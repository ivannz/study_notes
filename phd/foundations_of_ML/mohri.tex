\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}
\newcommand{\tr}{\text{tr}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\nil}{\mathbf{0}}
\newcommand{\Rcal}{{\mathcal{R}}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Fcal}{\mathcal{F}}

\title{Mehryar Mohri: Foundations Of Machine Learning}
\author{Nazarov Ivan}
\begin{document}

\section{20160919: lecture \#1} % (fold)
\label{sec:20160919_lecture_1}

The course starts with Basics and then moves to advanced topics. There will be
a mailing list and some homework.

\subsection{Introduction to ML} % (fold)
\label{sub:introduction_to_ml}
Definitions and concepts, problems of learning, and probability tools.

ML consists of computational methods to use ``experience'' to make predictions and
is associated with statistics, probability and optimization.

Complexities: \begin{itemize}
    \item Sample complexity: how many sample are needed to learn effectively;
    \item Space and computational complexity: how much resources the algorithm
    requires and its relation to its efficiency.
\end{itemize}

Typical application: spam filtering, NLP (sentiment analysis, morphological analysis),
speech recognition and synthesis, image annotation and recognition, Game AI, autonomous
systems, medical diagnostics, fraud and intrusion detection.
\noindent In fact, Games are serving as a platform for creating data for ML.
\noindent Personalized medicine: providing medical solutions based on the patients
medical history.

Broader categorization: \begin{itemize}
    \item Classification: $0-1$ error -- harder than in regression, the labels might
    have hierarchy or ``lattice'' structure (need for a more sophisticated loss);
    \item Regression: proximity-based error measures;
    \item Ranking: preference mining, recover ordering;
    \item Clustering: partition the data into homogeneous, yet distinct groups;
    \item Dimensionality reduction: a lower-dimensional manifold representing the
    data and preserves its properties, or recovering invariant representations (main
    question is how well the reduced representation works in a subsequent task of
    classification or regression);
\end{itemize}

General objective of ML research: \begin{itemize}
    \item theoretical: what can be learned? What is ``efficiency''? What are the
    guarantees?
    \item algorithmic: how does the solution scale with the data? Does it handle
    different tasks or generalize?
\end{itemize}

This course will present the most mathematically established algorithms.

% subsection introduction_to_ml (end)

\subsection{Definitions and terminology} % (fold)
\label{sub:definitions_and_terminology}

Examples, features, labels, and data. Poor features, random features make every
ML algorithm perform poorly. Choice of the features is extremely critical, and
gradually there must be more algorithms that learn features automatically.

Data: \begin{itemize}
    \item train sample (typically labelled);
    \item test sample (unlabelled);
    \item validation sample (typically labelled).
\end{itemize}

The general learning scenarios: \begin{itemize}
    \item batch learning: train and test phases are separated;
    \item online learning: crucial for iterative optimization, train-test steps
    are intermixed;
\end{itemize}

Queries: \begin{itemize}
    \item passive: the learner receives its data all-at-once;
    \item active: the learner can actively participate in data collection and has
    a dedicated internal process to make decision about acquiring new data for more
    efficient learning.
\end{itemize}

Batch scenarios: \begin{itemize}
    \item unsupervised: no labelled data;
    \item supervised: use labelled data for predictions on new data;
    \item semi-supervised: use labelled and unlabelled data to make predictions
    about new data;
    \item transduction: use the train and test sets to make prediction about the
    available test set (the test cases are so difficult that it is still hard to
    make accurate predictions, test data provides with additional distributional
    information);
\end{itemize}

Stages of learning: \begin{enumerate}
    \item divide the available data in to train, validation and test;
    \item train algorithm and prior knowledge about features to learn an algorithm
    $\hat{\mathcal{A}}(\Theta)$;
    \item use validation to select the best hyper-parameter $\Theta$;
    \item use test set to gauge the generalization error.
\end{enumerate}

Let $X$ be the input e space and $Y$ -- the output space. There is a loss function
$L: Y\times Y \mapsto \Real$: $L(\hat{y}, y)$ -- the cost of predicting $\hat{y}$
when the actual target is $y$. A hypotheses set $H\subseteq Y^X$. Training data
$(x_i,y_i)_{i=1}^m \in X\times Y$.
Problem setup: \begin{itemize}
    \item deterministic case: $\exists f:X\mapsto Y$ measurable, not necessarily from
    $H$, such that $y_i = f(x_i)$;
    \item stochastic case: there exits a conditional distribution given the
    input \\$y_i \sim p(y|x=x_i)$.
\end{itemize}

Errors: \begin{itemize}
    \item the generalization error $R(h) = \ex_{(y, x)\sim D} L(h(x), y)$ for $\in H$
    -- the thing we care about;
    \item Empirical error: $\hat{R}(h) = m^{-1} \sum_{i=1}^m L(h(x_i), y_i)$;
    \item the Bayes error: $R^* = \inf_{h \text{ -- meas}} R(h)$ the smallest achievable
    theoretical error (not necessarily feasible);
\end{itemize}
In deterministic case $R^*=0$ in stochastic -- $R^*\geq 0$.

Algorithm works imperfectly -- ususally it is blamed on the noise (as a measure of
how poorly you are doing). Binary case for $x\in X$:
\[ \text{noise}(x) = \min\bigl\{\pr(y=1|x), \pr(y=0|x)\bigr\} \,, \]
-- the error of the Bayes classifier and $\ex \text{noise(x)} = R^*$ -- the Bayes
error.
\textbf{First homework}: define the noise for other loss functions and problems
setups.

One learning challenge: find an accurate predictor for \textbf{all points} while
having access to a \textbf{finite} set of points. So the question is which technique
to use. One possible approach is to use Empirical Risk Minimization procedure:
choose the hypothesis with the lowest training sample error
\[ \hat{h} = \argmin_{h\in H} \hat{R}(h) \,. \]
In most cases ERM is terrible without complexity accounting. \textbf{Learning is not
fitting}, and does not constitute making all correct predictions on the train set.
Thus we arrive at the notion of the complexity of the hypothesised space.

The choice of the hypothesis set is very important. generalization error has ``U''
shape, penalty term is monotonic increasing, and the train error is monotonic decreasing.
The left leg of ``U'' is underfitting, and the right leg -- overfitting. Ideally one
would want to balance these phenomena.

Structural risk minimization: given a set on nested hypothesis spaces $(H_n)_{n\geq 0}$
with $H_n \subset H_{n+1}$ for all $n$ and
\[ h = \argmin_{h\in H_n, n\geq 0} \hat{R}_S(h) + \text{penalty}(H_n, m) \,. \]
This is an NP-hard problem. The problem of learning: when it is good enough to stop.
Now, $\text{penalty}$ depends on the complexity term, and a term depending on the
index and sample size.

Regularization-based algorithms: for $\lambda \geq 0$
\[ h = \argmin_{h\in H}\hat{R}(h) + \lambda \Omega(h) \,, \]
where $\Omega:H \mapsto \Real$ is the regularization (complexity) term.

\textbf{Second homework}: prove the SRM guarantees.

% subsection definitions_and_terminology (end)

\subsection{The tools} % (fold)
\label{sub:the_tools}

The probability tools: \begin{itemize}
    \item the Union bound: $\pr(\cup_{n\geq1} A_n) \leq \sum_{n\geq1} \pr(A_n)$;
    \item inversion: if $\pr(X\geq \epsilon) \leq f(\epsilon)$, then for any $\delta>0$
    \[ X < f^{-1}(\delta) \text{ with probability at least } 1-\delta\,;\]
    \item Jensen's inequality: if $f$ is convex then $f(\ex X) \leq \ex f(X)$;
    \item Markov's inequality: if $X$ is non-negative then $\pr(X>\epsilon) \leq \frac{\ex X}{\epsilon}$;
    \item Chebychev's inequality: $\pr(|X - \ex X| > \epsilon) \leq \frac{\ex(X - \ex X)^2}{\epsilon^2}$;
    \item Hoeffding's inequality: if $(X_i)_{i=1}^m$ are independent and $X_i \in [a_i, b_i]$
    almost surely then 
    \[ \pr\bigl(m^{-1} \sum_{i=1}^m \ex X_i - m^{-1} \sum_{i=1}^m X_i > \epsilon \bigr)
        \leq \text{exp}\Bigl(-\frac{2 m^2\epsilon^2}{\sum_{i=1}^m(b_i-a_i)^2}\Bigr) \,, \]
    \item McDiarmid's inequality: if $X_i$ are independent and take values in $U$, and
    $f:U^m \mapsto \Real$ is Lipschitz and
    \[ \sup_{x_1, \ldots, x_m, x_i'}
        \bigl| f(x_1, \ldots, x_i, \ldots, x_m) - f(x_1, \ldots, x_i', \ldots, x_m) \bigr|
        \leq c_i\,, \]
    then
    \[ \pr\bigl( |f(X_1, \ldots, X_m) - \ex f(X_1, \ldots, X_m) | > \epsilon)
        \leq 2 \text{exp}\Bigl(-\frac{2 \epsilon^2}{\sum_{i=1}^m c_i^2}\Bigr) \,, \]
    \item Chernoff's bounds: for any $t> 0$ one has:
    \[ \pr(X\leq \epsilon) = \pr(e^{tX} \leq e^{t\epsilon}) \leq e^{-t\epsilon}\ex e^{tX} \,, \]
    and then take the $\inf_{t>0}\ldots$ to best minimize the left-hand side probability.
\end{itemize}

% subsection the_tools (end)

\subsection{Learning guarantees} % (fold)
\label{sub:learning_guarantees}
Fundamentals of learning. How to understand complexity of a hypothesis?

The sample $(x_i, y_i)_{i=1}^m \sim D$ iid. True, theoretical error is:
\[ R(h) = \pr_{(x, y)\sim D} (h(x)\neq c(x)) = \ex_{(x, y)\sim D} 1_{h(x)\neq c(x)} \,, \]
Empirical error:
\[ \hat{R}_S(h)
    = \ex_{(x, y)\sim \hat{D}} 1_{h(x)\neq c(x)}
    = \ex_{(x, y)\sim S} 1_{h(x)\neq c(x)} \,,
\]
where the $\hat{D}$ is the empirical distribution of the sample data $S$. Now, note
that $\ex \hat{R}_S(h) = R(h)$ (we need only distributional identity here). In cases
where the training sample is not drawn from the same distributions (drift, and domain
adaptation).

Hoeffding's inequality readily gives:
\[ \pr(R(h) - \hat{R}(h) \geq \epsilon ) \leq e^{-2m\epsilon^2}
    \text{ and }
    \pr(\hat{R}(h) - R(h) \geq \epsilon ) \leq e^{-2m\epsilon^2}
\,.\]
By the union bound:
\[ \pr(|\hat{R}(h) - R(h)|\geq \epsilon ) \leq 2 e^{-2m\epsilon^2} \,.\]
But this is all pretty and good for a single hypothesis.

A learning algorithm yields a hypothesis based on the sample: $\mathcal{A}(S) = h_S$
-- the data dependent hypothesis. $h_S$ is a random variable, and we need a bound
that holds for all hypothesis $h\in H$ -- a uniform bound.

For a \textbf{finite} hypothesis set $H$ Hoeffding's inequality readily gives a
bound (using the union bound):
\begin{align*}
    \pr(\max_{h\in H} |\hat{R}(h) - R(h)|\geq \epsilon )
    &= \pr(\cup_{h\in H} \{|\hat{R}(h) - R(h)|\geq \epsilon\} ) \\
    &\leq \sum_{h\in H}\pr(|\hat{R}(h) - R(h)|\geq \epsilon )
     \leq 2 |H| e^{-2m\epsilon^2} \,.
\end{align*}
Thus for any $\delta > 0$, with probability at least $1-\delta$ one has
\[ \forall h\in H \,,\,
    R(h) \leq \hat{R}(h) + \sqrt{\frac{\log|H|
    + \log \frac{2}{\delta}}{2m}}
\,,\]
using the inversion principle.

Occam's razor: \textbf{ceteris paribus} one should be choosing the simplest hypothesis
set.

What about an infinite hypothesis set? Reduce the infinite case to a finite case, or
in what cases it is impossible. Foe example, it is possible to discretize the hypothesis
set. Or one can project the hypothesis case onto a finite sample.

Rademacher complexity, Growth function, VC dimension and Lower bounds. VC dimension is
not just convenient, but is also very fundamental.
\textbf{To be continued in the second part.}

% subsection learning_guarantees (end)

% section 20160919_lecture_1 (end)

\section{20160919: lecture \#2} % (fold)

\label{sec:20160919_lecture_2}

\subsection{Rademacher complexity} % (fold)
\label{sub:rademacher_complexity}

Empirical Rademacher complexity. Let $G$ be a family of maps from $Z$ to $[a, b]$,
and suppose $S = (z_i)_{i=1}^m \in Z$ is the train sample. Let $(\sigma_i)_{i=1}^m\in\{-1, 1\}$
be independent uniform random variables -- the Rademacher variables. The hypothesis
class is more complex whenever it can approximate any random label assignment arbitrarily
close:
\[ \hat{\mathcal{R}}_S(G)
    = \ex_{\sigma} \sup_{g\in G} m^{-1} \sum_{i=1}^m \sigma_i g(z_i)
    \,, \]
-- the correlation with ``purely random label assignment''. This measures how well
$G$ can mimic randomness, or, in other words how rich the hypothesis class $G$ is.
It is sample dependent, capturing the complexity precisely on the observed sample.
Taking the expectation with respect to any sample $S$ of size $m$ gives the Rademacher
complexity of $G$:
\[ \mathcal{R}_m(G) = \ex_{S\sim D^m} \hat{\mathcal{R}}_S(G) \,. \]

If we have a set $G$ of hypotheses $Z\mapsto [0,1]$ then for any $\delta>0$ with
probability at least $1-\delta$ for all $g\in G$ one has:
\[ \ex_{z\sim D} g(z)
    \leq m^{-1} \sum_{i=1}^m g(z_i) + 2\mathcal{R}_m(G)
    + \sqrt{\frac{\log\frac{1}{\delta}}{2m}}
    \,,\]
and
\[ \ex_{z\sim D} g(z)
    \leq m^{-1} \sum_{i=1}^m g(z_i) + 2\hat{\mathcal{R}}_S(G)
    + 3\sqrt{\frac{\log\frac{2}{\delta}}{2m}}
    \,.\]

\noindent \textbf{Proof}: apply McDiarmid's inequality to
\[\Phi(S)
    = \sup_{g\in G} \ex_{z\sim D} g(z) - \ex_{z\sim S} g(z)
    = \sup_{g\in G} \ex g - \hat{\ex}_S g
    \,, \]
-- the deviation of the empirical expectation form the true average. But we need
to know the bounds of $\Phi(S)$. Change one example in $S$ to get $S'$:
\[ \Phi(S') - \Phi(S)
    \leq \sup_{g\in G} \hat{\ex}_{S'} g  - \hat{\ex}_S g
    = \sup_{g\in G} m^{-1}(g(z_i') - g(z_i))
    \leq m^{-1} \,, \]
and then the McDiarmid's inequality is applied for $\frac{\delta}{2}$ to get:
\[ \Phi(S) \leq \ex_S \Phi(S) + \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,, \]
with probability at least $1-\frac{\delta}{2}$. What is left is bounding the remaining
expectation.

What about $\ex_S \Phi(S)$? Introduce the ghost sample $S'$ of similar size $m$:
\begin{align*}
    \ex_S \Phi(S)
        &= \ex_S \sup_{g\in G} \ex g - \hat{\ex}_S g \\
        &= \ex_S \sup_{g\in G} \ex_{S'} \hat{\ex}_{S'} g - \hat{\ex}_S g \\
        & \bigl[\text{sub-additivity of sup}\bigr]\\
        &\leq \ex_S \ex_{S'} \sup_{g\in G} \hat{\ex}_{S'} g - \hat{\ex}_S g \\
        & \bigl[\text{permuting } S \text{ and } S' \text{ and taking sup}\bigr] \\
        &\leq \ex_{S',S,\sigma} \sup_{g\in G} m^{-1} \sum_{i=1}^m \sigma_i(g(z_i') - g(z_i)) \\
        & \bigl[\text{the expectations are identical and } \sigma \text{ are uniform}\bigr] \\
        &\leq 2 \ex_S \ex_\sigma \sup_{g\in G} m^{-1} \sum_{i=1}^m \sigma_i g(z_i)
        = 2 \mathcal{R}_m(G) \,.
\end{align*}
This analysis uses outer expectations, and we just assume that the suprema are measurable
-- this is true for most hypotheses classes.

Now use the probability concentration inequality: by McDiarmid's inequality one
has
\[ \mathcal{R}_m(G) \leq \hat{\mathcal{R}}_S(G) + \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,, \]
with probability at least $1-\frac{\delta}{2}$. Using the union bound we get: for
all $g\in G$
\[ \ex g \leq \hat{\ex}_S g + 2 \hat{\mathcal{R}}_S(G) + 3 \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,, \]
with at least $1-\delta$ probability altogether.

Notation shift: $Z \to X\times Y$, $G \to H$ and $g(z) = L(h(x), y)$. So how can we
derive the complexity of the hypothesis set, and not the losses (compositions) of a
hypothesis.

Binary classification: the Rademacher complexity of $G$ is half that of $H$. Define
\[ G = \{(x,y)\mapsto 1_{h(x)\neq y}\,:\, h\in H\} \,. \]
Note that the hypotheses take values in $\{-1,+1\}$. Then
\begin{align}
    \mathcal{R}_m(G)
        &= \ex_{S, \sigma} \sup_{g\in G} m^{-1} \sum_{i=1}^m \sigma_i g(z_i) \\
        &= \frac{1}{2} \ex_{S, \sigma} \sup_{h\in H} m^{-1} \sum_{i=1}^m \sigma_i (1 - y_i h(x_i)) \\
        &= 0 + \frac{1}{2} \ex_{S, \sigma} \sup_{h\in H} m^{-1} \sum_{i=1}^m - \sigma_i y_i h(x_i) \\
        &[\text{the distribution of } y_i \sigma_i \text{ is uniform}]\\
        &= \frac{1}{2} \ex_{S, \sigma} \sup_{h\in H} m^{-1} \sum_{i=1}^m \sigma_i h(x_i) \,.
\end{align}
Thus we have the following for the hypothesis set $H$: with probability at least $1-\delta$
for all $g\in G$
\[ R(h) \leq \hat{R}_S(h) + \hat{\mathcal{R}}_S(H) + 3\sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,.\]

Estimating the Rademacher complexity -- use the definition, which reduces to ERM.
However, it is computationally hard for most hypotheses sets. One can use Monte-Carlo
to this end.

% subsection rademacher_complexity (end)

\subsection{The growth function} % (fold)
\label{sub:the_growth_function}

The growth function is the size of the ``worst'' sample: the maximum number of ways
to assign labels to a sample of size $m$ using all hypothesis from the set $H$.
For all $m\geq 0$
\[ \Pi_H(m)
    = \max_{(x_i)_{i=1}^m \subseteq X}
        \bigl| \{ (h(x_i))_{i=1}^m\,:\,h\in H \} \bigr|
    = \max_{S \in X^m} |G_{|S}|
    \,. \]
It is very strict, since it also takes into account samples which could be extremely
rare with respect to the true distribution.

\textbf{Massart's lemma}. Let $A\subseteq \Real^m$ is a finite set, with
$R = \max_{x\in A} \|x\|_2$. Then the following holds:
\[ \ex_\sigma \sup_{x\in A} m^{-1} \sum_{i=1}^m \sigma_i x_i
    \leq \frac{R\sqrt{2\log|A|}}{m} \,. \]
Use the Chernoff's technique, then the Jensen's inequality, the union bound and finally
the Hoeffding's inequality. Ultimately this gives an upper bound on the Rademacher
complexity of $G$:
\begin{align*}
    \mathcal{R}_m(G)
    &= \ex_S \ex_\sigma \sup_{g\in G} m^{-1} \sum_{i=1}^m\sigma_i g(x_i) \\
    &= \ex_S \ex_\sigma \sup_{u\in G_{|S}} m^{-1} \sum_{i=1}^m\sigma_i u_i \\
    &\leq \ex_S \frac{\sqrt{m} \sqrt{2 \log|G_{|S}|}}{m}
    \leq \ex_S \sqrt{\frac{2 \log \Pi_G(m)}{m}} \,,
\end{align*}
form Massart's lemma and the definition on the growth function.

The growth function is related to the VC-dimension of a hypothesis set.

% subsection the_growth_function (end)

\subsection{VC-dimension} % (fold)
\label{sub:vc_dimension}

The dimension is the size of the largest sample $S$ that admits arbitrary labeling
by a hypothesis from $H$ ($H$ shatters $S$):
\[ \text{VCdim}(H) = \max\{m \geq 0 \, : \, \Pi_H(m) = 2^m \} \,, \]
Again, it is the worst case measure -- very conservative, -- in practice bad samples
can even be negligible. It suffices to present a sample $S$ of size $m$ such that
$H$ shatters $S$. To give an upper bound one must show that for no sample $d+1$
shattering is possible.

Typical VC-dimensions: \begin{itemize}
    \item intervals on the real line: $\text{VCdim}(H) = 2$;
    \item hyperplanes in $\Real^d$: $\text{VCdim}(H) = d+1$ the number of free parameters
    of a hyperplane $\beta_0 + \beta'x$ n $\Real^d$, but not true in general (in
    a plane -- XOR configuration for example, proof uses Radon's theorem);
    \item hyper-rectangles: $\text{VCdim}(H) = 2d$;
    \item convex polygons on the plane: $\text{VCdim}(H) = 2d + 1$;
    \item sine functions on a line: $\text{VCdim}(H) = \infty$;
\end{itemize}

Computing the VC dimension of composite classes, unions, or intersections.

\textbf{Sauer's lemma}: if $H$ is a hypothesis set with $\text{VCdim}_H(m) = d$
then the growth function $\Pi_H(m)$ is upper-bounded by
\[ \sum_{i=0}^d C^i_m \leq \biggl(\frac{em}{d}\biggr)^d  = O(m^d) \,, \]
for all $m\geq 0$.
\textbf{Proof}: for $d\leq m$ one has \begin{align}
    \sum_{i=0}^d C^i_m
        &\leq \sum_{i=0}^d C^i_m \biggl(\frac{m}{d}\biggr)^{d-i} 
         \leq \sum_{i=0}^m C^i_m \biggl(\frac{m}{d}\biggr)^{d-i}
         \leq \biggl(\frac{m}{d}\biggr)^d \sum_{i=0}^m C^i_m \biggl(\frac{d}{m}\biggr)^i \\
        &\leq \biggl(\frac{m}{d}\biggr)^d \biggl(1+\frac{d}{m}\biggr)^m
         \leq \biggl(\frac{em}{d}\biggr)^d \,,
\end{align}
since $1 - x \leq e^{-x}$.

Proof of the VCdim upper bound proceeds by induction on $m+d$. It is clearly true
for $m=1$ and $d=0$ or $d=1$. For arbitrary $(m, d)$ suppose the inequality is true
for lesser $m+d$. If $S'= \{x_1. \ldots, x_{m-1}\}$, and $G_1 = G_{|S'}$ and
\[ G_2 = \{g'\subseteq S'\,:\, g'\in G\,\text{and}\, g'\cup\{x_m\} \in G \}\,, \]
then $|G_1| + |G_2| = |G|$. And the rest is in the book by Mohri (cf. pp. 46-47).

Now, it is time for the generalization bound with VC dimension. For any $\delta>0$
with $1-\delta$ for any $h\in H$
\[ R(h)
    \leq \hat{R}(h) + \sqrt{\frac{2d\log \frac{em}{d}}{m}}
                    + \sqrt{\frac{\log\frac{1}{\delta}}{2m}}
\,,\]
where $H$ is a set of hypotheses taking values $\{\pm 1\}$.

In the consistent case, when the training error can reach $0$, the square root
in the upper bound disappears.

% subsection vc_dimension (end)

\subsection{Lower bound based on the VC dimension} % (fold)
\label{sub:lower_bound_based_on_the_vc_dimension}

Let $H$ be a hypothesis set with $\text{VCdim}(H)=d$, $d>1$. Then for any learning
algorithm $L$ there exists a distribution $D$, and a input-target relation $f\in H$
with
\[ \pr_{S\sim D^m} \bigl(R_D(h_S, f) > \frac{d-1}{32m}\bigr) \geq \frac{1}{100} \,. \]

Take a sample $S = (x_i)_{i=1}^d$ of size $d$ which is shattered by $H$. Define a
tricky distribution: with high probability return the same point $x_0$, but with
remaining probability mass return any other point uniformly:
\[ \pr(X=x_1) = 1-8\epsilon
    \text{ and } \pr(X=x_i) = \frac{8\epsilon}{d-1} \forall i=2,\ldots, d
    \,, \]

No-free lunch theorem: VC-dimension non-realizable case. For any learning algorithm $L$
there exists $D$ over $X\times \{0,1\}$ such that
\[ \pr_{S\sim D^m} \biggl(R_D(h_S) - \inf_{h\in H} R_D(h) > \sqrt{\frac{d}{320 m}} \biggr)
    \geq \frac{1}{64}
    \,. \]

% subsection lower_bound_based_on_the_vc_dimension (end)

% section 20160919_lecture_2 (end)

%

\section{20160920 lecture \#1} % (fold)
\label{sec:20160920_lecture_1}

Today we will cover SVM and kernel methods.

\subsection{Support vector machine} % (fold)
\label{sub:support_vector_machine}

Reminder: binary classifcation problem
$X\subseteq \Real^N$, a sample $S = (x_i, y_i)_{i=1}^m \in X\times \{-1, +1\}$, and
the hypotheses space is $h:X \mapsto\{-1, +1\}$, and we want to find the $h\in H$ with
the samllest generalization error $R(h) = \ex_{(x, y)\sim D} 1_{h(x)\neq y}$.

Linear classification: \begin{itemize}
    \item hyperplanes;
    \item et c.
\end{itemize}

SVM -- separable case, SVM -- unseparable case, and 

Fins the hyperplane that best spaerates the classes.

Suppose there exists a hyperplane separating classes from one another. THe set of hypothese is :
\[ H = \{x\mapsto \text{sgn}(\beta'x + \beta_0) \, : \, \beta\in \Real^N, \beta_0\in \Real\} \,.\]
Each separating hyperplane has a so called geometric margin:
\[\text{margin} = \min_{i=1}^m \frac{|\beta'x_i + \beta_0|}{\|\beta\|} \,,\]
where $\beta\in \Real^N$, and $\beta_0 \in \Real$.

But there are many hyperplanes, so Vapnik an Chervonenkis came up with the idea to
find a hyperplane maximizing the margin and separating the classes. Now observe
that for all $t>0$:
\begin{align*}
    \min_{i=1}^m \frac{|t \beta'x_i + t \beta_0|}{\|t \beta\|}
        &= \min_{i=1}^m \frac{|t \beta'x_i + t \beta_0|}{\|t \beta\|} \\
        &= \min_{i=1}^m \frac{|\beta'x_i + \beta_0|}{\|\beta\|} \,,
\end{align*}
whence by separability we can fix $\min_{i=1}^m |\beta'x_i + \beta_0|$ to $1$
\begin{align*}
    \rho &= \max_{\beta,\beta_0} \min_{i=1}^m \frac{|\beta'x_i + \beta_0|}{\|\beta\|} \\
    &= \max_{\beta, \beta_0, \min_{i=1}^m |\beta'x_i + \beta_0| = 1} \frac{1}{\|\beta\|} \\
    &= \min_{\beta, \beta_0, \min_{i=1}^m y_i(\beta'x_i + \beta_0) \geq 1} \|\beta\| \\
    & = \ldots \,.
\end{align*}

Thus the SVM problem in a separable case becomes:
\[ \min_{\beta, \beta_0} \frac{1}{2} \|\beta\|^2 \,,\]
subject to $y_i(\beta'x_i + \beta_0) \geq 1$ for all $i=1, \ldots, m$. This is a
convex problem (and affine constraints) and thus has a unique solution. Using KKT
conditions one can find the solution, and formulate the Dual problem. Form the
Lagrangian:
\[ L = \frac{1}{2}\beta\|^2 + \sum_{i=1}^m \alpha_i\bigl(1-y_i(\beta'x_i + \beta_0)\bigr) \,.\]
KKT conditions are: \begin{description}
    \item[Primal] $\forall i=1,\ldots, m$ we must have $y_i(\beta'x_i + \beta_0) \geq 1$;
    \item[Dual] $\forall i=1,\ldots, m$ we must have $\alpha \geq 0$;
    \item[Complementary slackness] $\alpha_i\bigl(1-y_i(\beta'x_i + \beta_0)\bigr) = 0$ for all $i=1,\ldots, m$;
    \item[First-order] $\frac{\partial}{\partial \beta} L = 0$ and $\frac{\partial}{\partial \beta_0} L = 0$;
\end{description}
This yields the following Dual:
\[\max_\alpha \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i'x_j \,, \]
subject to $\sum_{i=1}^m \alpha_i y_i = 0$ and $\alpha_i \geq 0$. The solution is
given by:
\[ h(x) = \text{sgn}(\sum_{i=1}^m \alpha_i y_i x_i'x + \beta_0 \,,\]
where $\beta_0 = y_i - \sum_{j=1}^m \alpha_j y_j x_j'x_i$ for all $i=1,\ldots, m$
with $\alpha_i > 0$.

Sparsity. Iff $h_S$ is the learned hypothesis, thel the LOO error is given by:
\[ \hat{R}_{\text{loo}}(L) = m^{-1} \sum_{i=1}^m 1_{h_{S_{-i}}(x_i) \neq y_i} \,, \]
where $S_{-i}$ is the sample with $i$-th example removed. LOO error is an almost
unbiased estimate of the generalization error 
\[ \ex_{S\sim D^m} \hat{R}_{\text{loo}}
    = m^{-1}\sum_{i=1}^m \ex_{S'\sim D^{m-1}} \ex_{(x, y)\sim D} 1_{h_{S'}(x) \neq y}
    = \ex_{S'\sim D^{m-1}} R_{S'}(h_{S'})
    \,, \]

If $h_S$ is the optimal separating hyperplane and $N_{\text{sv}}(S)$ is the number
of support vectors, then
\[\ex_{S\sim D^m} R(h_S) \leq \ex_{S\sim D^{m+1}} \frac{N_{\text{sv}}(S)}{m+1} \,. \]
But this is a weak guarantee, since it is only on averages.

The non-separable case uses the so called slack variables, which permits imperfect
separation. The objective is the same: minimize $\frac{1}{2}\|\beta\|^2$ with respect
to $\beta$ and $\beta_0$, subject to
\[ y_i(\beta'x_i + \beta_0) \geq 1 - \xi_i \,,\]
with $\xi_i \geq 0$ for all $i=1,\ldots, m$. But this time we must penalize the slacks:
for $C \geq 0$ one wants
\[ \frac{1}{2}\|\beta\|^2 + C \sum_{i=1}^m \xi_i \to \min_{\beta, \beta_0} \,, \]
subject to the same conditions (one could use squares in the penalty term). 
The optimization problem is still convex. The trade-off parameter $C$ can be chosen
using cross-validation, like $K$-fold CV (or $m$-fol -- LOO). CV is connected to
SRM (see the homework).

Equivalent SVM problem:
\[ \frac{1}{2}\|\beta\|^2 + C \sum_{i=1}^m \bigl(1-y_i(\beta'x_i + \beta_0)\bigr)_+ \,. \]
One could use hinge loss $(1-yh(x))_+$ or the quadratic hinge loss $(1-yh(x))_+^2$.

The non-separable dual problem is
\[\max_\alpha \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i'x_j \,, \]
subject to $\sum_{i=1}^m \alpha_i y_i = 0$ and $0 \leq \alpha_i \leq C$. The solution
is given by:
\[ h(x) = \text{sgn}(\sum_{i=1}^m \alpha_i y_i x_i'x + \beta_0 \,,\]
where $\beta_0 = y_i - \sum_{j=1}^m \alpha_j y_j x_j'x_i$ for all $i=1,\ldots, m$
with $0 < \alpha_i < C$.

What about the margin guarantees of SVM? In bioinformatics the dimensionality of
the feature space if very large, so what about the complexity.

% subsection support_vector_machine (end)

\subsection{Margin theory} % (fold)
\label{sub:margin_theory}
Confidence margin versus the geometric margin. For any confidence margin $\rho>0$,
the $\rho$-margin function is defined by such an $\Phi_\rho$ that
\[ 1_{x < 0} \leq \Phi_\rho(x) \leq 1_{x < \rho} \,,\]
and $\Phi_\rho \in [0,1]$ non increasing and linear over $[0, \rho]$. For a sample
$S = (x_i)_{i=1}^m$ and a real-values hypothesis $h$ the empirical margin loss is
\[\hat{R}_\rho(h)
    = m^{-1} \sum_{i=1}^m \Phi_\rho(y_i h(x_i))
    \leq m^{-1} \sum_{i=1}^m 1_{y_i h(x_i) < \rho}
    \,. \]
The right-hand side is the share of examples in the training set that in which the
classifier has confidence less than $\rho$.

General margin bound: for any $H$ of real valued functions, for any $\rho>0$ and
$\delta\in(0,1)$
\[ R(h)
    \leq \hat{R}_\rho(h) + \frac{2}{\rho} \mathcal{R}_m(H)
    + \sqrt{\frac{\log\frac{1}{\delta}}{2m}}
    \,, \]
with probability at least $1-\delta$.

\textbf{proof}: Put $\tilde{H} = \{z = (x,y) \mapsto yh(x) \,:\, h\in H\}$, and
consider $\tilde{\mathcal{H}} = \{\Phi_\rho \circ f\,:\, f\in \tilde{H}\}$.
Then use the theorems for bounded functions in $[0,1]$.
By theorem 3 one has
\[ \ex g(z) \leq m^{-1} \sum_{i=1}^m g(z_i)
            + 2\mathcal{R}_m(\tilde{\mathcal{H}}) + \sqrt{\frac{\log\frac{1}{\delta}}{2m}}
    \,,\]
but since $\Phi_\rho$ is $\frac{1}{\rho}$-Lipschitz, then by Talagrand's lemma one
has
\begin{align*}
    \mathcal{R}_m(\Phi_\rho\circ \tilde{H})
    &\leq \frac{1}{\rho} \mathcal{R}_m(\tilde{H}) \\
    &= \frac{1}{\rho m} \ex_{S,\sigma} \sup_{h\in \tilde{H}} \sum_{i=1}^m \sigma_i y_i h(x_i) \\
    &=\bigl[\text{uniformity of } \sigma_i \bigr] \\
    &= \frac{1}{\rho} \mathcal{R}_m(H) \,.
\end{align*}
But $\Phi_\rho$ majorizes the $0-1$ loss: $1_{yh(x) < 0} \leq \Phi_\rho(yh(x))$.

The Rademacher Complexity of a linear hypothesis:
Let $S = (x_i)_{i=1}^m \subseteq \{x\,:\,\|x\|\leq R\}$ and put
\[ H = \{x\mapsto x'\beta\,:\,\|\beta\|\leq \Lambda\} \,. \]
Then
\[ \hat{\mathcal{R}}_S(H) \leq \frac{R \Lambda}{\sqrt{m}} \,. \]

Thus the margin bound for the linear classifiers is: if $X\subseteq \{x\,:\, \|x\|\leq R\}$
and $H = \{x\mapsto x'\beta \,:\, \|\beta\| \leq \Lambda\}$, then for any $\delta > 0$
\[ R(h) \leq \hat{R}_\rho(h) + 2\frac{R\Lambda}{\rho \sqrt{m}}
    + 3\sqrt{\frac{\log \frac{2}{\delta}}{2m}}
    \,, \]
with probability at least $1-\delta$. In the bad-distribution case the margin $\rho$
would have to be chosen too small.

This follows directly from the general margin bound and bound on $\hat{\mathcal{R}}_S(H)$
for linear classifiers.

\noindent The SVM precisely tries to minimize the right-hand side. To do better than
SVM is to minimize the ramp loss, but it is non-convex, so solving it is troublesome.

To prove this kind of result one needs to work with a different loss function.

% subsection margin_theory (end)

% section 20160920_lecture_1 (end)

\section{20160920 Lecture \#2} % (fold)
\label{sec:20160920_lecture_2}

The upper bound on the Rademacher complexity of linear hypotheses:
\[\hat{\mathcal{R}}_S(H) \leq \sqrt{\frac{R^2 \Lambda^2}{m}} \,, \]
for $H = \{x\mapsto\beta'x \,:\, \|\beta\|\leq \Lambda \}$ and
$S\subseteq \{x\,:\,\|x\|\leq R\}$. \textbf{Proof}:
\begin{align*}
    \hat{\mathcal{R}}_S(H)
        &= \frac{1}{m} \ex_\sigma \sup_{\|\beta\|\leq \Lambda} \sum_{i=1}^m \sigma_i \beta'x_i \\
        &= \frac{1}{m} \ex_\sigma \sup_{\|\beta\|\leq \Lambda} \beta' \sum_{i=1}^m \sigma_i x_i \\
        &\leq \frac{1}{m} \ex_\sigma \sup_{\|\beta\|\leq \Lambda} \|\beta\| \bigl\|\sum_{i=1}^m \sigma_i x_i \bigr\| \\
        &= \frac{\Lambda}{m} \ex_\sigma \bigl\|\sum_{i=1}^m \sigma_i x_i \bigr\| \\
        &\bigl[\text{using Jensen's inequality}\bigr]\\
        &\leq \frac{\Lambda}{m} \sqrt{\ex_\sigma \bigl\|\sum_{i=1}^m \sigma_i x_i \bigr\|^2} \\
        &\bigl[\text{pairwise independence of } \sigma_i\bigr]\\
        &\leq \frac{\Lambda}{m} \sqrt{\ex_\sigma \sum_{i=1}^m \sigma_i^2 x_i'x_i } \\
        &\leq \frac{\Lambda}{m} \sqrt{m R^2 } \,.
\end{align*}
Thus argument shows that it is possible to learn in high-dimensional feature spaces.

\subsection{Kernel methods} % (fold)
\label{sub:kernel_methods}

Kernel methods allow efficient manipulation of inner products in high dimension.
Also kernel methods result in non-linear decision boundary. Another reason is that
kernel methods expand the theory of classification to abstract input feature spaces.
Non-linear mapping $\phi:X\mapsto F$ -- from the input to a very high-dimensional
space.

Kernel method: take a kernel function $K:X\times X\mapsto \Real$ such that there
exists $\phi:X\mapsto F$ such that $K(x, y) = \langle \phi(x), \phi(y)\rangle_F$.
Thus $K$ is often seean as a similarity measure (incidence in a unordered graph,
closeness-proximity). Usually $K$ cam be efficiently computed, and the choice of
$K$ is up to the researcher, which entails flexibility.

$K$ is a kernel iff for all $m\geq1$, $(c_i)_{i=1}^m \in \Real$ and $(x_i)_{i=1}^m\in X$
one has $c'Kc \geq 0$, for $K = (K(x_i, x_j))_{i,j=1}^m$ and symmetric.
Kernels can be normalized, and there are many other arithmetic operations that
preserve the kernel property.

Gaussian kernels:
\[ k(x, y) = \text{exp}\biggl\{-\frac{\|x-y\|^2}{2\sigma^2}\biggr\} \,, \]
the sigmoid kernel:
\[ k(x, y) = \text{tanh}\bigl(a x'y + b \bigr) \,, \]
for $a, b\geq 0$, and it is useful for neural networks.

To get a kernel SVM just plug in the kernel instead of the inner products.

The generalization. Let $S\subseteq \{x\,:\, \|\phi(x)\|_\Hcal \leq R\}$ and
\[ H = \{x\mapsto \langle\beta, \phi(x)\rangle
    \,:\, \beta \in \Hcal\,, \|\beta\|_\Hcal \leq \Lambda\}
    \,. \]
Then
\[\hat{\mathcal{R}}_S(H)
    \leq \frac{\Lambda\sqrt{\tr K}}{m}
    \leq \frac{\Lambda\sqrt{m R^2}}{m}
    = \frac{\sqrt{R^2 \Lambda^2}{m}}
    \,. \]
\textbf{proof} is exactly the same as in the ordinary inner product case.

The Representer theorem states that the minimizer of
\[L\bigl((h(x_i))_{i=1}^m\bigr) + \Omega(\|h\|) \to \min_{h\in \Hcal}\,,\]
is an element of the pre-hilbert space $\Hcal_0$ guven by
\[ \Hcal_0 = \{\sum_{i=1}^m \alpha_i \phi(x_i)\,:\, (\alpha_i)_{i=1}^m \in \Real \} \,. \]

Linear regression can be extended to the kernel ridge regression, ranking algorithm
and et c.

It is interesting to define a kernel over string data. The idea is that strings
are similar if they share common sub-strings. This can be done using the so called
weighted transducers: weighted directed graphs with input-output configurations.
In such transducers one can define the function $T(x,y)$ as the sum of all possible
accepting paths with input $x$ and output $y$.
Now, $K$ is a rational kernel if there exists $T$ for some weighted transducer $T$.
For instance the composition can be defined as $T_1:\Sigma^*\times \Delta^* \mapsto \Real$
and $T_2:\Delta^*\times\Omega^* \mapsto \Real$ as
\[ (T_1\circ T_2)(x, z) = \sum_{y\in \Delta^*} T_1(x, y) T_2(y, z) \,, \]
much like multiplication of infinite matrices.

The inverse of a transducer $T:\Sigma^*\times \Delta^* \mapsto \Real$ is $T^{-1}$
obtained by swapping input with output in $T$: $T^{-1}:\Delta^* times \Sigma^*\mapsto \Real$
is $T^{-1}(x, y) = T(y, x)$.
For any transducer $T$ the function $K = T\circ T^{-1}$ is a positive definite symmetric
rational kernel.
\textbf{proof}: $K(x, y) = \sum_{z\in \Delta^*} T(x, z) T(z, y)$, then
$K(x, y) = \lim_{n\to \infty} K_n(x, y)$ where
$K_n(x, y) = \sum_{z\in\Delta^*, |z| \leq n} T(x, z) T(z, y)$.

Transducer learning is an interesting problem.

Kernels can be defined on many other discrete structures: automata, graphs, images,
parse trees et c.

Finally, negative kernels. What other kernels one can construct from a norm in a
Hilbert space? Negative definite kernels: a function $K:X\times X\mapsto\Real$
is saidto be negative defined symmetric iff it is symmetric and for all $m\geq 1$, 
$S=(x_i)_{i=1}^m\in X$ and $c\in\Real^{m\times 1}$ with $\one'c = 0$ one has
$c'Kc \leq 0$ for the gram matrix $K$ of $K$ on $S$.

If $K$ is NDS and $K(x, y) = 0$ iff $x=y$, then there exists a Hilbert space with
$K(x, y) = \|\phi(x) - \phi(y)\|_\Hcal$ for some $\phi:X\mapsto\Hcal$. Also $K$ is
NDS iff $e^{-tK}$ is a PDS for all $t>0$. Furthermore, for any $x_0$ the kernel $K'$
defined as 
\[K'(x, y) = K(x, x_0) + K(y, x_0) - K(x, y) - K(x_0, x_0) \,,\]
is NDS if and only if $K$ is PDS.

Deeper kernels: given a set of PDS kernels $(K_j)_{j=1}^p$ one can use a composite
kernel
\[K_\mu = \sum_{j=1}^p \mu_j K_j \,,\]
with $\sum_{j=1}^p \mu_j = 1$. One could create a window selection method by introducing
sparsity onto $\mu$ (or greedily, like in LARS).

% subsection kernel_methods (end)

% section 20160920_lecture_2 (end)

%

\section{20160921 Lecture \#1} % (fold)
\label{sec:20160921_lecture_1}

\subsection{Boosting and ensemble methods} % (fold)
\label{sub:boosting_and_ensemble_methods}

Weak learning: such an algorithm that $\gamma>0$ produces hypotheses which on average
does a little bit better than random guess: for all $\delta>0$ one has for all $D$ and
$C$
\[ \pr_{S\sim D} \bigl(R(h_S) \ leq \frac{1}{1} - \gamma\bigr) \geq 1-\delta \,. \]
Main idea: use weak learning to create a strong learner, and combine base classifiers
in such a way as to improve generalization error.

Suppose one has a family of base classifiers $H\subseteq \{-1,+1\}^X$. At each round
$t=1,\ldots, T$ on does:\begin{itemize}
    \item maintain distribution $D_t$ over the training sample $S=(x_i, y_i)_{i=1}^m$;
    \item adds a new hypothesis to the pool based on $D_t$;
    \item form a convex combination of hypotheses $f_t = \sum_{k=1}^t \alpha_k h_k$
    for $\alpha_k\geq 0$;
    \item computes the training error of $h_t$: 
        \[\epsilon_t = \pr_{(x, y)\sim D_t} 1_{h_t(x)\neq y} = \ex_{D_t} \epsilon_{ti}\,; \]
    \item updates the distribution $D_t$;
\end{itemize}
AdaBoost algorithm updates the distribution with
\[ D_{t+1}(dz)
    \propto e^{-\alpha_t g_t(z) } D_t(dz)
    \propto e^{-\sum_{k=1}^t \alpha_k g_k(z)} D_1(dz)
    \,, \]
with $g_t(z) = g_t(x,y) = y h_t(x)$. Now for $\epsilon_t = \ex_{(x, y)\sim D_t} 1_{h_t(x)\neq y}$
\[ \alpha_t = \frac{1}{2} \log \frac{1-\epsilon_t}{\epsilon_t} \,. \]
It is crucial that $\epsilon_t \leq \frac{1}{2}-\gamma$, since 
\[ \hat{R}_S(h_T) \leq \text{exp}\Bigl\{-2\sum_{k=1}^T \bigr(\frac{1}{2}-\epsilon_t\bigr)^2 \Bigr\} \,, \]
whence $\hat{R}(h_T) \leq e6{-2\gamma}$ and $h_T(x) = \text{sgn} f_T(x)$. Notice
that $\gamma$ is not needed to be known in advance, hence the name ``adaptive boosting''.

\textbf{proof} using the definition of $D_{T+1}$:
\begin{align*}
    \hat{R}(h_T)
        &\leq m^{-1}\sum_{i=1}^m 1_{y_i f_T(x_i) < 0} \\
        &= m^{-1}\sum_{i=1}^m e^{- y_i f_T(x_i) < 0} \\
        &= \sum_{i=1}^m D_{T+1}(i) \prod_{k=1}^T Z_k
        = \prod_{k=1}^T Z_k \,,
\end{align*}
Now
\begin{align*}
    Z_t &= \sum_{i=1}^m D_t(i) e^{-\alpha_t y_i h_t(x_i)} \\
        &= \sum_{i\,:\,y_i h_t(x_i) \geq 0} D_t(i) e^{-\alpha_t} 
            + \sum_{i\,:\,y_i h_t(x_i) < 0} D_t(i) e^{\alpha_t} \\
        &= (1-\epsilon_t) e^{-\alpha_t} + \epsilon_t e^{\alpha_t} \\
        & [\text{minimize with respect to }\alpha_t ]\\
        &= 2\sqrt{\epsilon_t(1-\epsilon_t)}
        \leq \text{exp}(-2(\frac{1}{2} - \epsilon_t)^2) \,.
\end{align*}
This is a multiplicatively separable upper bound in $(\epsilon_t)_{t=1}^T$ on the
empirical error. AdaBoost assigns the same total probability mass to correctly classified
and misclassified instances, since $(1-\epsilon_t)e^{-\alpha} = \epsilon_t e^\alpha$.

Minimizing $0-1$ loss is difficult due to non-convexity, and also it is hard to
minimize the whole combination at once. We thus use a greedy strategy and majorize
the loss. Finally it is possible to extend the algorithm from binary classifiers
to the ones taking values in $[-1,1]$: we will be minimizing the upper bound on
$Z_t$ and still get the same expression for $\alpha_t$.

AdaBoost is similar to coordinate descent over the convex and differentiable objective
function
\[ F(\alpha)
    = \sum_{i=1}^m e^{-y_i f(x_i)}
    = \sum_{i=1}^m e^{-y_i \sum_{t=1}^T \alpha_t h_t(x_i)} 
    \,.\]
The direction on the descent corresponds to the base classifier with the smallest
error. Thus ut could be possible to use other loss functions.

\textbf{Practical uses}. Base learners are usually the decision trees (or even stumps).
\begin{itemize}
    \item data is in $\Real^N$;
    \item each stump is associated with each feature component;
    \item each coordinate is pre-sorted $\mathcal{O}(N m \log m)$;
    \item at each round find the best component (feature) and threshold.
\end{itemize}
We do not need to find the best classifier, it suffices to find one that does barely
better than $0\%$. This results in total complexity $\mathcal{O}(mNT + Nm \log M)$,
but stumps are not weak learners!

Does AdaBoost overfir? Assume that $\text{VCdim}(H) = d$ and for a fixed $T$ define
\[\mathcal{F}_T
    = \{ \text{sgn}(\sum_{t=1}^T \alpha_t h_t  - b)
        \,:\, \alpha_t, b\in \Real\}\,,\]
then
\[ \text{VCdim}(\mathcal{F}_T) \leq 2(d+1)(T+1)\log_2((T+1)e) \,. \]

The mystery of AdaBoost is about why the test error gradually decreases with each
next round. Alternative is to use the margin theory, since it justifies SVM in high-
dimensional feature spaces.

Theorem Let $H$ be a set of functions mapping $X\mapsto\Real$. The the convex hull
of $H$ be defined as 
\[\text{conv}(H) = \{\sum_{k=1}^p \mu_k h_k\,:\, \}\,.\]
Then $\hat{\mathcal{R}}_S(\text{conv}(H)) = \hat{\mathcal{R}}_S(H)$.
\textbf{proof}
\begin{align*}
    \hat{\mathcal{R}}_S(\text{conv}(H))
    & = \ex_\sigma \sup_{(h_k)_{k=1}^p\in H\,,\mu\geq0\,,\|\mu\|_1\leq 1}
        m^{-1} \sum_{i=1}^m \sigma_i \sum_{k=1}^p \mu_k h_k(x_i) \\
    & = \ex_\sigma \sup_{(h_k)_{k=1}^p\in H} \sup_{\mu\geq0\,,\|\mu\|_1\leq 1}
        m^{-1} \sum_{k=1}^p \mu_k \sum_{i=1}^m \sigma_i h_k(x_i) \\
    & = \ex_\sigma \sup_{(h_k)_{k=1}^p\in H}
            m^{-1} \max_k \sum_{i=1}^m \sigma_i h_k(x_i) \\
    & = \ex_\sigma \sup_{h\in H} m^{-1} \sum_{i=1}^m \sigma_i h(x_i)
    = \hat{\mathcal{R}}_S(H)\,,
\end{align*}
thus
\[ R(h) \leq \hat{R}_\rho(h) + \frac{2}{\rho} \mathcal{R}_m(H)
        + \sqrt{\frac{\log\frac{1}{\delta}}{2m}}
    \,, \]
which justifies convex ensemble methods which favour large margins.
This is direct consequence of the margin upper bound and the convex result.
Notice that the upper bound does not depend on the number of rounds.

Empirical margin loss of AdaBoost:
\[ \hat{\pr} \bigl( \frac{yf(x)}{\|\alpha\|_1} \leq \rho \bigr)
    \leq \bigl[(1-2\gamma)^{1-\rho} (1+2\gamma)^{1+\rho} \bigr]^\frac{T}{2}
    \,,\]
and for $\rho < \gamma$ the term in the square brackets it less than $1$, which
means that the bound decreases in $T$. For the bound to be convergent on should
have $\rho >> \mathcal{O}(\frac{1}{\sqrt{T}})$

Does AdaBoost maximize the margin? In general no.
What about outliers? AdaBoost assigns much larger weights to examples hard to classify,
which means that the additional base classifiers tend to focus on a single example.
This means that AdaBoost can be used for outlier detection, based on high point
weights of the data distribution $D_{T+1}$.

AdaBoost has the weakest base learners possible. Is it plausible? Let the edge of
a binary classifier be defined as 
\[\gamma_t(D)
    = \frac{1}{2} - \epsilon_t
    = \frac{1}{2} \sum_{i=1}^m D(i) y_i h_t(x_i)
    \,.\]
AdaBoost learning assumption is that there exists $\gamma>0$ such that for all
distributions on the training sample and any base classifier $h_t$ with 
\[\gamma_t(D) \geq \gamma \,.\]
Game theoretic view: use the von Neumann theorem for zero-sum games for mixed strategies
that for any two-person zero-sum game defined by a payoff matrix $M$
\[\min_p \max_q p'Mq = \max_q \min_p p'Mq \,, \]
which means that there is an equilibrium in mixed strategies in the game.

% subsection boosting_and_ensemble_methods (end)

% section 20160921_lecture_1 (end)

\section{20160921 Lecture \#2} % (fold)
\label{sec:20160921_lecture_2}

\textbf{AdaBoost} can be represented as this zero-sum game:
\begin{itemize}
    \item teacher: selects points $(x_i)_{i=1}^m$;
    \item learner: learns a base classifier $(h_t)_{t=1}^T \in H$;
    \item payoff matrix: $M_{it} = y_i h_t(x_i)$.
\end{itemize}
Then the minmax and maxmin in mixed strategies gives:
\[ 2\gamma^*
    = \min_D \max_{h\in H} \sum_{i=1}^m D(i) y_i h(x_i)
    = \max_\alpha \min_{i=1}^m y_i \sum_{t=1}^T \frac{\alpha_t h_t(x_i) }{\|\alpha\|_1}
    = \rho^*
    \,.\]
Hence weak learning conditions implies non-zero margin, which means that it is possible
to search for a non-zero margin. And AdaBoost suboptimally searches for the corresponding
$\alpha$. Further more weak learning implies linear separability with the margin
$2\gamma^* > 0$.

There is a relation ship between AdaBoost and linear classifiers: the base learners
represent basis functions (features). And AdaBoost yileds a sparse solution. The
margin in AdaBoost is with respect to $\|\cdot\|_\infty$ (the dual to $\|\cdot\|_1$
c.f. p.96 of Boyd).

The $\gamma$ is single for \textbf{all} $D$ in the weak learner condition. In fact
the edge of each new hypothesis decreases with $t$: if $\gamma_t$ approaches zero
then the guarantees are lost. The WL assumption is violated in cases when the error
is precisely $\frac{1}{2}$.

Convex optimization LP problem: $\rho\to \max_\alpha$ subject to
\[ y_i (\alpha'x_i) \geq \rho\,\text{ and }\, \|\alpha\|_1 = 1\,. \]

Theoretical guarantees: \begin{itemize}
    \item AdaBoost is not designed to maximize the margin;
    \item there is a need for regularization of $\alpha$ (to combat overfitting).
\end{itemize}
Weaker aspects: \begin{itemize}
    \item need to fix $T$ beforehand;
    \item need for fixing the family of base learners: overfitting or low margin;
    \item noise severely damages AdaBoost's accuracy (Dietterich, 2000).
\end{itemize}
AdaBoost variations:\begin{itemize}
    \item \textbf{arc-gv} to maximiza the margin (Reyzin and Shapire, 2006);
    \item $L_1$-regularized AdaBoost (Raetsch et al., 2001; Cortes et al. 2014),
    which optimizes the margin-based upper bound;
    \item DeepBoost has favourable learning guarantees and outperforms AdaBoost
    with $L_1$ (Cortes et al. 2014).
\end{itemize}

Now onto the max-entropy methods.

\subsection{Logistic regression and max-ent models} % (fold)
\label{sub:logistic_regression_and_max_ent_models}

Probabilistic models: \begin{itemize}
    \item density estimation -- provides probability showing confidence in the prediction;
    \item classification;
\end{itemize}
We shall discuss basic notions of information theory: for a randon variable $X$ taking 
at most $N$ values (discrete) \textbf{entropy} is the amount
of randomness in $H(X) = -\ex \log p_X$, for pmf $p_X(x) = \pr(X=x)$. Properties:
\begin{itemize}
    \item measures the uncertainty of $X$;
    \item $H(X)\geq 0$ since $p_X \leq 1$;
    \item Jensen's inequality implies 
    \[ H(X) = \ex\log\frac{1}{p_X} \leq \log \ex \frac{1}{p_X} = \log N\,;\]
\end{itemize}
Relative enetropy, the Kullback-lEiler divergence:
\[\text{KL}(p\|q)
    = \ex_p\log\frac{p}{q}
    = \sum_{x\in X}p(x) \log\frac{p(x)}{q(x)}
    \,, \]
show the extra information needed (lost) when using $q$ instead of $p$. KL divergence
is asymmetric, non-negative and definite, meaning that $\text{KL}(p\|q) = 0$ implies
$p=q$.

Rel. entropy can be generalized to the notion of Bregman Divergence:
if $F$ is convex, then
\[ B_F(x\|y) = F(x) - F(y) - \langle \nabla F(x), x-y\rangle \,, \]
Unnormalized relative entropy is a particular notion of the Bregman divergence
on a probability distribution simplex.
The conditional relative entropy is given by
\[ \ex_{X\sim r} \text{KL}(p(\cdot|X)\|q(\cdot|X)) \,. \]

% subsection logistic_regression_and_max_ent_models (end)

\subsection{Density estimation problem} % (fold)
\label{sub:density_estimation_problem}

Given a sample $S$ one wants to estimate the generating density of the DGP. One
solution is to use the ML estimation:
\begin{align*}
    p_{ML} &= \argmax_{p\in \mathcal{P}} \pr(S|p)\\
    &= \argmax_{p\in \mathcal{P}} \prod_{i=1}^m p(z_i) \,.
\end{align*}
The ML solution corresponds to minimization of the KL divergence of $p$ from the 
empirical distribution $\hat{p}_S$ of $S = (z_i)_{i=1}^m$:
\[ \text{KL}(\hat{p}_S\|p) \to \min_{p\in \mathcal{P}} \,. \]

maximum a posteriori principle:
\[ p_{MAP}
    = \argmax_{p\in \mathcal{P}} \pr(p|S)
    = \argmax_{p\in \mathcal{P}} \frac{\pr(S|p)}{\pr(S)} \pr(p)
    = \argmax_{p\in \mathcal{P}} \pr(S|p) \pr(p) \,. \]

Density estimation in the presence of features. A sample $S=(x_i)_{i=1}^m \in X$,
and a feature map $\phi: X\to \Real^N$. Assume that $X$ is finite (wlog). Now suppose
$\phi_j$ are in $H$ and $\|\Phi\|_\infty \leq \Lambda$.

Idea: empirical feature average close to expectation.
For any $\delta > 0$
\[ \bigl\| \ex_{x\sim D} \phi(x) - \ex_{x\sim S} \phi(x) \bigr\|_\infty
    \leq 2\mathcal{R}_m(H) + \Lambda \sqrt{\frac{\log \frac{2}{\delta}}{2m}}\,,\]
with probability at least $1-\delta$. \textbf{Maxent principle}: find a distribution
$p$ that is closest to a prior distribution $p_0$ (agnostic, typically uniform)
while verifying 
\[ \bigl\| \ex_{x\sim p} \phi(x) - \ex_{x\sim S} \phi(x) \bigr\|_\infty
    \leq \beta \,.\]
And measure closeness using the relative entropy. Thus the problem becomes:
\[ \text{KL}(p\|p_0) \to \min_{p\in \Delta} \,,\] 
subject to 
\[ \bigl\| \ex_{x\sim p} \phi(x) - \ex_{x\sim S} \phi(x) \bigr\|_\infty
    \leq \beta \,.\]
This is a convex problem, and thus has a unique solution. If $\beta=0$ then this
problem is the standard Maxent (unregularized Maxent), and for $\beta > 0$ yields
regularized Maxent. If $p_0$ is uniform over $X$, then minimizing the divergence
of $p_0$ from $p$ is equivalent to maximizing the entropy of $p$.

Thus the problem becomes:
\[ \sum_{x\in X} p(x) \log p(x) \to \min \,, \]
subject to $\sum_{x\in X} p(x) = 1$, $p \geq 0$ and
\[ \bigl|\sum_{x\in X} p(x) \phi_j(x) - m^{-1} \sum_{i=1}^m \phi_j(x_i) \bigr|
    \leq \beta \,,\]
for all $j=1,\ldots, N$.

A simple family of distributions is the family of Gibbs distributions:
\[ p_w \propto \text{exp}(w' \phi(x)) \,, \]
for $w\in \Real^N$. It includes many distributions: Gaussian et c.

The dual problem is given by:
\[ \text{KL}(p\|p_0) \infty_{p\in \Delta} + \infty 1_C(\ex_{x\sim p} \phi(x)) \,,\]
with $C = \{u\,:\, \|u - \ex_{x\sim S} \phi(x) \|_\infty \leq \beta \}$. (Cortes
et al., 2015) show that this problem of regularized Maxent coincides with the ML
within the Gibbs family.

The next step is to choose the best Bregman divergence, since Maxent does not care
about the particular divergence.

\noindent Next is the conditional Maxent models.

% subsection density_estimation_problem (end)

% section 20160921_lecture_2 (end)

% 

\section{20160922 Lecture \#1} % (fold)
\label{sec:20160922_lecture_1}

Today we will continue with the Maxent principle, and then start a new topic.

\subsection{Conditional Maxent models} % (fold)
\label{sub:conditional_maxent_models}

The idea is to use conditional distribution for each class, and obviously it is
suitable for the multiclass classification problem. This allows different features
for each class. The model we are going to discuss multinomial regression, of which
the special case if the logistic regression.

The data $S=(x_i, y_i)_{i=1}^m\in X\times Y$ iid according to $D$. THe set of targets
$Y$ is finite: $Y=\{1,\ldots,K\}$, or $Y=\{0, 1\}^K$. Features are given by a mapping
$\phi:X\times Y \mapsto \Real^N$. THe problem is to find an accurate conditional
probability model of a class given $x$: $\pr(\cdot|X=x)$ for $x\in X$ based on $\Phi$.

The same path as in Maxent models:
\[ \| \ex_{\stackrel{x\sim\hat{p}}{y\sim D_{|x}}} \Phi(x, y)
     - \ex_{(x, y)\sim S} \Phi(x, y) \|_\infty
     \leq 2\mathcal{R}_m(H) + \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,,
     \]
with probability at least $1-\delta$. Maxent principle instructs to find a distribution
$p(\cdot|x)$ closest to $p_0(\cdot|x)$ while maintaining
\[ \| \ex_{\stackrel{x\sim\hat{p}}{y\sim p_{|x}}} \Phi(x, y)
     -\ex_{(x, y)\sim S} \Phi(x, y) \|_\infty \leq \beta \,. \]
and closeness is measured in terms of conditional relative entropy. The optimization
problem is
\begin{align*}
    \min_{p(\cdot|x)\in \Delta} &\sum_{x\in X}\hat{p}(x) \text{KL}(p(\cdot|x)\|p_0(\cdot|x)) \\
    \text{s.t.} & \|\ex_{x\sim \hat{p}} \ex_{y\sim p(\cdot|x)} \Phi(x, y)
        - \ex_{(x, y)\sim S} \Phi(x, y)\|_\infty \leq \beta \,.
\end{align*}
And in this case the solution coincides with the regularized conditional maximum
likelihood of a Gibbs-family conditional distribution.
\begin{align*}
    \lambda \|w\|_1 + (\text{\textbf{or} } \lambda \|w\|^2_2)
    m^{-1} w' \sum_{i=1}^m \Phi(x_i, y_i) + 
    m^{-1} \sum_{i=1}^m \log\sum_{y\in Y} e^{w'\Phi(x_i, y)} \,.
\end{align*}
One can replace the soft-max to with the hard-max: and it looks like it maximizies
a margin
\[ \max_{y\in Y}\bigl\{ w'\Phi(x_i, y) - w'\Phi(x_i, y_i) \bigr\} \,. \]
Typically $\Phi(x, y) = e_y \otimes \Gamma(x)$. Prediction is routine:
\[ \hat{y}(x) 
    \in \argmax_{y\in Y}\pr(y|x) 
    = \argmax_{y\in Y} w'\Phi(x, y)
    \,. \] 
For binary classification the soft-max can be simplified to use a sigmoid and a base
class:
\begin{align*}
    \lambda \|w\|_1 + (\text{\textbf{or} } \lambda \|w\|^2_2)
    m^{-1} \sum_{i=1}^m \log (1 + e^{-y_i w'\Psi(x_i)}) \,,
\end{align*}
where $\Psi(x) = \Phi(x, +1) - \Phi(x, -1)$ is the margin between classes.

Various solution techniques: SGD, coordinate descent et c. In coordinate descent
this is similar to AdaBoost with logistic loss.

A generalization bound for logistic regression. If $\pm \Phi_j(x, y) \in H$ for
all $j=1,\ldots, N$, then for all $\delta>0$ for all $f:x\mapsto w'\Phi(x)$
\[ R(f) \leq m^{-1} \sum_{i=1}^m \log_{u_0} \bigl(1+e^{-y_i f(x_i)}\bigr)
    + 4 \|w\|_1 \mathcal{R}_m(H) + \sqrt{\frac{\log\log_2 2\|w\|_1}{m}}
    + \sqrt{\frac{\log \frac{2}{\delta}}{m}}
    \,, \]
with probability at least $1-\delta$ over samples $S$ of size $m$, and $u_0 = \log(1+e^{-1})$.

\noindent Deep boosting with logistic loss coincides with the conditional structural
Maxent models.

% subsection conditional_maxent_models (end)

\subsection{Online learning} % (fold)
\label{sub:online_learning}

We contrast the batch setup with the situation where the data arrives in stream
fashion. Importance: \begin{itemize}
    \item active area of research;
    \item allows efficient algorithms.
\end{itemize}
PAC learning make natural but strong assumptions: fixed distribution $D$, training
and test distributions coincide, and the samples are iid.

Online learning assumes adversarial situation, and uses the worst case distribution.
Also the train and test phases are mixed. The performance measure is based on the
mistakes made to measure the difficulty of the task, or regret.

Two major parts of online learning: \begin{itemize}
    \item prediction with expert advice;
    \item linear classification.
\end{itemize}

The general setting: there are $T$ rounds and the protocol is as follows
\begin{enumerate}
    \item receive instance $x_t\in X$ and advice $\hat{y}_{ti}\in \Gamma$ from the pool;
    \item predict $\hat{y}_t \in \Gamma$;
    \item receive label $y_t \in Y$;
    \item suffer loss $L(\hat{y}_t, y_t)$;
\end{enumerate}
the loss function is $L:\Gamma \times Y \mapsto \Real$. Assume $\Gamma=Y$.
The goal is to minimize the hindsight loss:
\[\text{regret}_T = \sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i=1}^N L(\hat{y}_{ti}, y_t) \,. \]

Halving algorithm: suppose there is the best expert in the pool and the prediction
is based on majority voting. And every time a mistake is made at least half of experts
are dropped. A modification is the weighted majority algorithm, which adjusts weights
of mispredicting experts instead of dropping them.

However, no deterministic algorithm can achieve a regret $R_t = o(T)$ with the binary
loss. It is better to use randomized weighted majority.

The convex loss case: the exponential weighted average. We use this update:
\[P_{t+1}(d\theta) \propto e^{-L_t(\xi_t(\theta), y_t)} P_t(d\theta) \,,\]
and the prediction is just the average $\hat{y}_t(x) = \ex_{\theta\sim P_t} \xi_t(\theta, x)$.
The theorem: if $L$ is convex over $\Gamma$ and takes values in $[0,1]$, then for
any $\eta>0$ and any $(y_t)_{t=1}^T\in Y$ the regret at $T$ is satisfies
\[ \text{regret}_T = \min_{\theta\in \Theta} \sum_{t=1}^T L(\hat{y}_t, y_t) - L(\hat{y}_t(\theta), y_t)
    \leq \frac{\log N}{\eta} + \frac{\eta T}{8} \,. \]
For $\eta = \sqrt{\frac{8\log N}{T}}$ one has
\[ \text{regret}_T \leq \sqrt{\frac{T \log N}{2}} \,.\]

The doubling trick: divide the future into periods $[2^k, 2^{k+1}-1]$ of length
$2^k$ with $k\leq T$, and $T\geq 2^n-1$ then
\[ \text{regret}_T
    \leq \frac{\sqrt{2}}{\sqrt{2}-1} \sqrt{\frac{T \log N}{2}}
    + \sqrt{\frac{\log N}{2}}
    \,. \]

\textbf{Linear classification}
In batch case the points are drawn according to a distribution $D$, but if there
is no distribution, online learning guarantees a bound on the regret.

The most famous algorithm of online regression is the Perceptron algorithm, which
consists of learning the weights of the linear classifier, and updating the weight
vector in case of a misprediction. Its objective is
\[ T^{-1} \sum_{t=1}^T \max\{0, -y_t (w'x_t) \} = \ex_{(x, y)\sim S} f(w, (x, y)) \,,\]
for $f(w, (x, y)) = - y (w'x)$. This the Perceptron coincides with the SGD since
the update is
\[ w_{t+1} \leftarrow w_t - \eta \nabla_w f(w_t, (x_t, y_t)) 1_{y_t (w'x_t) < 0} \,. \]

\textbf{Novikoff theorem} A bound on the perceptron algorithm: if $\|x_t\| \leq R$
for all $t\leq T$, and for some $\rho>0$ and $v\in \Real^N$ one has for all $t\leq T$
\[ \rho \leq \frac{y_t (v'x_t)}{\|v\|} \,, \]
then the number of mistakes made by the perceptron algorithm is bound by $\frac{R^2}{\rho^2}$.

If $I$ is the set of $t$ at which an update was made then summing the conditions
over $t\in I$
\begin{align*}
    |I|\rho
        &\leq \frac{v'\sum_{t\in I} x_t y_t }{\|v\|} \\
        &= \frac{v'\sum_{t\in I} (w_{t+1} - w_t) }{\|v\|} \\
        &= \frac{v'w_{T+1} }{\|v\|} \\
        &\leq \| w_{T+1} \| \\
        &\leq \| w_{t^*} + y_{t^*} w_{t^*}'x_{t^*} \| \\
\end{align*}

A modification is to use voted perceptron algorithm, and predict according to
\[ \text{sgn} \bigl( (\sum_{t\in I} c_t w_t)' x_t \bigr) \,. \]
The vectors $(x_t)_{t\in I}$ are the support vectors for the perceptron. In the
non-separable case the algorithm does not converge.

% subsection online_learning (end)

% section 20160922_lecture_1 (end)

\section{20160922 Lecture \#2} % (fold)
\label{sec:20160922_lecture_2}

The guarantees do not hold in the non-separable case: the algorithm might not converge.
The analysis is based on the leave-one-out error: the expected value of which is
the generalization error.
Theorem:
\[\ex_{s\sim D^m} R(h_S)
    \leq \ex_{S'\sim D^{m+1}} \frac{\min\{M(S'), \frac{R^2_{m+1}}{\rho^2_{m+1}}\}}{m+1}
    \,, \]
\textbf{proof} let $S\sim D^{m+1}$ be linearly separable sample and pick $x\in S$.
If $h_{S-\{x\}}$ misclassifies the example $x$, then $x$ must be a support vector,
and thus
\[ \,.\]

The guarantee of Novikoff holds in a linearly separable case, but can we offer anything
in the non-separable case? We can:
\[ M_T \leq \inf_{\rho>0\,,\|u\|_2\leq 1}
    \sum_{t\in I} \bigl( 1-\frac{y_t (u'x_t)}{\rho}\bigr)_+
        + \sqrt{\frac{\sum_{t\in I} \|x_t\|^2}{\rho}}
    \,. \]
when $\|x_t\|\leq R$ for all $t\in I$ this implies:
\[ M_T \leq \inf_{\rho>0\,,\|u\|_2\leq 1}
    \frac{R}{\rho} + \sqrt{\|L_rho(u)\|_1}
    \,, \]
where $L_\rho(u) = \bigl((1-\frac{y_t (u'x_t)}{\rho})_+\bigr)_{t\in I}$.

One can formulate a dual of the perceptron algorithm:\begin{itemize}
    \item set $\alpha\leftarrow \alpha_0$;
    \item for$t=1,\ldots, T$ do \begin{enumerate}
        \item get $x_t$;
        \item put $\hat{y}_t \leftarrow \text{sgn}\bigl(\sum_{s=1}^T \alpha_s y_s (x_s' x_t)\bigr)$;
        \item get $y_t$;
        \item if $\hat{y}_t\neq y_t$, then $\alpha_s \leftarrow \alpha_t + 1$;
    \end{enumerate}
    \item return $\alpha$;
\end{itemize}
Since it depends on the inner products only, this means that it can be extended
to kernels to get a kernel perceptron algorithm.

Online linear classification algorithm: Winnow algorithm.
\begin{itemize}
    \item set $w_1\leftarrow N^{-1}$;
    \item for $t=1,\ldots, T$ do \begin{enumerate}
        \item get $x_t$;
        \item put $\hat{y}_t \leftarrow \text{sgn}\bigl(w_t' x_t\bigr)$;
        \item get $y_t$;
        \item if $\hat{y}_t\neq y_t$, then \begin{itemize}
            \item $Z_t \leftarrow \sum_{i=1}^N w_{ti} \text{exp}(\eta y_t x_{ti})$;
            \item set $w_{t+1,i} \leftarrow Z_t^{-1} w_{ti} \text{exp}(\eta y_t x_{ti})$;
        \end{itemize}
        \item otherwise $w_{t+1} \leftarrow w_t$;
    \end{enumerate}
    \item return $w_{T+1}$;
\end{itemize}
Winnow algorithm has favourable bounds when a sparse solution is known to exist.
% Online convex optimization, bandit problems et c.

\subsection{Ranking} % (fold)
\label{sub:ranking}

The goal is to learn a ranking function: $U:X\mapsto\Real$ -- the ``utility'' or
priority function. It is more favourable: sometimes not just relevant examples are
needed, but just the best ones.

Rank aggregation: for $n$ candidates and $k$ voters each giving a ranking of the
candidates, one has to find the ordering as close as possible to these, measured
by pairwise misranking. This is NP-hard even for $k=4$.

Fortunately, one can suggest a relatively efficient ML algorithms for this.
There are two kinds of algos:\begin{itemize}
    \item score-based;
    \item ...
\end{itemize}

\textbf{Score-based}: the goal is to learn a scoring function $h:U\mapsto\Real$.
A major drawback is that it imposes a linear total ordering on $U$, which is not
usually the case in real situations. The preferences might be intransitive, or even
incomparable.

Training data: a sample of iid labelled pairs from $U\times U$ according to $D$:
$S = ((x_i, x_i', y_i))_{i=1}^m\in U\times U\times \{-1, 0, +1\}$. for any $i=1,\ldots, m$
$y_i = 0$ indicates `don care' ranking or incomparability, whereas $y_i>0$ iff
$x_i \prec x_i'$. The problem is fond the hypothesis as close to the true preference
as possible:
\[ R(h) = \pr_{(x,x')\sim D} \bigl( f(x,x') \neq 0\,\&\,f(x,x')(h(x')-h(x)) \leq 0 \bigr)
    \,,\]
-- the pairwise misranking error. The empirical error is
\[ \hat{R}_S(h) = \pr_{(x,x',y)\sim S} \bigl( y \neq 0\,\&\,y (h(x')-h(x)) \leq 0 \bigr)
    \,.\]
The relation $(x,x')\in \mathcal{R} \Leftrightarrow f(x',x) = 1$ might not be transitive
or even antisymmetric.

Distributional assumptions: $m$ points, labels for pairs gives squared number of
examples $\mathcal{O}m^2$, and there is a dependency issue. This issue is that if
$(x_1,x_2)$ and $(x_2,x_3)$ implies that $(x_1,x_3)$ must also be in the sample.

We assume distribution over $m$ pairs and independence in the sample. Confidence
margin and the empirical margin loss are defined as
\[ \hat{R}_\rho(h) = m^{-1} \sum_{i=1}^m \Phi_\rho(y_i (h(x_i') - h(x_i))) \,. \]
and $\Phi_\rho(x) \leq 1_{x\leq \rho}$.

Margin Rademacher complexities: 
\[\mathcal{R}_m^{D_1}(H) = \ex_{D_1} \hat{\mathcal{S}}_1(H) \,, \]
where $D_1$ and $D_2$ are marginal distributions of the true distribution $D$ over
the pairs.

We can get a ranking margin bound:
\[ R(h) \leq \hat{R}_\rho(h)
    + \frac{2}{\rho}(\mathcal{R}_m^{D_1}(H)+\mathcal{R}_m^{D_2}(H))
    + \sqrt{\frac{\log\frac{1}{\delta}}{2m}}
    \,. \]

\textbf{Ranking with SVM} The idea is to look for a linear function that maximizes
the pairwise misranking margin: $\frac{1}{2}\|w\|^2 + C \sum_{i=1}^m \xi_i$ to
$\min_{w, \xi_i}$ subject to
\[ y_i w'( \Phi(x_i') - \Phi(x_i)) \leq 1 - \xi_i \,,\]
for $\xi_i\geq 0$. The decision function is $h(x) = w'\Phi(x) + b$. If $\Psi(z_i)$
is defined as $\Phi(x_i')-\Phi(x_i)$ then this coincides with SVM, which means that
it can be used with kernels.

Boosting for ranking, CD rankBoost.

$H\subseteq \{0,1\}^X$, $\epsilon^0_t+\epsilon^+_t+\epsilon^-_t=1$, and
\[ \epsilon^s_t
    = \pr_{(x,x')\sim D_t} \Bigl(\text{sgn}\bigl( f(x,x')(h(x')-h(x)) \bigr) = s \Bigr)
    \,.\]

Coordinate descent RankBoost is given by the following pseudoalgorithm:
\begin{enumerate}
    \item $D_1\propto 1$;
    \item for $t=1,\ldots, T$ do: \begin{enumerate}
        \item $h_t\leftarrow $ base ranking with the smallest $\epsilon^-_t-\epsilon^+_t$;
        \item set $\alpha_t \leftarrow \frac{1}{2}\log\frac{\epsilon^+_t}{\epsilon^-_t}$;
        \item put $Z_t \leftarrow \epsilon^-_t + 2(\epsilon^-_t\epsilon^+_t)^\frac{1}{2}$;
        \item $D_{t+1}(x,x',y) \propto D_t(x,x',y) \text{exp} \{ -y\alpha_t(h_t(x')-h_t(x)) \}$;
    \end{enumerate}
    return $\phi_T(x) = \sum_{t=1}^T \alpha_t h_t(x)$;
\end{enumerate}

Again this can be viewed as a coordinate descent problem for the ranking objective
function:
\[ F(\alpha) = \sum_{(x,x',y)\in S} \text{exp}\{-y(\phi_T(x')-\phi_T(x))\} \,. \]
Coordinate Descent idea: find the best direction and then the best step.

\[\frac{\partial F(\alpha + \eta e_t)}{d \eta}
    = -(\epsilon^-_t - \epsilon^+_t) \,,\]
which means that one needs to find the best classifier in that direction.

\textbf{Bipartite ranking} We are given two samples $S_-$ and $S_+$, and
we must find $h$ that minimizes
\[R(h) = \pr_{x_+\sim D_+\,,x_-\sim D_-} \bigl( h(x_+) > h(x_-) \bigr) \,. \]

Bipartite ranking results are described in terms of the ROC-AUC.

In AdaBoost the constant base learner $h=1$ automatically equalizes the contribution
of negative and positive examples, thus immediately resolving issues with class
imbalance. Thus asymptotically AdaBoost achieves the same optimum of the CD RankBoost
objective.
Since
\[F_{ada}(\alpha) = F_-(\alpha) + F_+(\alpha)\,,\]
whereas
\[F_{rank}(\alpha) = F_-(\alpha) F_+(\alpha)\,.\]

If for $(x,x')\in S_-\times S_+$ one has $D(x,x') = D_-(x) D_+(x)$, then
\[ D_{t+1} \propto D_{t-}(x) e^{\alpha_t h(x)} D_{t+}(x') e^{-\alpha_t h(x')}
    \,. \]

The area under the ROC is given by
\[ \text{AUC}(h)
    = \frac{1}{mm'}\sum_{i=1}^m \sum_{j=1}^{m'} 1_{h(x_j') > h(x_i)}
    = \pr_{(x,x')\sim \hat{D}_-\hat{D}_+}(h(x') > h(x))
    = 1 - \hat{R}(h)
    \,. \]

ROC-AUC is not for classification, but for ranking, so one should just compare the
misclassification error. Furthermore one should consider the statistical significance
of the AUC difference.

\textbf{Preference-based ranking} 

% subsection ranking (end)

% section 20160922_lecture_2 (end)

%

\section{20160923 Lecture \#1} % (fold)
\label{sec:20160923_lecture_1}

Domain adaptation and sample bias correction. The assumptions made in earlier lectures
are in a way strong: the data is iid and the discrete is fixed in time and the train,
test samples ale drawn according to the same distribution. In the real world this
is the usual situation: take spam detection -- the spammers adapt to filters, and
thus augment the distribution.

\subsection{Domain adaptation} % (fold)
\label{sub:domain_adaptation}

The outline:
\begin{itemize}
    \item divergence measure -- discrepancy;
    \item theoretical guarantees;
    \item DA algorithm and its enhancements;
\end{itemize}
DA appears in: \begin{itemize}
    \item sentiment analysis;
    \item language modelling, part-of-speech tagging;
    \item statistical parsing;
    \item speech recognition, computer vision;
\end{itemize}

Formalization: source domain $(Q, f_Q)$, and target $(P, f_P)$, where $f$ is the
labelling function. On input one receives a labelled sample $S$ from the source
distribution, and an unlabelled sample $T$ from the target. The problem is to
find a hypothesis in $H$ with small expected loss with respect to the target
domain:
\[ \mathcal{L}_P(h, f_P) = \ex_{x\sim P} L(h(x), f_P(x)) \,. \]
The deterministic scenario can be easily transformed into a stochastic labelling
one.

This problem is harder: not just the finiteness of the sample, but additionally
the source and target distributions are different.

\textbf{Sample bias correction}, is a special case of DA with $f_P = f_Q$, and
$\text{supp}(Q)\subsetq \text{supp}(P)$.

\subsubsection{Distribution mismatch} % (fold)
\label{ssub:distribution_mismatch}

The main question is what distance should be used to compare the distributions,
especially in the case of different supports. Possible ways: Hellinger, Kullback-
Leibler, total variations ($L_1$), quadratic variation ($L_2$) or $L_p$.
\textbf{KL} is infinite if the supports are not nested.

The choice of a loss function $L$ and a hypothesis set $H$ is important.
Assume the loss is bounded by $M$, then for all $h\in H$
\[ \bigl| \mathcal{L}_Q(h, f) - \mathcal{L}_P(h, f) \bigr|
    \leq M L_1(Q, P)
    \,.\]
This follows from the properties of $\ex$.

Discrepancy:
\[ \text{disc}(Q,P) = \max_{h,h'} \bigl| \mathcal{L}_P(h,h') - \mathcal{L}_Q(h,h')\bigr| \,. \]
It is symmetric, and satisfies the triangle inequality, but is not a distance in
general: $\text{disc}(Q,P) = 0$ does not imply $P=Q$.

For $L_q$ loss bounded by $M$, for any $\delta>0$
\[ \text{disc}(Q,P)
    \leq \text{disc}(\hat{Q},\hat{P})
    + 4q(\hat{\mathcal{R}}_S(H) + \hat{\mathcal{R}}_T(H))
    + 3 M \biggl( \sqrt{\frac{\log\frac{4}{\delta}}{2m}}
                + \sqrt{\frac{\log\frac{4}{\delta}}{2n}} \biggr)
    \,,\]
with probability at least $1-\delta$.

Let $K$ be a universal kernel and $H = \{ h\in \Hcal_K\,:\,\|h\|_\Hcal\leq \Lambda \}$.
Then for the $L_2$ loss the discrepancy is a distance. Note that 
\[ \Psi(x) = \ex_{x\sim P} h^2(x) - \ex_{x\sim Q} h^2(x) \,, \]
is Lipschitz.

Two questions: \begin{itemize}
    \item difference between the average loss of a hypothesis $h$ on $P$ against $Q$;
    \item ...
\end{itemize}
Generalization bound. If
\[ \mathcal{L}_Q(h^*_Q, f) = \min_{h\in H} \mathcal{L}_Q(h, f) \,,\]
then
\[ \mathcal{L}_P(h, f_P)
    \leq \mathcal{L}_Q(h, h^*_Q)
    + \mathcal{L}_P(h^*, f_P)
    + \text{disc}(P,Q)
    \,, \]

Consider a regularized ERM:
\[F_{\hat{Q}} = \lambda \|h\|^2_\Hcal + \hat{R}_{\hat{Q}}(h) \,,\]
where $\Hcal$ is the canonical RKHS induced by a kernel $K$. This has the following
guarantee: if $K(x,x) \leq R^2$ and $L$ is $\mu$-Lipschitz loss in the first argument,
and $f_P\in H$ then for all $(x, y)\in X\times Y$
\[ | L(h'(x), y) - L(h(x), y) |
    \leq \mu R \sqrt{\frac{\text{disc}(\hat{P}, \hat{Q}) + \mu \eta}{\lambda}}
    \,, \]
where $\eta = \max\{L(f_Q(x), f_P(x))\,:|, x\in \text{supp}(\hat{Q}) \}$.

Regularized ERM:
\[ | L(h'(x), y) - L(h(x), y) |
    \leq \frac{R\sqrt{M}}{\lambda}\biggl(
    \delta + \sqrt{\delta^2 + 4\lambda \text{disc}(\hat{P}, \hat{Q})}
    \biggr) \,, \]
where
\[\delta = \min_{h\in \Hcal_K}\biggl\|
    \ex_{x\sim \hat{Q}} (h(x) - f_Q(x))\Phi_K(x)
    - \ex_{x\sim \hat{P}} (h(x) - f_P(x))\Phi_K(x)
    \biggr\|_K
\,.\]
Thus the empirical discrepancy is very crucial in all generalization bounds. But
how can it be reduced further?

Change the weights of the distribution $Q$:
\[ q^* = \argmin_{\text{supp}(q) \subseteq \text{supp}(\hat{Q})} 
        \text{disc}(\hat{P}, q)
    \,,\]
which gives this optimization problem:
\[ \min_h F_{q^*}(h)
    = \sum_{i=1}^m q^*(x_i) L(h(x_i), y_i) + \lambda \|h\|^2_K
    \,. \]

Finding the $q^*$. View it as a game:
\[ \hat{Q}' = \argmin_{\hat{Q}'\in Q} \max_{h,h'\in H}
    \bigl| \mathcal{L}_{\hat{P}}(h',h) - \mathcal{L}_{\hat{Q}'}(h',h) \bigr|
    \,. \]
This optimal value can be bounded by a max-min bound.

% subsubsection distribution_mismatch (end)

% subsection domain_adaptation (end)

\subsection{Multiple source domain adaptation} % (fold)
\label{sub:multiple_source_domain_adaptation}

Given distributions and corresponding hypotheses: $(h_i)_{i=1}^k$ -- hypotheses
good for $(D_i)_{i=1}^k$ -- different domains for the same target labelling function
$f$. All individual hypotheses perform well in their domain $\mathcal{L}(D_i, h_i, f) \leq \epsilon$
with
\[ \mathcal{L}(D_i, h_i, f) = \ex_{x\sim D_i}L(h_i(x), f(x)) \,.\]
The loss $L$ is convex, non-negative, bounded and continuous.

The unknown target distribution is a mixture of source distributions $(D_i)_{i=1}^k$,
and the task is to find a hypothesis that performs well with respect to any target, 
and choice of $(\lambda_i)_{i=1}^k$: convex combination rule for $z_i\geq 0$
\[ h_z(x) = \sum_{i=1}^k z_i h_i(x) \,,\]
and distribution weighted combination
\[ h_z(x) = \sum_{i=1}^k \frac{z_i D_i(x)}{\sum_{j=1}^k z_j D_j(x)} h_i(x) \,.\]
Note that for some distribution any convex combination performs poorly.

For $z=\lambda$ one has due to convexity 
\[ \mathcal{L}(D_T, h_\lambda, f)
    = \ex_{x\sim D_T} L(h_\lambda(x), f(x))
    \leq \ex_{x\sim D_T} \sum_{i=1}^k
        \frac{z_i D_i(x)}{\sum_{j=1}^k z_j D_j(x)} L(h_i(x), f(x))
    = \sum_{i=1}^k \sum_{j=1}^k \lambda_j \ex_{x\sim D_j}
        \frac{\lambd _i D_i(x)}{\sum_{j=1}^k \lambd _j D_j(x)} L(h_i(x), f(x))
    \,, \]

Zero-sum game: \begin{itemize}
    \item NATURE: select a target distribution $D_i$;
    \item LEARNER: select a distribution weighted hypohtesis $h_z$;
    \item PAYOFF: suffer $\mathcal{L}(D_i, h_z, f)$;
    \item then by von Neumann theorem the value of the game is at most $\epsilon$.
\end{itemize}
The minimiax theorem posits that there exists a mixture $\sum_{j=1}^k \alpha_j h_{z_j}$
of distribution weighted hypotheses that does well for any distribution mixture.

\textbf{Brouwer's fixed point theorem} is useful here (used NE proof): for any compact
convex non-empty $A$ and any continuous function $f:A\mapsto A$, there exists $x\in A$
with $f(x) = x$.

Put $\mathcal{L}_i^z = \mathcal{L}(D_i, h_z, f)$. Define a mapping $\phi$ by
\[ \phi(z)_i = \frac{z_i \mathcal{L}_i^z}{\sum_{j=1}^k z_j \mathcal{L}_j^z} \,, \]
from the simplex $\{z\,:\, z\geq 0\,,\sum_{j=1}^k z_j = 1\}$.
Suppose $z>0$ and thus $\phi$ is continuous, which means that there exists $z$ such
that $\phi(z) = z$, whence
\[ \mathcal{L}_i^z = \sum_j=1}^k z_j \mathcal{L}_j^z = \gamma \,.\]
Thus this $z$ balances the losses across different domains. But we need to make
sure that $z>0$. 

Bounding loss: for a fixed point $z$ one has
\begin{align*}
    \mathcal{L}(D_z, h_z, f)
        &= \sum_{x\in X} L(h_z(x), f(x)) \sum_{j=1}^k z_j D_j(x) \\ 
        &= \sum_{j=1}^k z_j \sum_{x\in X} L(h_z(x), f(x)) D_j(x) \\ 
        &= \sum_{j=1}^k z_j \mathcal{L}_j^z  = \gamma \,. 
\end{align*}
Now, if the target distribution mixture parameters are known, we have $\gamma\leq \epsilon$.

Details on the continuity of $L$ of the discontinuity of the mixing coefficients:
smear $\eta > 0$ across all domains
\[ h_z(x) = \sum_{i=1}^k \frac{z_i D_i(x) + \frac{\eta}{k}}{\eta + \sum_{j=1}^k z_j D_j(x)} h_i(x)
    \,;\]
This implies that for any target function $f$ and any $\delta>0$, there exist $\eta>0$
and $z$ such that $\mathcal{L}(D_T, h_\lambda, f) \leq \epsilon + \delta$.

R\'enyi divergence:
\[ D_\alpha(P\|Q)
    = \frac{1}{\alpha - 1} \ex_P \biggl(\frac{P(x)}{Q(x)}\biggr)^{\alpha - 1}
    \,. \]

% subsection multiple_source_domain_adaptation (end)

% section 20160923_lecture_1 (end)

\section{20160923 Lecture \#2} % (fold)
\label{sec:20160923_lecture_2}

\subsection{Transductive learning} % (fold)
\label{sub:transductive_learning}

Inductive: receive a training sample and using it make a prediction on a test sample;
Transductive: receive both a labelled and unlabelled test sample and make a prediction
for that same unlabelled examples. A closely-related concept is semi-supervised
scenario. Examples: network predictions in computational biology for biosequences,
web graph predictions, NLP applications.

Knowing the test object during training seems to be more favourable than in the
semi-supervised setting. Does it make sense to compute a ``expected'' loss in this
case, since the test object is seen only once.

Two possible transductive settings: in the first setting there is a full sample
$X$ of size $m + u$ (labelled and unlabelled). The learner receives a sample $S=(x_i)_{i=1}^m$
drawn uniformly without replacement from $X$ as well as the labels $(y_i)_{i=1}^m$.

Errors:\begin{itemize}
    \item training error $\hat{R}_S(h) = \ex_{x, y\sim S} L(h(x), y)$;
    \item test error $R_T(h) = u^{-1} \sum_{i=1}^u L(h(x_{m+i}), y_{m+i})$;
    \item full sample error $R_X(h) = \frac{1}{m+u}(u R_T(h) + m \hat{R}_S(h))$;
\end{itemize}

Another setting is when there is a distribution $D$ over $X$, and the learner receives
a train sample $S$ and a test sample $T$. The setting are related:
\[ \ex_{S\sim D^m, T\sim D^u}
        1_{\sup_{h\in H} R_T(h) - \hat{R}_S(h) > \epsilon} 
    = \ex_{X\sim D^{m+u}} \ex_{(S, T) = X}
        1_{\sup_{h\in H} R_T(h) - \hat{R}_S(h) > \epsilon}
    \,, \]
and since it is simpler to study the first setting, we shall focus on this one.

Bounds: VC, PAC, Stability, and Rademacher. Transductive Rademacher complexity:
\[\mathcal{R}_{m+u}(G) = \ex_\sigma \sup_{g\in G} \sum_{i=1}^{m+u} \sigma_i g(x_i) \,, \]
with augmented Rademacher variables $\sigma \in \{-\frac{m}{u}, \frac{u}{m}\}$ and
``balancing'' probabilities.
Since the full sample is available it is possible to minimize the Rademacher complexity
term directly.


Transductive SVM: fake the unknown labesl for the test set.
\[ \frac{1}{2} \|w\|^2
    + C\sum_{i=1}^m L(w'x_i + b, y_i)
    + C'\sum_{i=1}^u L(w'x_{m+i} + b, y_{m+i})
    \to \min_{w, b,(y_{m+i})_{i=1}^u }
    \,.\]
This algorithm is hard in the classification case, but it is trivial in the regression
case. Also it does not yet have any theoretical guarantees, and it has exponential
complexity due to labels. The algorithm tends to label the unlabelled examples uniformly.

Local transductive regression:
\[ \|h\|_K + C \sum_{i=1}^m L(h(x_i), y_i)
     + C' \sum_{i=1}^u L(h(x_{m+i}), \tilde{y}_{m+i})
     \,, \]
with a pds kernel $K$ and pseudo labels $\tilde{y}$ obtained via a local weighted
average or any other local regression algorithm form neighbourhood of radius $r$.
For example, one can get surrogates from a $k$-NN regression.

Graph regularization algorithm. A weighted graph $G=(X, E)$ and a hypothesis $h:X\mapsto \Real$.



% subsection transductive_learning (end)

% section 20160923_lecture_2 (end)

\end{document}
