\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}
\newcommand{\tr}{\text{tr}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\nil}{\mathbf{0}}
\newcommand{\Rcal}{{\mathcal{R}}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Fcal}{\mathcal{F}}

\title{Mehryar Mohri: Foundations Of Machine Learning}
\author{Nazarov Ivan}
\begin{document}

\section{20160919: lecture \#1} % (fold)
\label{sec:20160919_lecture_1}

The course starts with Basics and then moves to advanced topics. There will be
a mailing list and some homework.

\subsection{Introduction to ML} % (fold)
\label{sub:introduction_to_ml}
Definitions and concepts, problems of learning, and probability tools.

ML consists of computational methods to use ``experience'' to make predictions and
is associated with statistics, probability and optimization.

Complexities: \begin{itemize}
    \item Sample complexity: how many sample are needed to learn effectively;
    \item Space and computational complexity: how much resources the algorithm
    requires and its relation to its efficiency.
\end{itemize}

Typical application: spam filtering, NLP (sentiment analysis, morphological analysis),
speech recognition and synthesis, image annotation and recognition, Game AI, autonomous
systems, medical diagnostics, fraud and intrusion detection.
\noindent In fact, Games are serving as a platform for creating data for ML.
\noindent Personalized medicine: providing medical solutions based on the patients
medical history.

Broader categorization: \begin{itemize}
    \item Classification: $0-1$ error -- harder than in regression, the labels might
    have hierarchy or ``lattice'' structure (need for a more sophisticated loss);
    \item Regression: proximity-based error measures;
    \item Ranking: preference mining, recover ordering;
    \item Clustering: partition the data into homogeneous, yet distinct groups;
    \item Dimensionality reduction: a lower-dimensional manifold representing the
    data and preserves its properties, or recovering invariant representations (main
    question is how well the reduced representation works in a subsequent task of
    classification or regression);
\end{itemize}

General objective of ML research: \begin{itemize}
    \item theoretical: what can be learned? What is ``efficiency''? What are the
    guarantees?
    \item algorithmic: how does the solution scale with the data? Does it handle
    different tasks or generalize?
\end{itemize}

This course will present the most mathematically established algorithms.

% subsection introduction_to_ml (end)

\subsection{Definitions and terminology} % (fold)
\label{sub:definitions_and_terminology}

Examples, features, labels, and data. Poor features, random features make every
ML algorithm perform poorly. Choice of the features is extremely critical, and
gradually there must be more algorithms that learn features automatically.

Data: \begin{itemize}
    \item train sample (typically labelled);
    \item test sample (unlabelled);
    \item validation sample (typically labelled).
\end{itemize}

The general learning scenarios: \begin{itemize}
    \item batch learning: train and test phases are separated;
    \item online learning: crucial for iterative optimization, train-test steps
    are intermixed;
\end{itemize}

Queries: \begin{itemize}
    \item passive: the learner receives its data all-at-once;
    \item active: the learner can actively participate in data collection and has
    a dedicated internal process to make decision about acquiring new data for more
    efficient learning.
\end{itemize}

Batch scenarios: \begin{itemize}
    \item unsupervised: no labelled data;
    \item supervised: use labelled data for predictions on new data;
    \item semi-supervised: use labelled and unlabelled data to make predictions
    about new data;
    \item transduction: use the train and test sets to make prediction about the
    available test set (the test cases are so difficult that it is still hard to
    make accurate predictions, test data provides with additional distributional
    information);
\end{itemize}

Stages of learning: \begin{enumerate}
    \item divide the available data in to train, validation and test;
    \item train algorithm and prior knowledge about features to learn an algorithm
    $\hat{\mathcal{A}}(\Theta)$;
    \item use validation to select the best hyper-parameter $\Theta$;
    \item use test set to gauge the generalization error.
\end{enumerate}

Let $X$ be the input e space and $Y$ -- the output space. There is a loss function
$L: Y\times Y \mapsto \Real$: $L(\hat{y}, y)$ -- the cost of predicting $\hat{y}$
when the actual target is $y$. A hypotheses set $H\subseteq Y^X$. Training data
$(x_i,y_i)_{i=1}^m \in X\times Y$.
Problem setup: \begin{itemize}
    \item deterministic case: $\exists f:X\mapsto Y$ measurable, not necessarily from
    $H$, such that $y_i = f(x_i)$;
    \item stochastic case: there exits a conditional distribution given the
    input \\$y_i \sim p(y|x=x_i)$.
\end{itemize}

Errors: \begin{itemize}
    \item the generalization error $R(h) = \ex_{(y, x)\sim D} L(h(x), y)$ for $\in H$
    -- the thing we care about;
    \item Empirical error: $\hat{R}(h) = m^{-1} \sum_{i=1}^m L(h(x_i), y_i)$;
    \item the Bayes error: $R^* = \inf_{h \text{ -- meas}} R(h)$ the smallest achievable
    theoretical error (not necessarily feasible);
\end{itemize}
In deterministic case $R^*=0$ in stochastic -- $R^*\geq 0$.

Algorithm works imperfectly -- ususally it is blamed on the noise (as a measure of
how poorly you are doing). Binary case for $x\in X$:
\[ \text{noise}(x) = \min\bigl\{\pr(y=1|x), \pr(y=0|x)\bigr\} \,, \]
-- the error of the Bayes classifier and $\ex \text{noise(x)} = R^*$ -- the Bayes
error.
\textbf{First homework}: define the noise for other loss functions and problems
setups.

One learning challenge: find an accurate predictor for \textbf{all points} while
having access to a \textbf{finite} set of points. So the question is which technique
to use. One possible approach is to use Empirical Risk Minimization procedure:
choose the hypothesis with the lowest training sample error
\[ \hat{h} = \argmin_{h\in H} \hat{R}(h) \,. \]
In most cases ERM is terrible without complexity accounting. \textbf{Learning is not
fitting}, and does not constitute making all correct predictions on the train set.
Thus we arrive at the notion of the complexity of the hypothesised space.

The choice of the hypothesis set is very important. generalization error has ``U''
shape, penalty term is monotonic increasing, and the train error is monotonic decreasing.
The left leg of ``U'' is underfitting, and the right leg -- overfitting. Ideally one
would want to balance these phenomena.

Structural risk minimization: given a set on nested hypothesis spaces $(H_n)_{n\geq 0}$
with $H_n \subset H_{n+1}$ for all $n$ and
\[ h = \argmin_{h\in H_n, n\geq 0} \hat{R}_S(h) + \text{penalty}(H_n, m) \,. \]
This is an NP-hard problem. The problem of learning: when it is good enough to stop.
Now, $\text{penalty}$ depends on the complexity term, and a term depending on the
index and sample size.

Regularization-based algorithms: for $\lambda \geq 0$
\[ h = \argmin_{h\in H}\hat{R}(h) + \lambda \Omega(h) \,, \]
where $\Omega:H \mapsto \Real$ is the regularization (complexity) term.

\textbf{Second homework}: prove the SRM guarantees.

% subsection definitions_and_terminology (end)

\subsection{The tools} % (fold)
\label{sub:the_tools}

The probability tools: \begin{itemize}
    \item the Union bound: $\pr(\cup_{n\geq1} A_n) \leq \sum_{n\geq1} \pr(A_n)$;
    \item inversion: if $\pr(X\geq \epsilon) \leq f(\epsilon)$, then for any $\delta>0$
    \[ X < f^{-1}(\delta) \text{ with probability at least } 1-\delta\,;\]
    \item Jensen's inequality: if $f$ is convex then $f(\ex X) \leq \ex f(X)$;
    \item Markov's inequality: if $X$ is non-negative then $\pr(X>\epsilon) \leq \frac{\ex X}{\epsilon}$;
    \item Chebychev's inequality: $\pr(|X - \ex X| > \epsilon) \leq \frac{\ex(X - \ex X)^2}{\epsilon^2}$;
    \item Hoeffding's inequality: if $(X_i)_{i=1}^m$ are independent and $X_i \in [a_i, b_i]$
    almost surely then 
    \[ \pr\bigl(m^{-1} \sum_{i=1}^m \ex X_i - m^{-1} \sum_{i=1}^m X_i > \epsilon \bigr)
        \leq \text{exp}\Bigl(-\frac{2 m^2\epsilon^2}{\sum_{i=1}^m(b_i-a_i)^2}\Bigr) \,, \]
    \item McDiarmid's inequality: if $X_i$ are independent and take values in $U$, and
    $f:U^m \mapsto \Real$ is Lipschitz and
    \[ \sup_{x_1, \ldots, x_m, x_i'}
        \bigl| f(x_1, \ldots, x_i, \ldots, x_m) - f(x_1, \ldots, x_i', \ldots, x_m) \bigr|
        \leq c_i\,, \]
    then
    \[ \pr\bigl( |f(X_1, \ldots, X_m) - \ex f(X_1, \ldots, X_m) | > \epsilon)
        \leq 2 \text{exp}\Bigl(-\frac{2 \epsilon^2}{\sum_{i=1}^m c_i^2}\Bigr) \,, \]
    \item Chernoff's bounds: for any $t> 0$ one has:
    \[ \pr(X\leq \epsilon) = \pr(e^{tX} \leq e^{t\epsilon}) \leq e^{-t\epsilon}\ex e^{tX} \,, \]
    and then take the $\inf_{t>0}\ldots$ to best minimize the left-hand side probability.
\end{itemize}

% subsection the_tools (end)

\subsection{Learning guarantees} % (fold)
\label{sub:learning_guarantees}
Fundamentals of learning. How to understand complexity of a hypothesis?

The sample $(x_i, y_i)_{i=1}^m \sim D$ iid. True, theoretical error is:
\[ R(h) = \pr_{(x, y)\sim D} (h(x)\neq c(x)) = \ex_{(x, y)\sim D} 1_{h(x)\neq c(x)} \,, \]
Empirical error:
\[ \hat{R}_S(h)
    = \ex_{(x, y)\sim \hat{D}} 1_{h(x)\neq c(x)}
    = \ex_{(x, y)\sim S} 1_{h(x)\neq c(x)} \,,
\]
where the $\hat{D}$ is the empirical distribution of the sample data $S$. Now, note
that $\ex \hat{R}_S(h) = R(h)$ (we need only distributional identity here). In cases
where the training sample is not drawn from the same distributions (drift, and domain
adaptation).

Hoeffding's inequality readily gives:
\[ \pr(R(h) - \hat{R}(h) \geq \epsilon ) \leq e^{-2m\epsilon^2}
    \text{ and }
    \pr(\hat{R}(h) - R(h) \geq \epsilon ) \leq e^{-2m\epsilon^2}
\,.\]
By the union bound:
\[ \pr(|\hat{R}(h) - R(h)|\geq \epsilon ) \leq 2 e^{-2m\epsilon^2} \,.\]
But this is all pretty and good for a single hypothesis.

A learning algorithm yields a hypothesis based on the sample: $\mathcal{A}(S) = h_S$
-- the data dependent hypothesis. $h_S$ is a random variable, and we need a bound
that holds for all hypothesis $h\in H$ -- a uniform bound.

For a \textbf{finite} hypothesis set $H$ Hoeffding's inequality readily gives a
bound (using the union bound):
\begin{align*}
    \pr(\max_{h\in H} |\hat{R}(h) - R(h)|\geq \epsilon )
    &= \pr(\cup_{h\in H} \{|\hat{R}(h) - R(h)|\geq \epsilon\} ) \\
    &\leq \sum_{h\in H}\pr(|\hat{R}(h) - R(h)|\geq \epsilon )
     \leq 2 |H| e^{-2m\epsilon^2} \,.
\end{align*}
Thus for any $\delta > 0$, with probability at least $1-\delta$ one has
\[ \forall h\in H \,,\,
    R(h) \leq \hat{R}(h) + \sqrt{\frac{\log|H|
    + \log \frac{2}{\delta}}{2m}}
\,,\]
using the inversion principle.

Occam's razor: \textbf{ceteris paribus} one should be choosing the simplest hypothesis
set.

What about an infinite hypothesis set? Reduce the infinite case to a finite case, or
in what cases it is impossible. Foe example, it is possible to discretize the hypothesis
set. Or one can project the hypothesis case onto a finite sample.

Rademacher complexity, Growth function, VC dimension and Lower bounds. VC dimension is
not just convenient, but is also very fundamental.
\textbf{To be continued in the second part.}

% subsection learning_guarantees (end)

% section 20160919_lecture_1 (end)

\section{20160919: lecture \#2} % (fold)

\label{sec:20160919_lecture_2}

\subsection{Rademacher complexity} % (fold)
\label{sub:rademacher_complexity}

Empirical Rademacher complexity. Let $G$ be a family of maps from $Z$ to $[a, b]$,
and suppose $S = (z_i)_{i=1}^m \in Z$ is the train sample. Let $(\sigma_i)_{i=1}^m\in\{-1, 1\}$
be independent uniform random variables -- the Rademacher variables. The hypothesis
class is more complex whenever it can approximate any random label assignment arbitrarily
close:
\[ \hat{\mathcal{R}}_S(G)
    = \ex_{\sigma} \sup_{g\in G} m^{-1} \sum_{i=1}^m \sigma_i g(z_i)
    \,, \]
-- the correlation with ``purely random label assignment''. This measures how well
$G$ can mimic randomness, or, in other words how rich the hypothesis class $G$ is.
It is sample dependent, capturing the complexity precisely on the observed sample.
Taking the expectation with respect to any sample $S$ of size $m$ gives the Rademacher
complexity of $G$:
\[ \mathcal{R}_m(G) = \ex_{S\sim D^m} \hat{\mathcal{R}}_S(G) \,. \]

If we have a set $G$ of hypotheses $Z\mapsto [0,1]$ then for any $\delta>0$ with
probability at least $1-\delta$ for all $g\in G$ one has:
\[ \ex_{z\sim D} g(z)
    \leq m^{-1} \sum_{i=1}^m g(z_i) + 2\mathcal{R}_m(G)
    + \sqrt{\frac{\log\frac{1}{\delta}}{2m}}
    \,,\]
and
\[ \ex_{z\sim D} g(z)
    \leq m^{-1} \sum_{i=1}^m g(z_i) + 2\hat{\mathcal{R}}_S(G)
    + 3\sqrt{\frac{\log\frac{2}{\delta}}{2m}}
    \,.\]

\noindent \textbf{Proof}: apply McDiarmid's inequality to
\[\Phi(S)
    = \sup_{g\in G} \ex_{z\sim D} g(z) - \ex_{z\sim S} g(z)
    = \sup_{g\in G} \ex g - \hat{\ex}_S g
    \,, \]
-- the deviation of the empirical expectation form the true average. But we need
to know the bounds of $\Phi(S)$. Change one example in $S$ to get $S'$:
\[ \Phi(S') - \Phi(S)
    \leq \sup_{g\in G} \hat{\ex}_{S'} g  - \hat{\ex}_S g
    = \sup_{g\in G} m^{-1}(g(z_i') - g(z_i))
    \leq m^{-1} \,, \]
and then the McDiarmid's inequality is applied for $\frac{\delta}{2}$ to get:
\[ \Phi(S) \leq \ex_S \Phi(S) + \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,, \]
with probability at least $1-\frac{\delta}{2}$. What is left is bounding the remaining
expectation.

What about $\ex_S \Phi(S)$? Introduce the ghost sample $S'$ of similar size $m$:
\begin{align*}
    \ex_S \Phi(S)
        &= \ex_S \sup_{g\in G} \ex g - \hat{\ex}_S g \\
        &= \ex_S \sup_{g\in G} \ex_{S'} \hat{\ex}_{S'} g - \hat{\ex}_S g \\
        & \bigl[\text{sub-additivity of sup}\bigr]\\
        &\leq \ex_S \ex_{S'} \sup_{g\in G} \hat{\ex}_{S'} g - \hat{\ex}_S g \\
        & \bigl[\text{permuting } S \text{ and } S' \text{ and taking sup}\bigr] \\
        &\leq \ex_{S',S,\sigma} \sup_{g\in G} m^{-1} \sum_{i=1}^m \sigma_i(g(z_i') - g(z_i)) \\
        & \bigl[\text{the expectations are identical and } \sigma \text{ are uniform}\bigr] \\
        &\leq 2 \ex_S \ex_\sigma \sup_{g\in G} m^{-1} \sum_{i=1}^m \sigma_i g(z_i)
        = 2 \mathcal{R}_m(G) \,.
\end{align*}
This analysis uses outer expectations, and we just assume that the suprema are measurable
-- this is true for most hypotheses classes.

Now use the probability concentration inequality: by McDiarmid's inequality one
has
\[ \mathcal{R}_m(G) \leq \hat{\mathcal{R}}_S(G) + \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,, \]
with probability at least $1-\frac{\delta}{2}$. Using the union bound we get: for
all $g\in G$
\[ \ex g \leq \hat{\ex}_S g + 2 \hat{\mathcal{R}}_S(G) + 3 \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,, \]
with at least $1-\delta$ probability altogether.

Notation shift: $Z \to X\times Y$, $G \to H$ and $g(z) = L(h(x), y)$. So how can we
derive the complexity of the hypothesis set, and not the losses (compositions) of a
hypothesis.

Binary classification: the Rademacher complexity of $G$ is half that of $H$. Define
\[ G = \{(x,y)\mapsto 1_{h(x)\neq y}\,:\, h\in H\} \,. \]
Note that the hypotheses take values in $\{-1,+1\}$. Then
\begin{align}
    \mathcal{R}_m(G)
        &= \ex_{S, \sigma} \sup_{g\in G} m^{-1} \sum_{i=1}^m \sigma_i g(z_i) \\
        &= \frac{1}{2} \ex_{S, \sigma} \sup_{h\in H} m^{-1} \sum_{i=1}^m \sigma_i (1 - y_i h(x_i)) \\
        &= 0 + \frac{1}{2} \ex_{S, \sigma} \sup_{h\in H} m^{-1} \sum_{i=1}^m - \sigma_i y_i h(x_i) \\
        &[\text{the distribution of } y_i \sigma_i \text{ is uniform}]\\
        &= \frac{1}{2} \ex_{S, \sigma} \sup_{h\in H} m^{-1} \sum_{i=1}^m \sigma_i h(x_i) \,.
\end{align}
Thus we have the following for the hypothesis set $H$: with probability at least $1-\delta$
for all $g\in G$
\[ R(h) \leq \hat{R}_S(h) + \hat{\mathcal{R}}_S(H) + 3\sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,.\]

Estimating the Rademacher complexity -- use the definition, which reduces to ERM.
However, it is computationally hard for most hypotheses sets. One can use Monte-Carlo
to this end.

% subsection rademacher_complexity (end)

\subsection{The growth function} % (fold)
\label{sub:the_growth_function}

The growth function is the size of the ``worst'' sample: the maximum number of ways
to assign labels to a sample of size $m$ using all hypothesis from the set $H$.
For all $m\geq 0$
\[ \Pi_H(m)
    = \max_{(x_i)_{i=1}^m \subseteq X}
        \bigl| \{ (h(x_i))_{i=1}^m\,:\,h\in H \} \bigr|
    = \max_{S \in X^m} |G_{|S}|
    \,. \]
It is very strict, since it also takes into account samples which could be extremely
rare with respect to the true distribution.

\textbf{Massart's lemma}. Let $A\subseteq \Real^m$ is a finite set, with
$R = \max_{x\in A} \|x\|_2$. Then the following holds:
\[ \ex_\sigma \sup_{x\in A} m^{-1} \sum_{i=1}^m \sigma_i x_i
    \leq \frac{R\sqrt{2\log|A|}}{m} \,. \]
Use the Chernoff's technique, then the Jensen's inequality, the union bound and finally
the Hoeffding's inequality. Ultimately this gives an upper bound on the Rademacher
complexity of $G$:
\begin{align*}
    \mathcal{R}_m(G)
    &= \ex_S \ex_\sigma \sup_{g\in G} m^{-1} \sum_{i=1}^m\sigma_i g(x_i) \\
    &= \ex_S \ex_\sigma \sup_{u\in G_{|S}} m^{-1} \sum_{i=1}^m\sigma_i u_i \\
    &\leq \ex_S \frac{\sqrt{m} \sqrt{2 \log|G_{|S}|}}{m}
    \leq \ex_S \sqrt{\frac{2 \log \Pi_G(m)}{m}} \,,
\end{align*}
form Massart's lemma and the definition on the growth function.

The growth function is related to the VC-dimension of a hypothesis set.

% subsection the_growth_function (end)

\subsection{VC-dimension} % (fold)
\label{sub:vc_dimension}

The dimension is the size of the largest sample $S$ that admits arbitrary labeling
by a hypothesis from $H$ ($H$ shatters $S$):
\[ \text{VCdim}(H) = \max\{m \geq 0 \, : \, \Pi_H(m) = 2^m \} \,, \]
Again, it is the worst case measure -- very conservative, -- in practice bad samples
can even be negligible. It suffices to present a sample $S$ of size $m$ such that
$H$ shatters $S$. To give an upper bound one must show that for no sample $d+1$
shattering is possible.

Typical VC-dimensions: \begin{itemize}
    \item intervals on the real line: $\text{VCdim}(H) = 2$;
    \item hyperplanes in $\Real^d$: $\text{VCdim}(H) = d+1$ the number of free parameters
    of a hyperplane $\beta_0 + \beta'x$ n $\Real^d$, but not true in general (in
    a plane -- XOR configuration for example, proof uses Radon's theorem);
    \item hyper-rectangles: $\text{VCdim}(H) = 2d$;
    \item convex polygons on the plane: $\text{VCdim}(H) = 2d + 1$;
    \item sine functions on a line: $\text{VCdim}(H) = \infty$;
\end{itemize}

Computing the VC dimension of composite classes, unions, or intersections.

\textbf{Sauer's lemma}: if $H$ is a hypothesis set with $\text{VCdim}_H(m) = d$
then the growth function $\Pi_H(m)$ is upper-bounded by
\[ \sum_{i=0}^d C^i_m \leq \biggl(\frac{em}{d}\biggr)^d  = O(m^d) \,, \]
for all $m\geq 0$.
\textbf{Proof}: for $d\leq m$ one has \begin{align}
    \sum_{i=0}^d C^i_m
        &\leq \sum_{i=0}^d C^i_m \biggl(\frac{m}{d}\biggr)^{d-i} 
         \leq \sum_{i=0}^m C^i_m \biggl(\frac{m}{d}\biggr)^{d-i}
         \leq \biggl(\frac{m}{d}\biggr)^d \sum_{i=0}^m C^i_m \biggl(\frac{d}{m}\biggr)^i \\
        &\leq \biggl(\frac{m}{d}\biggr)^d \biggl(1+\frac{d}{m}\biggr)^m
         \leq \biggl(\frac{em}{d}\biggr)^d \,,
\end{align}
since $1 - x \leq e^{-x}$.

Proof of the VCdim upper bound proceeds by induction on $m+d$. It is clearly true
for $m=1$ and $d=0$ or $d=1$. For arbitrary $(m, d)$ suppose the inequality is true
for lesser $m+d$. If $S'= \{x_1. \ldots, x_{m-1}\}$, and $G_1 = G_{|S'}$ and
\[ G_2 = \{g'\subseteq S'\,:\, g'\in G\,\text{and}\, g'\cup\{x_m\} \in G \}\,, \]
then $|G_1| + |G_2| = |G|$. And the rest is in the book by Mohri (cf. pp. 46-47).

Now, it is time for the generalization bound with VC dimension. For any $\delta>0$
with $1-\delta$ for any $h\in H$
\[ R(h)
    \leq \hat{R}(h) + \sqrt{\frac{2d\log \frac{em}{d}}{m}}
                    + \sqrt{\frac{\log\frac{1}{\delta}}{2m}}
\,,\]
where $H$ is a set of hypotheses taking values $\{\pm 1\}$.

In the consistent case, when the training error can reach $0$, the square root
in the upper bound disappears.

% subsection vc_dimension (end)

\subsection{Lower bound based on the VC dimension} % (fold)
\label{sub:lower_bound_based_on_the_vc_dimension}

Let $H$ be a hypothesis set with $\text{VCdim}(H)=d$, $d>1$. Then for any learning
algorithm $L$ there exists a distribution $D$, and a input-target relation $f\in H$
with
\[ \pr_{S\sim D^m} \bigl(R_D(h_S, f) > \frac{d-1}{32m}\bigr) \geq \frac{1}{100} \,. \]

Take a sample $S = (x_i)_{i=1}^d$ of size $d$ which is shattered by $H$. Define a
tricky distribution: with high probability return the same point $x_0$, but with
remaining probability mass return any other point uniformly:
\[ \pr(X=x_1) = 1-8\epsilon
    \text{ and } \pr(X=x_i) = \frac{8\epsilon}{d-1} \forall i=2,\ldots, d
    \,, \]

No-free lunch theorem: VC-dimension non-realizable case. For any learning algorithm $L$
there exists $D$ over $X\times \{0,1\}$ such that
\[ \pr_{S\sim D^m} \biggl(R_D(h_S) - \inf_{h\in H} R_D(h) > \sqrt{\frac{d}{320 m}} \biggr)
    \geq \frac{1}{64}
    \,. \]

% subsection lower_bound_based_on_the_vc_dimension (end)

% section 20160919_lecture_2 (end)

%

\section{20160920 lecture \#1} % (fold)
\label{sec:20160920_lecture_1}

Today we will cover SVM and kernel methods.

\subsection{Support vector machine} % (fold)
\label{sub:support_vector_machine}

Reminder: binary classifcation problem
$X\subseteq \Real^N$, a sample $S = (x_i, y_i)_{i=1}^m \in X\times \{-1, +1\}$, and
the hypotheses space is $h:X \mapsto\{-1, +1\}$, and we want to find the $h\in H$ with
the samllest generalization error $R(h) = \ex_{(x, y)\sim D} 1_{h(x)\neq y}$.

Linear classification: \begin{itemize}
    \item hyperplanes;
    \item et c.
\end{itemize}

SVM -- separable case, SVM -- unseparable case, and 

Fins the hyperplane that best spaerates the classes.

Suppose there exists a hyperplane separating classes from one another. THe set of hypothese is :
\[ H = \{x\mapsto \text{sgn}(\beta'x + \beta_0) \, : \, \beta\in \Real^N, \beta_0\in \Real\} \,.\]
Each separating hyperplane has a so called geometric margin:
\[\text{margin} = \min_{i=1}^m \frac{|\beta'x_i + \beta_0|}{\|\beta\|} \,,\]
where $\beta\in \Real^N$, and $\beta_0 \in \Real$.

But there are many hyperplanes, so Vapnik an Chervonenkis came up with the idea to
find a hyperplane maximizing the margin and separating the classes. Now observe
that for all $t>0$:
\begin{align*}
    \min_{i=1}^m \frac{|t \beta'x_i + t \beta_0|}{\|t \beta\|}
        &= \min_{i=1}^m \frac{|t \beta'x_i + t \beta_0|}{\|t \beta\|} \\
        &= \min_{i=1}^m \frac{|\beta'x_i + \beta_0|}{\|\beta\|} \,,
\end{align*}
whence by separability we can fix $\min_{i=1}^m |\beta'x_i + \beta_0|$ to $1$
\begin{align*}
    \rho &= \max_{\beta,\beta_0} \min_{i=1}^m \frac{|\beta'x_i + \beta_0|}{\|\beta\|} \\
    &= \max_{\beta, \beta_0, \min_{i=1}^m |\beta'x_i + \beta_0| = 1} \frac{1}{\|\beta\|} \\
    &= \min_{\beta, \beta_0, \min_{i=1}^m y_i(\beta'x_i + \beta_0) \geq 1} \|\beta\| \\
    & = \ldots \,.
\end{align*}

Thus the SVM problem in a separable case becomes:
\[ \min_{\beta, \beta_0} \frac{1}{2} \|\beta\|^2 \,,\]
subject to $y_i(\beta'x_i + \beta_0) \geq 1$ for all $i=1, \ldots, m$. This is a
convex problem (and affine constraints) and thus has a unique solution. Using KKT
conditions one can find the solution, and formulate the Dual problem. Form the
Lagrangian:
\[ L = \frac{1}{2}\beta\|^2 + \sum_{i=1}^m \alpha_i\bigl(1-y_i(\beta'x_i + \beta_0)\bigr) \,.\]
KKT conditions are: \begin{description}
    \item[Primal] $\forall i=1,\ldots, m$ we must have $y_i(\beta'x_i + \beta_0) \geq 1$;
    \item[Dual] $\forall i=1,\ldots, m$ we must have $\alpha \geq 0$;
    \item[Complementary slackness] $\alpha_i\bigl(1-y_i(\beta'x_i + \beta_0)\bigr) = 0$ for all $i=1,\ldots, m$;
    \item[First-order] $\frac{\partial}{\partial \beta} L = 0$ and $\frac{\partial}{\partial \beta_0} L = 0$;
\end{description}
This yields the following Dual:
\[\max_\alpha \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i'x_j \,, \]
subject to $\sum_{i=1}^m \alpha_i y_i = 0$ and $\alpha_i \geq 0$. The solution is
given by:
\[ h(x) = \text{sgn}(\sum_{i=1}^m \alpha_i y_i x_i'x + \beta_0 \,,\]
where $\beta_0 = y_i - \sum_{j=1}^m \alpha_j y_j x_j'x_i$ for all $i=1,\ldots, m$
with $\alpha_i > 0$.

Sparsity. Iff $h_S$ is the learned hypothesis, thel the LOO error is given by:
\[ \hat{R}_{\text{loo}}(L) = m^{-1} \sum_{i=1}^m 1_{h_{S_{-i}}(x_i) \neq y_i} \,, \]
where $S_{-i}$ is the sample with $i$-th example removed. LOO error is an almost
unbiased estimate of the generalization error 
\[ \ex_{S\sim D^m} \hat{R}_{\text{loo}}
    = m^{-1}\sum_{i=1}^m \ex_{S'\sim D^{m-1}} \ex_{(x, y)\sim D} 1_{h_{S'}(x) \neq y}
    = \ex_{S'\sim D^{m-1}} R_{S'}(h_{S'})
    \,, \]

If $h_S$ is the optimal separating hyperplane and $N_{\text{sv}}(S)$ is the number
of support vectors, then
\[\ex_{S\sim D^m} R(h_S) \leq \ex_{S\sim D^{m+1}} \frac{N_{\text{sv}}(S)}{m+1} \,. \]
But this is a weak guarantee, since it is only on averages.

The non-separable case uses the so called slack variables, which permits imperfect
separation. The objective is the same: minimize $\frac{1}{2}\|\beta\|^2$ with respect
to $\beta$ and $\beta_0$, subject to
\[ y_i(\beta'x_i + \beta_0) \geq 1 - \xi_i \,,\]
with $\xi_i \geq 0$ for all $i=1,\ldots, m$. But this time we must penalize the slacks:
for $C \geq 0$ one wants
\[ \frac{1}{2}\|\beta\|^2 + C \sum_{i=1}^m \xi_i \to \min_{\beta, \beta_0} \,, \]
subject to the same conditions (one could use squares in the penalty term). 
The optimization problem is still convex. The trade-off parameter $C$ can be chosen
using cross-validation, like $K$-fold CV (or $m$-fol -- LOO). CV is connected to
SRM (see the homework).

Equivalent SVM problem:
\[ \frac{1}{2}\|\beta\|^2 + C \sum_{i=1}^m \bigl(1-y_i(\beta'x_i + \beta_0)\bigr)_+ \,. \]
One could use hinge loss $(1-yh(x))_+$ or the quadratic hinge loss $(1-yh(x))_+^2$.

The non-separable dual problem is
\[\max_\alpha \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i'x_j \,, \]
subject to $\sum_{i=1}^m \alpha_i y_i = 0$ and $0 \leq \alpha_i \leq C$. The solution
is given by:
\[ h(x) = \text{sgn}(\sum_{i=1}^m \alpha_i y_i x_i'x + \beta_0 \,,\]
where $\beta_0 = y_i - \sum_{j=1}^m \alpha_j y_j x_j'x_i$ for all $i=1,\ldots, m$
with $0 < \alpha_i < C$.

What about the margin guarantees of SVM? In bioinformatics the dimensionality of
the feature space if very large, so what about the complexity.

% subsection support_vector_machine (end)

\subsection{Margin theory} % (fold)
\label{sub:margin_theory}
Confidence margin versus the geometric margin. For any confidence margin $\rho>0$,
the $\rho$-margin function is defined by such an $\Phi_\rho$ that
\[ \Phi_\rho(x) \leq 1_{x < \rho} \,,\]
and $\Phi_\rho \in [0,1]$ non increasing and linear over $[0, \rho]$. For a sample
$S = (x_i)_{i=1}^m$ and a real-values hypothesis $h$ the empirical margin loss is
\[\hat{R}_\rho(h)
    = m^{-1} \sum_{i=1}^m \Phi_\rho(y_i h(x_i))
    \leq m^{-1} \sum_{i=1}^m 1_{y_i h(x_i) < \rho}
    \,. \]
The right-hand side is the share of examples in the training set that in which the
classifier has confidence less than $\rho$.

General margin bound: for any $H$ of real valued functions, for any $\rho>0$ and
$\delta\in(0,1)$
\[ R(h)
    \leq \hat{R}_\rho(h) + \frac{2}{\rho} \mathcal{R}_m(H)
    + \sqrt{\frac{\log\frac{1}{\delta}}{2m}}
    \,, \]
with probability at least $1-\delta$.

\textbf{proof}: Put $\tilde{H} = \{z = (x,y) \mapsto yh(x) \,:\, h\in H\}$, and
consider $\tilde{\mathcal{H}} = \{\Phi_\rho \circ f\,:\, f\in \tilde{H}\}$.
Then use the theorems for bounded functions in $[0,1]$.
By theorem 3 one has
\[ \ex g(z) \leq m^{-1} \sum_{i=1}^m g(z_i)
            + 2\mathcal{R}_m(\tilde{\mathcal{H}}) + \sqrt{\frac{\log\frac{1}{\delta}}{2m}}
    \,,\]
but since $\Phi_\rho$ is $\frac{1}{\rho}$-Lipschitz, then by Talagrand's lemma one
has
\begin{align*}
    \mathcal{R}_m(\Phi_\rho\circ \tilde{H})
    &\leq \frac{1}{\rho} \mathcal{R}_m(\tilde{H}) \\
    &= \frac{1}{\rho m} \ex_{S,\sigma} \sup_{h\in \tilde{H}} \sum_{i=1}^m \sigma_i y_i h(x_i) \\
    &=\bigl[\text{uniformity of } \sigma_i \bigr] \\
    &= \frac{1}{\rho} \mathcal{R}_m(H) \,.
\end{align*}
But $\Phi_\rho$ majorizes the $0-1$ loss: $1_{yh(x) < 0} \leq \Phi_\rho(yh(x))$.

The Rademacher Complexity of a linear hypothesis:
Let $S = (x_i)_{i=1}^m \subseteq \{x\,:\,\|x\|\leq R\}$ and put
\[ H = \{x\mapsto x'\beta\,:\,\|\beta\|\leq \Lambda\} \,. \]
Then
\[ \hat{\mathcal{R}}_S(H) \leq \frac{R \Lambda}{\sqrt{m}} \,. \]

Thus the margin bound for the linear classifiers is: if $X\subseteq \{x\,:\, \|x\|\leq R\}$
and $H = \{x\mapsto x'\beta \,:\, \|\beta\| \leq \Lambda\}$, then for any $\delta > 0$
\[ R(h) \leq \hat{R}_\rho(h) + 2\frac{R\Lambda}{\rho \sqrt{m}}
    + 3\sqrt{\frac{\log \frac{2}{\delta}}{2m}}
    \,, \]
with probability at least $1-\delta$. In the bad-distribution case the margin $\rho$
would have to be chosen too small.

This follows directly from the general margin bound and bound on $\hat{\mathcal{R}}_S(H)$
for linear classifiers.

\noindent The SVM precisely tries to minimize the right-hand side. To do better than
SVM is to minimize the ramp loss, but it is non-convex, so solving it is troublesome.

To prove this kind of result one needs to work with a different loss function.

% subsection margin_theory (end)

% section 20160920_lecture_1 (end)

\section{20160920 Lecture \#2} % (fold)
\label{sec:20160920_lecture_2}

The upper bound on the Rademacher complexity of linear hypotheses:
\[\hat{\mathcal{R}}_S(H) \leq \sqrt{\frac{R^2 \Lambda^2}{m}} \,, \]
for $H = \{x\mapsto\beta'x \,:\, \|\beta\|\leq \Lambda \}$ and
$S\subseteq \{x\,:\,\|x\|\leq R\}$. \textbf{Proof}:
\begin{align*}
    \hat{\mathcal{R}}_S(H)
        &= \frac{1}{m} \ex_\sigma \sup_{\|\beta\|\leq \Lambda} \sum_{i=1}^m \sigma_i \beta'x_i \\
        &= \frac{1}{m} \ex_\sigma \sup_{\|\beta\|\leq \Lambda} \beta' \sum_{i=1}^m \sigma_i x_i \\
        &\leq \frac{1}{m} \ex_\sigma \sup_{\|\beta\|\leq \Lambda} \|\beta\| \bigl\|\sum_{i=1}^m \sigma_i x_i \bigr\| \\
        &= \frac{\Lambda}{m} \ex_\sigma \bigl\|\sum_{i=1}^m \sigma_i x_i \bigr\| \\
        &\bigl[\text{using Jensen's inequality}\bigr]\\
        &\leq \frac{\Lambda}{m} \sqrt{\ex_\sigma \bigl\|\sum_{i=1}^m \sigma_i x_i \bigr\|^2} \\
        &\bigl[\text{pairwise independence of } \sigma_i\bigr]\\
        &\leq \frac{\Lambda}{m} \sqrt{\ex_\sigma \sum_{i=1}^m \sigma_i^2 x_i'x_i } \\
        &\leq \frac{\Lambda}{m} \sqrt{m R^2 } \,.
\end{align*}
Thus argument shows that it is possible to learn in high-dimensional feature spaces.

\subsection{Kernel methods} % (fold)
\label{sub:kernel_methods}

Kernel methods allow efficient manipulation of inner products in high dimension.
Also kernel methods result in non-linear decision boundary. Another reason is that
kernel methods expand the theory of classification to abstract input feature spaces.
Non-linear mapping $\phi:X\mapsto F$ -- from the input to a very high-dimensional
space.

Kernel method: take a kernel function $K:X\times X\mapsto \Real$ such that there
exists $\phi:X\mapsto F$ such that $K(x, y) = \langle \phi(x), \phi(y)\rangle_F$.
Thus $K$ is often seean as a similarity measure (incidence in a unordered graph,
closeness-proximity). Usually $K$ cam be efficiently computed, and the choice of
$K$ is up to the researcher, which entails flexibility.

$K$ is a kernel iff for all $m\geq1$, $(c_i)_{i=1}^m \in \Real$ and $(x_i)_{i=1}^m\in X$
one has $c'Kc \geq 0$, for $K = (K(x_i, x_j))_{i,j=1}^m$ and symmetric.
Kernels can be normalized, and there are many other arithmetic operations that
preserve the kernel property.

Gaussian kernels:
\[ k(x, y) = \text{exp}\biggl\{-\frac{\|x-y\|^2}{2\sigma^2}\biggr\} \,, \]
the sigmoid kernel:
\[ k(x, y) = \text{tanh}\bigl(a x'y + b \bigr) \,, \]
for $a, b\geq 0$, and it is useful for neural networks.

To get a kernel SVM just plug in the kernel instead of the inner products.

The generalization. Let $S\subseteq \{x\,:\, \|\phi(x)\|_\Hcal \leq R\}$ and
\[ H = \{x\mapsto \langle\beta, \phi(x)\rangle
    \,:\, \beta \in \Hcal\,, \|\beta\|_\Hcal \leq \Lambda\}
    \,. \]
Then
\[\hat{\mathcal{R}}_S(H)
    \leq \frac{\Lambda\sqrt{\tr K}}{m}
    \leq \frac{\Lambda\sqrt{m R^2}}{m}
    = \frac{\sqrt{R^2 \Lambda^2}{m}}
    \,. \]
\textbf{proof} is exactly the same as in the ordinary inner product case.

The Representer theorem states that the minimizer of
\[L\bigl((h(x_i))_{i=1}^m\bigr) + \Omega(\|h\|) \to \min_{h\in \Hcal}\,,\]
is an element of the pre-hilbert space $\Hcal_0$ guven by
\[ \Hcal_0 = \{\sum_{i=1}^m \alpha_i \phi(x_i)\,:\, (\alpha_i)_{i=1}^m \in \Real \} \,. \]

Linear regression can be extended to the kernel ridge regression, ranking algorithm
and et c.

It is interesting to define a kernel over string data. The idea is that strings
are similar if they share common sub-strings. This can be done using the so called
weighted transducers: weighted directed graphs with input-output configurations.
In such transducers one can define the function $T(x,y)$ as the sum of all possible
accepting paths with input $x$ and output $y$.
Now, $K$ is a rational kernel if there exists $T$ for some weighted transducer $T$.
For instance the composition can be defined as $T_1:\Sigma^*\times \Delta^* \mapsto \Real$
and $T_2:\Delta^*\times\Omega^* \mapsto \Real$ as
\[ (T_1\circ T_2)(x, z) = \sum_{y\in \Delta^*} T_1(x, y) T_2(y, z) \,, \]
much like multiplication of infinite matrices.

The inverse of a transducer $T:\Sigma^*\times \Delta^* \mapsto \Real$ is $T^{-1}$
obtained by swapping input with output in $T$: $T^{-1}:\Delta^* times \Sigma^*\mapsto \Real$
is $T^{-1}(x, y) = T(y, x)$.
For any transducer $T$ the function $K = T\circ T^{-1}$ is a positive definite symmetric
rational kernel.
\textbf{proof}: $K(x, y) = \sum_{z\in \Delta^*} T(x, z) T(z, y)$, then
$K(x, y) = \lim_{n\to \infty} K_n(x, y)$ where
$K_n(x, y) = \sum_{z\in\Delta^*, |z| \leq n} T(x, z) T(z, y)$.

Transducer learning is an interesting problem.

Kernels can be defined on many other discrete structures: automata, graphs, images,
parse trees et c.

Finally, negative kernels. What other kernels one can construct from a norm in a
Hilbert space? Negative definite kernels: a function $K:X\times X\mapsto\Real$
is saidto be negative defined symmetric iff it is symmetric and for all $m\geq 1$, 
$S=(x_i)_{i=1}^m\in X$ and $c\in\Real^{m\times 1}$ with $\one'c = 0$ one has
$c'Kc \leq 0$ for the gram matrix $K$ of $K$ on $S$.

If $K$ is NDS and $K(x, y) = 0$ iff $x=y$, then there exists a Hilbert space with
$K(x, y) = \|\phi(x) - \phi(y)\|_\Hcal$ for some $\phi:X\mapsto\Hcal$. Also $K$ is
NDS iff $e^{-tK}$ is a PDS for all $t>0$. Furthermore, for any $x_0$ the kernel $K'$
defined as 
\[K'(x, y) = K(x, x_0) + K(y, x_0) - K(x, y) - K(x_0, x_0) \,,\]
is NDS if and only if $K$ is PDS.

Deeper kernels: given a set of PDS kernels $(K_j)_{j=1}^p$ one can use a composite
kernel
\[K_\mu = \sum_{j=1}^p \mu_j K_j \,,\]
with $\sum_{j=1}^p \mu_j = 1$. One could create a window selection method by introducing
sparsity onto $\mu$ (or greedily, like in LARS).

% subsection kernel_methods (end)

% section 20160920_lecture_2 (end)

%


\end{document}
