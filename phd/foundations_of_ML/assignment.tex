\documentclass[a4paper]{article}
\usepackage{fullpage}

\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}
\newcommand{\tr}{\text{tr}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\nil}{\mathbf{0}}
\newcommand{\Rcal}{{\mathcal{R}}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Fcal}{\mathcal{F}}

\title{Foundations Of Machine Learning: assignment}
\author{Nazarov Ivan}
\begin{document}

\section{Assignment \#1} % (fold)
\label{sec:assignment_1}

\subsection{Problem A} % (fold)
\label{sub:problem_a}

\subsubsection{Task 1} % (fold)
\label{ssub:task_1}

Let $f:(0, +\infty)\mapsto\Real$ be a function with an inverse $f^{-1}$, and let
$X$ be an rv, and suppose $\pr(X > t)\leq f(t)$ for all $t>0$. Then for any $\delta>0$,
one has
\[ \pr\bigl(X>f^{-1}(\delta)\bigr) \leq \delta \,, \]
since $f$ is invertible. Therefore for al $\delta>0$
\[ 1-\delta \leq \pr\bigl(X \leq f^{-1}(\delta)\bigr) \,. \]

% subsubsection task_1 (end)

\subsubsection{Task 2} % (fold)
\label{ssub:task_2}

Let $X$ be a non-negative rv taking integer values. Consider it expectation:
\[ \ex X = \sum_{n\geq 0} n \pr(X=n) \,. \]
However $\{X=n\} = \{X\geq n\}\setminus \{X\geq n+1\}$, whence
\[ \ex X = \sum_{n\geq 0} n \bigl(\pr(X\geq n) - \pr(X\geq n+1)\bigr) \,. \]
Now since the terms in the summation are non-negative it is possible to reorder
so that
\begin{align*}
	\ex X
    &= \sum_{n\geq 0} n \bigl(\pr(X\geq n) - \pr(X\geq n+1)\bigr) \\
    &= \sum_{n\geq 0} n \pr(X\geq n)
      - \sum_{n\geq 0} n \pr(X\geq n+1) \\
    &= \sum_{n\geq 0} n \pr(X\geq n)
      - \sum_{n\geq 0} (n+1) \pr(X\geq n+1)
      + \sum_{n\geq 0} \pr(X\geq n+1) \\
    &= \pr(X\geq 0) + \sum_{n\geq 1} \pr(X\geq n) \,,
\end{align*}
whence the result follows.

% subsubsection task_2 (end)

% subsection problem_a (end)

\subsection{Problem B} % (fold)
\label{sub:problem_b}

Let $D$ be a distribution over $X$, and $F:X\mapsto\{-1,+1\}$ be the true deterministic
labelling function. WE a re interested in finding a good approximation of the label
bias of $(D, f)$, i.e. $P_+$ defined by
\[ p_+ = \pr_{x\sim D}\bigl(f(x) = +1)\bigr) \,. \]

Let $S=(x_i)_{i=1}^m$ be an iid sample drawn from $D$. Since the random variable
$f(X)$ for $X\sim D$ is binary, it can have only one kind on distribution -- Bernoulli.
Thus one can use an ML estimator: since $(f(x_i))_{i=1}^m$ is an iid sample as well,
the log-likelihood of labels of the sample $S$ is
\[ \mathcal{L} = \sum_{i=1}^m 1_{\{f(x_i)=+1\}} \log p_+ + 1_{\{f(x_i)=-1\}} \log(1-p_+) \,, \]
whence the MLE of $p_+$ is given by: $\hat{p}_+(S) = m^{-1} \sum_{i=1}^m 1_{\{f(x_i)=+1\}}$.
This is an unbiased estimator of $p_+$:
\[ \ex_{S\sim D^m} \hat{p}_+(S)
  = m^{-1} \sum_{i=1}^m \ex_{x_i\sim D} 1_{\{f(x_i)=+1\}} 
  = \ex_{x\sim D} 1_{f(x)=+1} = p_+ \,. \]
Now $1_{\{f(x_i)=+1\}}$ are iid $0-1$ random variables with expectation $p_+$, which
means that by the two-sided Hoeffding inequality for any $\epsilon>0$ one has:
\[ \pr_{S\sim D^m}\bigl(
    \bigl| \sum_{i=1}^m 1_{\{f(x_i)=+1\}} - m p_+ \bigr| > m\epsilon
  \bigr) \leq 2 \text{exp}\biggl\{\frac{-2m^2\epsilon^2}{m}\biggr\}
  \,, \]
whence $\pr_{S\sim D^m}( |\hat{p}_+(S) - p_+| > \epsilon ) \leq 2 e^{-2m\epsilon^2}$.
Therefore for any $\delta>0$ it is true that
\[ |\hat{p}_+(S) - p_+| \leq \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,, \]
with probability at least $1-\delta$ over $S\sim D^m$.
Furthermore, since the upper bound on the probability is exponential in $m$,
the first Borell-Cantelli lemma implies that for all $\epsilon>0$
\[ \pr_{S\sim D_\infty} \bigl(
  |\hat{p}_+(S_{:m}) - p_+| > \epsilon
    \text{ i.o. over } m \bigr) = 0 \,, \]
where $D_\infty$ is a distribution over infinite iid sequences in $X$. Therefore
one has $\hat{p}_+(S_{:m}) \to p_+$ almost surely.

% subsection problem_b (end)

\subsection{Problem C} % (fold)
\label{sub:problem_c}

\textbf{TODO}

% subsection problem_c (end)

\subsection{Problem D} % (fold)
\label{sub:problem_d}
Recall that the empirical loss $\hat{R}_S(h)$ of $h$ is given by
\[ \hat{R}_S(h) = \ex_{(x,y)\sim S} L(h(x), y) \,, \]
where $(x,y)\sim S$ means empirical distribution over the sample data $S=(z_i)_{i=1}^m$.
The true loss of $h$ is $R(h) = \ex_{(x,y)\sim D} L(h(x), y)$. Consider the $0-1$
loss $L(h(x), y) = 1_{\{h(x)\neq y\}}$.

Consider nested hypotheses sets $(H_k)_{k\geq1}$ with $H_k\subset H_{k+1}$ of hypotheses
taking values in $\{-1,+1\}$, and put $H=\cup_{k\geq1} H_k$. Furthermore for any
$h\in H$ let $k: H\mapsto\mathbb{N}$ be defined by $k(h) = \inf\{k\geq1\,:\, h\in H_k\}$
-- the index of the first hypothesis set $H_k$ that houses $h$. For any $k\geq1$
define the objective function $F_k$ by
\[ F_k(h) = \hat{R}_S(h) + \mathcal{R}_m(H_k) + \sqrt{\frac{\log k}{m}} \,. \]

\subsubsection*{step 1} % (fold)
\label{ssub:step_1}
% \url{http://bactra.org/weblog/926.html}

First, by definition of the Rademacher complexity the inclusion $H_k\subset H_{k+1}$
implies that
\[ \mathcal{R}_m(H_k) \leq \mathcal{R}_m(H_{k+1}) \,, \]
which in turn implies that for any $h\in H_k$ it is true that $k\geq k(h)$ one has
\[ \mathcal{R}_m(H_k) + \sqrt{\frac{\log k}{m}}
  \geq \mathcal{R}_m(H_{k(h)}) + \sqrt{\frac{\log {k(h)}}{m}} \,. \]
Therefore $F_k(h) \geq F_{k(h)}(h)$, and for all $k\geq1$ and $h\in H_k$ it is true
that
\[ R(h) - F_k(h) \leq R(h) - F_{k(h)}(h) \leq \sup_{h\in H} R(h) - F_{k(h)}(h) \,. \]
Conversely, for any $f\in H$
\[ \sup_{k\geq1} \sup_{h\in H_k} R(h) - F_k(h)
    \geq \sup_{h\in H_{k(f)}} R(h) - F_{k(f)}(h)
    \geq R(f) - F_{k(f)}(f)
  \,, \]
which ultimately implies that
\[ \sup_{k\geq1}\sup_{h\in H_k} R(h) - F_k(h)
    = \sup_{h\in H} R(h) - F_{k(h)}(h)
  \,. \]

% subsubsection* step_1 (end)

\subsubsection*{step 2} % (fold)
\label{ssub:step_2}

Now, the main theorem on the one-sided Rademacher complexity bound (slide~15) states,
that for a hypothesis class $H_k$ one has for all $\epsilon>0$
\[ \pr_{S\sim D^m}\Bigl( \{S\,:\,
    \sup_{h\in H_k} R(h) - \hat{R}_S(h) > \mathcal{R}_m(H_k) + \epsilon\}
  \Bigr) \leq \text{exp}\bigl\{-2m\epsilon^2\bigr\}
  \,. \]
If $\epsilon_k = \epsilon + \sqrt{\frac{\log k}{m}}$, then for all $k\geq 1$ the main
theorem implies that
\[ \pr_{S\sim D^m}\Bigl( \{S\,:\,
    \sup_{h\in H_k} R(h) - \hat{R}_S(h) - \mathcal{R}_m(H_k) - \sqrt{\frac{\log k}{m}} > \epsilon\}
  \Bigr) \leq \text{exp}\bigl\{-2m\epsilon_k^2\bigr\}
  \,. \]
However for any $k\geq 1$ it is true that $\epsilon_k^2 \geq \epsilon^2 + \frac{\log k}{m}$,
whence
\[ \text{exp}\bigl\{-2m\epsilon_k^2\bigr\}
  \leq k^{-2} \text{exp}\bigl\{-2m\epsilon^2\bigr\}
  \,,\]
By the Union Bound one has
\[ \pr_{S\sim D^m}\Bigl(
  \bigcup_{k\geq 1} \{S\,:\, \sup_{h\in H_k} R(h) - F_k(h) > \epsilon\}
  \Bigr) \leq e^{-2m\epsilon^2} \sum_{k\geq 1} k^{-2}
  = \frac{\pi^2}{6} e^{-2m\epsilon^2}
  \,. \]
Now, step 1 and the definition of the least upper bound imply that
\begin{align*}
  \bigcup_{k\geq 1} \{S\,:\, \sup_{h\in H_k} R(h) - F_k(h) > \epsilon\}
    &= \{S\,:\, \sup_{k\geq 1} \sup_{h\in H_k} R(h) - F_k(h) > \epsilon\}\\
    &= \{S\,:\, \sup_{h\in H} R(h) - F_{k(h)}(h) > \epsilon\} \,,
\end{align*}
whence for any $\epsilon>0$
\[ \pr_{S\sim D^m}\Bigl(
  \sup_{h\in H} R(h) - F_{k(h)}(h) > \epsilon
  \Bigr) \leq 2 e^{-2m\epsilon^2}
  \,. \]

% subsubsection* step_2 (end)

\subsubsection*{step 3} % (fold)
\label{ssub:step_3}

The SRM-optimal hypothesis $f^*$ is given by the minimizer of the objective:
\[ f^* = \argmin_{h\in H_k, k\geq1} F_k(h) \,, \]
and the best (lowest risk) hypothesis $h^*$ is given by
\[ h^* = \argmin_{h\in H} R(h) \,. \]
The hypothesis $f^*$ is observable and feasible, whereas the $h^*$ is unknown.

\noindent Fix some $\delta>0$. By the uniform concentration result from step 2 one
has
\[ \sup_{h\in H} R(h) - F_{k(h)}(h) \leq \sqrt{\frac{\log\frac{4}{\delta}}{2m}}
  \,,\]
with probability at least $1-\frac{\delta}{2}$ over $S\sim D^m$. However the SRM
$f^*$ hypothesis is such that $F_{k(f^*)}(f^*) \leq F_{k(h)}(h)$ for all $h\in H$,
since by step 1
\[\argmin_{h\in H_k, k\geq1} F_k(h) = \argmin_{h\in H} F_{k(h)}(h) \,. \]
Therefore with the same probability one has
\[ R(f^*) \leq \hat{R}_S(h^*) + \mathcal{R}_m(H_{k(h^*)})
        + \sqrt{\frac{\log k(h^*)}{m}} + \sqrt{\frac{\log\frac{4}{\delta}}{2m}}
  \,. \]

\noindent Now, the one-sided Rademacher complexity bound for $H_{k(h^*)}$ implies
that
\[ \hat{R}_S(h^*) \leq R(h^*) + \mathcal{R}_m(H_{k(h^*)})
  + \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,, \]
with probability at least $1-\frac{\delta}{2}$ over $S\sim D^m$.
Applying the union bound yields this result: for all $\delta>0$
\[ R(f^*) \leq R(h^*) + 2\mathcal{R}_m(H_{k(h^*)})
  + \sqrt{\frac{\log k(h^*)}{m}}
  + \sqrt{\frac{\log\frac{4}{\delta}}{2m}}
  + \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \,, \]
with probability at least $1-\delta$ over $S\sim D^m$.

% subsubsection* step_3 (end)

\subsubsection*{step 4} % (fold)
\label{ssub:step_4}

Since $x\mapsto\sqrt{x}$ and $x\mapsto \log x$ are concave one has
\begin{align*}
  \sqrt{\frac{\log\frac{4}{\delta}}{2m}} + \sqrt{\frac{\log\frac{2}{\delta}}{2m}}
  &= 2 \Biggl( \frac{1}{2}\sqrt{\frac{\log\frac{4}{\delta}}{2m}}
      + \frac{1}{2} \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \Biggr)
  \leq 2 \sqrt{\frac{\frac{1}{2} \log\frac{4}{\delta}+\frac{1}{2} \log\frac{2}{\delta}}{2m}} \\
  &\leq 2 \sqrt{\frac{\log\bigl(\frac{1}{2}\frac{4}{\delta} + \frac{1}{2} \frac{2}{\delta}\bigr)}{2m}}
  = 2 \sqrt{\frac{\log\frac{3}{\delta}}{2m}}
  = \sqrt{\frac{2\log\frac{3}{\delta}}{m}} \,.
\end{align*}
Therefore we end up with the following result: for any $\delta>0$
\[ R(f^*) \leq R(h^*) + 2\mathcal{R}_m(H_{k(h^*)})
          + \sqrt{\frac{\log k(h^*)}{m}}
          + \sqrt{\frac{2\log\frac{3}{\delta}}{m}}
  \,, \]
with probability at least $1-\delta$ over $S\sim D^m$.

% subsubsection* step_4 (end)

\subsubsection*{step 5} % (fold)
\label{ssub:step_5}

Suppose we are told the index $n$ of the hypothesis set the best hypothesis $h^*$
belongs to. Then SRM would degenerate into ERM, because information can be leveraged
to find a better $f^*$:
\[ f^* = \argmin_{h\in H_n} F_n(h) = \argmin_{h\in H_n} \hat{R}_S(h) \,,\]
since the remaining terms in $F_n(h)$ depend on $n$ only. As $f^*$ is known, one
can compute $n^* = k(f^*)$ and by applying the one-sided Rademacher complexity
bound for a hypothesis class $H_{n^*}$ get this: for any $\delta>0$
\[ R(f^*) \leq \hat{R}_S(f^*) + \mathcal{R}_m(H_{n^*})
          + \sqrt{\frac{\log\frac{2}{\delta}}{2m}}
  \,, \]
with probability at least $1-\frac{\delta}{2}$ over $S\sim D^m$. Another application
of the theorem for the class $H_n$ yields a generalization bound for $h^*$:
\[ \hat{R}_S(h^*) \leq R(h^*) + \mathcal{R}_m(H_n)
                  + \sqrt{\frac{\log\frac{2}{\delta}}{2m}}
  \,, \]
with probability at least $1-\frac{\delta}{2}$ over $S\sim D^m$. Since $k(f^*) \leq n$
and $\hat{R}_S(f^*) \leq \hat{R}_S(h^*)$, one can arrive at this conclusion:
\begin{align*}
  R(f^*)
    &\leq R(h^*) + \mathcal{R}_m(H_{n^*}) + \mathcal{R}_m(H_n)
     + 2 \sqrt{\frac{\log\frac{2}{\delta}}{2m}} \\
    &\leq R(h^*) + 2\mathcal{R}_m(H_n)
     + \sqrt{\frac{2\log\frac{2}{\delta}}{m}} \,,
\end{align*}
with probability at least $1-\delta$ over $S\sim D^m$. 

\noindent If $n \geq k(h^*)$, then $\mathcal{R}_m(H_{k(h^*)}) \leq \mathcal{R}_m(H_n)$
and the bounds are asymptotically equivalent. In the case of a finite sample knowing
$n$ and using ERM clearly offers a tighter guarantee than SRM:
\[ \sqrt{\frac{2\log\frac{2}{\delta}}{m}}
    \leq \sqrt{\frac{\log n}{m}}
      + \sqrt{\frac{2\log\frac{3}{\delta}}{m}}
  \,. \]

% subsubsection* step_5 (end)

% subsection problem_d (end)

% section assignment_1 (end)

\section{Assignment \#2} % (fold)
\label{sec:assignment_2}

\subsection{Problem A} % (fold)
\label{sub:problem_a}

\subsubsection{Task 1} % (fold)
\label{ssub:task_1}

Consider a hypothesis class, which consists of a single hypothesis $h_0:X\mapsto\{-1,+1\}$.
Its empirical Rademacher complexity over the sample $S \sim D$ iid is given by
\[ \hat{\mathcal{R}}_S(\Hcal)
    = m^{-1} \ex_\sigma \sup_{h\in \Hcal} \sum_{i=1}^m \sigma_i h(x_i)
    = m^{-1} \sum_{i=1}^m h_0(x_i) \ex_{\sigma_i, \sigma_{-i}} \sigma_i
    = 0
  \,, \]
where $(\sigma_i)_{i=1}^m$ are iid Rademacher variables. The Rademacher complexity
upper bound for this hypothesis class implies that for any $\delta > 0$:
\[ \bigl|\hat{R}_S(h_0) - R(h_0)\bigr|  
    \leq 0 + 2 \sqrt{\frac{\log\frac{2}{\delta}}{2m}}
    \,, \]
with probability at least $1-\delta$.

Consider an alternative definition of the empirical Rademacher complexity:
\[ \hat{\mathcal{R}}_S'(\Hcal)
    = m^{-1} \ex_\sigma \sup_{h\in \Hcal}
        \Bigl| \sum_{i=1}^m \sigma_i h(x_i) \Bigr|
    \,, \]
and its theoretical counterpart 
\[ \mathcal{R}_m'(\Hcal) = \ex_{S\sim D^m} \hat{\mathcal{R}}_S'(\Hcal)
  \,. \]
Fix some sample $S$, and let's find an upper bound for $\hat{\mathcal{R}}_S'(\Hcal)$:
\[ \hat{\mathcal{R}}_S'(\Hcal)
    = m^{-1} \ex_\sigma \Bigl| \sum_{i=1}^m \sigma_i h_0(x_i) \Bigr|
    \leq m^{-1} \ex_\sigma \sum_{i=1}^m | \sigma_i h_0(x_i) |
    \leq m^{-1} \sum_{i=1}^m |h_0(x_i)|
    \,, \]
by the triangle inequality and the fact that $\ex_\sigma |\sigma_i| = 1$.
This implies that the theoretical Rademacher complexity of this class is can be
upper-bounded by
\[ \mathcal{R}_m'(\Hcal)
  \leq \ex_{S\sim D^m} m^{-1} \sum_{i=1}^m |h_0(x_i)|
  = \ex_{x\sim D} |h_0(x)|
  \leq \sqrt{\ex_{x\sim D} h_0^2(x)}
  \,, \]
by the linearity of expectation and the Cauchy-Schwartz inequality.

Note that this derivation did not rely on the iid assumption of the Rademacher variables,
which suggests that using the independence might improve the bound. Inded, let's
use the Cauchy-Schwartz inequality from the start on the theoretical Rademacher
complexity:
\begin{align*}
  \mathcal{R}_m'(\Hcal)
    &= m^{-1} \ex_{S, \sigma} \Bigl| \sum_{i=1}^m \sigma_i h_0(x_i) \Bigr|
    \leq m^{-1} \sqrt{\ex_S \ex_\sigma \biggl( \sum_{i=1}^m \sigma_i h_0(x_i)\biggr)^2 } \\
    &= m^{-1} \sqrt{\ex_S \biggl(
         \ex_\sigma \sum_{i=1}^m \sigma_i^2 h_0^2(x_i)
        +  \ex_\sigma \sum_{i\neq j} \sigma_i\sigma_j h_0(x_i) h_0(x_j)
      \biggr) } \\
    &= m^{-1} \sqrt{\ex_S \sum_{i=1}^m h_0^2(x_i)}
    = \sqrt{\frac{\ex_{x\sim D} h_0^2(x)}{m}} \,,
\end{align*}
where we used $\ex_\sigma \sigma_i^2 = 1$, joint independence $(\sigma_i)_{i=1}^m$
and the fact that $S\sim D^m$.

% subsubsection task_1 (end)

\subsubsection{task 2} % (fold)
\label{ssub:task_2}

In this task we need to provide upper bounds for the Rademacher complexity of various
transformations and typical combinations of hypothesis classes. Let $H$ and $H'$
be some classes of binary hypotheses $X\mapsto\{-1, 1\}$, and $\alpha\in \Real$.

\paragraph{$\mathcal{R}_m(\alpha H)$:} % (fold)
\label{par:r_m_alpha_h}

To get the result for $\mathcal{R}_m(\alpha H)$, first, note that
\begin{equation}
  \sup_{h\in H} \sum_{i=1}^m \alpha \sigma_i h(x_i)
    =\begin{cases}
      |\alpha| \sup_{h\in H} \sum_{i=1}^m \sigma_i h(x_i), &\text{ if }\alpha \geq 0 \,,\\
      |\alpha| \sup_{h\in H} \sum_{i=1}^m (-\sigma_i) h(x_i), &\text{ otherwise}\,.
    \end{cases}
\end{equation}
whence we have
\begin{align*}
  \mathcal{R}_m(\alpha H)
    &= \ex_{S\sim D^m} \hat{\mathcal{R}}_S(\alpha H)
    = \ex_{S, \sigma} \sup_{h\in H} m^{-1} \sum_{i=1}^m \alpha \sigma_i h(x_i) \\
    &= |\alpha| \ex_S \bigl(
         1_{\alpha\geq 0} \ex_\sigma \sup_{h\in H} m^{-1} \sum_{i=1}^m \sigma_i h(x_i) 
       + 1_{\alpha < 0} \ex_\sigma \sup_{h\in H} m^{-1} \sum_{i=1}^m (-\sigma_i) h(x_i)
       \bigr)\\
    &= |\alpha| \ex_S \bigl(
         1_{\alpha\geq 0} \hat{\mathcal{R}}_S(H)
       + 1_{\alpha < 0} \hat{\mathcal{R}}_S(H)
       \bigr)
    = |\alpha| \mathcal{R}_m(H) \,,
\end{align*}
since $(\sigma_i)_{i=1}^m \overset{\mathcal{D}}{\sim} (-\sigma_i)_{i=1}^m$.

% paragraph r_m_alpha_h (end)

\paragraph{$\mathcal{R}_m(H+H')$:} % (fold)
\label{par:r_m_h_hh}

By definition of $\mathcal{R}_m(H+H')$:
\begin{align*}
  \ex_{S, \sigma} \sup_{h\in H+H'} m^{-1} \sum_{i=1}^m \sigma_i h(x_i)
    &= \ex_{S, \sigma}
      \sup_{h\in H, h'\in H'} m^{-1} \sum_{i=1}^m \sigma_i h(x_i)
                            + m^{-1} \sum_{i=1}^m \sigma_i h'(x_i) \\
    &\leq \ex_{S, \sigma} \Bigl(
            \sup_{h\in H, h'\in H'} m^{-1} \sum_{i=1}^m \sigma_i h(x_i)
          + \sup_{h\in H, h'\in H'} m^{-1} \sum_{i=1}^m \sigma_i h'(x_i)
        \Bigr)\\
    &= \ex_{S, \sigma} \sup_{h\in H} m^{-1} \sum_{i=1}^m \sigma_i h(x_i)
       + \ex_{S, \sigma} \sup_{h'\in H'} m^{-1} \sum_{i=1}^m \sigma_i h'(x_i) \,,
\end{align*}
hence $\mathcal{R}_m(H+H') \leq \mathcal{R}_m(H) + \mathcal{R}_m(H')$.

% paragraph r_m_h_hh (end)

\paragraph{$\mathcal{R}_m(\max\{H, H'\})$:} % (fold)
\label{par:r_m_max_h_hh}

This case is a little bit more involved than the previous ones, and in order to
prove it we shall borrow tricks from the proof of Talagrand's inequality, i.e. the
contraction lemma, which is concerned with compositions of hypotheses with Lipschitz
functions. The identity $\max\{a, b\} = \frac{1}{2}(a+b+|a-b|)$ and sub-additivity
of the supremum imply:
\begin{align*}
  \mathcal{R}_m(\max\{H,H'\})
    &= \ex_{S, \sigma} m^{-1} \frac{1}{2} \sup_{h, h'\in H\times H'}
            \sum_{i=1}^m \sigma_i \Bigl((h(x_i) + h'(x_i)) + |h(x_i) - h'(x_i)|\Bigr)\\
    &\leq \frac{1}{2} \ex_{S, \sigma}
            m^{-1} \sup_{h, h'\in H\times H'} \sum_{i=1}^m \sigma_i (h(x_i) + h'(x_i))
          + \frac{1}{2} \ex_{S, \sigma}
            m^{-1} \sup_{h, h'\in H\times H'} \sum_{i=1}^m \sigma_i |h(x_i) - h'(x_i)| \\
    &\leq \frac{1}{2} \mathcal{R}_m(H+H')
          + \frac{1}{2} \ex_S \ex_\sigma
            m^{-1} \sup_{h, h'\in H\times H'} \sum_{i=1}^m \sigma_i |h(x_i) - h'(x_i)| \,,
\end{align*}
where $S \sim \mathcal{D}^m$ and $\sigma\in \text{Ber}(\{-1, 1\})^m$. Since the
Rademacher variables $\sigma$ are iid Bernoulli the inner expectation of the last
term can be decomposed into
\begin{align*}
  \ex_\sigma m^{-1} \sup_{h, h'\in H\times H'}
      \sum_{i=1}^m \sigma_i |h(x_i) - h'(x_i)|
    = \ex_{\sigma_{-i}} m^{-1} \frac{1}{2} &\Bigl(
      \sup_{h_1, h_1'\in H\times H'}
        (U_{-i}(h_1,h_1') + |h_1(x_i) - h_1'(x_i)|) \\
      &+ \sup_{h_2, h_2'\in H\times H'}
        (U_{-i}(h_2,h_2') - |h_2(x_i) - h_2'(x_i)|) \Bigr) \,,
\end{align*}
where $U_{-i}(h,h') = \sum_{j\neq i} \sigma_j |h(x_j) - h'(x_j)|$. Since the sum
of the suprema is always equal to the supremum of the sum over the product we have
that the right hand side equals
\begin{equation*}
  \frac{1}{2} \ex_{\sigma_{-i}} m^{-1}
      \sup_{h_1, h_2\in H, h_1', h_2'\in H'}
        U_{-i}(h_1,h_1') + U_{-i}(h_2,h_2')
        + |h_1(x_i) - h_1'(x_i)| - |h_2(x_i) - h_2'(x_i)| \,.
\end{equation*}
Therefore, from $|a|-|b|\leq |a-b|$ and the triangle inequality we can upper bound
the RHS by
\begin{equation*}
  \frac{1}{2} \ex_{\sigma_{-i}} m^{-1}
      \sup_{h_1, h_2\in H, h_1', h_2'\in H'}
        U_{-i}(h_1,h_1') + U_{-i}(h_2,h_2')
        + |h_1(x_i) - h_2(x_i)| + |h_1'(x_i) - h_2'(x_i)| \,,
\end{equation*}
which, due to symmetry of the expression under the supremum with respect to pairs
$h_1 \leftrightarrow h_2$ and $h_1' \leftrightarrow h_2'$, becomes
\begin{multline*}
  \frac{1}{2} \ex_{\sigma_{-i}} m^{-1}
      \sup_{h_1, h_2\in H, h_1', h_2'\in H'}
        U_{-i}(h_1,h_1') + U_{-i}(h_2,h_2')
        + (h_1(x_i) - h_2(x_i)) + (h_1'(x_i) - h_2'(x_i)) \\
  = \frac{1}{2} \ex_{\sigma_{-i}} m^{-1}
      \sup_{h_1, h_2\in H, h_1', h_2'\in H'}
        U_{-i}(h_1,h_1') + U_{-i}(h_2,h_2')
        + (h_1(x_i) + h_1'(x_i)) - (h_2(x_i) + h_2(x_i)) \,.
\end{multline*}
Now, this supremum can be decomposed in two suprema with relabelled variables:
\begin{multline*}
  \ex_{\sigma_{-i}} \frac{1}{2} m^{-1} \Bigl(
      \sup_{h, h'\in H\times H'} \bigl(U_{-i}(h,h') + (h(x_i) + h'(x_i))\bigr)
      + \sup_{h, h'\in H\times H'} \bigl(U_{-i}(h,h') - (h(x_i) + h'(x_i))\bigr)
    \Bigr) \\
    = \ex_{\sigma_{-i}} \ex_{\sigma_i} m^{-1}
      \sup_{h, h'\in H\times H'} U_{-i}(h,h') + \sigma_i (h(x_i) + h'(x_i)) \,,
\end{multline*}
where the last transformation uses the definition of the expectation with respect
to $\sigma_i$. Applying similar reasoning repeatedly to the remaining terms in $U_{-i}$
we finally arrive at the following inequality:
\begin{equation*}
  \ex_\sigma m^{-1} \sup_{h, h'\in H\times H'} \sum_{i=1}^m \sigma_i |h(x_i) - h'(x_i)|
    \leq \ex_\sigma m^{-1} \sup_{h, h'\in H\times H'} \sum_{i=1}^m \sigma_i (h(x_i) + h'(x_i))
    = \hat{\mathcal{R}}_S(H+H')
    \,,
\end{equation*}
and therefore $\mathcal{R}_m(\max\{H,H'\}) \leq \mathcal{R}_m(H+H')$. The upper bound
for the sum of hypothesis classes yields the required result.

% paragraph r_m_max_h_hh (end)

% subsubsection task_2 (end)

% subsection problem_a (end)

\subsection{Problem C} % (fold)
\label{sub:problem_c}

\subsubsection{task 5} % (fold)
\label{ssub:task_5}

Consider the following modified SVM problem:
\begin{equation}
  \frac{1}{2}\|\beta\|^2 + C\max_{i=1}^m \xi_i \to \min_{\beta_0, \beta, \xi_i}
\end{equation}
subject to
\begin{equation*}
  y_i(\beta_0 + \beta_0'x_i) \geq 1 - \xi_i\, \forall i=1,\ldots, m\,,
\end{equation*}
and $\xi_i \geq 0$.

The $\max$-norm $\|\xi\|_\infty$ is a convex function of $(\xi_i)_{i=1}^m$, which
means that the objective function is convex. Furthermore the constraints are linear
in $\beta_0$, $\beta$, and $\xi_i$. Suppose $(\beta^*_0, \beta^*, \xi^*)$ is a solution
to this problem with $\xi^*_i < \xi^*_j$ for some $i, j\in\{1,\ldots, m\}$. Then
$0\leq \xi^*_i < \xi^*_j \leq \|\xi^*\|_\infty$ for any $k\neq i$. Therefore, the
value of the objective does not change if $\xi^*_i$ is adjusted by $\delta_i = \xi^*_j - \xi^*_i > 0$,
since
\begin{equation*}
  \max_{k=1}^m \xi^*_k
    = \max\{\xi^*_i, \xi^*_j, \|\xi^*_{-i,j}\|_\infty\}
    = \max\{\xi^*_i + \delta_i, \xi^*_j + 0, \|\xi^*_{-i,j} + 0\|_\infty\}
    = \|\xi^* + \delta\|_\infty \,,
\end{equation*}
where $\delta = e_i \delta_i$ and $e_i$ is the $i$-th coordinate unit vector in
$\Real^{m\times 1}$. Furthermore, $\hat{\xi} = \xi^* + \delta$ is feasible:
$\hat{\xi}_k = \xi^*_k + \delta_k \geq \xi^*_k$, and
\begin{equation*}
  y_k(\beta^*_0 + {\beta^*}'x_i)
    \geq 1 - \xi^*_i > 1 - \xi^*_j = 1 - \hat{\xi}_i \,.
\end{equation*}
Therefore any solution with a discrepancy in the slack variable values has a can
brings about a feasible point with the same parameters $\beta_0$, and $\beta$, yet
$\xi^*$ having the same components, that yields the same value of the objective.
Thus, since margin minimization is the goal, it suffices to consider solutions with
$(\xi_i)_{i=1}^m = \one \xi$, for $\xi \geq 0$.

Therefore, the following minimization produces a subset of solutions to the original
problem, which nonetheless result in the same value of the functional:
\begin{align*}
  \frac{1}{2}\|\beta\|^2 + C\xi
      &\to \min_{\beta_0, \beta, \xi} \\
  \text{subject to :}\,
      &\,y_i(\beta_0 + \beta_0'x_i) \geq 1 - \xi\,, \forall i=1,\ldots, m \\
      &\,\xi \geq 0 \,.
\end{align*}
The Lagrangian of the simplified problem is
\begin{equation*}
  \mathcal{L}
    = \frac{1}{2} \|\beta\|^2 + C \xi
    + \sum_{i=1}^m \alpha_i (1 - \xi - y_i (\beta_0 + \beta'x_i))
    + \mu ( -\xi) \,.
\end{equation*}
The simplified problem is convex, because the objective is convex and the constraints
are linear in $\beta_0$, $\beta$, and $\xi$. By Slater's constraints qualifications
the strong duality holds, whence the KKT conditions are necessary:
\begin{align*}
  \text{\textbf{Primal}}\,:\quad
        & (\beta_0 + \beta_0'x_i) \geq 1 - \xi\,, \xi\geq 0\,; \\
        %
  \text{\textbf{Dual}}\,:\quad
        & \alpha_i\geq 0\,, \mu\geq 0\,; \\
        %
  \text{\textbf{Complementary Slackness}}\,:\quad
        & \alpha_i(1 - \xi - y_i(\beta_0 + \beta'x_i)) = 0\,, \mu \xi = 0\,; \\
        %
  \text{\textbf{First order}}\,:\quad
        &\frac{\partial \mathcal{L}}{\partial \beta_0}\,
          :\, \sum_{i=1}^m \alpha_i y_i = 0\,; \\
        &\frac{\partial \mathcal{L}}{\partial \beta}\,
          :\, \beta - \sum_{i=1}^m x_i y_i \alpha_i = 0\,; \\
        &\frac{\partial \mathcal{L}}{\partial \xi}\,
          :\, C  - \sum_{i=1}^m \alpha_i - \mu = 0\,.
\end{align*}
With these conditions the Lagrangian can be simplified to
\begin{align*}
  \mathcal{L}
    &= \frac{1}{2} \|\beta\|^2 + C \xi
    + \sum_{i=1}^m \alpha_i
    - \xi \sum_{i=1}^m \alpha_i
    - \beta_0 \sum_{i=1}^m \alpha_i y_i
    - \beta'\sum_{i=1}^m x_i \alpha_i y_i
    - \xi \mu \\
    &= \frac{1}{2} \|\beta\|^2
    + \bigl(C - \sum_{i=1}^m \alpha_i - \mu\bigr) \xi
    + \sum_{i=1}^m \alpha_i
    - \beta_0 \sum_{i=1}^m \alpha_i y_i
    - \beta' \sum_{i=1}^m x_i \alpha_i y_i \\
    &= \sum_{i=1}^m \alpha_i - \frac{1}{2}
        \sum_{i,j=1}^m \alpha_j y_j (x_j'x_i) y_i \alpha_i \,.
\end{align*}
Since $\mu \geq 0$ and it does not affect the simplified objective, the dual problem
becomes
\begin{align*}
  \sum_{i=1}^m \alpha_i - \frac{1}{2}
        \sum_{i,j=1}^m \alpha_j y_j (x_j'x_i) y_i \alpha_i
    &\to \max_{\alpha_i} \\
  \text{subject to :}\,
      &\,\sum_{i=1}^m \alpha_i y_i = 0
      \,,\alpha_i \geq 0
      \,,\sum_{i=1}^m \alpha_i \leq C \,.
\end{align*}
It worth noting that this problem differs from the dual of the $L_1$-SVM problem
in the constraints only:
\begin{align*}
  \sum_{i=1}^m \alpha_i - \frac{1}{2}
        \sum_{i,j=1}^m \alpha_j y_j (x_j'x_i) y_i \alpha_i
    &\to \max_{\alpha_i} \\
  \text{subject to :}\,
      &\,\sum_{i=1}^m \alpha_i y_i = 0
       \,,0 \leq \alpha_i \leq C \,.
\end{align*}

% subsubsection task_5 (end)

% subsection problem_c (end)

% section assignment_2 (end)
\end{document}
