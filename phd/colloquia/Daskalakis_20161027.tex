\documentclass[a4paper]{article}
\usepackage{fullpage}

\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}
\newcommand{\tr}{\text{tr}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\nil}{\mathbf{0}}
\newcommand{\Rcal}{{\mathcal{R}}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Fcal}{\mathcal{F}}

\title{Mechanism Design for Learning Agents}
\author{Nazarov Ivan}
\begin{document}

Talk about Computer Science and Economics.

An algorithmist considers an algorithm as an algorithmic procedure that takes an
input and computes a function on it. However, usually the algorithm is influenced
by the inputs, and how trustworthy it is.

The intersection of CS and Econ is Online Markets and Advertising, Public Goods
Allocation, Sharing Economy, Auctions, Game bots, Crypto-currencies, et c.
The input in all of these application comes from agents, which may be malevolent.

The goal thus becomes, to align the incentives of the input providers with the desired
quality of input. Or make the algorithms robust to input manipulation.

Suppose we want to compute the maximum of some numbers.
The input $(x_i)_{i=1}^n$ and the goal is to compute $\max_{i=1}^n x_i$, with payoffs
to each agent being $p_i = x_i 1_{x_i = \max_{j=1}^n x_j}}$. In this algorithm, the
agents are inclined to misreport their $x_i$ and instead provide $x_i=+\infty$. Thus
in this simple algorithm we learn nothing about the reality.

A better algorithm is The Vickery auction (or the $2$-nd price auction):
collect the reported bids $b_i$, select $i^* \in \argmax_{i=1}^n b_i$ and charge
the $i^*$-th agent (winner) the second largest bid: $b^* = \max_{j\neq i^*} b_j$.
The main result is that the optimal rational strategy is not to misreport the value
and set $b_i = x_i$.

Another way to compute the maximum is to use an algorithm based on Solon's law.
Suppose a big public expenditure is due, and only a subset of agents was eligible
to pay. If someone didn't want to pay for the assigned public expense, could report
another agent to be richer (or more able) than him. If the second in disagrees,
then the first can propose the exchange of property (riches) before paying (antidosis).
The outcome is the maximum (the richest person).

\textbf{Nisan-Ronen'99}: How much more difficult the optimization is on the ``stretgic''
input, compared to the ``honest'' input?

Information completeness and asymmteriy:\begin{itemize}
	\item what information does the algorithm have on its inputs?
	\item what information about the algorithm do the inputs providers have?
	\item is there access to the private information?
\end{itemize}

Computational complexity: \begin{itemize}
	\item computational, communicational complexity;
	\item centralized, distributed;
\end{itemize}

\section{Combinatorial Auctions} % (fold)
\label{sec:combinatorial_auctions}

Suppse there are $[m]$ indivisible items, and $[n]$ bidders each with its private
valuation function $v_i:\mathcal{P}([m]) \mapsto \Real_+$. The goal is to partition
the items $\uplus_{i=1}^n S_i = [m]$ in such a manner as to maximize the welfare of
from allocating set $S_i$ to the $i$-th bidder and charging him $p_i$, namely to maximize
$\sum_{i=1}^m v_i(S_i)$. The Vickery's auction solves this in the case of $m=1$.

Thus comes in the Vickery-Clarke-Groves mechanism: similar to Vickery but with a twist.
\textbf{VCG'73}
\begin{enumerate}
	\item ask the bidders to submit their valuation functions $(\hat{v}_i)_{i=1}^n$
	-- the reported valuation functions;
	\item Choose the  partition $(S_i)_{i=1}^n$ maximizing $\sum_{i=1}^n \hat{v}_i(S_i)$;
	\item Charge the ``Clarke payments'':
	\begin{equation}
		p_i\bigl((S_j)_{j=1}^n\bigr)
			= \argmax_{\uplus_{j\neq i} P_j = [m]}
				\sum_{i\neq j} \hat{v}_j(P_j) - \sum_{i\neq j} \hat{v}_j(S_j) \,;
	\end{equation}
\end{enumerate}
The Clarke prices ensure that the truthful reporting strategy dominates the misreporting,
hence it chooses the real optimal allocation.

Reality check:\begin{itemize}
	\item It asks for the valuation function -- this function has exponential complexity:
	\begin{itemize}
		\item there is a communication issue (specify $2^m$ numbers);
		\item Solution: consider succinct functions, of use lazy evaluation (query the
		valuation function);
	\end{itemize}
	\item What if the $2$-nd step, the Clarke prices, cannot be efficiently computed?
	\begin{itemize}
		\item what if only approximations are accessible? Do approximations destroy
		the truthfulness property of VCG?
		\item Are there truthful approximately optimal mechanisms?
	\end{itemize}
\end{itemize}

Consider sub-modular valuation function, which are analogues of convex function over
$\Real$. A set function $f$ is sub-modular iff for all $S\subseteq T$ and every $j\notin T$
\begin{equation}
	f(S\cup\{j\}) - f(S) \geq f(T\cup\{j\}) - f(T)\,.
\end{equation}
\textbf{Vondrak'08} A combination auction wit sub-modular bidders. With \emph{valuation
query access} to \textbf{true} bidder valuation can achieve 1-e^{-1} fraction of the
optimal welfare.

\textbf{Vondrak'11} If a mechanism has a query access to bidders reported valuations and
guarantees $m^{-10^2}$ fraction of optimal welfare then it must make exponentially many
queries. Even if each bidder's valuation can be succinctly described (polynomial complexity
in $m$) then no polynomial mechanism can get better than $n^{-10^2}$-fraction (\wat) of the
optimal welfare.

% section combinatorial_auctions (end)

\section{Overcoming the truthfulness barriers} % (fold)
\label{sec:overcoming_the_truthfulness_barriers}

Combine any subset of:\begin{itemize}
	\item more powerful queries, e.g. demand queries: give the prices $p_i$
	what is $\argmax_{S\subseteq [m]} v(S) - \sum_{i\in S} p_i$;
	\item Bayesian assumptions: assume some distribution over the valuations;
\end{itemize}

\noindent ...

\textbf{Someone'16} Polynomial-time $O(\sqrt{\log m})$-approximately correct
algorithm.

Additive, unit-demand $\subseteq$ sub-modular
$\subseteq$ demand-queries
$\subseteq$ sub-additive;

Meanwhile, in the real world the auctions are being conducted in various environments,
though the theory say that they do not quite work. Moreover most environments may encourage
non-truthful bidding.

What about $m$ parallel Vickery auctions over a single-item? Can this be used to
probe for truthful values? No, though it is simple, it is non-truthful. Also it may
be challenging for non-additive bidders to bid, since they need to anticipate how
other bidders behave.

Classic microeconomic theory: Nash/Bayesian Nash equilibrium. But uses really strong
modelling assumptions, and is computationally hard. Also stationarity is not observed
in the real datasets.

% section overcoming_the_truthfulness_barriers (end)

\section{No-regret learning} % (fold)
\label{sec:no_regret_learning}
\noindent A generic point of view on learning.

Fix some mechanism $M$, and suppose it is replayed $T$ times with the same set of
bidders. An algorithm choosing a bid $b_i^t$ for $t$ is ``no-regret'' iff for any
(adaptively chosen even adversarial) sequence of other bids $(b_{-i}^t)_{t=1}^T$:
\begin{equation}
	\frac{1}{T} \sum_{i=1}^T \ex u_i(b_i^t, b_{-i}^t)
		\geq \max_{b^*} \frac{1}{T} \sum_{i=1}^T \ex u_i(b^*, b_{-i}^t) - o(1)
		\,,
\end{equation}
i.e the expected average utility under arbitrary behaviour of other bidders is larger
than the best expected utility from a fixed bid. Many online learning algorithms satisfy
this property (FTRL, FTPL, online mirror descent, Boosting, et c.).

% The important question: are equilibria or no-regret learning feasible in any non-truthful
% combinatorial auction? In Bayesian setting the answer is ``no''.

% section no_regret_learning (end)

\section{No-envy vs. No-regret learning} % (fold)
\label{sec:no_envy_vs_no_regret_learning}

Fix a mechanism $M$: \begin{itemize}
	\item suppose $n$ bidders engage in a repeated execution of $M$;
	\item $b_i^t$ is the $i$-th bidder's action during round $t$: \begin{itemize}
		\item in SiSPAs this is a vector of bids on each item;
		\item in more complexi mechanisms, it might be more complex;
	\end{itemize}
	\item An algorithm that chooses a bid $b_i^t$ for all $t$ is ``no-envy'' iff
	for any (adaptively chosen) $(b_{-i}^t)_{t=1}^T$
	\begin{equation}
		\frac{1}{T} \sum_{i=1}^T \ex u_i(b_i^t, b_{-i}^t)
			\geq \max_{S^*} \bigl(v_i(S^*) - \frac{1}{T} \sum_{i=1}^T \ex p_{S^*,i}(b_{-i}^t) \bigr)
			- o(1) \,,
	\end{equation}
	i.e. in a price taking environment.
	\item Enters the Walrasian equilibrium.
\end{itemize}

Nash equilibrium $\subseteq$
Correlated equilibrium $\subseteq$
Coarse correlated equilibrium $=$
No-regret Learning Outcomes $\subseteq$
No-envy learning outcomes.

Unfortunately these results are not robust to bidder collusions.

% section no_envy_vs_no_regret_learning (end)

\end{document}
