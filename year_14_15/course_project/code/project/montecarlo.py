# -*- coding: UTF-8 -*-
from numpy.random import RandomState

def mc_run( generator, kernel, replications, **op ) :
	import time as tm
	import numpy as np
	import multiprocessing as mp
## Analyse the data generated by "generator" with "kernel":
##   kernel -- the kernel invoked on each generated replication;
##   generator -- the replication generator.
	if type( replications ) == int :
		replications = range( replications )
	if op.get( 'parallel', False ) :
		num_workers = op.get( 'processes', mp.cpu_count( ) )
## Setup the pool of workers. Initialize worker ID dispenser.
		worker_id = mp.Value( 'i', 0, lock = True )
## Initialize the entropy array: Generate 32bit values uniformly at random.
		if op.get( 'debug', False ) :
			entropy = np.ones( num_workers, dtype = np.int )
		else :
## The main problem is that it is possible that each worker reads its seed from
##  the same source and at the same time, which would make each worker process
##  gnerate identickal samples. In nix and like systems the os.fork() actually makes
##  a clone of the calling process, which includes duplicating the internal state
##  of the random number generator. Therefore each worker must reinitilize the
##  cricual components wit hthe entropy "hints" provided by the parent process.
##  http://stackoverflow.com/questions/2396209/best-seed-for-parallel-process
			entropy = np.random.randint( 0x7FFFFFFF, size = num_workers )
## Alternative way for UNIX systems is to read from /dev/random.
##	with open( "/dev/random", "rb" ) as dev :
##		seeds = [ struct.unpack( 'I', dev.read( 4 ) )[ 0 ] for c in cli ]
## Each worker must make a personal copy of the generator's context and use it
##  for the simulation purposes.
		worker_pool = mp.Pool( processes = num_workers,
			initializer = wrk_startup, initargs = (
				worker_id, entropy, generator, kernel, op ) )
## The pool is ready: get the replications and the chunksize
		chunksize = int( np.sqrt( ( len( replications ) + num_workers - 1 ) // num_workers ) )
## Launch the pool
		if op.get( 'quiet', True ) :
## Run the experiment synchronously
			result = worker_pool.map( wrk_kernel, replications, chunksize = chunksize )
		else :
## Run the experiment asynchronously
			result = worker_pool.map_async( wrk_kernel, replications, chunksize = chunksize )
## Wait until the jobs are complete
			tick = tm.time( )
			while not result.ready( ) :
				tm.sleep( 1 )
## Track the progress
				print( "%.3f\r" % ( tm.time( ) - tick ) )
			result = result.get( )
		worker_pool.close( )
		worker_pool.join( )
		return result
	else :
## For a serieal run, just do the same initialization steps
		generator.initialize( )
		rnd = RandomState( )
		generator.set_rnd( rnd )
		tick = tm.time( )
		result = [ ( 0, i, kernel( i, generator, **op ), ) for i in replications ]
		if not op.get( 'quiet', True ) :
			print( "%.3f\r" % ( tm.time( ) - tick ) )
		return result

local_id = None ; rnd = None ; generator = None ; kernel = None
arguments = dict( { } )
def wrk_startup( WRK, seed, gen, ker, op = dict() ) :
	import copy as cp
	global local_id, generator, rnd, kernel, arguments
## Atomically increment the worker wrk_id
	with WRK.get_lock( ) :
		local_id = WRK.value
		WRK.value += 1
## Initialize own context: this works since the processes live in separate address spaces.
	generator, kernel, arguments = cp.deepcopy( gen ), cp.deepcopy( ker ), cp.deepcopy( op )
	generator.initialize( )
	rnd = RandomState( seed[ local_id ] )
	generator.set_rnd( rnd )

def wrk_kernel( i ) :
	global local_id, kernel, generator, arguments
	return ( local_id, i, kernel( i, generator, **arguments ) )
