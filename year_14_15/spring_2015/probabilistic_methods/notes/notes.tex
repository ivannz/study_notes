\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Rbar}{{\bar{\mathbb{R}}}}
\newcommand{\Cplx}{\mathbb{C}}

\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ical}{\mathfrak{I}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\borel}{\mathcal{B}}
\newcommand{\one}{\mathbf{1}}

\newcommand{\ex}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\pr}{\mathbb{P}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}
\newcommand{\sign}{\mathop{\text{sgn}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Probabilistic methods in modelling}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

Theory of Stochastic processes
Brownian motion
Martingales
Poisson point processes

Generalizationo of the central limit theorem:
-- normal distribution
-- stable processes
-- Fisher-Tipett-Gendenko thorem for maxima

Based on the law of large numbers:
-- Extreme value theory

Additional chapters of probability

Elementary probability: Kolmogorov axioms, Measure theoretic approach.
Bernoulli law of large numbers.
Stochastic processes require measure theoretic foundations for their definition on some abstract space.

%% \selectlanguage{russian}
\section{Lecutre \#1} % (fold)
\label{sec:lecutre_1}


Suppose $\brac{X_k}_{k=1}^n$ is a finite collection of random variables which are independent and identically distributed.

What are the asymptotics of functions of $\brac{X_k}_{k=1}^n$ and their probability distributions.

Consider a measure space $(\Omega, \Fcal, P)$ and an RV $X(\omega)$ from $\Omega$ to $(\Xcal, \Sigma)$ on it.
This induces a measure in $\Xcal$ -- the image space. see \textbf{image~1}

\[\mathbb{P}_X \defn X_\# \mathbb{P} = \mathbb{P}\brac{X\in A} = \int 1_{X^{-1}(A)}d\mathbb{P}\]

For limiting theorems the image space is sufficient, but we need convergence in distributions. However, abstract spaces would be needed in studies of stochastic processes (and endowed with filtration).

Consider $X\in \Real$.
How is the distribution of $X$ defined?

It can be defined using the semi-ring of half-closed intervals $\ploc{a,b}$, but the most popular, and equivalent, method is via a function with specific properties -- \textbf{C}umulative \textbf{D}istribution \textbf{F}unction.

Since every random variable induces a distribution on its image space, and it is always possible to construct a probability space and a measurable function such that they induce the required law in the image space (Skorohod's representation, see [Williams, 1991, p.~34]).
Thus random variables can be partitioned into classes identified by their distribution function.
In the following random variables and their corresponding distributions can be used interchangeably.

A map $F:\Real\to \clo{0,+\infty}$ is a distribution function if \begin{enumerate}
	\item $F$ is non decreasing;
	\item $F(-\infty) = 0$ and $F(+\infty) = 1$.
	\item $F$ is right-continuous and bounded;
\end{enumerate}
In short $F$ must be c\'adl\'ag with a crucial requirement that $F(\Real)=1$.

Modern statistical physics was founded on Gibbs's idea.
A physical system might be in different states, thus Gibbs and Bolzmann postulated that each state has some likelihood.

What is the likelihood of picking an even number in $\mathbb{Z}$? Suppose there is some probability measure on $\mathbb{Z}$ given by $\brac{p_n}_{n\in \mathbb{Z}}$ 
\[\sum_{n\text{ div } 2} p_n\]

Let $A\subseteq \mathbb{Z}$ then the density of $A$ is \[\rho(A) \defn \lim_{n\to+\infty, m\to -\infty} \frac{\abs{\obj{\induc{k\in \mathbb{Z}}\,n\leq k\leq m }}}{n+m+1}\]
However $\rho(\cdot)$ fails to be countably sub-additive.

Classification of distribution functions on $\Real$. \begin{description}
	\item[Atomic:] \hfill \\
		there is a countable and measurable subset $A\in \borel(\Real)$ and a collection $\brac{p_k}_{k\in A}$ such that
			\[F(x) = \sum_{k\in A} p_k 1_{\obj{k}}(x)\]
	\item[Singular:] \hfill \\
		all the rest. 
	\item[Absolutely continuous:] \hfill \\
		there exists a lebesgue-measurable map $p:\Real\to\Real$ with $F(x)=\int p(x) dx$
\end{description}

Lebesgue theorem: every distribution function can be represented as a weighted sum of three basic types of distributions.


Classification of local behaviour of distributions in $\Real$.

Consider some $x_0\in \Real$ and consider the difference $F(x_0+\Delta)-F(x_0-\Delta)$ for $\Delta > 0$.
The function $F$ has the singularity order $\alpha$ if \[F(x_0+\Delta)-F(x_0-\Delta) = \Omega(\Delta^\alpha)\]

\begin{description}
	\item[Atomic:] $\alpha = 0$;
	\item[Continuous:] $\alpha = 1$;
\end{description}

If $F(x)=\min\obj{\sqrt{x}, 1} 1_{\clop{0,+\infty}}(x)$ then $0$ has singularity order $\frac{1}{2}$.

Cantor distribution has uncountably many points with $\alpha = 0$, and uncountably many points with $\alpha = \frac{\log 2}{\log 3}$.
A good idea is to represent any $x\in \clo{0,1}$ in base-3 representation. The ``interesting'' points have either $0$ or $2$ in their base-3 form.

\noindent\textbf{Problem \#1}\hfill \\
	Given $U\sim \mathcal{U}\clo{0,1}$ and $Y=f(U)$. Find $f$ such that the density $p_Y$ in given by
	\[\lambda e^{-\lambda y} 1_{\clop{0,+\infty}}\]
	If $F(x)$ is absolutely continuous and $X\sim F$, then $F(X)\sim \mathcal{U}\clo{0,1}$. (Skorokhod)

\noindent\textbf{Problem \#2}\hfill \\
	Suppose $(X,Y)$ is uniformly distributed inside the unit circle. Let $\rho \defn \sqrt{X^2+Y^2}$. What is the distribution of $(x',y')$ with the radius given by $r = \sqrt{-\log \rho}$.
	Using the Jacobian transformation, the distribution of the transformed pair is jointly gaussian.

% section lecutre_1 (end)

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}

A shorter lecture, no definition of expectation, but full of functions:
\begin{itemize}
	\item Probability generating functions;
	\item Moment generating functions;
	\item Characteristic functions.
\end{itemize}

Suppose $X$ is a real-valued random variable with law $F_X$. If $F_X$ is absolutely continuous with respect to $dx$, then $F_X(x) = \int_{-\infty}^x f dx$.

The expectation of $X$ is defined as \[\ex(X) \defn \int X d{F_X}\] and in the case of absolutely continuous distribution it is true that \[\int X d{F_X} = \int X f(x) dx\]

Riemann-Stieltjes integrals are used (but those who know, can use a better Lebesgue-Stieltjes integral).
\[\int d dF = \lim_{\Delta\to 0} \sum_i g(x_i) \brac{F(\xi_{i+1}) - F(\xi_i)}\]
Lebesgue integral is pedagogically more difficult, though more abstract, flexible and overall better.
% Geometric measure theory.

Cauchy distribution does not allow $\ex X$, however it has the mean in terms of the \textbf{main value} (integration over a symmetric interval $\clo{-M,+M}$).
% \rus{Уравнение Власова}

The expectation of a function $g$ of $X$ is $\ex\brac{g(X)} = \int g(X) dF_X$.

The $k$-th moment of an RV is $\ex X^k$ and the $k$-th central moment is $\ex\brac{ X - \ex X}^2$.
The second central moment of $X$ is the variance of $X$ : $\mathbb{D}(X) \defn \ex\brac{ X - \ex X}^2$.

Jensen's inequality. For any convex function $g$ of $X$ it is true that 
\[g\brac{\ex X}\leq \ex g(X) \]

Fatou's lemma \[\int \liminf X_n d\mu \leq \liminf \int X_n d\mu\]

Holder's inequality for nonnegative measurable $f$ and $g$ \[\int fg d\mu \leq \brac{\int f^p d\mu}^\frac{1}{p}\brac{\int g^q d\mu}^\frac{1}{q}\] whenever $\sfrac{1}{p}+\sfrac{1}{q}=1$.

Minkowski inequality \[\brac{\int {(f+g)}^p d\mu}^\frac{1}{p}\leq \brac{\int f^p d\mu}^\frac{1}{p} + \brac{\int g^p d\mu}^\frac{1}{p}\]

The probability generating function is defined for any $z\in \Cplx$ with $\abs{z}\leq 1$ as \[G_X(z) \defn \ex z^X\]
If $X$ has discrete distribution, then $G_X(z) = \sum_{n\geq 0} z^n \Pr(X=n)$.
Since the series is analytical in a finite disc, it is differentiable.
Thus \[G_X^{(k)}(z) = \sum_{n\geq 0} \frac{n!}{(n-k)!} z^{n-k} \Pr(X=n) \]
whence $G_X^{(k)}(0) = \ex \brac{X(X-1)\ldots (X-k+1)}$

Factorial exponent: $x^{\underline{k}} = \prod_{s=1}^k (X-s+1)$.
\[x^{\underline{k}} - \brac{x-1}^{\underline{k}} = k \cdot \brac{x-1}^{\underline{k-1}} \]

The moment generating function of $X$ is given by \[S_X(t) \defn G(e^t) = \ex(e^{tX})\]
Differentiation of the MGF and evaluation at $t=0$ yields moments of $X$.
Indeed, if the integral converges uniformly, then $S_X^{(k)}(t) = \induc{\ex X^k e^{tX}}_{t=0} = \ex X^k$.

\emph{A gentle reminder}\hfill \\
For any pair of independent random variables the expectation of their product equals the product of their expectations.

Thus the mgf of a sum of independent $X,Y$ results in \[S_{X+Y}(t) = \ex e^{t(X+Y)} = \ex e^{tX} e^{tY} = \ex e^{tX} \ex e^{tY} = S_X(t) S_Y(t)\]

The cumulants of $X$ (or semiinvariants) are defined as the coefficients in the following formal series expansion of $\log S_X(t)$ as
\[\log S_X(t) \defn \sum_{n\geq 0} \frac{t^n}{n!} \kappa_n^X\]

Using the formal differentiation of the series with respect to $t$ yields
$\kappa_n^X = \induc{\frac{d^n}{dt^n} \log S_X(t)}_{t=0}$.

Since $S_X(t)$ is additive with respect to $X$ if the summands are independent,
Kurtosis, \rus{эксцесс} -- measures how far the probability is shifted to the tails.

\textbf{Martsinkevich's theorem}\hfill\\
$\log S_X(t)$ is a polynomial if and only if $X\sim\mathcal{N}(\mu,\sigma)$.


Solve exercises 1.8, 1.11, 1.12 and 1.13 on pp.~42-43.

% section lecture_2 (end)

\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}

%% Missed the first part of the lecture
Tracey-Widam distribution

\textbf{T}otally \textbf{A}symmatric \textbf{S}imple \textbf{E}xclusion \textbf{P}rocess: when a particles on a lattice can jump and shift to the right, but not over another particle.

% Random matrix theory -- Ivan Corvin

Asymptotic (high dimensional) representation theory:
KPZ scaling and Tracey-Widam

The (weak) Law of Large Numbers:
\[\pr\brac{\abs{ \frac{\sum_{k=1}^n X_k}{n}-\mu }>\epsilon} \to 0\]


\noindent \textbf{Definition} of weak convergence. \hfill\\
A sequence of random variables $\brac{X_n}_{n\geq1}$ converges weakly to $X$, or in distribution, if for all $a,b\in \Rbar$, such that $a,b$ are atoms, probability concentrations:
\[\pr\brac{a<X_n<b}\to \pr\brac{a<X<b}\]

\noindent \textbf{Proposition} 1\hfill \\
$X_k\overset{D}{\to}X$ if and only if $F_{X_n}\to F_X$ at any point of continuity of $F$.

Continuity is required, because the CDf is c\'adlag (only right continuous).

Let $\brac{x_k}_{k\geq0}$ be a monotonous deterministic sequence, which converges to $\mu$.
If $x_k\uparrow$, then CDFs of a deterministic sequence do converge to the CDF of their limit. And not otherwise.

\noindent \textbf{Proposition} 1\hfill \\
$X_k\overset{D}{\to}X$ if and only if $F_{X_n}\to F_X$ at any point of continuity of $F$.

\noindent \textbf{Proposition} 2 (Helly)\hfill \\
For every sequence of distribution functions $\brac{F_n}_{n\geq1}$ it is possible to select a subsequence, such that $F_{n_k}$ converges to some function $F$ at every point of continuity of $F$ and such that $F$ is monotonous.

Note that the function $F$ is non-normalised.

Sketch:
Use the rational numbers $\mathbb{Q}$ as the \textbf{knots} of convergence of $F$. Enumerate them and choose the ``diagonal''.

Proof. Suppose $\brac{q_n}_{n\geq 1}$ enumerates $\mathbb{Q}$.
The sequence $\brac{F_n(q_1)}_{n\geq1}$ is bounded, hence it has a convergent subsequence, whence exists $\brac{k_n}\uparrow\infty$ such that $F_{k_n}(q_1)$ converges to some $F(q_1)$.
Consider $\brac{F^1_n}_{n\geq 1}\defn\brac{F_{k_n}}_{n\geq1}$ as a new sequence, and apply the same argument to it recursively.

Finally we get a sequence $\brac{F^m_n}_{n\geq1}$ such that $F^m_n(q_l)$ converges to $F(q_l)$ for all $l\leq m$ as $n\to \infty$, either by being itself a subsequence or by the choice of the subsequence $k_n$.

Use Cantor's diagonal argument: form a new sequence $\brac{F^n_n}_{n\geq1}$. This sequence converges at any $q\in\mathbb{Q}$ to $F(q)$.

Now to prove that $F$ is continuous.

If $\lim_{q\uparrow x}F(q) = \lim_{p\downarrow x}F(p)$, then set $F(x)$ to the limit.

The function then becomes either continuous, or left limited and right continuous, since $F_n$ are monotonous and right-continuous.

Counterexample:
Suppose $X_n=n$. Then $F_n(x) = 1_{\ploc{-\infty, x}}(n)$.

In order to get a requirement in Helly's theorem, which guarantees that the limiting function is a distribution function, one has to require the following ``tightness'' condition (uniform property -- nor all $n$):
\[\forall \epsilon>0\,\exists L>0\,\text{s.t.}\,\sup_{n\geq1}\pr\brac{\abs{X_n}>L}\leq \epsilon\]

% See the photographs of the explanation of the method of closed loops.

% Алгебра -- рай, а недифференцируемость -- нечистая чила

\noindent \textbf{The main technical result}\hfill\\

The sequence of distribution function $F_n$ converges weakly to some $F$ if $\phi_n\to \phi$, their associated characteristic functions converge, and $\phi$ is continuous at $0$.

\noindent \textbf{Proposition}\hfill\\

Suppose $X_n\to X$ and $\phi_n(s)\defn \ex\brac{e^{isX_n}}$. Then there exists a limiting function of $\phi_n$ such that $\phi_n\to \phi = \ex\brac{e^{isX}}$.

Let's show that if $X_n\overset{D}{\to} X$ then
\[\int g(X) dF_n \to \int g(X)dF\]
for any bounded continuous function $g$.

\textbf{Proof}\hfill\\
Split the integration region into two regions:
\[\int f dF_n = \int_\obj{\abs{X}>R} g dF_n + \int_\obj{\abs{X}\leq R} g dF_n\]

The tightness condition implies that there exists $R$ such that
\[F_n\brac{\obj{\abs{X}>R}}< \frac{\epsilon}{2}\]
which means that $\int_\obj{\abs{X}>R} g dF_n$ is bounded above by $M\frac{\epsilon}{2}$ where $\abs{g}\leq M$.

Since $g$ is continuous on a compact $\obj{\abs{X}\leq R}$, for $\epsilon>0$ there exists $\delta>0$ such that on each $\delta$-range the change of $g$ is not greater that $\epsilon$.
therefore 
\[\int_\obj{\abs{X}\leq R} g dF_n = \int_\obj{\abs{X}\leq R} (g-g^\epsilon) dF_n + \int_\obj{\abs{X}\leq R} g^\epsilon dF_n \leq F_n\brac{\obj{\abs{X}\leq R}}\frac{\epsilon}{2} + \int_\obj{\abs{X}\leq R} g^\epsilon dF_n\]

where $g^\epsilon$ is an $\epsilon$ step function.

However
\[\int_\obj{\abs{X}\leq R} g^\epsilon dF_n\]
is a finite sum $\sum_i g^\epsilon$

Therefore if $X_n\overset{D}{\to} X$ then characteristic function s converge.


% Feller -- Probability: a neat proof

\noindent\textbf{Proposition} \hfill\\
If
\[\phi(s) = \int e^{isx} dF(x)\]
then the following equivalence is true for any $\delta>0$ and $y$:
\[\frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds
=\frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF(x)\]
for a gaussian smoothing kernel $e^{-\frac{{(x-y)}^2}{2\delta^2}}$.

Indeed,
\[e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}}
= e^{-isy} e^{-\frac{\delta^2 s^2}{2}}\int e^{isx} dF(x)
 = e^{-isy}\int e^{isx} e^{-\frac{\delta^2 s^2}{2}} dF(x)\]
whence
\begin{align*}
	\frac{1}{2\pi} \int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds
	&= \frac{1}{2\pi} \int e^{-isy} \int e^{isx} e^{-\frac{\delta^2 s^2}{2}} dF(x) ds\\
	&= \frac{1}{2\pi} \int \int e^{is(x-y)-\frac{\delta^2 s^2}{2}} dF(x) ds
\end{align*}
Since
\[is(x-y)-\frac{\delta^2 s^2}{2}
= \frac{1}{2}2s\delta \frac{i(x-y)}{\delta}-\frac{\delta^2 s^2}{2} \pm \frac{1}{2}\frac{(x-y)^2}{\delta^2}
= -\frac{1}{2}\Big( \big( \frac{i(x-y)}{\delta} - s\delta \big)^2 + \frac{(x-y)^2}{\delta^2} \Big)
\]
we get
\begin{align*}
	\ldots
	&= \frac{1}{2\pi} \int \int e^{-\frac{{(x-y)}^2}{2\delta^2}} e^{\frac{\delta^2}{2}\brac{-i\frac{x-y}{\delta^2}+s}^2} dF(x) ds\\
	&= \frac{1}{2\pi} \int e^{-\frac{{(x-y)}^2}{2\delta^2}} \int e^{\frac{\delta^2}{2}\brac{-i\frac{x-y}{\delta^2}+s}^2} ds dF(x)\\
	&= \frac{1}{2\pi} \int e^{-\frac{{(x-y)}^2}{2\delta^2}} \frac{\sqrt{2\pi}}{\delta} dF(x)
% Missed some details...
\end{align*}

\noindent\textbf{Another lemma}\hfill\\
\noindent The characteristic functions of different distributions are never identical.

Indeed, if characteristic functions coincide, but but distributions differ at least in some point, then there is a contradiction

Suppose $\phi_n(x) = \ex\brac{e^{isX_n}}$ converges to some $\phi$ -- and $\phi$ is continuous at $x=0$, then there exists a random variable, such that $\phi(s) = \ex\brac{e^{isX}}$ and $X_n\to X$ weakly.

Suppose $Y_k$ are iid and $X_n = \sum_{k=1}^n Y_k$ with $\ex\brac{e^{isY}} = \psi(s) = e^{-\abs{s}}$.

Then $\phi_n(s) = \ex\brac{e^{isX}} = \brac{\phi(s)^n}$, but it converges to $1_{s=0}$.

% the probability is smeared along the infinity/

\begin{align*}
	\frac{1}{2\pi}\int e^{-sy} \phi_n(s) e^{-\frac{\delta^2 s^2}{2}}ds
	& = \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF_n(x)
\end{align*}

Helly's theorem implies that there is a convergent subsequence $F_{n_k}$ that converges to some $F$.

Then analogously to a previously stated proposition
\[\frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF_n(x)
\to \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF\]

Using Lebesgue's Dominated convergence theorem it is true that
\[\frac{1}{2\pi}\int e^{-sy} \phi_{n_k}(s) e^{-\frac{\delta^2 s^2}{2}}ds
\to \frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds\]

Therefore
\[\frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds
= \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF\]
Since $F$ is an arbitrary limiting function, this implies that any other convergent subsequence converges to the same $F$, whence $F_n\to F$.

To show that $F$ is a CDF we do the following: think of letting $\delta\to \infty$ and fixing $y=0$. Then
\[\frac{\delta^2}{\sqrt{2\pi}}\int \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds
= \int e^{-\frac{x^2}{2\delta^2}}dF \leq \int dF\]

However as $\delta\to\infty$ it is true that
\[\int \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds\to \phi(0) = 1\]
since $\phi(0)$ is continuous at $0$.

Thus $1 \leq \int dF$. But $\int dF$ cannot strictly exceed $1$, whence $F$ is a distribution function.

\noindent\textbf{Lyapunov's Central Limit Theorem} \hfill\\
Suppose $X_k$ are i.i.d. with $\ex X_k = \mu$ and $\ex X_k^2 = \sigma^2$. Introduce the following sequence:
\[Y_n = \frac{\sum_{k=1}^n X_k - n \mu }{\sqrt{n}\sigma}\]
Then $Y_n\overset{D}{\to}\mathcal{N}(0,1)$

\textbf{Proof}\hfill\\

If $Z_n = \sum_{k=1}^n X_k - n\mu$. Then $\ex Z_n = 0$ and $\Var Z_n = n \sigma^2$ due to independence.

Let $\phi_n(s) = \ex\brac{e^{isZ_n}}$. Then
\[\phi_n(s) = 1 + 0 - \frac{n\sigma^2}{2} s^2 + o(s^2)\]

Now
\[\phi_{Y_n}(s) = \ex\brac{e^{is\frac{Z_n}{\sigma \sqrt{n}}}} = \phi_n(\frac{s}{\sigma\sqrt{n}})\]
whence
\[\phi_{Y_n}(s) = 1 - \frac{s^2}{2} + o(s^2)\]


Another way: due to independence of $X_k$ and using the product-independence property of characteristic function
\[\phi_n(s) = \brac{\phi_{X-\mu}(s)}^n = \brac{ 1 - \frac{n\sigma^2}{2} s^2 + o(s^2) }^n\]

Therefore
\[\phi_{Y_n}(s) = \phi_n(\frac{s}{\sqrt{n}\sigma}) = \brac{ 1 - \frac{s^2}{2n} + o(s^2) }^n\]
where $o(\cdot)$ is uniformly infinitesimal with respect to $n$.

Using a well known limit we get the following limit:
\[\phi_{Y_n}(s) \to e^{-\frac{s^2}{2}}\]

an example:
Suppose $\brac{X_n}_{n\geq1}$ are independent identically distributed such that $\phi_X(s) = 1+i\mu s + o(s)$, where $o(\cdot)$ is an infinitesimal.

Let \[Y_n = \frac{\sum_{k=1}^n X_k - n \mu }{n}\]
then
\[\phi_{Y_n}(s) = \phi_{\sum}(\frac{s}{n})
= \brac{\phi_X(\frac{s}{n})}^n = \brac{1+i\mu s + o(s)}^n
\to e^{is\mu}\]

The necessary and sufficient condition for $\phi(s) = 1+i s\mu+o(s)$ is
\[\lim_{R\to\infty} R\cdot \brac{F_X(-R) + 1 - F_X(R)} = 0\]
% Khinchine's law of large numbers.

% Tutorials
% Homework У2.1, У2.6, У2.7, У2.8, У2.9, У3.9, У3.3, У3.6

% section lecture_3 (end)

\section{lecture \# 4} % (fold)
\label{sec:lecture_4}

Gamma distribution.
\[\gamma(k,\beta) \defn \frac{\beta^k}{\Gamma(k)} s^{k-1} e^{-\beta s}\]

It has all the moments, since the exponent decays very rapidly, faster than any polynomial.

Some examples of the $\gamma$ distribution are \begin{description}
	\item[$\chi^2_k$] A $\chi^2$ random variable is actually $\gamma\brac{\frac{k}{2},\frac{1}{2}}$;
	\item[$\text{Exp}(\lambda)$] An exponential distribution is a particular case of the Gamma distribution: $\text{Exp}(\lambda) \sim \gamma(1,\lambda)$.
\end{description}

This distribution is asymmetric.
\[\frac{\sum_{k=1}^n G_k- }{}\]

Limiting behaviour of products of random variables.

Suppose $\brac{X_k}_{k=1}^n$ are non-negative random variables. Consider the asymptotics of \[P_n \defn \brac{e^{-\mu n}\prod_{k=1}^n G_k}^{\sqrt{n}}\]

The natural approach is to use $Y_k\defn \log X_k$ and consider the asymptotics of the logarithm of the product: the normalised sum
\[S_n \defn \log P_n \frac{\sum_{k=1}^n Y_k - \mu n}{\sqrt{n}}\]

Since this converges in distribution to a standard normal random variable: $S_n \overset{\mathcal{D}}{\to} \mathcal{N}(0,1)$. Thus the products converge in distribution to a log-normal random variable:
\[\frac{1}{\sqrt{2\pi\sigma^2}} e^\frac{-\log^2 x}{2\sigma^2}\frac{1}{x}\]

What happens to the central limit theorem, when its assumptions fail.
\begin{description}
	\item[Independence] Stochastic processes CLT;
	\item[Identical distribution] The most useful generalisation of the CLT;
	\item[Moments] What if $\ex(X_k) = \mu$ and $\text{Var}(X_k) = \sigma^2$ are violated?
\end{description}

Consider the Cauchy distributed random variable. IS $\brac{X_n}_{n\geq1}\sim \text{Cauchy}$, then their normalised sum is still Cauchy (consider the characteristic function).

What precludes a distribution from having finite variance?

The main moment is the computation of the asymptotics at zero of the characteristic function (the Fourier transform of the distribution).

Consider the following a bit contrived density:
\[p(x) = 1_\obj{\abs{x}\leq 1}\frac{A}{2} + 1_\obj{\abs{x}>1}\frac{A}{2\abs{x}^{\alpha+1}}\]
where $A=\frac{\alpha}{\alpha+1}$ and $\alpha$ - the Levy exponent. The exponent characterises asymptotics convergences of the CDF to the horizontal asymptotes: $\abs{x}^{-\alpha}$. In the limit we get the Pareto-Levy family of distributions.

Let's use the characteristic functions.
\[\phi(s)\defn\int e^{-isx}p(x)dx\]
Consider the asymptotics near zero (the problems at infinity i time domain transform to the problems near zero in the frequency domain).

Since $p(x)$ is symmetrical
\begin{align*}
	\phi(s) &= \ex\brac{e^{-isX} } = \int e^{-isx}p(x)dx \\
	&= \int_{-1}^1 \frac{A}{2} e^{-isx} dx + \brac{\int_1^\infty + \int_{-\infty}^{-1}} \frac{A}{2} \frac{e^{-isx} }{\abs{x}^{\alpha+1}} dx
\end{align*}

Now the first integral is just \begin{align*}
	\int_{-1}^1 e^{-isx} dx &= \int_{-1}^1 \cos{sx} - i\sin{sx} dx \\ 
	& = \int_{-1}^1 \cos{sx} dx - i \int_{-1}^1 \sin{sx} dx \\
	& =  \induc{\frac{\sin{sx}}{s}}_{-1}^1 - i \induc{\frac{-\cos{sx}}{s}}_{-1}^1\\
	& = \frac{\sin{s}}{s}-\frac{\sin{-s}}{s} = 2 \frac{\sin{s}}{s} = 2 \frac{\sin\abs{s}}{\abs{s}}
\end{align*}
while the second is a little bit more intricate:
\begin{align*}
	\brac{\int_1^\infty + \int_{-\infty}^{-1}} \frac{e^{-isx} }{\abs{x}^{\alpha+1}} dx
	& = \int_1^\infty \frac{\cos{sx}-i\sin{sx}}{x^{\alpha+1}} dx + \int_{-\infty}^{-1} \frac{\cos{sx}-i\sin{sx}}{\brac{-x}^{\alpha+1}} dx \\
	& = \int_1^\infty \frac{\cos{sx}-i\sin{sx}}{x^{\alpha+1}} dx + \int_1^\infty \frac{\cos(-sy)-i\sin(-sy)}{y^{\alpha+1}} dy \\
	& = \int_1^\infty \frac{2\cos{sx}}{x^{\alpha+1}} dx = 2\int_1^\infty \frac{\cos{\abs{s}x}}{x^{\alpha+1}} dx \\
	& = 2\abs{s}^\alpha \int_1^\infty \frac{\cos{\abs{s}x}}{\brac{\abs{s}x}^{\alpha+1}} \abs{s}dx
	= \clo{\xi = \abs{s}x}
	= 2\abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi
\end{align*}

Thus the characteristic function simplifies to
\[\phi(s) = A \frac{\sin\abs{s}}{\abs{s}} + A \abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi \]

The integral can be dealt with in the following way: if $\alpha>0$ then \begin{align*}
	\int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi
	& = \int_{\abs{s}}^\infty \frac{1}{\xi^{\alpha+1}} d\xi - \int_{\abs{s}}^\infty \frac{\xi - \cos\xi}{\xi^{\alpha+1}} d\xi\\
	& = \induc{-\frac{\xi^{-\alpha}}{\alpha}}_{\abs{s}}^\infty - \int_{\abs{s}}^\infty \frac{\xi - \cos\xi}{\xi^{\alpha+1}} d\xi
\end{align*}
%%  FINISH AT HOME!!!

\begin{align*}
A \abs{s}^\alpha \int_\abs{s}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt
&= A \abs{s}^\alpha \int_\abs{s}^\infty \frac{1}{t^{\alpha+1}} dt - A \abs{s}^\alpha \int_\abs{s}^\infty \frac{1-\cos(t)}{t^{\alpha+1}}dt
\end{align*}


It is possible to show (?) that the very last integral in the right hand side is asymptotically equal to
\[\int_\abs{s}^\infty \frac{1-\cos(t)}{t^{\alpha+1}}dt \sim \Ical_1(\alpha) + o(\abs{s}^{2-\alpha})\]
which means that
\begin{align*}
A \abs{s}^\alpha \int_\abs{s}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt
&= \abs{s}^\alpha \frac{1}{\alpha} \abs{s}^{-\alpha} + A \Ical_1(\alpha)\abs{s}^\alpha + o(s^2)
\end{align*}
Thus
\[\phi(s) = 1 - A \Ical_1(\alpha) \abs{s}^\alpha + o(s^2)\]

Since the variables are independent, then the sum has the product characteristic function
\[\ex\brac{ e^{-is\frac{S_n}{n^\frac{1}{\alpha}}} }
= \brac{1 - A \Ical_1(\alpha) \frac{\abs{s}^\alpha}{n} + o(s^2)}^n \to e^{-A \Ical_1(\alpha)\abs{s}^\alpha}\]

Thus it turns out that if $F(x)\underset{x\to\infty}{\sim} \abs{x}^{-\alpha}$,
$F(x)\underset{x\to-\infty}{\sim} 1-\abs{x}^{-\alpha}$ and the tails are symmetric,
then
\[\frac{\sum_{k=1}^{n}X_k}{n^\frac{1}{\alpha}}\to X\]
where the random variable $X$ has the characteristic function given by
\[\phi(s) = e^{-\frac{\alpha}{\alpha+1} \Ical_1(\alpha)\abs{s}^\alpha}\]
-- the Levy-Pareto characteristic function.

When $\alpha=2$, then the infinitesimal in the asymptotic expansion balances the major component, and inside there will emerge a divergent integral $\int \frac{dt}{t}$.
Thus the normalisation must be different:
\[\frac{\sum_{k=1}^n X_k}{\sqrt{n\log n}}\sim \mathcal{N}(0,1)\]

The last case is, when the density is asymmetric.
\[p(x) = 1_\obj{\abs{x}\leq 1}\frac{A}{2}
	+ 1_\obj{x>1}\frac{A(1+\beta)}{2\abs{x}^{\alpha+1}}
	+ 1_\obj{x<-1}\frac{A(\beta-1)}{2\abs{x}^{\alpha+1}}\]
where the parameter $\beta \in \clo{0,1}$ is the mass defect.
\begin{align*}
	\phi(s) &= \int e^{-isx}p(x)dx\\
	&= \int_\clo{-1,1} e^{-isx}\frac{A}{2}dx
		+ \int_1^\infty e^{-isx}\frac{A(1+\beta)}{|x|^{\alpha+1}}dx
		+ \int_{-\infty}^{-1} e^{-isx}\frac{A(1-\beta)}{|x|^{\alpha+1}}dx\\
	&= \frac{A}{2} \int_\clo{0,1} \cos(sx)dx 
		+ \frac{A(1+\beta)}{2}\int_1^\infty \frac{e^{-isx}}{x^{\alpha+1}}dx 
		+ \frac{A(1-\beta)}{2}\int_{-\infty}^{-1} \frac{e^{-isx}}{(-x)^{\alpha+1}}dx\\
	&= \frac{A}{2} \frac{\sin(s)}{s} 
		+ \frac{A(1+\beta)}{2} \int_1^\infty \frac{\cos(sy) - i\sin(sy)}{y^{\alpha+1}}dy \\
		&\quad + \frac{A(1-\beta)}{2} \int_{-\infty}^{-1} \frac{\cos(sy) - i\sin(sy)}{(-y)^{\alpha+1}}dy\\
	&= \frac{A}{2} \frac{\sin(s)}{s} 
		+ A \int_1^\infty \frac{\cos(sy)}{y^{\alpha+1}}dy 
		- i A\beta \int_1^\infty \frac{\sin(sy)}{y^{\alpha+1}}dy\\
	&= \Big[ \text{substitution: } t = \abs{s} x; \cos\text{--even function}\Big ]\\
	&= \frac{A}{2} \frac{\sin\abs{s}}{\abs{s}} 
		+ A \abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt
		- i A\beta \abs{s} \int_1^\infty \sign(s) \frac{\sin(t)}{t^{\alpha+1}}dt
\end{align*}
The integral
\[\Ical_2(\alpha) = \int_1^\infty \sign(s) \frac{\sin(t)}{t^{\alpha+1}}dt\]
converges when $\alpha\in\brac{0,1}$. Therefore 
\[\phi(s) = 1 - A \Ical_1(\alpha)\abs{s}^\alpha \brac{1+i\beta \sign(s) \frac{\Ical_2(\alpha)}{ \Ical_1(\alpha)}} + o(s^2)\]
From analysis it is known (?) that
\[\frac{\Ical_2(\alpha)}{ \Ical_1(\alpha)} = \tg\frac{\pi \alpha}{2}\]
whence the limiting distribution must have the following characteristic function:
\[\phi(s) = \exp\brac{-\frac{\alpha}{\alpha+1} \Ical_1(\alpha)\abs{s}^\alpha \brac{1+i\beta \sign(s) \tg\frac{\pi \alpha}{2} } }\]

Now the case when $\alpha=1$.
\begin{align*}
	\phi(s) &= \int e^{-isx}p(x)dx
	= \int_\clo{-1,1} e^{-isx}\frac{A}{2}dx
		+ \int_1^\infty e^{-isx}\frac{A(1+\beta)}{x^{\alpha+1}}dx
		+ \int_{-\infty}^{-1} e^{-isx}\frac{A(1-\beta)}{x^{\alpha+1}}dx
\end{align*}

% see the lecture notes 5.8, 5.9

% for small $s\to 0$
% iA\beta \abs{s}^\alpha\sign(s) \int_{\abs{s}}^\infty \frac{\sin t}{t^2} dt

% \int_{\abs{s}}^\infty \frac{\sin t}{t^2} dt = 
% \int_{\abs{s}}^1 \frac{1}{t} dt + \int_{\abs{s}}^1 \frac{\sin(t)-t}{t^2} dt + \int_1^\infty \frac{\sin t}{t^2} dt
% = iA\beta s\log \abs{s} + \text{const} i s + o(s^2)

Using the convergence of characteristic functions, extracting the divergent integral, regularizing it -- the basic idea (complex in general, simple locally). 

Stability
\[\frac{\sum_{k=1}^n X_k - \mu n}{n^\frac{1}{\alpha}} \sim X_k\]

if $0<\alpha<1$ then ...

% section lecture_4 (end)

\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

Mathematical statistics.
% Probability theory -- we have some understanding
We have an experiment, the data is random, and we attempt to to infer something about the underlying physical phenomenon.

Suppose $\brac{X_k}_{k=1}^n$ is some sample.
Using the strong law of large numbers \[\frac{\sum_{k=1}^n X_k}{n}\overset{\text{a.s}}{\to}\]
Does not tell us anything about the speed of convergence.

The model of the data plays very important role. We consider 
\[X_k \sim C + \xi_i;\quad \xi_i\sim \mathcal{D}(0,1)\,\text{iid}\]
Here All probabilistic properties are governed by the noise term.
The distribution function of the vectors of the noise terms is given by 
\[F\brac{y_1, \ldots, y_n;\Theta} \Pr\brac{\xi_k\leq y_k;\Theta} = \prod_{k=1}^n F_{\xi_k}(y_k)\]

The basic properties of distribution functions:
\begin{itemize}
	\item Normalisation: $0\leq F\leq 1$ and $\lim_{x\to -\infty} F(x)=0$ and $\lim_{x\to \infty} F(x)=1$;
	\item Right-continuous and left-limited non-decreasing map;
	\item It has a density if the Radon-Nikodym derivative exists.
\end{itemize}

Multivariate distribution:
\begin{itemize}
	\item \[p(x,y) = \frac{\partial}{\partial x}\frac{\partial}{\partial y} F(x,y)\]
\end{itemize}

% The law of large numbers
The central limit theorem

Suppose we have $\brac{\xi_k}_{k\geq1}\sim \mathcal{D}(\mu, \sigma^2)$ iid., Then
\[\frac{\frac{1}{n}\sum_{k=1}^n \xi_k - \mu}{\sfrac{\sigma}{\sqrt{n}}} \overset{\mathcal{D}}{\to} \mathcal{N}(0,1)\]

\noindent\textbf{The Chebyshev inequality}\hfill\\
Suppose $\xi$ is some random variable, then 
\[\Pr\brac{\abs{\xi-\mu}> \epsilon}\leq \frac{\Var{\xi}}{\epsilon^2}\] 

\noindent\textbf{Chernoff's inequality}\hfill\\
Suppose $\xi$ is a real-valued random variable.
\[\Pr\brac{\xi>\epsilon}\leq \inf_{\lambda>0} \ex\exp\brac{\lambda\xi - \lambda\epsilon}\]

Since $1_\obj{\xi\>x}\leq e^{\lambda \xi - \lambda x}$ and expectations preserve monotonicity, we have for every $\lambda>0$
\[\Pr\brac{\xi>\epsilon}\leq \ex\brac{e^{\lambda \xi - \lambda x}}\]
whence the inequality for the infumum over all $\lambda$ follows.

Similarly being a real function of a real value $\xi$ we have the following inequality
\[1_\obj{\xi\>x}\leq \frac{\abs{\xi}^p}{x^p}\]
form which Chebyshev's inequality follows as well (not only the markov's inequality).

\subsection{Random variate generation} % (fold)
\label{sub:random_variate_generation}

Suppose it is necessary to generate a random variable $\theta$ with a known distribution $F$ using the uniform random variable $U\sim\mathcal{U}\clo{0,1}$.

If the function $F$ is invertible ($F$ has no atoms), then $\eta \defn F^{-1}(U)\sim F$.

Indeed, \[\Pr\brac{\eta\leq x} = \Pr\brac{F^{-1}(U)\leq x} = \Pr\brac{U\leq F(x)} = F(x)\]

For example, for $F\sim \text{Exp}(\lambda)$ it is true that $F(x)= 1-e^{-\lambda x}$, whence \[Y \defn -\frac{1}{\lambda}\log{(1-U)}\] has is exponentially distributed. Notice, that $1-U\sim\mathcal{U}\clo{0,1}$, which implies that \[-\frac{1}{\lambda}\log U \sim F\]

The inversion method is not always convenient. Take normally distributed random variables for instance.

% subsection random_variate_generation (end)

\subsection{Box-M\"uller method} % (fold)
\label{sub:box_muller_method}

Suppose we need to generate a pair of normally distributed random variates $X,Y$.
Find the joint distribution of independent $(r,\phi)$ such that the following random
vectors have a normal join distribution and are independent
\[\xi_1 = r \cos\phi\,\text{and}\,\xi_2 = r \sin\phi\]

Consider an infinitesimal patch of a plane
\[(\xi_1, \xi_2)\in\clo{r,r+\Delta r}\times \clo{\phi,\phi+\Delta \phi}\sim
\frac{1}{2\pi} e^{-\frac{r^2}{2}} \Delta_r r\sin \Delta_\phi\]
Note that $\sin\Delta_\phi\approx \Delta_\phi$ for an infinitesimal $\Delta_\phi$. At the same time
\[\clo{r,r+\Delta r}\times \clo{\phi,\phi+\Delta \phi} \sim p(r,\phi) \Delta_r\Delta_\phi\] 

Therefore
\[p(r,\phi) = \frac{1}{2\pi} r e^{-\frac{r^2}{2}}\]

Thus the marginal distribution of $\phi$ is uniform on $\ploc{0,2\pi}$, and $F_r(x) = 1 - e^{-\frac{x^2}{2}}$.

% subsection box_muller_method (end)

Suppose we have a sample $\brac{Y_k}_{k=1}^n$ iid with $F(x)$. The empirical distribution function of $Y$ given the sample is 
\[\hat{F}_n(x) \defn \frac{1}{n}\sum_{k=1}^n 1_\ploc{-\infty, x}(Y_k)\]
The expectation of $\hat{F}_n(x)$ for any $x$ is 
\[\ex \hat{F}_n(x) = \frac{1}{n} n \ex1_\ploc{-\infty, x}(Y) = \Pr\brac{Y\leq x} = F(x)\]

The variance of $\hat{F}_n(x)$ at any $x$ is given by
\[\Var\brac{\hat{F}_n(x)} = \frac{1}{n^2}n \Var{1_\ploc{-\infty, x}(Y)}\]
where independence assumption has been used.
The variance on an indicator is
\[\Var{1_A(Y)} = \ex 1_A(Y) - \brac{\ex 1_A(Y)}^2 = \Pr(Y\in A)\brac{1-\Pr(Y\in A)}\]
Thus \[\Var\brac{\hat{F}_n(x)} = \frac{1}{n} F(x)\brac{1-F(x)}\]

Using the central limit theorem we get
\[\frac{\hat{F}_n(x) - F(x)}{\sqrt{\frac{1}{n} F(x)\brac{1-F(x)}}}\overset{\mathcal{D}}{\to} \mathcal{N}(0,1)\]
The convergence rate is $\frac{1}{\sqrt{n}}$.

There is another way to define the empirical distribution function -- though the order statistics
\[\hat{F}_n(x) = \frac{1}{n}\sum_{k=1}^n 1_\ploc{-\infty,x}(Y_{(k)})\]

\noindent\textbf{Glivenko-Cantelli theorem}\hfill\\
\[\nrm{\hat{F}_n(x) - F(x)}_\infty \overset{\text{a.s}}{\to} 0\]

If $F(x)$ is invertible, $u_k\sim\mathcal{U}\clo{0,1}$ -- iid and $Y_k\sim F$ -- iid, then it is easy to see that 
\[\Pr\brac{\nrm{\hat{F}_n(x) - F(x)}_\infty>z} = \Pr\brac{\sup_{u\in\clo{0,1}}{\hat{U}_n(u) - u}>z}\]
where $\hat{U}_n(u) = \frac{1}{n}\sum_{k=1}^n 1_\ploc{0,F^{-1}(Y_k)}(u)$.

\noindent\textbf{Kolmogorov's theorem}\hfill\\
\[\Pr\brac{\sqrt{n}\nrm{\hat{F}_n(x) - F(x)}_\infty>z} = 2\sum_{k\geq1} {(-1)}^{k-1} e^{-2k^2z^2}\]

\noindent\textbf{Dworezky-Kifer-Wolfowitz theorem}\hfill\\
\[\Pr\brac{\sqrt{n}\nrm{\hat{F}_n(x) - F(x)}_\infty>z} \leq 2 e^{-2z^2}\]

\noindent\textbf{Pike's theorem}\hfill\\
Suppose $\brac{\xi_k}_{k=1}^{n+1}\sim \text{Exp}(1)$.
The distribution of the ordered statistics coincides with 
\[U_{(k)} = \frac{\sum_{i=1}^k\xi_i}{\sum_{i=1}^{n+1}\xi_i}\]

%% See Renyi 1953 On the theory of Order Statistics

% section lecture_5 (end)

\section{Lecture \# 6} % (fold)
\label{sec:lecture_6}

Suppose $\brac{X_k}_{k=1}^n$ is independent and identically distributed like Cauchy, then $\frac{\sum_{k=1}^n X_k}{n}\sim \text{Cauchy}$ and this property is known as stability.

If $p(x)\sim \frac{1}{\abs{x}^{\alpha+1}}$ for $\alpha\in\brac{0,1}$, then $\frac{\sum_{k=1}^n X_k}{n}\sim n^{\frac{1}{\alpha}-1} X_i$.

This looks very much like the renormalisation transformation in self-similarity studies.

What is the reason for this strange result? The limiting distributions for extreme values.

\subsection{Extreme value distribution} % (fold)
\label{sub:extreme_value_distribution}

Other properties of characteristic functions.

Let $X,Y$ be two random variables with distributions $F$ and $G$ respectively. Then \[\Pr\brac{\max\obj{X,Y}\leq x} = F(x)G(x)\]
since $\obj{\max{X,Y}\leq x} = \obj{X\leq x}\cap \obj{Y\leq x}$ and $X\perp Y$.

This fact can be used for deriving distribution of maxima.

Let's get back to the example of Cauchy distribution. The maximum of two Cauchy distributed random variables is distributed according to
\[F^2(x) = \brac{\frac{1}{2} + \frac{1}{\pi}\arctan x}^2\]
and $F(x)\approx 1 - \frac{1}{\pi x}$.

Indeed 
\begin{align*}
	\tan \xi
	&= \frac{\cos \frac{\pi}{2} - \xi}{\sin \frac{\pi}{2} - \xi}\\
	\tan \xi &\overset{\xi\to \frac{\pi}{2}}{=} \frac{\cos \frac{\pi}{2} - \xi \to }{\sin \frac{\pi}{2} - \xi}\sim \frac{1}{\frac{\pi}{2} - \xi}\\
	x\sim \brac{\pi\brac{1-F(x)}}^{-1} \Rightarrow F(x)\sim 1-\frac{1}{\pi x}
\end{align*}

Therefore the asymptotics of the sum is
\[F_{\sum_{k=1}^n X_i} (x) = F_{nX}(x) = F_X(\frac{x}{n})\approx 1 - \frac{n}{\pi x}\]

The largest sum component dominates the sum, and the normalisation kills off the meagre sum components and retains the scaled ``giant''.

However the maximum has the right tail asymptotically equal to
\[F_{\max_{k=1}^n X_i} (x) = \brac{F_X(x)}^n \approx \brac{1 - \frac{1}{\pi x}}^n \approx 1 - \frac{n}{\pi x}\]

% subsection extreme_value_distribution (end)

\subsection{The Fisher-Tipett-Gnedenko} % (fold)
\label{sub:the_fisher_tipett}

Gnedeko proved the exclusiveness of the limiting distributions.

Reasoning in complete analogy to the limiting stable distributions, one has to decide on the recentering and rescaling.

\noindent \textbf{Definition}\hfill\\
Let $F(x)$ be some distribution function. Define the \emph{typical maximum value} in the sample of size $n$ as the solution to the following equation: \[F(\bar{x}_{(n)}) = 1-\frac{1}{n}\]

The expected number of sample elements larger than $\bar{x}_{(n)}$ is $n\frac{1}{n}$, over the random samples of size $N$ from $F(x)$.

Let's use $X_{(n)}$ to renormalise the maxima.

\noindent\textbf{Theorem}\hfill\\
Suppose $\brac{X_k}_{k=1}^n$ is an independent sample. Denote by $X_{(k)}$ the order statistics of the sample. Recall that for all $k\leq n-1$
\[x_{(k)}\leq X_{(k+1)}\]
The order statistics are obtained via a random permutation and are dependent.

The maximum of a random sample of size $n$ is \[X_{(n)} = \max_k X_k\] and its CDF is $F_{(n)}(x) = F^n(x)$.

Let's looks for the limiting distribution of the form 
\[F_{(n)}(\xi \bar{x}_{(n)}) \overset{n\to \infty}{\to} e^{-\phi(\xi)} \]

This renormalise makes sense if $F(x)<1$ for all $x\in \Real$, for otherwise the limiting distribution is a step function.

It is more practical to consider the complimentary distribution function $\bar{F}(x) = 1-F(x)$.

\begin{align*}
	F_{(n)}(\xi \bar{x}_{(n)}) &= \brac{1-\bar{F}(\xi \bar{x}_{(n)})}^n\\
	n \ln\brac{1-\bar{F}(\xi \bar{x}_{(n)})} \to -\phi(\xi)
\end{align*}

Now if $\bar{F}(\xi \bar{x}_{(n)})\to 0$, then asymptotically 
\[- n \bar{F}(\xi \bar{x}_{(n)}) \to -\phi(\xi)\]
since $\ln (1-\epsilon)\approx -\epsilon$.

On the othe hand, by definition of $\bar{x}_{(n)}$ it is true that $
n\bar{F}(\bar{x}_{(n)}) \equiv 1$. Therefore we look for
\[\lim_{n\to \infty} - \frac{n \bar{F}(\xi \bar{x}_{(n)})}{ n\bar{F}(\bar{x}_{(n)}) }\]

Note that for all $t\in \clo{ \bar{x}_{(n)}\leq \bar{x}_{(n+1)}}$ it is true that 
\[
\frac{\bar{F}(\bar{x}_{(n+1)})}{ \bar{F}(\bar{x}_{(n)}) } \frac{\bar{F}(\xi \bar{x}_{(n)})}{ \bar{F}(\bar{x}_{(n)}) } \leq
\frac{\bar{F}(\xi t)}{ \bar{F}(t) } \leq
\frac{\bar{F}(\bar{x}_{(n)})}{ \bar{F}(\bar{x}_{(n+1)}) }
\frac{\bar{F}(\xi \bar{x}_{(n+1)})}{ \bar{F}(\bar{x}_{(n+1)}) }
\]

Where $\frac{\bar{F}(\bar{x}_{(n+1)})}{ \bar{F}(\bar{x}_{(n)}) }\to 1$ and $\frac{\bar{F}(\bar{x}_{(n)})}{ \bar{F}(\bar{x}_{(n+1)}) }\to 1$.
%% Use limsups and liminfs

Therefore
\[\phi(\xi) = \lim_{n\to\infty} \frac{\bar{F}(\bar{x}_{(n+1)})}{\bar{F}(\bar{x}_{(n+1)}) } = \lim_{t\to\infty} \frac{\bar{F}(t \xi)}{\bar{F}(t) }\]

Next notice that for any $\xi_1, \xi_2\geq 0$ it is true that
\[\phi(\xi_1\xi_2) = \lim_{t\to\infty} \frac{\bar{F}(t \xi_1\xi_2)}{\bar{F}(t\xi_1) } \frac{\bar{F}(t \xi_1)}{\bar{F}(t) }\]
whence \[\phi(\xi_1\xi_2) = \phi(\xi_1)\phi(\xi_2)\]

A continuous positive solution to this functional equation is given by $\phi(\xi) = \xi^{-\alpha}$.
In order for $\phi$ to be a meaningful solution, it must be true that the limiting function $e^{-\phi(\xi)}$ be a distribution function.

This the limiting distribution is
\[G(\xi) = e^{-\xi^{-\alpha}}\]

Therefore the tail of the distribution
\[\bar{F}(\xi) = 1 - F(\xi) = 1-e^{-\xi^{-\alpha}}\approx \xi^{-\approx}\]

The limiting distribution if called Fr\'ech\'et distribution and it emerges when the tail is polynomial and $\bar{x}_{(n)}\sim n^\frac{1}{\alpha}$.

For light tailed distributions this derivation will yield $\alpha=\infty$. This means that it is necessary to change the normalisation for this class of distributions. The scaling is $F_{(n)}(\bar{x}_{(n)} + \eta)$. In this case the limiting exponent satisfies
\[\phi(\eta) =\lim_{t\to\infty} \frac{\bar{F(t\eta)}}{\bar{F(t)}}\]
the functional equation is thus $\phi(\eta_1+\eta_2)=\phi(\eta_1)\phi(\eta_2)$.

Thus the limiting distribution is the Gumbel's distribution
\[F(\eta) = e^{-e^{-\beta\eta}}\]

If the distribution $X\sim F$ has compact support, i.e. $X_i\leq X_*$, then one can consider another random variable $Y_i = \frac{1}{x_*-X_i}$ and reduce this problem a previously solved.

Then for heavy tail $\alpha\in\ploc{0,\infty}$: Weibull's distribution
\[F(\theta) = e^{-{(x_*-\theta)}^\alpha}\]
Since the exponential decay to zero is so fast, that it would yield a step function.

% subsection the_fisher_tipett (end)

% section lecture_6 (end)

\section{Lecture \# 7} % (fold)
\label{sec:lecture_7}

Suppose we have a sample  $\brac{Y_k}_{k=1}^n$ of independent And identically distributed random variables with $F(x)$.

The empirical distribution function is
\[\hat{F}_n(x) = \frac{1}{n}\sum_{k=1}^n 1_{\ploc{-\infty, x}}(Y_k)\]

The divergence of the eCDF from the true CDF with respect to the $
sup$-norm is equivalent in distribution to
\[\Pr\brac{\sup_{x}\nrm{\hat{F}_n(x) - F(x)}_\infty > z} = \Pr\brac{\sup_{u\in\clo{0,1}}\nrm{\hat{U}_n(u) - u}_\infty > z}\]
where 
\[\hat{U}_n(u) = \frac{1}{n}\sum_{k=1}^n 1_{\clo{0, u}}\brac{F^{-1}(Y_k)}\]

Kolmogorov's theorem
\[\Pr\brac{\sqrt{n} \sup_{u\in\clo{0,1}}\nrm{\hat{U}_n(u) - u}_\infty \leq z} = 2\sum_{k\geq1} {(-1)}^{k-1} e^{-2 k^2 z^2}\]


Pike's theorem
%%%%% Suppose $\brac{\xi_k}_{k=1}^{n+1}\sim \text{Exp}(1)$.
%%%%% The distribution of the ordered statistics coincides with 
%%%%% \[U_{(k)} = \frac{\sum_{i=1}^k\xi_i}{\sum_{i=1}^{n+1}\xi_i}\]


An intermediate result:
\[\lim_{n\to \infty} \Pr\brac{\sqrt{n} \sup_{x}\nrm{\hat{F}_n(x) - F(x)}_\infty > z} = \Pr\brac{\sqrt{n} \abs{W_0(t)} > z}\]

A scalar process on a probability space $\brac{\Omega, \Fcal, \Pr}$ is an $\Fcal$ measurable function $X = X(\omega, t)$ for $\omega\in \Omega$ and $t\in \Tcal$ is a non-random parameter.

A Gaussian random process is a scalar process $X_t$ \begin{itemize}
	\item $\brac{X_{t_k}}_{k=1}^p\sim \Ncal_p\brac{\brac{m(t_i)}_{i=1}^p, \Sigma^p}$ for any $\brac{t_k}_{k=1}^m$ of time slices;
	\item The covariance matrix is given by 
	\[\Sigma^p_{ij} = \ex\brac{X_{t_i} - m(t_i)}\brac{X_{t_j} - m(t_j)}\]
\end{itemize}

A Wiener process is a Gaussian process $W_t$, $t\geq 0$ with $m(t) = 0$ and $\ex\brac{W_t W_s} = \min\obj{t,s}$.

The Brownian Bridge is a process defined over $t\in\clo{0,1}$ by
\[W_0(t) \defn W(t) - t W(1)\]

To reiterate:
\[\Pr\brac{\sqrt{n} \sup_{u\in\clo{0,1}} \nrm{\hat{U}_n(u) - u}_\infty \leq z} \overset{n\to\infty}{\to} \Pr\brac{\sqrt{n} \sup_{u\in \clo{0,1}}\abs{W_0(u)} > z}\]

Consider the ordered statistics $\brac{U_{(k)}}_{k=1}^n$ of a uniformly distributed random variables:
\[\hat{U}_n(U_{(k)}) = \frac{k}{n}\]

Therefore 
\[\Pr\brac{\sqrt{n} \sup_{u\in\clo{0,1}} \nrm{\hat{U}_n(u) - u}_\infty \leq z}  = \Pr\brac{\sqrt{n} \max_{k=1}^n \abs{U_{(k)} - \frac{k}{n}} \leq z} \]

According to Pike's theorem
\[U_{(k)} = \frac{\sum_{i=1}^k \xi_j}{\sum_{j=1}^{n+1} \xi_j}\]

Thus 
\[\Pr\brac{\sqrt{n} \max_{k=1}^n \abs{U_{(k)} - \frac{k}{n}} \leq z}  = \Pr\brac{\max_{k=1}^n \abs{ n \sum_{i=1}^k \xi_j - k\sum_{j=1}^{n+1} \xi_j} \leq \sqrt{n} z\sum_{j=1}^{n+1} \xi_j } \]

Next
\[ = \Pr\brac{\max_{k=1}^n \abs{ \frac{1}{\sqrt{n}} \sum_{i=1}^k (\xi_j-1) - \frac{k}{n\sqrt{n}} \sum_{j=1}^{n+1} (\xi_j-1) - \frac{1}{n}} \leq \frac{1}{n} z\sum_{j=1}^{n+1} \xi_j } \]

Consider for $\brac{\xi_i}_{i\geq1}$ independent and identically distribution $\xi_i\sim\text{exp}(1)$:
\[W_n(t) = \frac{1}{\sqrt{n}} \sum^{\floor{n t}}_{i=1} (\xi_i - 1)\]

First of all $\ex W_n(t) = 0$ and the covariance is given by
\[\ex W_n(t)W_n(s)  = \frac{1}{n} \sum^{\floor{n t}}_{j=1} \sum^{\floor{n s}}_{i=1} (\xi_j - 1) (\xi_i - 1) = \frac{1}{n^2}
\sum^{\floor{n t}\vee \floor{n s}}_{i=1} \ex {(\xi_i - 1)}^2 = 
\frac{1}{n} \floor{n t}\vee \floor{n s} = \floor{t}\vee \floor{s} + o(\frac{1}{n})\]

Thus from the Central Limit Theorem on $t\in \clo{0,1}$
\[W_n(t)\overset{\Dcal}{\to} W(t)\,\text{and}\, \frac{1}{\sqrt{n}} \sum_{i=1}^k (\xi_j-1) - \frac{t}{n\sqrt{n}} \sum_{j=1}^{n+1} (\xi_j-1) \to W(t)-tW(1)\]

Since 
\[\frac{1}{n} z\sum_{j=1}^{n+1} \xi_j \overset{\Pr}{\to} z\]

Thus the theorem is proved.


\subsection{Extreme Value Distribution} % (fold)
\label{sub:extreme_value_distribution}

A sample $\brac{Y_k}_{k=1}^n$ of independent and identically distributed random variables with $F(x)$. The distribution of 
\[\Pr\brac{\max_{i=1}^n Y_i \leq x} = \prod_{i=1}^n \Pr\brac{Y_i \leq x} = F^n(x)\]

There are three types of limiting distributions of maxima (properly scaled and centred) \begin{description}
	\item[Fr\'ech\'et]\hfill\\
	\[ e^{-x^{-\alpha}} 1_{\clop{0,\infty}}(x) \]
	\item[Gumbel]\hfill\\
	\[ e^{-e^{-x}} 1_{\clop{0,\infty}}(x) \]
	\item[Weibull (Wallodi)]\hfill\\
	\[ e^{-{(-x)}^\alpha} 1_{\brac{-\infty, 0}}(x) \]
\end{description}


The normal distribution is 
\[F(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-\frac{u^2}{2}}du\]

recall the inversion method $Y_{(n)} = F^{-1}(U_{(n)})$.

Now using Pike's theorem
\[U_{(n)} = 1-\frac{ \xi_{n+1}}{\sum_{j=1}^{n+1} \xi_j}\]

Next
\[U_{(n)} = 1-\frac{ \xi_{n+1}}{n+1}\brac{1 + \frac{1}{n+1}\sum_{j=1}^{n+1} {(\xi_j-1)}}^{-1}\]

Using the CLT we get the following
\[U_{(n)} \overset{\Dcal}{\approx} 1-\frac{\xi_{n+1}}{n+1}\brac{1+o(\frac{1}{\sqrt{n+1}})}\]

So $1-U_{(n)}$ decays as $n\to \infty$, thus we apply the gaussian inversion to it.

Integrating the Gaussian distribution by parts yields
\[\int_x^\infty \frac{1}{u} de^{-\frac{u^2}{2}} =  \induc{ \frac{1}{u} e^{-\frac{u^2}{2}} }_x^\infty - \int_x^\infty \frac{1}{u^2} e^{-\frac{u^2}{2}}du\]

Then 
\[= - \frac{1}{x} e^{-\frac{x^2}{2}}
+ \int_x^\infty \frac{1}{u^3} de^{-\frac{u^2}{2}} 
= - e^{-\frac{x^2}{2}} \brac{\frac{1}{x} + o(\frac{1}{x^3})}\]

Thus
\[F(x) = 1 - \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \brac{\frac{1}{x} + o(\frac{1}{x^3})}\]

let's invert this for values of $y$ close to zero.

\[ x = F^{-1}(y) = \sqrt{ 2\log{\frac{1}{x}+o(\frac{1}{x^3})} + \log \frac{1}{1-y}\frac{1}{\sqrt{2\pi}}} = L_y(x) \]

Then 
\[L_y'(x) \leq - \frac{L_y(x)}{2x}\]

whence an approximate solution of the inversion is
\[ x\approx \sqrt{-\log{(1-y)2\sqrt{\pi}}}\]

Therefore
\[Y_{(n)} = F^{-1}(U_{(n)}) \approx F^{-1}\brac{1-\frac{\xi_{n+1}}{n+1}\brac{1+o(\frac{1}{\sqrt{n+1}})}}\]

whence 
\[Y_{(n)} \approx \sqrt{ \log \frac{n+1}{\xi_{n+1} \sqrt{2\pi} } } \]

Using the first approximation of $x$ into $L_y(x)$ we can obtain a more precise approximation to $x$ and thus the final distribution:

\[ Y_{(n)} \approx \sqrt{ \log \frac{n+1}{\xi_{n+1} \sqrt{2\pi} } - 2\log \log \frac{{(n+1)}^2}{\xi_{n+1}^2 \sqrt{2\pi} }  + o(1) }  \]

Setting 
\[m_n^2 \defn \log \frac{n+1}{\xi_{n+1} \sqrt{2\pi} } \]

we get 
\[Y_{(n)} \approx \sqrt{ m_n^2 - \frac{2\log \xi_{n+1}}{m_n^2} }\]

Using Taylor's expansion (? STRANGE)
\[Y_{(n)} \approx m_n\brac{ 1 - \frac{2\log \xi_{n+1}}{m_n} }\]

Thus 
\[\Pr\brac{ Y_{(n)} < z } \approx \Pr\brac{ m_n\brac{ 1 - \frac{2\log \xi_{n+1}}{m_n} } < z } \]


\textbf{CHECK THE CONSTANTS!!!!!}

Redoing this: the first approximation id $x$ is
\[x_0 \approx \sqrt{ \log\frac{1}{(1-y)\sqrt{2\pi}} }\]

Then 
\[x = \sqrt{ \log\frac{1}{(1-y)\sqrt{2\pi}} + \log\brac{ \frac{1}{x} + o(\frac{1}{x^3}) }}\]

Substituting $x_0$ instead of $x$ and neglecting the infinitesimal yields:
\[ x \approx \sqrt{ \log\frac{1}{(1-y)\sqrt{2\pi}} - \frac{1}{2} \log\log\frac{1}{(1-y)\sqrt{2\pi}} }\]

Then 
\[ Y_{(n)}^2 \approx \brac{
\log\frac{n+1}{\xi_{n+1}\sqrt{2\pi}}
- \frac{1}{2} \log\log\frac{n+1}{\xi_{n+1}\sqrt{2\pi} + o(1)}
}\]

So
\[ Y_{(n)} \approx \sqrt{ - \log \xi_{n+1} + \log\frac{n+1}{2\log{(n+1)} \sqrt{8\pi} + o(1) } }\]

Whence
\[ Y_{(n)} \approx \brac{-\log \xi_{n+1} + m_n + o(1) }^\frac{1}{2}\]

Thus using the Taylor once again
\[ Y_{(n)} \approx m_n^\frac{1}{2} \brac{-\frac{\log \xi_{n+1}}{m_n} + 1 }\]

Next 
\[ \sqrt{m_n} Y_{(n)} - m_n\approx - \log \xi_{n+1}\]

Thus
\[ \Pr\brac{\sqrt{m_n} \brac{Y_{(n)} - \sqrt{m_n}}} \approx \Pr\brac{- \log \xi_{n+1} < z }\]

So
\[\Pr\brac{- \log \xi_{n+1} < z } = \Pr\brac{\xi_{n+1} > e^{-z} } = e^{-e^{-z}}\]

And $m_n\approx \sqrt{ 2 \log n}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Laplace distribution
\[F(x) = 1-e^{-\frac{x}{2}} 1_{x\geq 0} + e^{\frac{x}{2}} 1_{x\leq 0}\]

The inverse is given for $y\in \clo{\frac{1}{2},1}$ by
\[ F^{-1}(y) = - \log 2(1-y)\]

Since $\max_{i=1}^n Y_i \sim \log\frac{n+1}{2\xi_{n+1}}$ 
we have
\[\Pr\brac{ \log\frac{n+1}{2} - \log \xi_{n+1} < z } \approx e^{-e^{-z}}\]

% subsection extreme_value_distribution (end)

% section lecture_7 (end)

\section{LEcture \# 8} % (fold)
\label{sec:lecture_8}

\subsection{Discrete extreme value theory} % (fold)
\label{sub:discrete_extreme_value_theory}
Suppose we have a collection of $\brac{X_k}_{k=1^n}$ independent and identically distributed with values $\brac{1, -1}$ with probabilities $\brac{\frac{1}{2},\frac{1}{2}}$.

Define the cumulative sum $S_n = \sum_{k=1}^n X_k$ -- this is a discrete 1-D random walk.

According to the Law of large numbers the mean displacement 
\[\frac{S_n}{n} \overset{\Pr}{\to} 0\]

An extreme value is the defined by the event $E_\alpha \defn \obj{ \sfrac{S_n}{n} = \alpha } = \obj{ S_n = n\alpha }$ for $\alpha\in \clo{-1,1}$.

Fluctuations around $0$ have of the order $\sqrt{n}$, that is why a large deviation is defined as $n\alpha$.

The number of upward and downward steps is equal to $N_+ = \frac{1+\alpha}{2}n$ and $N_- = \frac{1-\alpha}{n}$ respectively. With $S-n = N_+-N_-$

\subsubsection{Lazy random walk} % (fold)
\label{ssub:lazy_random_walk}

Suppose $X_k = 0$ with positive probability $p_0$, while $X_k = 1$ with $p_+$ and $X_k=-1$ with $p_-$. 

Then $\ex \frac{S_n}{n} = p_+ - p_-$, where $S_n = \frac{N_+-N_-}{n}$ with $N_++N_0+N_- = n$ and $N_i$ is the number of steps in the direction $i$.

Even the most improbable event occurs with some highly probable chances.

% subsubsection lazy_random_walk (end)

\subsubsection{General theory} % (fold)
\label{ssub:general_theory}

Suppose $\Acal$ is some finite alphabet with $\abs{\Acal}$.

Consider a probability distribution $P = \brac{P_a}_{a\in \Acal}$ on $\Acal$.

Consider a string $s\in \Acal^n$ and define $n_a(s)$  as the $\abs{\obj{\induc{i=1,\ldots,n} s_i = a }}$. Then $\sum_{a\in\Acal} n_a(s) = n$.

Suppose the symbols in $s$ are independent and identically distributed random variables on $\Acal$ with probability distribution $P$.

Consider the probability space $\Acal^n$ with the product probability measure $\Pr(s) = \prod_{a\in \Acal} p_a^{n_a(s)}$.
Use the discrete $\sigma$-algebra.

One-point distribution of one symbol on $k=1,\ldots,n$ (as opposed to distributions of finite vectors). 

Is the random process is stationary, then the probability of encountering a symbol $a\in \Acal$ is independent of is place in $s$.

How does the typical and atypical string look like. Typicality is defined as the ratio of occurrences of a particular symbol.

The number of strings grows exponentially with $n$.

A characteristic vector of string $s\in\Acal^n$ is the vector $n = \brac{n_a(s)}_{a\in \Acal}$.

The number of string with a given characteristic vector $m = \brac{m_a}_{a\in \Acal}$ is given by 
\[\Pr(\vec{m}) \defn \frac{n!}{\prod_{a\in \Acal} m_a!}\] 

The number of different characteristic vectors is given by 
\[\frac{(n+\abs{\Acal}-1)!}{(\abs{\Acal}-1)!n!} = \frac{(n+1)\ldots (n+\abs{\Acal}-1)}{(\abs{\Acal}-1)!}\]
the number of combinations of size $n$ of $k$ symbols with repetition.
Thus the number of possible characteristic types is polynomial in the size of the string. One can draw a conclusion that the complexity of strings grows slower than the size of the string.

Consider a characteristic vector $n$, then the probability of a string of this type is \[\Pr\Big(\Big. s \big.\big\rvert \vec{m}\Big.\Big) = \prod_{a\in \Acal} p_a^{n_a}\]
and the number of strings of the type is $\Pr(\vec{m})$
Thus the chance of a particular string is
\[\Pr(s) = \Pr\Big(\Big. s \big.\big\rvert \vec{m}\Big.\Big) \Pr(\vec{m}) = \prod_{a\in \Acal} p_a^{n_a}\]

Since the Stirling's formula is $n! = n^n e^{-n} \sqrt{2 \pi n} \big(1+o(\frac{1}{n})\big)$, we have
\begin{align*}
	\frac{n!}{\prod_{a\in \Acal} m_a!} \prod_{a\in \Acal} p_a^{n_a}
	& \approx \frac{n^n e^{-n} \sqrt{2 \pi n} }{\prod_{a\in \Acal} m_a^{m_a} e^{-m_a} \sqrt{2 \pi m_a} } \prod_{a\in \Acal} p_a^{n_a} \\
	& \approx \prod_{a\in \Acal} \brac{p_a \frac{n}{m_a}}^m_a \\
	& = \text{exp}\brac{ n \big( \sum_{a\in \Acal} \eta_a \ln\frac{p_a}{\eta_a} \big) } = e^{-n\text{KL}\big( \eta \lvert \rvert P\big)}
\end{align*}

where $\eta_a = \frac{m_a}{n}$. 

The relative entropy, or Kullback-Leibler entropy, describes the differences in distributions $\eta$ with respect to $P$:
\[\text{KL}\big( \eta \lvert \rvert P\big) = \sum_{a\in \Acal} \eta_a \ln \frac{\eta_a}{p_a} = \sum_{a\in \Acal} p_a \frac{\eta_a}{p_a} \ln \frac{\eta_a}{p_a} \]
Where $\frac{\eta_a}{p_a}$ is the Radon-Nikodym derivative of $\eta$ relative to $p$.

For two measures $Q,P$ on the same space, if $Q<<P$ then
\[\text{KL}\big( Q \lvert \rvert P\big) = \int \frac{dQ}{dP} \ln \frac{dQ}{dP} dP\]
where $\frac{dQ}{dP}$ is the Radon-Nikodym derivative of $Q$ with respect to $P$ :
\[Q(E) = \int_E \frac{dQ}{dP} dP\]

The Radon-Nikodym derivative of $Q$ with respect to $P$ when $Q,P<<dx$ and $Q<<P$ is
\[\int_E \frac{q}{p} dP = \int_E \frac{q}{p} p dx = \int_E q dx = \int_E dQ = \int_E \frac{dQ}{dP} dP\]
whence $\frac{dQ}{dP} = \frac{q}{p}$ $P$-almost surely. (and thus $dx$-almost surely).

So far we have got the following result
\begin{align*}
	\Pr\brac{\brac{n \eta_a}_{a\in \Acal}} &\approx e^{- n \text{KL}\brac{\eta \lvert\rvert p}}
\end{align*}

If $\Phi(x)$ is a convex function, then for all $x,x_0\in \Real^d$ the following is true
\[ \Phi(x) \geq \Phi(x_0) + \nabla \Phi(x_0) \big(x-x_0\big) \]

If $\Phi(x) = \sum_{k=1}^d x_k \ln x_k$ for $x\in \Real_+^d$, then $\Phi(x)$ is convex.

Around $\brac{p_a}_{a\in \Acal}$:
\[\sum_{a\in \Acal} p_a \ln p_a + \sum_{a\in \Acal} \big(1 + \ln p_a \big) \big(x_a - p_a\big)\]
whence $\sum_{a\in \Acal} x_a \ln\frac{x_a}{p_a} \geq 0$.

% The distribution of possible probability vectors $\brac{p_a}_{a\in \Acal}$ is given by 
% \[\text{Dir}\brac{\brac{p_a}_{a\in \Acal}} = \frac{\prod_{a\in \Acal} \Gamma(\alpha_a)}{\Gamma\brac{\sum_{a\in \Acal} \alpha_a }}\prod_{a\in \Acal} p_a^{\alpha_a-1}\]

Let's refine the approximation above.

Consider the 
\[-\frac{1}{n}\ln\Pr\brac{\brac{n \eta_a}_{a\in \Acal}} \overset{n\to\infty}{\to} \text{KL}\brac{\eta \lvert\rvert p} \]

\noindent\textbf{Sanov's theorem}\hfill\\
Suppose $A$ is a closed subset of a probability simplex $S_\Acal$. Then 
\[-\frac{1}{n}\ln \Pr\brac{A}\to \min_{\eta\in A} \text{KL}\brac{\eta \lvert\rvert p} \]
and inside $A$ the conditional probability concentrates around the points where the minimum is achieved.

% subsubsection general_theory (end)

% subsection discrete_extreme_value_theory (end)

\subsection{Continuous extreme values theory} % (fold)
\label{sub:continuous_extreme_values_theory}

\subsubsection{Extreme values: Cram\'er's theorem} % (fold)
\label{ssub:extreme_values_cramer_theorem}

One of the earliest theorems on extreme values (1938).

Is investinages the distribution of the normalised sum of independent and identically distributed random variables $\brac{X_k}_{k=1}^n$.

Again the law of large numbers tells us that
\[\frac{\sum_{k=1}^n X_k}{n} \to \ex X = \mu\]

A large deviation is the following event :
\[\obj{ \sum_{k=1}^n X_k - n\mu \neq 0 }\]

The intuition suggests that the following asymptotics holds
\[\Pr\brac{a < \bar{X}_n < b} \sim e^{-n \rho(a,b)}\] as $n \to \infty$ where $\bar{X}_n = \frac{\sum_{k=1}^n X_k}{n}$.

Let's derive heuristically a general form of $\rho$.

If one has a sub-additive sequence, them there is one general trick in approaching the limit.

We want to show that 
\[-\frac{1}{n}\ln \Pr(a<\bar{X}_n<b) \overset{n\to\infty}{\to} \rho(a,b)\]

Observe that for $n,m\geq 1$
\[-\ln \Pr(a<\bar{X}_{n+m}<b) \leq - \ln \Pr(a<\bar{X}_n<b) - \ln \Pr(a<\bar{X}_m<b)\]

Indeed, the sequence of these logarithms is subadditive.
For this event it is true that
\[a<\bar{X}_{n+m}<b \Leftrightarrow a<\frac{n \bar{X}_n + m \bar{X}_m}{n+m}<b \]
and this is true when both $a<\bar{X}_n<b$ and $a<\bar{X}_m<b$ due to convex linear combination.

We have 
\[\Pr\brac{a<\bar{X}_m<b} = \Pr\brac{a<\frac{\sum_{k=n+1}^{n+m}X_k}{m}<b}\]
since $X_k$ are identically distributed.

Thus 
\[\Pr\brac{a<\bar{X}_{n+m}<b} \geq \Pr\brac{a<\bar{X}_n<b \cup a<\bar{X}_m<b} = \Pr\brac{a<\bar{X}_n<b}\Pr\brac{a<\bar{X}_m<b} \]

whence 
\[\ln \Pr\brac{a<\bar{X}_{n+m}<b} \geq \ln \Pr\brac{a<\bar{X}_n<b} + \ln\Pr\brac{a<\bar{X}_m<b} \]

and
\[-\ln \Pr\brac{a<\bar{X}_{n+m}<b} \leq -\ln \Pr\brac{a<\bar{X}_n<b}  - \ln\Pr\brac{a<\bar{X}_m<b} \]

Suppose $\brac{S_k}_{k\geq1}$ is a subadditive sequence, i.e $S_{n+m}\leq S_n + S_m$, then there exist $\lim_{n\to \infty} \frac{S_n}{n}$.

Indeed, fix some arbitrary $n_0\geq1$. Then for any $n > n_0$ it is true that $n = q n_0 + r$.  Hence 
\[S_n \leq S_{n-n_0} + S_{n_0}\leq \ldots \leq S_{n-q n_0} + q S_{n_0} = S_r + q S_{n_0}\]

Therefore 
\[S_n \leq \frac{S_r}{n} + \frac{q}{q n_0 + r}S_{n_0}\leq \frac{S_r}{n} + \frac{S_{n_0}}{n_0}\]

thus
\[\bar{S} = \limsup_{n\to \infty} \frac{S_n}{n} \leq \limsup_{n\to \infty} \frac{S_r}{n} + \frac{S_{n_0}}{n_0} = \frac{S_{n_0}}{n_0}\]

Therefore $\bar{S}\leq \frac{S_{n_0}}{n_0}$. Since $n_0$ is arbitrary, 
\[\liminf_{n_0\to\infty} \frac{S_{n_0}}{n_0} \geq \bar{S}\]

But $\frac{S_{n_0}}{n_0}$ is the same sequence as $\frac{S_n}{n}$! Now since $\liminf a_n \geq \limsup a_n$ for any sequence $a_n$, it must be true that
\[\bar{S}\leq \liminf_{n\to\infty} \frac{S_n}{n} \leq \limsup_{n\to\infty} \frac{S_n}{n} \leq \bar{S}\]

Given the above observation we have the existence
\[-\frac{1}{n}\ln \Pr\brac{a<\bar{X}_n<b} \to \rho(a,b)\]

We want not only a result concerning the density of the limiting distribution of the mean, which by the LLN is \[p_{\bar{X}_n}(x)\to \delta(x-\mu)\]
but also some speed of convergence:
\[p_{\bar{X}_n}(x)\sim e^{-n \rho^*(x)}\]

\[ -\frac{1}{n} \ln \int dx \]

Then 
\[-\frac{1}{n}\ln \Pr\brac{a<\bar{X}_n<b} \to \min_{x\in (a,b)} \rho^*(x)\]

The characteristic function is
\[\phi_{\bar{X}_n}(s) = \brac{\phi_X\big(\frac{s}{n}\big)}^n = e^{n\eta(\frac{s}{n})}\]

Now the inverse Fourier transform is 
\[p_{\bar{X}_n}(x) = \frac{1}{2\pi}\int e^{-isx } \phi_{\bar{X}_n}(s) ds = \frac{1}{2\pi}\int e^{-isx + n \eta(\frac{s}{n})} ds\]

whence
\[p_{\bar{X}_n}(x) = \frac{n}{2\pi}\int e^{n\brac{-iux + \eta(u)}} du\]

\noindent \textbf{Saddle point method} \hfill\\
Suppose we want to integrate 
\[\int_\Real e^{n\Phi(u)} du\]
where $\Phi$ is some analytic function.

Now, obviously 
\[\Phi(u) = \Re \Phi(u) + i \Im\Phi(u)\]
with the imaginary part governing its phase shift, while the real part influencing the amplitude.

If $\Im\Phi(u)$ varies a lot along a contour, then due to high oscillations they will contribute a small value to the integral.

Thus we want to deform the contour, so that the oscillations are insignificant.

Thus we must deform the contour so that it passed through the point $u^*$ of $\frac{d}{d z}\Phi(z) = 0$. The real and imaginary parts are harmonic functions due to Cauchy-Riemann conditions.

The imaginary part of $\Phi(z)$ must be locally constant.

Using thus deformation
\[\int_\Real e^{n\Phi(u)} du \approx \int_{\epsilon(u^*)} e^{n\brac{\Phi(u^*) + \frac{1}{2}\Phi''(u^*) \brac{u-u^*}^2}} du\]

whence 
\[\approx e^{n\Phi(u^*)} \int_{\epsilon(u^*)} e^{\frac{n}{2}\Phi''(u^*) {(u-u^*)}^2} du \approx e^{n\Phi(u^*)} \sqrt{\frac{2\pi}{n\abs{\Phi''(u^*)}}}\brac{1 + O(\frac{1}{n})}\]

Deformation $v\in \Cplx$ : $u = u^* + t v$.

\noindent \textbf{Getting back to Cram\'er} \hfill\\

Suppose the Laplace transform of the distribution of iid $X_k$ is given by
\[e^{\kappa(s)} = \int_{-\infty}^{+\infty} e^{sx} p(x) dx\]
exists for $s$ in some interval $\brac{\alpha,\beta}$.

And $p(x) \sim e^{-\beta x}$ for $x\to \beta-$, $p(x) \sim e^{-\alpha x}$ for $x\to \alpha+$. This condition is stronger than what is required for the LLN.

We need the asymptotics of
\[p_{\bar{X}_n}(x) = \frac{n}{2\pi}\int e^{n\brac{-iux + \eta(u)}} du\]
Note that $\kappa(is) = \eta(s)$ for all $\Re s \in \brac{\alpha, \beta}$.

Thus $\eta(s)$ is defined when $\Im s\in \brac{-\beta, -\alpha}$ and the integral is analytic.

The largest value of $\Re \eta(s)$ on $\Im s = \xi \in \brac{-\beta, -\alpha}$ is attained when $\Re s = 0$.

Then $\phi(u) = e^{\eta(u)}$ implies that $\abs{\phi(u)} = e^{\Re \eta(u)}$. However 
\[\abs{\phi(u)} = \abs{\int_\Real e^{iux} p(x) dx} = e^{-\Im u} \abs{ \int_\Real e^{i\Re u x} p(x) dx } \leq e^{-\Im u} 1 \]

Second observation: $\kappa(s)$ is convex and $\lim_{s\to \alpha+} \kappa(s) = \lim_{s\to \beta-} \kappa(s) = +\infty$.

Indeed, 
\begin{align*}
	1 &= \int e^{sx - \kappa(s)} p(x) dx \\
	e^{\kappa(s)} \kappa'(s) &= \int xe^{sx} p(x) dx \\
	\kappa'(s) &= \int xe^{sx - \kappa(s)} p(x) dx = \ex_s X
\end{align*}

Where $\ex_s$ is the expectation with respect to a distribution with density $e^{sx - \kappa(s)} p(x)$.

Then
\begin{align*}
	e^{\kappa(s)} \kappa'(s) &= \int xe^{sx} p(x) dx \\
	e^{\kappa(s)} \kappa''(s) + e^{\kappa(s)} \brac{\kappa'(s)}^2 &= \int x^2 e^{sx} p(x) dx \\
	\kappa''(s) + \brac{\kappa'(s)}^2 &= \int x^2 e^{sx-\kappa(s)} p(x) dx \\
	\kappa''(s) + \brac{\kappa'(s)}^2 &= \ex_s X^2 \\
	\kappa''(s) &= \ex_s X^2 - \brac{\ex_s X}^2\geq 0
\end{align*}

Therefore $\kappa(s)$ can be described as an envelope of tangents:
\[ \kappa(s) = \max_{y} \brac{sy - \kappa^*(y)} \]
where $\kappa^*(y)$ is the Legendre's transform of $\kappa(s)$:
\[ \kappa^*(x) = \max_{s} \brac{sx - \kappa(y)} \]

Recall 
\[\Phi(u) = - iux +\eta(u)\]

From the first observation it follows that $u^* = -is^*$, where
\[s^* = \min_{s}\Phi(-is) = \min_{s} -sx + \kappa(s) = - \kappa^*(x)\]

Therefore we get the following asymptotics
\[p_{\bar{X}_n}(x) = e^{-n\kappa^*(x)} \sqrt{\frac{n}{2\pi \abs{\kappa''(s^*)}}}\brac{1+O(\frac{1}{n})}\]

where $\kappa^*(x)$ is the Legendre transform of the logarithm of the Laplace transform of the original density.

% subsubsection extreme_values_cramer_theorem (end)

% subsection continuous_extreme_values_theory (end)

% section lecture_8 (end)

\section{Lecture \# 9} % (fold)
\label{sec:lecture_9}

Fractals and the multifractal formalism reside at the intersection of the measure theory and geometry. In 60s it it was discovered that there are fractal (self-similar) models in physics.

There are different definitions of fractal dimensions.
Dimensions in the topological sense is different from some intuitively similar dimensionality measure.

Consider a fyord. What is its coastline length? Given different resolution, the result will converge to the true length.

\subsection{Koch's snowflake} % (fold)
\label{sub:koch_s_snowflake}

Is an iterative process which on the $n+1$-th step adds an equilateral triangle at the middle third of each edge of the flake at the $n$-th integration.

\begin{tabular}{c|c|c}
$n$ & \# sides & perimeter \\
$0$ & $3$ & $3$ \\
$1$ & $3\cdot 4$ & $3 \cdot 4 \frac{1}{3}$ \\
$2$ & $3\cdot 4^2$ & $3 \cdot 4^2 \frac{1}{3^2}$ \\
$\vdots$ & $\cdots$ & $\cdots$\\
$k$ & $3 \cdot 4^k$ & $3 \cdot 4^k \frac{1}{3^k} = \frac{4^k}{3^{k-1}}$
\end{tabular}

% subsection koch_s_snowflake (end)

How could one define a value for dimensionality of a set geometrically?

Splitting a region into a finite number and counting how to reconstruct it won't do.


Take a bounded line segment and cover it with a covering of some $\epsilon$ templates, $\epsilon$-balls, for example. This would require a certain finte amount of templates $N_\epsilon$. Then $N_\epsilon\propto \epsilon^{-1}$.

For a bounded planar region, we have $S_\epsilon\propto \epsilon^{-2}$.

Consider a metric space $(X,d)$ and a bounded set $M\subseteq X$: the diameter of a set is finite. The box dimensionality of a set is
\[d_B(M)\defn -\lim_{\epsilon\to 0} \frac{\ln N_\epsilon(M)}{\ln \epsilon}\]
where $N_\epsilon$ is the exact lower bound of the number of templates across all coverings of $M$ with templates of size $\epsilon$. Templates in a metric space are closed $\epsilon$-balls.

The Box dimensionality of the Koch's snowflake is
\begin{align*}
	N_\epsilon &= 3 4^n = 3 4^\frac{-\ln \epsilon}{\ln 3}\\
	d_B &= -\lim_{\epsilon\to 0} \frac{\ln 3 - \frac{\ln \epsilon}{\ln 3} \ln 4}{\ln \epsilon} = \frac{\ln 4}{\ln 3} > 1
\end{align*}
where the standard $\epsilon$-ball template was used.

But the box dimensionality is imperfect. Is there a better one?

\subsection{Hausd\"orf dimensionality} % (fold)
\label{sub:hausdorf_dimensionality}

Consider a unit interval $\clo{0,1}$ and a set 
\[M \defn \obj{ \induc{ n^{-\alpha} } n\geq 1}\]
for some $\alpha>0$.

Let's pick a standard template and some optimal $\epsilon$ cover.

Cover the set thusly : \begin{itemize}
	\item if the points spaced farther than $\epsilon>0$ apart should be covered with a separate $\epsilon$-ball;
	\item $\bar{n}_\epsilon$ points clustering near zero are covered by a single common $\epsilon$-ball.
\end{itemize}
The estimate is $\bar{n}_\epsilon ^ {-\alpha} < \epsilon$, and then $\bar{n}_\epsilon = O\big(\epsilon^\frac{1}{\alpha}\big)$. Thus $N_\epsilon = C\epsilon^\frac{1}{\alpha}$.

A better estimate is 
\[\frac{1}{n_*^\alpha} - \frac{1}{(n_*+1)^\alpha}\sim \frac{\alpha}{n_*^{\alpha+1}} < \epsilon\]
whence $n_*\sim \epsilon^\frac{-1}{\alpha+1}$ and $N_\epsilon > O(\epsilon^\frac{-1}{\alpha+1})$.

This implies that $d_B(M) \geq \frac{1}{\alpha+1}>0$.
However a countable set of points clustering at zero seems to have $0$-dimensionality.

\noindent\textbf{Definition}\hfill\\

Consider a bounded set $M\subseteq \Real^n$. Then for $\alpha>0$ the Hausd\"orf dimensionality is 
\[H_\alpha(M) = \lim_{\epsilon\downarrow 0} \inf_{\text{cover}} \sum_{k} \text{diam}(S_k)^\alpha\]
The limit inferior is taken over all finite or countable covers $S_k$ such that $S_k$ is an arbitrary subset with diameter less that $\epsilon$, where \[\text{diam}(A) \defn \sup_{x,y\in A} d(x,y)\]

Any procedure of measuring a two-dimensional set by a one dimensional measure should yield $\infty$.

Bijection approach.
A bijection must honour some geometric structure (a diffeomorphism -- a deformation when not only things do not get torn apart but also do not become broken by wild bending).

It is necessary to preserve some kind Lipschitz condition on the space (?).

The Hausd\"orf measure is additive. It is necessary to check that
the infimums are additive when the sets are disjoint.

If $\alpha<\beta$ then $H_\alpha(M) \geq H_\beta(M)$.
Indeed, consider some covering $\brac{S_k}$. Then 
\[\sum_k \text{diam}(S_k)^\alpha \geq \sum_k \text{diam}(S_k)^\beta\]

If $H_\alpha(M) > 0$ for some $\alpha$ then $H_\beta(M) = +\infty$ for all $\beta<\alpha$.
If $H_\alpha(M) < \infty$ for some $\alpha$ then $H_\beta(M) = 0$ for all $\alpha<\beta$.

Suppose $H_\alpha(M) < +\infty$, then there is an $\epsilon>0$ and a cover $S_k$ such that 
\[\sum_k \text{diam}(S_k)^\alpha < \infty\]

\[\text{diam}(S_k)^\beta \leq \epsilon^{\beta-\alpha} \text{diam}(S_k)^\alpha \]
since $\text{diam}(S_k)^\beta = \text{diam}(S_k)^{\beta-\alpha}\text{diam}(S_k)^\alpha$.
Thus
\begin{align*}
	\sum_k \text{diam}(S_k)^\beta &\leq \epsilon^{\beta - \alpha} \sum_k \text{diam}(S_k)^\alpha\\
	\inf \sum_k \text{diam}(S_k)^\beta &\leq \epsilon^{\beta - \alpha} \sum_k \text{diam}(S_k)^\alpha\\
	\inf \sum_k \text{diam}(S_k)^\beta &\leq \epsilon^{\beta - \alpha} \inf \sum_k \text{diam}(S_k)^\alpha\\
	H_\beta(M) &\leq \epsilon^{\beta - \alpha} H_\alpha(M)
\end{align*}

Thus there exists a unique value of $\alpha$ such that $H_\alpha(M) \in (0,\infty)$. Put 
\[\alpha_0(M) = \sup_{\beta>0} \obj{ H_\beta(M) = +\infty }\]
or alternatively
\[\alpha_0(M) = \inf_{\beta>0} \obj{ H_\beta(M) = 0 }\]

\noindent\textbf{Definition}\hfill\\
Thus the Hausd\"orf dimensionality is defined as 
\[d_H(M) \defn \alpha_0(M)\]

In fact $H_\alpha$ defines a measure on Borel sets of $(X, d)$. It coincides with the Lebesgue measure $dx^n$ if $\alpha = n$.

% subsection hausdorf_dimensionality (end)

% section lecture_9 (end)

\section{Lecture \# 10} % (fold)
\label{sec:lecture_10}

Consider a sample $\brac{X_k}_{k=1}^n$ of iid random variables distributed according to $F(x)$. Let $X_{(n)} \defn \max_{k=1,\ldots,n} X_k$. Using the inversion method the uniform variables are defined as $X_k = F^{-1}(U_k)$. Thus $\brac{U_k}_{k=1}^n\sim \mathcal{U}\clo{0,1}$.

Using Pike's theorem
\[U_{(n)} = 1 - \frac{e_{n+1}}{n+1}\brac{1+O(\frac{1}{\sqrt{n+1}})}\] with $e_{n+1}\sim\text{exp}(1)$.

Then we found $F^{-1}(1-z)$ when $z\to 0$.

Cauchy distribution:
\begin{align*}
	p_X(x) &= \frac{1}{\pi(1+x^2)} \\
	F(x) &= \frac{1}{2} + \frac{1}{\pi} \text{arctg}(x) \\
	F^{-1}(y) &= \text{tg}\big(\pi (y-\frac{1}{2}) \big) \\
\end{align*}

Now \begin{align*}
	F^{-1}(1-z)
	& = \text{tg}\big(\pi (\frac{1}{2}-z) \big)\\
	& = \frac{\sin(\frac{\pi}{2} - \pi z)}{\cos(\frac{\pi}{2} - \pi z)}\\
	& = \frac{\cos(\pi z)}{\sin(\pi z)} \sim \frac{1}{\pi z}
\end{align*}
thus 
\[X_{(n)} = F^{-1}\big( 1 - (1-U_{(n)}) \big) = \approx \frac{n+1}{\pi e_{n+1}}\]

Thus 
\[\lim_{n\to \infty}\Pr\big( \frac{\pi}{n}\max_{k=1,\ldots,n}X_k < x \big) = \Pr\big(\frac{1}{e_{n+1}} < x\big) = e^{-\frac{1}{x}} \approx 1-\frac{1}{x}\]

Further consider
\[S_n = \frac{1}{n}\sum_{k=1}X_k\]
then using the characteristic function we get $F_{S_n}(x) \sim \text{Cauchy}$, whence the sample mean value of Cauchy distributed random variates is dominated by the maximum value.

\subsection{Subgaussain random variables} % (fold)
\label{sub:subgaussain_random_variables}


Consider $\brac{Y_i}_{i=1}^n$  independent with $\ex Y_i = 0$ and $\ex Y_i^2 = 1$, then
\[\Pr\brac{ \frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i > x } \to \frac{1}{\sqrt{2\pi}} \int_x^\infty e^{-\frac{u^2}{2}}du \]

However
\begin{align*}
	\frac{1}{\sqrt{2\pi}} \int_x^\infty e^{-\frac{u^2}{2}}du = \frac{1}{\sqrt{2\pi}} \int_0^\infty e^{-\frac{(u+x)^2}{2}}du \leq \frac{e^{-\frac{x^2}{2}}}{2}
\end{align*}
for some $n\geq n_0(x)$.

Consider independent $\brac{Y_i}_{i=1}^n$ such that they are subgaussian: $\exists \sigma^2 > 0$ with \[\ex\big( e^{\lambda Y_i} \big) \leq \text{exp}\big( \frac{\sigma^2 \lambda^2}{2} \big)\]
then 
\[\Pr\brac{ \frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i > x }\leq e^{-\frac{x^2}{2}}\]

Indeed using Chernoff's inequality
\[\Pr\brac{\xi>\epsilon}\leq \inf_{\lambda>0} \ex\exp\brac{\lambda\xi - \lambda\epsilon}\]

we get
\[\Pr\brac{\frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i>\epsilon}\leq \inf_{\lambda>0} \exp\brac{ - \lambda\epsilon} \ex\exp\brac{\lambda\frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i}\]

since $Y_i$ is subgaussian, we have by the independence
\[\ex\exp\brac{\lambda\frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i} = 
\prod_{i=1}^n \ex\exp\brac{\frac{\lambda}{\sqrt{n}} Y_i} \leq 
\text{exp}\big( n \frac{\sigma^2 \lambda^2}{2n} \big)
\]

whence, minimizing
\[\inf_{\lambda>0} \ex\exp\brac{\lambda\frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i} \leq \ldots\]
and thus $\lambda = \frac{x}{\sigma^2}$

\noindent\textbf{H\"oefding inequality}\hfill\\
Consider a random variable $\xi$ with $\ex \xi = 0$ and $\xi\ni\clo{a,b}$ almost surely.
\[\ex\brac{\lambda \xi}\leq \exp\big( \frac{\lambda^2(b-a)^2}{8} \big)\]

Indeed, put
\[\phi(\lambda) = \log \ex\big(\lambda \xi\big) = \log \int_a^b e^{\lambda x} p(x) dx\]

Differentiating with respect to $\lambda$:
\[\phi'(\lambda) = \frac{\int_a^b x e^{\lambda x} p(x) dx}{\int_a^b e^{\lambda x} p(x) dx}\]

and 
\[\phi''(\lambda) = \frac{\int_a^b x^2 e^{\lambda x} p(x) dx}{\int_a^b e^{\lambda x} p(x) dx} - \bigg( \frac{\int_a^b x e^{\lambda x} p(x) dx}{\int_a^b e^{\lambda x} p(x) dx} \bigg)^2\]

Notice that
\[f(x) = \frac{\int_a^b e^{\lambda x} p(x) dx}{\int_a^b e^{\lambda x} p(x) dx}\]

is a density function of a random variable $\eta\in \clo{a,b}$. Furthermore $\ex \eta = \phi'(\lambda)$ and $\Var \eta^2 = \phi''(\lambda)$. The second moment is given by
\[\ex \eta^2 = \phi''(\lambda) + \big(\phi'(\lambda)\big)^2\]

Now
\[\bigg(\eta - \frac{b-a}{2}\bigg)^2 \leq \bigg(\frac{b-a}{2}\bigg)^2\]

whence 
\[\phi''(\lambda) \leq \bigg(\frac{b-a}{2}\bigg)^2 \]

integrating $\phi''$ twice over $\clo{0,\lambda}$ yields
\[\phi(\lambda) \leq \frac{\lambda^2}{2}\bigg(\frac{b-a}{2}\bigg)^2\]

which implies that $\xi$ is subgaussian.

% subsection subgaussain_random_variables (end)

\subsection{Estimation} % (fold)
\label{sub:estimation}

Consider $\brac{\xi_i}_{i=1}^n$ iid, and let $Y_i = \mu + \xi_i$, $\mu\in \Real$ -- unknown. Let $p_\xi(x)$ be the density function of $\xi$.

The joint density of observed $\brac{Y_i}_{k=1}^n$ is 
\[\prod_{i=1}^n p_\xi(y_i - \mu)\]

Suppose $p_0(x)$ is the true unknown density of $Y_i$ which we wish to approximate with a parametric family of distributions.

The empirical CDF is given by
\[\hat{F}_n(y) = \frac{1}{n}\sum_{k=1}^n 1_{\ploc{-\infty, y}}(y_i)\]

use a generalised function to compute a generalised derivative of the simple step function $\hat{F}_n$:
\[ \hat{p}(y) = \frac{1}{n} \sum_{k=1}^n \delta_y(y_i) \]

% a linera operator over the space of functions with finite support

The $\delta$ method, the method of moments
\[M(p_0) = \int_{-\infty}^\infty x p_0(x) dx\]

In general we wish to estimate some $\Phi(p_0)$. With this method we do this by
\[\hat{\Phi}\big((y_i)_{i=1}^n\big) = \Phi\big(\hat{p}(\cdot)\big)\]

Examples
\begin{itemize}
	\item $M(p_0)$: $\hat{M} = \frac{1}{n}\sum_{k=1}^n y_k$;
	\item $D(p_0)$: $\hat{D} = \frac{1}{n}\sum_{k=1}^n y_k^2 - \big(\frac{1}{n}\sum_{k=1}^n y_k\big)^2$;
	\item $\phi(p_0) = \ex_{p_0} e^{is\xi}$: $\hat{\phi} = \frac{1}{n}\sum_{k=1}^n e^{isy_k}$;
	\item $Q_\alpha(p_0)$ -- quantile: $\hat{Q}_\alpha$ is defined as $\abs{\obj{\induc{k=1,\ldots,n} y_k \leq \hat{Q}_\alpha}} = \floor{n\alpha}$.
\end{itemize}

In order to understand how well $p_0$ is approximated, we need a metric on the density functions $d$. With this metric we find
\[\theta^*(p_0) \defn \text{argmin}_{\theta \in\Theta} d\Big(p_0, p(\cdot,\theta)\Big) \]

The list of possible metrics: \begin{description}
	\item[Covariance distance]\hfill \\
	\[D_c(f,g)\defn \sup_{A\in \borel{\Real}} \abs{\int_A f dx - \int_A g dx}\]
	it is not employed much in estimation, since even using Scheffe's theorem, it is hard to use.
	For the empirical density we have
	\[D_c(\hat{p}, p(\cdot,\theta)) = \frac{1}{2} \int \abs{\hat{p} - p(\cdot,\theta)}dx = \frac{1}{2}\]
	\item[Kullback-Leibler]\hfill \\
	\[D_k(f,g) = \int f \log \frac{g}{f} dx\]
	it is used even though it is not in fact a metric due to asymmetry.
	It is easily calculated on the observed empirical data.
	Suppose $Y-i$ are iid with $p_0$ -- the true density, and let $p_1$ be an estimate, then $D_k(p_0^n, p_1^n) = nD_k(p_0,p_1)$.
\end{description}

\noindent\textbf{Scheffe's theorem}:
\[D_c(f,g) = \frac{1}{2} = \int \abs{f-g}dx\]
Indeed suppose $A_0 = \obj{ f\leq g }$. Then
\[D_c(f,g)\geq \int_{A_0} f-g dx\]
However (correction is needed.)
\[\int \abs{f-g} dx = \int_{A_0} f-g dx - \int_{A_0^c} g-f dx = \int_{A_0} f-g dx  - \int_{A_0} -f+g dx = 2\int_{A_0} f-g dx \]
Thus 
\[D_c(f,g)\leq \frac{1}{2} \int \abs{f-g} dx \]

Conversely for any measurable $A$ we have $A = (A\cap A_0) \cup (A\cap A_0^c)$, whence
\[\abs{\int_A f-g dx} = \abs{\int_{A\cap A_0} f-g dx + \int_{A\cap A_0^c} f-g dx} = \abs{\int_{A\cap A_0} f-g dx - \int_{A\cap A_0^c} g-f dx}\]

thus 
\[\abs{\int_A f-g dx} \leq \max\obj{ \int_{A\cap A_0} f-g dx, \int_{A\cap A_0^c} g-f dx } \leq \ldots\]

however
\[\ldots \leq \max\obj{ \int_{A_0} f-g dx, \int_{A_0^c} g-f dx } \leq \int_{A_0} f-g dx  = \frac{1}{2}\int \abs{f-g} dx\]

\noindent\textbf{Pinsker inequality}\hfill \\
\[2 D_c(f,g) \leq D_k(f,g)\]

next time...

% subsection estimation (end)

% section lecture_10 (end)

\section{Lecture \# 11} % (fold)
\label{sec:lecture_11}

Multifractal analysis

The definition of $\text{dim}(A)$.
\begin{description}
	\item[Box dimensionality] For a covering $(S_k)$ of $A$ with $\text{diam}(S)\leq \epsilon$:
	\[A \subseteq \sum_{k=1}^{N^*_\epsilon(A)} S_k \]
	where $N^*_\epsilon(A)$ is the number of templates $S_k$ in the most efficient covering of $A$ with templates of diameter $\epsilon$.
	So $\text{dim}_B(A) = \lim_{\epsilon \to 0} \frac{\log N^*_\epsilon(A)}{-\log \epsilon}$
	\item[Hausdorf] The Hausdorf outer-measure on $\mathcal{P}(\Xcal)$
	\[H_\alpha(A) = \lim_{\epsilon\downarrow 0} \inf \sum_k \text{diam}(S_k)^\alpha\]
	Indeed, $H_\alpha$ is an outer measure, since sub-additivity is due to $\inf$: \[H_\alpha(A\cup B) \geq H_\alpha(A)+H_\alpha(B)\]
	The definition of Hausdorff dimensionality is
	$\text{dim}_H(A) = \sup\big\{\big. \alpha\geq 0\big.\big\rvert H_\alpha(A) > 0 \big.\big\}$
\end{description}

For any $A\subseteq \Xcal$ the Hausdorff dimensionality is not greater than the Box dimensionality, due to templates in $H$ being arbitrary.

Suppose $\alpha = \text{dim}_B(A)$ and consider $H_\alpha(A)$ for the covering in Box dimensionality:
\[\inf_{*} \sum_n \text{diam}(S_n)^\alpha \leq \sum_k \text{diam}(S_k)^\alpha = \sum_k \epsilon^\alpha = N^*_\epsilon(A) \epsilon^\alpha\]
where the infimum is taken over the coverings of diameter at most $\epsilon$.

On the other hand $\alpha = \text{dim}_B(A)$:
\[\alpha = \lim_{\epsilon\to 0}\frac{\log N^*_\epsilon(A)}{-\log \epsilon}\]
whence $\log N^*_\epsilon(A) = -\alpha \big(\log \epsilon + o(1)\big)$ and
\[N^*_\epsilon(A) = \epsilon^{-\alpha}e^{-\alpha o(1)}\]

therefore
\[H_\alpha(A) = \lim_{\epsilon} \inf_{*} \sum_n \text{diam}(S_n)^\alpha \leq \lim_{\epsilon} \epsilon^{-\alpha}e^{-\alpha o(1)} \epsilon^\alpha \]

Once again: $\alpha = \text{dim}_B(A)$ implies that for sufficiently small $\epsilon>0$
\[\log N_\epsilon^*(M) = -\alpha \log \epsilon (1 - o(1)) \]
where $N_\epsilon^*$ is the number of templates of diameter $\epsilon$ in the most
efficient covering of $M$ by $(B_k)$. For any $\epsilon>0$ sufficiently close
to zero:
\[ N_\epsilon^*(M) = \epsilon^{-\alpha} \epsilon^{o(1)}\]
where $f\in o(1)$ if and only if $f(\epsilon)\to 0$ as $\epsilon\to0$.
Now for any $\beta>\alpha$ we have
\[
\inf_* \sum_k\text{diam}{S_k}^\beta
\leq \sum_k\text{diam}{B_k}^\beta
= N_\epsilon^* \epsilon^\beta
\]
Thus
\[
H_\beta(M)
= \lim_{\epsilon\downarrow0} \inf_* \sum_k\text{diam}{S_k}^\beta
\leq \lim_{\epsilon\downarrow0} N_\epsilon^* \epsilon^\beta
= \lim_{\epsilon\downarrow0} \epsilon^{\beta-\alpha} \epsilon^{o(1)}
= \lim_{\epsilon\downarrow0} \epsilon^{\beta-\alpha}
\]
because $\lim_{\epsilon\downarrow0} \epsilon^{o(1)} = 1$.
Thus for any $\beta > \text{dim}_B(M)$ the Hausdorff outer measure of $M$ must be
zero. Hence $\text{dim}_H(M)\leq \text{dim}_B(M)$.


\noindent\textbf{Frostman's lemma}\hfill\\
Consider a metric space $(X,d)$ with a finite measure $\mu$ on $\mathcal{B}(X)$. Let $M\subseteq X$ be such that $\mu(M) > 0$. Suppose $\alpha\geq 0$ is such that $\mu(S) \leq c \text{diam}(S)^\alpha$ for any measurable template $S\subseteq M$ with sufficiently small diameter. Then $\text{dim}_H(M) \geq \alpha$.

Indeed, consider the sum for this $\alpha$:
\[\sum_n \text{diam}(S_n)^\alpha \geq \sum_k \frac{1}{c} \mu(S_k) \geq \frac{1}{c} \mu(M)\]
where the sum is taken over members of a covering of diameter at most $\epsilon$. Thus
\[H_\alpha(M) = \lim_{\epsilon\to 0}\inf_* \sum_n \text{diam}(S_n)^\alpha \geq \frac{1}{c} \mu(M)\]
whence by definition of $\dim_H(M)$ it is true that $\dim_H(M) \geq \alpha$.


Consider tha box dimensionality of a cantor set $K$.
Choose $\epsilon = 3^{-n}$ whence $n=-\log_3 \epsilon$. Then $N_\epsilon(K) = 2^n$ and $N_\epsilon^*(K) \leq 2^n = \epsilon^{-\log_3 2}$. Thus
\[\text{dim}_H(K) \leq \text{dim}_B(K) = \lim_{\epsilon\to 0}\frac{\ln N_\epsilon^*(K)}{-\ln \epsilon}\leq \log_3 2 < 1\]

But for $\mu_\infty$ -- the measure on the cantor set, we have 
\[\mu_\infty(S_k) \leq 2^{-n} = \text{diam}(S_k)^{\log_3 2}\]

\subsection{Multifractals} % (fold)
\label{sub:multifractals}

Consider a measure $\mu$ on $(\Xcal, d)$. It has singularity of order $\alpha$ at $x\in \Xcal$ with $x\in \{\mu(S)>0\}$ if for any $\epsilon>0$
\[\mu(B_\epsilon(x)) \overset{\text{asy}}{\sim} \epsilon^\alpha\]
where $B_\epsilon(x) = \obj{\induc{y\in \Xcal}d(x,y)\leq\epsilon}$ is a closed ball in $(\Xcal, d)$.

Let $(\Omega, \Tcal)$ be a metrizable topological space and $\mu$ a finite measure on $\big(\Omega, \borel(\Omega)\big)$. Then for all measurable $B$ and every $\epsilon>0$ there exist $F$ closed and $G$ open in $(\Omega, \Tcal)$ with $F\subseteq B \subseteq G$ such that $\mu(G\setminus F) < \epsilon$.

A multifractal spectrum is a function $f$ such that 
\[f(\alpha) = \text{dim}_H\Big(\big\{\big. x\in \text{supp}\mu\big.\big\rvert \mu \text{ has singularity order } \alpha \text{ at } x \big.\big\}\Big)\]

For any covering $(S_k)$ of the support of $\mu$ with $\text{diam}(S_k)\leq \epsilon$ let
\[R_{\mu,\epsilon}(q) = \inf_* \sum_k \big(\mu(S_k)\big)^q\]
The R\'enyi exponent is $\tau_\mu(q)$ defined as follows:
\[\tau_\mu(q) = \lim_{\epsilon\to 0} \frac{\log R_{\mu,\epsilon}(q) }{\log \epsilon}\]
The R\'enyi exponent could be estimated numerically, in contrast to the multifractal spectrum (if the measure is given approximately as a set of atomic masses).

The good news is that the spectrum and the exponent are related.

An observation: for $R_{\mu,\epsilon}(q)$ there is the following estimate (using the Box dimensionality templates) for ``good'' sets is
\begin{align*}
	R_{\mu,\epsilon}(q)
	&= \inf_* \sum_k \big(\mu(S_k)\big)^q \\
	&\approx \sum_\alpha \sum_k \mu(S_k)^{\alpha q} \\
	&\approx \sum_\alpha \epsilon^{\alpha q} N^*_\epsilon(A_\alpha) \\
	&\approx \sum_\alpha \epsilon^{\alpha q - \text{dim}_B(A_\alpha)}\\
	&\approx \sum_\alpha \epsilon^{\alpha q - f(\alpha)}
\end{align*}
where $A_\alpha = \Big\{\Big. \Big.\Big\rvert \Big.\Big\}$

Now approximately
\[\tau_\mu(q) = \lim_{\epsilon\to0} \frac{\ln \sum_\alpha \epsilon^{\alpha q - f(\alpha)}}{\ln \epsilon} = \min_\alpha \big( \alpha q - f(\alpha) \big)\]
the Legendre transform of $f(\cdot)$. Thus multifractal spectrum is recovered from
\[f^*(\alpha) = \min_q \Big( \alpha q - \tau_\mu(q) \Big)\]
The R\'enyi exponent $\tau_\mu(q)$ is convex, since it is defined as a lower envelope.

It is possible to say that for any arbitrary function $f$ sufficiently well-behaved the function
\[f^*(q) = \min_\alpha\big(\alpha q - f(\alpha)\big)\]
is convex (?).

Thus for any $\alpha$ and $q$ we have $f_\mu(\alpha)\leq \alpha q - \tau(q)$ or $f_\mu(\alpha) + \tau(q)\leq \alpha q$, which is symmetric. Thus
\[f(\alpha)\leq f^*(\alpha) \defn \min_q \Big( \alpha q - \tau_\mu(q) \Big)\]

% were $f(\alpha)$ convex, $f^*(\alpha) = f(\alpha)$.
For what $q$ is the minimum attained in the definition of $f^*(\alpha)$? First order conditions imply $\alpha - \tau_\mu'(q) = 0$ provided the derivative exists. Using the symmetric condition above we get $f'(\alpha) = q$, whence it follows that the derivatives of $\tau$ and $f$ are mutually inverse functions.

The Legendre transform classically is defined as the first integral of the inverse of the (monotonous) derivative of a convex function.

% subsection multifractals (end)

% section lecture_11 (end)

\section{Lecture \# 12} % (fold)
\label{sec:lecture_12}

Distances in statistics:
\begin{description}
	\item[Total variation] Is defined as the maximal discrepancy of probability measures $Q$ and $P$ (with densities $q$ and $p$ respectively)
	\[D(p,q) = \sup_A\Big\lvert P(A) -  Q(A) \Big\rvert = \sup_A\Big\lvert \int_A p dx -  \int_A q dx \Big\rvert\]
	Using Scheffe's equivalent representation \[D(p,q) = \frac{1}{2}\int|p-q| dx\]
	\item[Kullback-Leibler] Is the relative entropy between $P$ and $Q$:
	\[ KL(p,q) = \int p \log\frac{p}{q} dx = \int \frac{p}{q}\log\bigg(\frac{p}{q}\bigg) q dx = \text{KL}\big( P \lvert \rvert Q\big)\]
	if the random variables $(\xi_k)_{k=1}^n\sim p$ and $(\eta_k)_{k=1}^n\sim q$ are each iid then the KL devergence is $KL(p^{(n)},q^{(n)}) = n KL(p,q)$.
	% \[\text{KL}\big( P \lvert \rvert Q\big) = \int \frac{dP}{dQ} \log \frac{dP}{dQ} dQ =  \int \frac{p}{q} \log \frac{p}{q} q dx\]
\end{description}

Let's show that (Pinsker's inequality)
\[K(p,q)\geq 2 D(p,q)\]
Put $A_0 = \{ p \geq q \}$ and $A_0^c = \{p < q\}$. Then 
\[D(p,q) = \frac{1}{2}\int_{A_0} p-q dx + \frac{1}{2}\int_{A_0^c} q - p dx \]
using the fact that $p$ and $q$ are densities of $P$ and $Q$ respectively:
\[D(p,q) = \int_{A_0} p-q dx = P(A_0) - Q(A_0)\]

Jensen's inequality for a convex $f$ yields
\[\ex f(X) \geq f\big(\ex X\big)\]

Since $f(x) = -\log x$ is convex the following is true:
\[KL(p,q) = \int_{A_0} -\log\frac{q}{p} p dx + \int_{A_0^c} -\log\frac{q}{p} p dx \]
\[\ldots = P(A_0) \int_{A_0} -\log\frac{q}{p} \frac{p}{P(A_0)} dx + P(A_0^c) \int_{A_0^c} -\log\frac{q}{p} \frac{p}{P(A_0^c)} dx \]
whence using Jensen's
\[KL(p,q) \geq  -P(A_0) \log \int_{A_0}\frac{q}{p} \frac{p}{P(A_0)} dx - P(A_0^c) \log \int_{A_0^c} \frac{q}{p} \frac{p}{P(A_0^c)} dx\]

reducing
\[\ldots \geq -P(A_0) \log \int_{A_0}\frac{q}{P(A_0)} dx -P(A_0^c) \log \int_{A_0^c} \frac{q}{P(A_0^c)} dx \]

yields
\[KL(p,q) \geq -P(A_0) \log \frac{Q(A_0)}{P(A_0)} -P(A_0^c) \log \frac{Q(A_0^c)}{P(A_0^c)} \]

Putting $\alpha = P(A_0)$ and $\beta = Q(A_0)$
\[KL(p,q) \geq -\alpha \log \frac{\beta}{\alpha} -(1-\alpha) \log \frac{1-\beta}{1-\alpha} = \alpha \log \frac{\alpha}{\beta} + (1-\alpha) \log \frac{1-\alpha}{1-\beta} \]

Finally, consider for an arbitrary $\lambda$
\[f(\beta) = \alpha \log \frac{\alpha}{\beta} + (1-\alpha) \log \frac{1-\alpha}{1-\beta} - \lambda (\alpha - \beta)^2\]

differentiating gives:
\[f'(\beta) = -\frac{\alpha}{\beta} + \frac{1-\alpha}{1-\beta} + 2\lambda (\alpha - \beta) = (\beta - \alpha)\Big(\frac{1}{(1-\beta)\beta} - 2\lambda\Big)\]

when $\lambda < 2$ we have
\[\frac{1}{(1-\beta)\beta} - 2\lambda > 0\]
implying that $f'(\beta) = 0$ for $\alpha=\beta$.
Therefore in the extremum we have $f(\alpha) = 0$ and $f(\beta)\geq f(\alpha) = 0$ for all $\beta\in [0,1]$, since $f(0)=f(1)=+\infty$.

\subsection{Hellinger's distance} % (fold)
\label{sub:hellinger_s_distance}

Consider the total variation distance
\[D(p,q) = \frac{1}{2}\int |p-q| dx = \frac{1}{2}\int (\sqrt{p}-\sqrt{q})(\sqrt{p}+\sqrt{q}) dx\]
whence using the Cauchy-Schwartz-Bunyakovski inequality one gets
\[D(p,q) \leq \frac{1}{2}\Big(\int (\sqrt{p}-\sqrt{q})^2dx\Big)^\frac{1}{2}\Big(\int (\sqrt{p}+\sqrt{q})^2dx\Big)^\frac{1}{2}\]
since $(a+b)^2\leq 2 a^2 + 2 b^2$ we have
\[\int (\sqrt{p}+\sqrt{q})^2dx \leq \int 2p+2q dx = 2 \int p+q dx = 4\]
Therefore
\[D(p,q) \leq \Big(\int (\sqrt{p}-\sqrt{q})^2dx\Big)^\frac{1}{2} \frac{1}{2}\Big(4\Big)^\frac{1}{2}\]

% subsection hellinger_s_distance (end)

\subsection{$\chi^2$-distance} % (fold)
\label{sub:chi2_distance}

The $\chi^2$ distance is an approximation of the Kullback-Leibler distance when $p\approx q$.

Consider the Taylor expansion of $\log(1+x)$. Note that 
\[\frac{d^k}{dx^k} \log(1+x) = -(-1)^kk!\frac{x^{-k}}{k}\]
whence Taylor-expanding around $a=0$ we get
\[\log(1+x) = \log(1) + \sum_{k\geq1} \frac{1}{k!}\bigg.\frac{d^k}{dx^k} \log(1+a)\bigg\rvert_{a=0} (x-0)^k = \sum_{k\geq1} \frac{-(-1)^k}{k!}k!\frac{x^k}{k} = -\sum_{k\geq1} \frac{(-1)^kx^k}{k}\]
truncating at the second term:
\[\log(1+x) = x - \frac{x^2}{2} + o(x^2) \]
whence $\log(1+x)\approx x-\frac{x^2}{2}$ for sufficiently small $x$ we get.
Therefore we get
\begin{align*}
	KL(p,q) &= -\int p\log\frac{q}{p}dx = -\int p \log \Big(1+\big(\frac{q}{p}-1\big)\Big) dx \\
	& \approx -\int p \Big(\big(\frac{q}{p}-1\big) - \frac{1}{2} \big(\frac{q}{p}-1\big)^2 \Big) dx = -\int (q-p) \Big(1 - \frac{1}{2} \big(\frac{q}{p}-1\big) \Big) dx \\
	& = -\int (q-p) dx - \int (q-p) (-1) \frac{1}{2} \frac{q-p}{p} dx\\
	& = \frac{1}{2}\int \frac{|q-p|^2}{p} dx
\end{align*}
Therefore for sufficiently close densities $p$ and $q$ the Kullback-Leibler divergence is well approximated by the $\chi^2$ distance
\[KL(p,q) \approx \frac{1}{2}\int \frac{|p-q|^2}{p}dx\]

% subsection chi2_distance (end)

\subsection{The maximum likelihood} % (fold)
\label{sub:the_maximum_likelihood}

Suppose there is a sample $\big(Y_k\big)_{k=1}^n\sim p^{(n)}_0$ -- the true density. Consider a parametric family $p^{(n)}(x;\theta)$ for $\theta\in \Theta$.
We wish to find 
\[\theta^*(p_0)=\text{argmin}_{\theta\in \Theta} KL(p^{(n)}_0, p^{(n)}(\cdot;\theta))\]

However minimizing the Kullback-Leibler divergence is equivalent to 
\[\theta^*(p_0)=\text{argmax}_{\theta\in \Theta} \int p^{(n)}_0 \log p^{(n)}(\cdot;\theta) dx^n\]

Using the empirical density to estimate $\int p^{(n)}_0 \log p^{(n)}(\cdot;\theta) dx^n$ we get the likelihood
\[\hat{L} = \int \delta(x-Y^{(n)}) \log p^{(n)}(\cdot;\theta) dx^n = \log p^{(n)}(Y^{(n)};\theta)\]
whence 
\[\hat{\theta}(Y^{(n)}) = \text{argmax}_{\theta\in \Theta} \log p^{(n)}(Y^{(n)};\theta)\]

if $Y^{(n)}$ is iid and $Y_i = \theta + \epsilon_i$ with $\epsilon_i\sim p_\epsilon$ iid with $\ex \epsilon_i = 0$ and $\ex \epsilon_i^2 = \sigma^2$, then
\[\hat{L} = \sum_{i=1}^n\log p_\epsilon(Y_i-\theta)\]
Using the law of large numbers we get
\[\frac{1}{n}\hat{L} \to \ex_{p_0}\log p_\epsilon(Y_1-\theta)\]
whence
\[\hat{\theta}_n \to \text{argmax}_{\theta}\ex_{p_0}\log p_\epsilon(Y_1-\theta) = \text{argmin}_{\theta} KL\Big(p_0,p_\epsilon(\cdot - \theta)\Big)\]

% subsection the_maximum_likelihood (end)

\subsection{Properties of MLE} % (fold)
\label{sub:properties_of_mle}
Illustrated for the case of a simple constant and spread estimation.

If $\epsilon\sim \Ncal(0,\sigma^2)$, then 
\[\hat{L} = -\frac{1}{2\sigma^2}\sum_{i=1}^n\big(Y_i-\theta\big)^2 - \frac{n}{2}\log(2\pi \sigma^2)\]
implying that $\hat{\theta} = \frac{1}{n}\sum_{i=1}^n Y_i$.
If $\epsilon$ are iid with $D(0,\sigma^2)$ then the ML estimate is unbiased $\ex \hat{\theta} = \theta$.
Its variance is $\ex (\hat{\theta}-\theta)^2 = \frac{\sigma^2}{n}$.

using the Central Limit theorem we get
\[\Pr\big(\sqrt{n} \frac{\hat{\theta}-\theta}{\sigma} \geq z\big)\to \int_z^\infty \frac{1}{\sqrt{2\pi}} e^{-\frac{s^2}{2}}ds \]

The sample variance is estimated using $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(Y_i - \hat{\theta})^2$ whence
\[\hat{\sigma}^2 = \sigma^2 + \frac{1}{n}\sum_{i=1}^n (\epsilon_i^2 - \sigma^2) - \frac{1}{n} \sum_{i=1}^n \epsilon_i^2\]
Therefore
\[\ex \hat{\sigma}^2 = \sigma^2 - \frac{1}{n}\sigma^2 \]
and the estimate is biased.

However the estimates $\hat{\sigma}^2$ and $\hat{\theta}$ are independent. It is based on the fact that for Gaussian random variables only lack of correlation implies independence.
\begin{align*}
	\ex (\hat{\theta}-\theta) (Y_i - \hat{\theta})
	&= \ex (\hat{\theta}-\theta) \big(\epsilon_i + (\theta - \hat{\theta}) \big) \\
	&= \ex (\hat{\theta}-\theta)\epsilon_i - \ex (\hat{\theta}-\theta)^2 \\
	&= \ex \Big(\frac{1}{n}\sum_{j=1}^n Y_j-\theta\Big)\epsilon_i - \frac{1}{n}\sigma^2 \\
	&= \frac{1}{n}\sum_{j=1}^n \ex (Y_j - \theta)\epsilon_i - \frac{1}{n}\sigma^2 \\
	&= \frac{1}{n}\sum_{j=1}^n \ex \epsilon_j\epsilon_i - \frac{1}{n}\sigma^2 \\
	&= \frac{1}{n}\sigma^2 - \frac{1}{n}\sigma^2 = 0
\end{align*}

% subsection properties_of_mle (end)

% section lecture_12 (end)

\section{Lecture \# 13} % (fold)
\label{sec:lecture_13}

The mid-term assessment is the following tasks to be handed due the 1st of April:
\begin{enumerate}
	\item None;
	\item \rus{У2.12, У2.13, У2.14, У2.18};
	\item None;
	\item \rus{У4.2, У4.11, У4.12, У4.16};
	\item \rus{Лекция~2, упр. в разд.~3.1}.
\end{enumerate}

\subsection{Legendre transform} % (fold)
\label{sub:legendre_transform}

This would be a small digression into the topic of convex analysis. The main beauty of the
convex analysis is that the objects studies admit dual representations.

Based on the analysis of functions.

Consider a linear space $L$ over $\Cplx$. A set $M$ is called convex if $x\alpha + y (1-\alpha)\in M$ for all $x,y\in M$ and for every $\alpha\in [0,1]$
A function $f: L\to \Real$, with $M\subseteq L$ is convex is its supergraph is convex:
\[\text{epi}(f) = \big\{\big. (x,y)\in \Real^{d+1} \big.\big\lvert y\geq f(x)\big.\big\}\]
this also requires that $M$ be convex.
If the function is not defined everywhere, then outside its domain it is extended to a map taking values $+\infty$.
This enables relaxation of the definition to arbitrary domains.

We will be considering the function with closed supergraph only. The function must be finite on the closure of its support.
In other terms only lower semi-continuous functions are considered.

If $(\Omega,\Tcal)$ is a topological space, then the function $f:\Omega\to\Real$ is lower semi-continuous at
$x\in \Omega$, $f(x)<+\infty$ if for every $\epsilon>0$ there exists $U\in \Tcal$ with $x\in U$ such that $f(y)\geq f(x)-\epsilon$.

A convex hull of $S$ is the intersection of all \emph{closed} convex sets $C$ with $S\subseteq C$. A corner point is a point $y$ in $M$ such that it is not an internal point of any closed interval $x\alpha + z(1-\alpha)\in M$ for $\alpha\in(0,1)$. Excluding corner point does not necessarily make a set non convex.

Geometrically, a convex function $f(x)$ is defined via the envelope of tangent hyperplanes:
\[f(x) = \sup_p \big\{ \brkt{p,x} - \phi(p) \big\}\]
where $\phi(p)$ is the elevation of the tangent hyperplane with normal $p$.

This expression implies the Young inequality:
\[f(x) + \phi(p) \geq \brkt{p,x}\]
since $f$ and $\phi$ are symmetric in this inequality., which begs the question whether it is possible to represent $\phi$ though $f$.
For example with this expression:
\[\phi(p) = \sup_x \big\{\brkt{x,p}-f(x)\big\}\]
which cloud yield a canonical representative of all functions that may generate $f$.

\textbf{A counter example}.\hfill \\
Consider $f(x) = \lvert x \rvert$. Then $f(x) = \max\{-x,x\}$ and $f(x) = \sup\{-1\cdot x, 1\cdot x\}$
which means that $\phi(\pm1)=0$. HOw to define $\phi(s)$ for $s\in (-1,1)$, since outside for finite values the tangent hyperplane would not be tanget to $f$ at all. Thus $\phi(p)=\infty$ for $p\notin [-1,1]$. At $p=0$ the function $\phi$ is allowed to take any positive value.

The loss of smoothness at points leads to failure of the dual to be strictly convex.
Thus a good canonical class is that of smooth strictly convex functions.

\textbf{An example}.\hfill \\
If $f$ is smooth convex function, $f=\alpha x^2$, then
\[\phi(x_0) = \alpha x_0^2 + 2\alpha x_0 (x-x_0)\]
whence 
\[\phi(x_0) = \alpha x_0^2 - 2\alpha x_0 x = \frac{p^2}{4 \alpha} - p x\]
for $p = 2\alpha x_0$, which implies that
\[\alpha x^2 = \sup_{p} \big\{ px - \frac{p^2}{4\alpha}\big\}\]
which is a canonical representative.

Support hyperplanes of $f$ are such that they lie entirely below $f$.
A convex function has at least one support hyperplane at any point $x$.

A precise definition:
Suppose $f$ is convex. The sub-gradient of $f$ at $x_0\in \Real^d$ is such a dual vector
$p\in\big(\Real^d)^*$ such that $f(x_0)+\brkt{p, x-x_0}\leq f(x)$ for all $x$.

A sub-differential of a convex $f$ at $x_0$ is a collection of all of its sub-gradients:
\[\partial f(x_0) = \big\{\big. p \big.\big\rvert \forall x f(x_0) + \brkt{p,x-x_0} \leq f(x) \big.\big\}\].
If $f(x) = \lvert x\rvert$, then
\[\partial f(x) = \bigg\{\begin{cases}
	+1, &\text{ if } x>0\\
	-1, &\text{ if } x<0\\
	\text{conv}\big(\{-1,+1\}\big) &\text{ otherwise}
\end{cases}\bigg.\]
For $f(x) = \alpha x^2$, $\partial f(x) = 2\alpha x$.
If the $\partial \lvert x\rvert$ is flipped (transposed), then it corresponds to a sub-differential of the Legendre transform of $\lvert x\rvert$.
Indeed, for $f(x) = \alpha x^2$, $\partial f(x) = 2\alpha x$ and $\phi(p) = \frac{p^2}{4\alpha}$ with $\partial \phi(p) = \frac{p}{2\alpha}$.

The Young's inequality $f(x) + \phi(p) \geq \brkt{p,x}$ yields pairs of adjoint points when the inequality turns into an equality.
Is this relation invertible? In $d=1$ case the inverse is $p=\partial \phi(x)$ is monotonous in $x$.
In $d>1$ case this property is called \emph{$k$-cyclical monotonicity}, with $k\geq2$: $x\mapsto p$ is cyclically monotonous if
\[ \sum_{i=1}^{k-1} \brkt{p_i, x_{i+1}-x_i} + \brkt{p_k, x_1-x_k} \leq 0\]
for all any $\big(x_i\big)_{i=1}^k$ and $p_i \in p(x_i)$.
Ordinary monotonicity is $2$-cyclical monotonicity. Indeed, 
\[\brkt{p_1, x_2-x_1}+\brkt{p_2, x_1- x_2} = \brkt{p_2-p_1, x_1-x_2}\leq 0\]

% subsection legendre_transform (end)

% section lecture_13 (end)

\section{Lecture \# 14} % (fold)
\label{sec:lecture_14}

\subsection{Probability spaces and filtrations} % (fold)
\label{sub:probability_spaces_and_filtrations}


Consider a sequence $(X_n)_{n\geq 1}\in \{0,1\}$ of independent and identically
distributed random variables with $\Pr(X_n = 0) = \frac{1}{2}$. Let $X$ be defied
as $X=0.X_1X_2\ldots = \sum_{n\geq1}\frac{1}{2^n} X_n$. Then $X\in [0,1]$ and
$X\sim\mathcal{U}[0,1]$.

Indeed, to prove this we need to show that $\Pr(a\leq X \leq b) = b-a$.
First observation -- $X_1$, -- tells us if $X$ is either in the left or the
right half of $[0,1]$. The second -- $X_2$, -- the halves of halves and so on.
For any dyadic interval $X\in(\frac{k}{2^n}, \frac{k+1}{2^n}]$ has the probability
$2^{-n}$. Thus exhausting the interval $[a,b]$ from within yields 
\[
\Bigl| \sum_{k=\lceil a2^n \lceil}^{\lfloor b2^n\rfloor} \frac{1}{2^n} - (b-a)\Bigr|
= \frac{1}{2^n}\Bigl| \lceil b2^n \lceil - b2^n - ( \lfloor a2^n \lfloor - a2^n ) + 1 \Bigr|
\leq 3 \frac{1}{2^n}
\]

Consider $X_n$ defined $\frac{1}{2^{n-1}}1_{I_n}(x)$ with 
\[I_n = \bigcup_{k=0}^{2^{n-1}}(\frac{2k+1}{2^n}, \frac{2(k+1)}{2^n}]\]
then any finite collection of such random variables is independent.

\subsubsection{A non-measurable set} % (fold)
\label{ssub:a_non_measurable_set}

Consider unit interval identified with a circumference and throw away $1$ (so we
end up with $[0,1)\to [0,2\pi)$). Consider partition of $[0,2\pi)$ defined through
equivalence relation $\theta_1\sim\theta_2$ if and only if $|\theta_1-\theta_2|\in \mathbb{Q}$
on the central angle
\[\mathcal{C}(\theta_0) = \Bigl\{\theta\in [0,2\pi)\Big\rvert \theta\sim \theta_0\Bigr\}\]
Since each class is countable while the circumference is a continuum, there must be
a continuum of distinct classes $\mathcal{C}(\theta)$. However every class is congruent
to one another.

Using the Axiom of Choice consider a choice function $g:\mathcal{P}[0,2\pi) \to [0,2\pi)$
(with $g(S)\in S$). Pick a representative in each class using $g$ and collect them
into
\[\mathcal{Z} = \bigcup_{\theta \in[0,2\pi)} g\bigl(\mathcal{C}(\theta)\bigr)\]
Then rotating $Z$ by every rational number and taking the union yields
\[S_1 = \bigcup_{q\in \mathbb{Q}} Z\oplus q\]

% subsubsection a_non_measurable_set (end)

\subsubsection{Elements of measure theory} % (fold)
\label{ssub:elements_of_measure_theory}

Measure theory is the formalisation of intuitive understanding of the lengths,
volumes et c.

A triple $\bigl(\Omega, \Fcal, \mu \bigr)$ is called a measure space (probability space)
if $\Fcal$ is a $\sigma$-algebra on $\Omega$ and $\mu:\Fcal\to[0,\infty]$ is a measure:
\begin{itemize}
	\item $\mu(\emptyset)=0$;
	\item for any measurable partition $(A_n)_{n\geq1}$ of a measurable $A$
	\[\mu(A) = \mu\Bigl(\uplus_{n\geq1} A_n\Bigr)=\sum_{n\geq1} \mu(A_n)\]
\end{itemize}
(with $\mu(\Omega) = 1$).

% subsubsection elements_of_measure_theory (end)

\subsubsection{Filtrations} % (fold)
\label{ssub:filtrations}

A \textbf{filtration} is a sequence of nested $\sigma$-algebras on $\Omega$
$(\Fcal_n)_{n\geq0} \subseteq \Fcal$ of $\Fcal$ with
\[\Fcal_0\subseteq\Fcal_1\subseteq\ldots\subseteq\Fcal_n\subseteq\Fcal\]

Time is a one-dimensional manifold with a parameter (smoothness and continuity).
A more correct mathematical formalisation of time is the collection of events
accumulated so far which coincides with filtration. Time point is the ``amount''
of information obtained so far.

% subsubsection filtrations (end)

% subsection probability_spaces_and_filtrations (end)

% section lecture_14 (end)

\section{Lecture \# 15} % (fold)
\label{sec:lecture_15}

\subsection{Finite Markov chains} % (fold)
\label{sub:finite_markov_chains}

Finiteness is in terms of a number of states.
Finite Markov chains have a limiting distribution, and the convergence to it
is exponential.

Dual approach to Finite Markov chains:
\begin{itemize}
	\item a random walk on an oriented graph of states;
	\item a stochastic matrix, that permits convergence results.
\end{itemize}

A homogeneous Markov chain is such that its parameters are static.

A homogeneous finite Markov chain is a sequence of states form a finite set of
states $i=1,\ldots m$ with a certain probability distribution of interstate transition.
\[\Pr( x_{t+1} = j\rvert X_t = i) = p_{ij}\]
The \textbf{transition kernel} is defined by a stochastic matrix $P = (p_{ij})_{i,j=1}^{m,m}$
with the following restriction $\sum_j p_{ij} = 1$ for all $i$ and $p_{ij}\geq0$.

A bi-stochastic matrix has an additional constraint that its columns sum to $1$.
The hyper-polygon of bi-stochastic matrices is determined by the convex hull of
the set of all $n!$ permutation matrices.

The probabilistic state of a finite Markov chain is a vector $(\pi_k)_{k=1}^m$ in
$[0,1]^{1\times m}$ of probabilities of $X_t$ at the moment $t$ being in the sate $k$.
Then 
\[\pi_k(t+1) = \sum_j \pi_j(t) p_{ji}\]
whence $\pi(t+1) = \pi(t)P$.

Every time one gets a matrix it is interesting to consider its eigenvalues and
eigenvectors. The matrix $P$ has a right eigenvector $v = \one$, since
the stochasticity of $P$ implies that $P\one = \one$

Eigenvalues of a matrix and its transpose coincide, since they are the roots of
the characteristic polynomial. The left eigenvector of $\lambda = 1$ is such $\pi$
that $\pi = \pi P$. The goal of this lecture is to find conditions appropriate
for the convergence of $\pi(t) = \pi(0) P^t$ to this eigenvector $\pi$.

Consider a directed graph of states $G=(V,E)$ where directed edges represent the
possibility of making a transition $j\to i$ whenever $(j,i)\in E$.
The graph $G$ must not have multiple edges, but might have loops, and every vertex
$j$ (state) must not be a sink (there is at least one outgoing edge from $j$ in $E$).

These conditions allow the interpretation of a directed graph as a finite state
Markov chains. The weighted incidence matrix of $G$ can be regarded the transition
kernel of the associated Markov chain.

Such matrix (with proper permutation of states) has block structure if the graph
has more than one weakly connected component.

When studying matrices, we are interested in their spectrum, while with graphs --
their connected components are of interest.

\subsubsection{Elementary graph theory} % (fold)
\label{ssub:elementary_graph_theory}

Consider a directed graph $G=(V,E)$. A vertex $v$ is \textbf{reachable} form $u$
($u\leadsto v$) if there exists an ordered path $\pi$ of edges from $E$, such that
\[ u = x_0 \to x_1 \to \ldots x_k \to\ \ldots \to x_p = v\]
where $(x_i,x_{i+1})\in E$ for all $i=1,\ldots, p$.

Vertices $u$ and $v$ are connected if $u\leadsto v$ and $v\leadsto u$.

A transitory vertex is such that it has only one outgoing edge. Such vertices only
add a delay in the random walk (increase the number of iterations) and for the
purposes of limit analysis are irrelevant (transitory vertices cannot have any other
stationary probability but $0$).

A class of connected vertices (state) is \textbf{absorbing} if no vertex of any
other class is reachable from it.

The idea is that the probability mass as the walk goes on redistributes itself
in favour of the absorbing classes. Thus the stationary distribution factorizes
into multiplicative components of the distribution of mass between classes
and the distribution within each class. Hence the understanding of the limiting
case may be reduced to studying a single absorbing class of states.

% subsubsection elementary_graph_theory (end)

\subsubsection{Back to Markov chains} % (fold)
\label{ssub:back_to_markov_chains}

A Markov chain is \textbf{irreducible} if it consists entirely of one absorbing
class (one strongly connected component in $G$). However nothing is simple and
the Markov chain might exhibit periodicity.

\noindent\textbf{Perron-Frobenius}\hfill\\
For any irreducible and aperiodic Markov chain with transition kernel $P$ there
exists a stationary distribution $\pi$ such that $\pi = \pi P$ and for any
distribution $\pi(0)$ it is true that $\pi(t) = \pi(0) P^t$ converges to $\pi$
in $l^1$ norm and
\[\|\pi(t) - \pi\|_1 < (1-\alpha)^t\]
for some $\alpha>0$. The $l^1$ norm is $\|\theta\|_1 = \sum_j |\theta_j|$.

Suppose that the transition kernel has strictly positive elements. Then since
the chain is finite there is $\lambda>0$ such that $p_{ij}\geq \lambda$ for all
$i$ and $j$.

Consider distributions $\pi$ and $\sigma$ and show that
\[\|\pi P-\sigma P\|_1 < \|\pi - \sigma \|\]
But 
\[
\|\pi P-\sigma P\|_1
= \sum_j | [\pi P]_j - [\sigma P]_j |
= \max_{|\xi_j|=1}  \sum_j \bigl([\pi P]_j - [\sigma P]_j \bigr) \xi_j
\]
since 
\[ [\pi P]_j = \sum_k \pi_k p_{kj} \]
we get this
\[
\|\pi P-\sigma P\|_1
= \max_{|\xi_j|=1}  \sum_j\sum_k \bigl(\pi_k p_{kj} - \sigma_k p_{kj} \bigr) \xi_j
= \max_{|\xi_j|=1}  \sum_k\bigl(\pi_k - \sigma_k \bigr) \sum_j p_{kj} \xi_j
\]
Let $\eta_k=\sum_j p_{kj} \xi_j$.

Since $\pi$ and $\sigma$ each sum to one, then some $\xi_j$ must have different
signs (?). Thus $|\eta_k| \leq 1-2\lambda$ and
\[
\|\pi P-\sigma P\|_1
\leq \max_{|\eta_k|\leq 1-2\lambda} \sum_k\bigl(\pi_k - \sigma_k \bigr)\eta_k
\leq (1-2\lambda) \|\pi - \sigma\|_1
\]

% subsubsection back_to_markov_chains (end)

% subsection finite_markov_chains (end)

% section lecture_15 (end)

\section{Lecture \# 16} % (fold)
\label{sec:lecture_16}

Consider a sample $(Y^n)\in \Real^n$ with unknown density $p(\cdot)$ from the parametric
family $p(x\rvert \theta)$ with $\theta\in \Theta\subseteq \Real^p$. Suppose that
$\Theta = \Theta_0\uplus \Theta_1$. Th goal is to discriminate the following hypothesis
$H_0: \theta\in \Theta_0$ from $H_1: \theta\in \Theta_1$.
 A statistical decision rule is a map $\phi:\Real^n \to\{0,1\}$ which is defined as
\[
\phi(x) = \begin{cases}
	0, &\to H_0\\
	1, &\to H_1\\
	0, &\to H_0,H_1\\
\end{cases}
\]

THe following errors a re possible:
\begin{itemize}
	\item reject $H_0$, when it is true; type-$1$ error;
	\item reject $H_1$, when it is true; type-$2$ error.
\end{itemize}
A criterion for the type-$1$ error:
\[\alpha(\phi, \theta) = \ex_{p(\cdot\rvert\theta)} \phi(X)\]
for $\theta\in \Theta_0$. This is equivalent to
\[\alpha(\phi, \theta) = \int_{\Real^n} \phi(X) p(X \rvert \theta) dX\]
For the type-$2$ error
\[\beta(\phi, \theta) = \int_{\Real^n} \Bigl(1-\phi(X)\Bigr) p(X \rvert \theta) dX\]
for $\theta\in \Theta_1$ whence
\[\beta(\phi, \theta) = 1-\ex_\theta \phi(Y^n)\]

We wish to find a decision rule with \begin{itemize}
	\item $\alpha(\phi,\theta)\leq \alpha$;
	\item $\beta(\phi,\theta)\to \min_\phi$.
\end{itemize}

The Bayesian risk is 
\[
r_\pi(\phi)
= \int_{\Theta_0} \pi(\theta) \alpha(\phi,\theta) d\theta
+ \int_{\Theta_1} \pi(\theta) \beta(\phi,\theta) d\theta
\]
for some prior distribution $\pi(\theta)$. Thus
\[
\ldots =
= \int_{\Theta_0} \pi(\theta) \ex_\theta \phi(Y^n) d\theta
+ \int_{\Theta_1} \pi(\theta) \bigl(1-\ex_\theta \phi(Y^n)\bigr) d\theta
\]
The Bayesian test is defines as
\[\bar{\phi}_\pi = \text{argmin}_{\phi} r_\pi(\phi)\]

Theorem:
\begin{align*}
	\bar{\phi}_\pi(Y^n) &= 1_{A}(Y^n)\\
\end{align*}
where
\[
A = \biggl\{
\int_{\Theta_1} \pi(\theta)p(X \rvert \theta) d\theta
\geq \int_{\Theta_0} \pi(\theta)p(X \rvert \theta) d\theta
\biggr\}
\]

Indeed, for an arbitrary decision rule $\phi$
\[
r_\pi(\phi)
= \int_{\Theta_0} \pi(\theta) \ex_\theta \phi(Y^n) d\theta
+ \int_{\Theta_1} \pi(\theta) \bigl(1-\ex_\theta \phi(Y^n)\bigr) d\theta
\]
transforms into
\begin{align*}
	r_\pi(\phi)
	&= \int_{\Theta_0} \pi(\theta)\int_{\Real^n} \phi(X) p(X \rvert \theta) dX d\theta
	+ \int_{\Theta_1} \pi(\theta) \int_{\Real^n} (1-\phi(X)) p(X \rvert \theta) dX d\theta\\
	&= \int_{\Real^n} \phi(X) \int_{\Theta_0} \pi(\theta)p(X \rvert \theta) d\theta dX
	+ \int_{\Real^n} (1-\phi(X)) \int_{\Theta_1} \pi(\theta) p(X \rvert \theta) d\theta dX\\
	&\geq \int_{\Real^n} \min_{y\in[0,1]} \Bigl\{
		y \int_{\Theta_0} \pi(\theta)p(X \rvert \theta) d\theta
	+ (1-y) \int_{\Theta_1} \pi(\theta) p(X \rvert \theta) d\theta \Bigr\}dX
	&\geq \int_{\Real^n} \min\Bigl\{
		\int_{\Theta_0} \pi(\theta)p(X \rvert \theta) d\theta,
		\int_{\Theta_1} \pi(\theta) p(X \rvert \theta) d\theta
	\Bigr\}dX
\end{align*}

How to compute integrals with respect to unknown priors? Use the mean-value theorem:
\[
\int_{\Theta_0} p(X\rvert \theta) \pi(\theta) d\theta
\approx C_0 \max_{\theta\in \Theta_0} p(X\rvert \theta)
\]
and similarly
\[
\int_{\Theta_1} p(X\rvert \theta) \pi(\theta) d\theta
\approx C_1 \max_{\theta\in \Theta_1} p(X\rvert \theta)
\]
whence the decision can be reduced to the maximum likelihood rule:
\[
\bar{\phi}_{t_\alpha}(X) = 1_{A(t_\alpha)}(X)
\]
where
\[
A(t_\alpha)
= \Biggl\{\max_{\theta\in \Theta_1} p(X\rvert \theta)
\geq t_\alpha \max_{\theta\in \Theta_0} p(X\rvert \theta)
\Biggr\}
\]
and $t_\alpha$ is computed from
\[\max_{\theta\in \Theta_0}\alpha(\bar{\phi}_{t_\alpha},\theta) \leq \alpha\]

\noindent\textbf{Fano inequality}\hfill\\
Consider a multivariate $X\sim p(\cdot)$ and hypotheses $H_k: p=p_k$ with prior
probabilities $\pi_k$ for $k=1,\ldots, M$. The parameter is discrete. The decision
rule $\phi(Y^n):\Real^n\to \{1,\ldots,M\}$. The mean error of $\phi$ is
\[P_\phi = \sum_{k=1}^M \pi_k \int_{\Real^n} 1_{\phi(X)\neq k} p_k(X) dX\]
where $p_k(X) = p(X\rvert \theta_k)$. Then
\[h[P_\phi] + P_\phi \log(M+1 )\geq \ex H(Y^n)\]
with
\begin{align*}
	h(x) &= - x \log x - (1-x) \log (1-x)\\
	H(X) &= - \sum_{k=1}^M \Pr(\theta=\theta_k\rvert X) \log \Pr(\theta=\theta_k\rvert X)\\
	\Pr(\theta=\theta_k\rvert X) &= \frac{\pi_k p_k(X)}{\sum_{k=1}^M \pi_k p_k(X)}
\end{align*}
The map $h$ obeys $h(x)\leq 1$ whence
\[1+P_\phi \log(M+1 )\geq h[P_\phi] + P_\phi \log(M+1)\geq \ex H(Y^n)\]
and $P_\phi \log(M+1)\geq \ex H(Y^n)-1$.

Indeed, put 
\[q_{\phi}(X) = \sum_{k\neq \phi(X)}\Pr(\theta=\theta_k\rvert X)\]
then the conditional entropy becomes
\[
H(X)
= - \sum_{k\neq \phi(X)} \Pr(\theta=\theta_k\rvert X) \log \Pr(\theta=\theta_k\rvert X)
- \Pr(\theta=\theta_{\phi(X)}\rvert X) \log \Pr(\theta=\theta_{\phi(X)}\rvert X)
\]
whence the first term becomes
\[
\sum_{k\neq \phi(X)}
\frac{\Pr(\theta=\theta_k\rvert X)}{q_{\phi}(X)} q_{\phi}(X) \Bigl(
-\log \frac{q_{\phi}(X)}{\Pr(\theta=\theta_k\rvert X)} - \log q_{\phi}(X)
\Bigr)
\]
and the second term
\[
\Pr(\theta=\theta_{\phi(X)}\rvert X) \log \frac{1}{\Pr(\theta=\theta_{\phi(X)}\rvert X)}
\]

By the conditional version of Jensen's inequality for $-\log x$ applied to the
conditional distribution (?) we get ($\ex_p \log f \leq \log \ex_p f$)
\[
H(X)
\leq  \log \sum_{k\neq \phi(X)} \frac{\Pr(\theta=\theta_k\rvert X)}{q_{\phi}(X)} q_{\phi}(X)
\Bigl(\frac{q_{\phi}(X)}{\Pr(\theta=\theta_k\rvert X)}\Bigr)
+ \ldots
= \log(M-1) + \ldots
\]


A stationary Gaussian sequence $Y_n$ from $\xi_n\sim \Ncal(0,1)$ can be derived
using
\[
Y_n = \sum_{k=-\infty}^n h_{n-k} \xi_k
\]
where $h_k$ are defined as such terms in the analytical expression of 
\[
\sum_{i\geq 0} h_i z^i = \text{exp}\{ \sum_{j=0}^\infty f_j z^j \}
\]
where
\[\phi_j = \int_{-\sfrac{1}{2}}^{\sfrac{1}{2}} e^{2\pi i \lambda j } \log f_Y( \lambda ) d\lambda \]

% section lecture_16 (end)

\section{Lecture \# 17} % (fold)
\label{sec:lecture_17}

\subsection{A sequence of uniform random variables} % (fold)
\label{sub:a_sequence_of_uniform_random_variables}

Constructing a countable sequence of independent random variables is done as
follows: pick a random number $x\sim \mathcal{U}[0,1]$ and consider its infinite
decimal expansion:
\[x = \sum_{k\geq 1}x_k \frac{1}{10^k}\]
then use the natural bijection between $\mathbb{N}^2\to\mathbb{N}$ to get the following
collection of random variables:
\[y_k = \sum_{n\geq 1} x_{\phi(k,n)}\]
these random variables are independent and identically uniformly distributed on $[0,1]$.
Using the inversion method one can get a sequence of independently and arbitrarily
distributed random variables.

% subsection a_sequence_of_uniform_random_variables (end)

\subsection{Wiener process} % (fold)
\label{sub:wiener_process}

A standard Wiener process is a random function $W(t)$ on $[0,1]$ with the following
properties:
\begin{itemize}
	\item $W(0) = 0$;
	\item independence of finite (non-overlapping) differences $\bigl(W(t_{i+1})-W(t_i)\bigr)_{i=1}^n$
	for any $n\geq 2$;	
	\item for any $t>s$ the difference $W(t)-W(s)\sim \Ncal(0, t-s)$.
\end{itemize}
the existence of a uncountable family of independent standard normal random variables
is assumed proven (?).

Thus $W(0) = 0$ and $W(1) \sim \Ncal(0,1)$. Consider a time $t=\frac{1}{2}$. How to
construct a variable $W(\frac{1}{2})$ given $W(1)$.
The density of a normal random variable $W(1)$ is 
\[p_1(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}\]
find the density
\[p_\frac{1}{2}(x|W(1)= x) = ?\]
Thus the joint density $p(x_1, x_2)$ must satisfy the requirements of a Wiener process.

After some thought the conditional density of $W(\frac{1}{2})$ given $W(1)$ is gaussain
with parameters $\mu = \frac{W(1)}{2}$ and $\sigma^2 = \frac{1}{2}$.
Now on the second iteration,
\begin{align*}
	W(\frac{1}{4})|W(\frac{1}{2}) &\sim \Ncal(w(\frac{1}{2})\frac{1}{2}, \frac{1}{4})\\
	W(\frac{3}{4})|W(\frac{1}{2}),W(1) &\sim \Ncal((W(\frac{1}{2})+W(1))\frac{1}{2}, \frac{1}{4})
\end{align*}
Thus on the $n$-th iteration we get the following:
\[
W_n(t) = \sum_{k=1}^{2^n-1} z_{n,k}(t) \cdot \xi_{\phi(n,k)}
\]
where 
\[
z_{n,k}(t) = \int_0^t 1_{( \frac{k}{2^n}, \frac{2k+1}{2^{n+1}} ]} - 1_{( \frac{2k+1}{2^{n+1}}, \frac{k+1}{2^n} ]} ds
\]
and $\xi\sim\Ncal(0,1)$.

Note that 
\[
\max_{0\leq s\leq 1} |f_n(s)| = \max_{n<\log_2 i \leq n+1} |\xi_i| \frac{1}{2^n}
\]
Thus
\begin{align*}
	\pr\Bigl( \max_{0\leq s\leq 1} |f_n(s)| \geq \epsilon_n \Bigr)
	&= \pr\Bigl( \max_{n<\log_2 i \leq n+1} \xi_i \geq 2^n \epsilon_n \Bigr)\\
	&= 1 - \pr\Bigl( \max_{n<\log_2 i \leq n+1} \xi_i < 2^n \epsilon_n \Bigr)\\
	&= 1 - \Bigl( \pr\Bigl( \xi < 2^n \epsilon_n \Bigr)\Bigr)^{2^n}\\
	&= 1 - \Bigl( 1 - \pr\Bigl( \xi \geq 2^n \epsilon_n \Bigr)\Bigr)^{2^n}\\
	&\leq 1 - \Bigl( 1 - 2^n \pr\Bigl( \xi \geq 2^n \epsilon_n \Bigr)\Bigr)\\
	&= 2^n \pr\Bigl( \xi \geq 2^n \epsilon_n \Bigr)
\end{align*}

The chance that $\xi\sim\Ncal(0,1)$ exceeds some threshold is given by
\begin{align*}
	\pr(|\xi| \geq a)
	&= 2 \pr(\xi \geq a )
	= 2 \int_a^\infty  \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}} du\\
	&= \frac{\sqrt{2}}{\sqrt{\pi}} \int_a^\infty e^{-\frac{u^2}{2}} du\\
	&= \frac{\sqrt{2}}{\sqrt{\pi}} \frac{1}{a} e^{-\frac{a^2}{2}}
\end{align*}
because
\begin{align*}
\int_a^\infty e^{-\frac{u^2}{2}} du
&= \int_a^\infty \frac{1}{u} d -e^{-\frac{u^2}{2}}\\
&= \frac{1}{a} e^{-\frac{a^2}{2}} - \int_a^\infty \frac{1}{u^2} e^{-\frac{u^2}{2}} du\\
&\leq \frac{1}{a} e^{-\frac{a^2}{2}}
\end{align*}

Therefore
\[
\pr\Bigl( \max_{0\leq s\leq 1} |f_n(s)| \geq \epsilon_n \Bigr)
\leq \frac{\sqrt{2}}{\sqrt{\pi}} \frac{2^n}{2^n \epsilon_n} e^{-2^{2n-1} \epsilon_n^2}
= \frac{\sqrt{2}}{\sqrt{\pi}} \frac{1}{\epsilon_n} e^{-2^{2n-1} \epsilon_n^2}
\]
Let $\epsilon_n = \frac{1}{\alpha^n}$ for $\alpha\in(1,2)$, whence
\[
\pr\Bigl( \max_{0\leq s\leq 1} |f_n(s)| \geq \alpha^{-n} \Bigr)
\leq \frac{\sqrt{2}}{\sqrt{\pi}} e^{n\ln \alpha - \frac{1}{2} \Bigl( \frac{2}{\alpha}\Bigr)^{2n}}
< e^{- \frac{1}{2} \Bigl( \frac{2}{\alpha}\Bigr)^{2n}}
\]

\noindent\textbf{The first Borel-Cantelli lemma}\hfill\\
Consider a probability space $(\Omega, \Fcal, \Pr)$ and a countable sequence of sets
$(E_n)_{n\geq1}\in \Fcal$ such that $\sum_{n\geq1} \pr\bigl(E_n\bigr)$ converges. Then
then
\[
\pr\bigl( E_n \text{- ininitely often} \bigr)
= \pr\bigl( \limsup E_n \bigr)
= \pr\bigl( \limsup \bigcap_{n\geq1} \bigcup_{k\geq n} E_k \bigr)
= 0
\]
Indeed, consider $G_n = \bigcup_{k\geq n} E_k$ and $G = \bigcap_{n\geq1} G_n$. Since
$\Fcal$ is a $\sigma$-algebra and these sets are formed via countable unions or
intersections. Thus for all $n\geq1$ due to the union bound
\[
\pr(G)
\leq \pr(G_n)
\leq \sum_{k\geq n} \pr(E_k)
\]
The fact that $\sum_{k\geq1} \pr(E_k)$ converges, implies that
\[
\limsup_{n\to 1\infty}\sum_{k\geq n} \pr(E_k) = 0
\]
whence
\[
\pr(G)
\leq \limsup_{n\to 1\infty} \sum_{k\geq n} \pr(E_k)
= 0
\]
and $\pr\bigl( E_n \text{- i.o.} \bigr) = 0$.

Therefore
\[ \pr\bigl( \max_{0\leq s\leq 1} |f_n(s)| \geq \alpha^{-n} \text{ -i.o.} \bigr) = 0 \]
because the series converge
\[ \sum_{n\geq 1} e^{- \frac{1}{2} \Bigl( \frac{2}{\alpha} \Bigr)^{2n}} < +\infty \]




% subsection wiener_process (end)

% section lecture_17 (end)

\section{Lecture \# 18} % (fold)
\label{sec:lecture_18}

\subsection{Markov chains} % (fold)
\label{sub:markov_chains}

What is meant by aperiodicity? A trivial example is a markov chain with countably
many states corresponding to a random walk on an integer mash with admissible
transitions being $\pm1$. This chain is $2$-periodic.

A period is the greatest common divisor (minimal) of the length of all simple cycles
passing through it. Consider an irreducible chain with states (vertices) with different
periods. Since any pairs of states are connected, thus it is possible to linger at
state $u$ with period $L_u$, then make a one-hop transition (due to irreducibility
every state is reachable from every other state) to a state $v$ with another period
$L_v$ and then return to $u$. Thus the cycle through $u$ becomes $L_v$. If $L_v$
is odd, then $L_u$ is not the greatest common divisor.

In general, is vertices in an irreducible chain have periods $k$ and $n$, then
connectivity implies that both must have same period with $\text{gcd}(k,n)$.

If $P$ is a transition kernel of an irreducible Markov chain, then there exists
$N\geq 1$ such that $P^n$ has non-zero elements for all $n\geq N$.

Suppose the kernel $P$ has all non-zero elements. We want to show that there exists
the stationary distribution $\pi^*$ with $\pi^*=\pi^*P$. Let's employ the principle of
contraction mappings: if $f:X\to X$ in a metric space $(X,d)$ is such that there exists
$|\beta| < 1$ such that $d(f(x), f(y)) < \beta d(x,y)$ for all $x,y\in X$, then
there a unique fixed point $z\to X$: $z=f(z)$. Thus in order to employ this principle
one just has to find a proper metric space of the distributions such that $\pi \to \pi P$
is a contraction.

First of all the distributions of a finite sate chain constitute
and $n$-dimensional simplex
\[S_n \bigl\{ \pi\in [0,1]^n \big\rvert \sum_k \pi_k = 1 \bigr\}\]
and define a metric between $\pi$ and $\sigma$ as follows:
\[
d(\pi,\sigma)
= \sum_k |\pi_k-\sigma_k|
\]
One neat trick is that basic convex optimization yields
\[
d(\pi,\sigma)
= \max_{s_k = \pm 1} \sum_k s_k(\pi_k-\sigma_k)
= \max_{|s_k| \leq 1} \sum_k s_k(\pi_k-\sigma_k)
\]
note that the functional to be maximized is additively separable.

Consider the effect of $P$ on $d$:
\begin{align*}
	d( \pi P,\sigma P)
	&= \max_{|s_k| \leq 1} \sum_k s_k ([\pi P]_k-[\sigma P]_k)\\
	&= \max_{|s_k| \leq 1} \sum_k \sum_j s_k (\pi_j P_{kj} - \sigma_j P_{kj})\\
	&= \max_{|s_k| \leq 1} \sum_j (\pi_j - \sigma_j ) \sum_k s_k P_{kj}\\
	&= \Bigl[\eta_j = \sum_k s_k P_{kj} \Bigr]
	&= \max_{|s_k| \leq 1} \sum_j (\pi_j - \sigma_j ) \eta_j
\end{align*}
It is not very easy to see that $|\eta_j| \leq 1 - 2\min_{i,k} P_{kj}$. Indeed,
it is impossible for all $s_k$ to have the same sign, because they sort of depend
on the differences $[\pi P]_k-[\sigma P]_k$ of elements of probability distributions
which should \textbf{sum} to $1$. Thus for $\lambda = \min_{i,k} P_{kj}$
\[-1+2\lambda \leq \eta_j \leq 1-2\lambda\]
and
\begin{align*}
	d( \pi P,\sigma P)
	&= \max_{|s_k| \leq 1} \sum_j (\pi_j - \sigma_j ) \eta_j\\
	&\leq \max_{|\eta_j| \leq 1-2\lambda} \sum_j (\pi_j - \sigma_j ) \eta_j\\
	&= (1-2\lambda) \max_{|s_j| \leq 1 } \sum_j (\pi_j - \sigma_j ) s_j\\
	&= (1-2\lambda) d(\pi,\sigma)
\end{align*}
whence $P$ is a contraction.

% subsection markov_chains (end)

% section lecture_18 (end)

\section{Lecture \# 19} % (fold)
\label{sec:lecture_19}

\subsection{maximum likelihood hypothesis testing} % (fold)
\label{sub:maximum_likelihood_hypothesis_testing}

Consider a random sample $Y^n = (Y_k)_{k=1}^n\sim p_\theta(\cdot)$ and two hypotheses:
\[H_0: \theta\in \Theta_0\text{ and }H_1: \theta\in \Theta_1\]
The decision rule is $\phi(Y^n) = 1_{T(Y^n)\geq t_\alpha}$
where 
\[T(Y^n) = \frac{\max_{\theta_0\in \Theta_0} p(Y^n|\theta_0)}{\max_{\theta_1\in \Theta_1} p(Y^n|\theta_1)}\]
and $t_\alpha$ is such that $\ex_{\theta_0} \phi(Y^n) \leq \alpha$ for $\theta_0\in \Theta_0$.

Let $Y_i\sim \Ncal(\mu, \sigma^2)$. Consider testing a simple hypothesis in the
setting when $\sigma^2$ is unknown:
\[H_0: \mu=\mu_0\text{ and }H_1: \mu\neq \mu_0\]
The decision rule in this case is
\[
\phi(Y^n) = I\Bigl\{ 
\max_{\theta_0\in \Theta_0} \log p(Y^n|\theta_0) - \max_{\theta_1\in \Theta_1} \log p(Y^n|\theta_1) \geq t_\alpha
\Bigr\}
\]
with 
\[
\pr\Bigl( \max_{\theta_0\in \Theta_0} \log p(Y^n|\theta_0) - \max_{\theta_1\in \Theta_1} \log p(Y^n|\theta_1) \geq t_\alpha \Bigr) = \alpha
\]
In the gaussain case 
\[
L(\theta|Y^n)
= \log p(Y^n|\theta)
= - \frac{n}{2} \log 2\pi
  - \frac{n}{2} \log \sigma^2
  - \frac{1}{2\sigma^2} \sum_{k=1}^n (Y_k - \mu)^2
\]
maximizing with respect to $\sigma$ with $\mu$ fixed yields
\[\hat{\sigma}^2_n = \frac{1}{n} \sum_{k=1}^n (Y_k - \mu)^2\]
whence
\[ \hat{L}_n(\mu|Y^n) = - \frac{n}{2} - \frac{n}{2} \log 2\pi \hat{\sigma}^2_n \]
maximizing $\hat{L}_n(\mu|Y^n)$ with respect to $\mu$ gives
\[
\hat{\mu}_n
= \frac{1}{n}\sum_{k=1}^n Y_k
\]
The test becomes
\[
\phi(Y^n)
 = I\Bigl\{ 
\frac{n}{2} \bigl( \log 2\pi \hat{\sigma}^2_n(\mu_0) 
 - \log 2\pi \hat{\sigma}^2_n(\hat{\mu}_n) \bigr)
\geq t_\alpha
\Bigr\}
\]
whence
\[
\phi(Y^n)
 = I\Bigl\{ 
\frac{\hat{\sigma}^2_n(\mu_0)}{\hat{\sigma}^2_n(\hat{\mu}_n)}
\geq t_\alpha'
\Bigr\}
\]

Expanding the numerator results in
\[
\frac{\hat{\sigma}^2_n(\mu_0)}{\hat{\sigma}^2_n(\hat{\mu}_n)}
= \frac{\sum_{k=1}^n (Y_k - \mu_0)^2}{\sum_{k=1}^n (Y_k - \hat{\mu}_n)^2}
= \frac{\sum_{k=1}^n (Y_k - \hat{\mu}_n)^2 + n( \hat{\mu}_n -\mu_0 )^2}{\sum_{k=1}^n (Y_k - \hat{\mu}_n)^2}
\]
and finally 
\[
\phi(Y^n)
 = I\Bigl\{ 
\frac{\sqrt{n} \sqrt{\hat{\mu}_n -\mu_0} }{\sfrac{\sqrt{\sum_{k=1}^n (Y_k - \hat{\mu}_n)^2}}{\sqrt{n-1}} }
\geq \hat{t}_\alpha
\Bigr\}
\]
since $\ex_{\theta_0}\phi(Y^n) = \alpha$ it must be true that
\[\pr\bigl(T(Y^n) \geq \hat{t}_\alpha\bigr) = \alpha\]
and 
\[
T(Y^n)
= \frac{\sqrt{n} \sqrt{\hat{\mu}_n -\mu_0} }{\sfrac{\sqrt{\sum_{k=1}^n (Y_k - \hat{\mu}_n)^2}}{\sqrt{n-1}} }
\]
The distribution of the test statistic is Student's-$t$ with $n-1$ degrees of freedom.

% subsection maximum_likelihood_hypothesis_testing (end)

\subsection{Discrete distribution case} % (fold)
\label{sub:discrete_distribution_case}

Consider a discrete random variable $(Y_i)_i\in \{S_1\ldots S_d\}$ with distribution
$\Pr(Y_i = S_k) = p(k)$. And let's study the following hypotheses:
\[ H_0: p = p_0\text{ and } H_1: p\neq p_0 \]
The statistical decision rule in this case is $ \phi(Y^n) = 1_{T(Y^n) \geq t_\alpha} $,
where 
\[
T(Y^n)
= \max_{p} \sum_{i=1}^n \log p(Y_i) - \sum_{i=1}^n \log p_0(Y_i)
\]
Now
\[
\sum_{i=1}^n \log p(Y_i)
= \sum_{i=1}^n \sum_{k=1}^d 1_{Y_i = S_k} \log p_k
= \sum_{k=1}^d \Bigl( \sum_{i=1}^n 1_{Y_i = S_k} \Bigr) \log p_k
= n \sum_{k=1}^d \hat{p}_k \log p_k
\]
and $\hat{p}_k$ is the empirical distribution function. Let's maximize $\sum_{i=1}^n \log p(Y_i)$.
We get an equivalent problem (by omitting $n$ multiplier)
\[
\max_{p} \sum_{k=1}^d \hat{p}_k \log p_k - \sum_{k=1}^d \hat{p}_k \log \hat{p}_k
= \min_{p} - \sum_{k=1}^d \hat{p}_k \log \frac{p_k}{\hat{p}_k}
\geq 0
\]
whence $p_k = \hat{p}_k$ for all $k$.

Therefore we get a test abase on the empirical Kullback-Leibler divergence and
the expression of the log-likelihood
\[
\{ T(Y^n) \geq t_\alpha \}
= \{
\sum_{k=1}^d \hat{p}_k \log \hat{p}(k) - 
\sum_{k=1}^d \hat{p}_k \log p_0(k)
\geq t_\alpha
\}
= \{ \text{KL}( \hat{p}\| p_0) \geq t_\alpha \}
\]

The Law of Large Numbers implies that under the null
\[ \hat{p}_k \overset{\pr}{\to} p_0(k) \]
Consider the logarithm
\begin{align*}
	\log \frac{\hat{p}_k}{p_0(k)}
	&= - \log \Bigl( \frac{p_0(k)}{\hat{p}_k} - 1 + 1 \Bigr)\\
	&\approx - \frac{p_0(k)}{\hat{p}_k} + 1
		+ \frac{1}{2} \Bigl( \frac{p_0(k)}{\hat{p}_k} - 1 \Bigr)^2
\end{align*}
therefore
\[
T(Y^n) = \frac{n}{2} \sum_{k=1}^d \frac{(p_0(k)-\hat{p}_k)^2}{\hat{p}_k}
\]

Theorem
\[ \lim_{n\to \infty} \pr\Bigl( T(Y^n) \leq a \Bigr) = \pr\Bigl( \chi^2_{d-1} \leq a\Bigr)\]
Indeed, let $\xi^n_k  = \sqrt{n}\frac{-p_0(k)+\hat{p}_k}{\sqrt{p_0(k)}}$. Thus
$\ex\xi^n_k = 0$ and 
\[
\ex \xi^n_i \xi^n_j
= \frac{1}{\sqrt{p_0(i)}\sqrt{p_0(j)}}
 \times \sum_{s,i=1}^n \frac{1}{n^2} \ex [1_{Y_i = S_k} - p_0(k)][1_{Y_s = S_j} - p_0(j)]
\]
after some simplification we get
\[
\ex \xi^n_i \xi^n_j
= 1_{i=j} \frac{p_0(j)}{\sqrt{p_0(i) p_0(j)}} - \sqrt{p_0(i) p_0(j)}
= 1_{i=j} - \sqrt{p_0(i) p_0(j)}
\]

Using the Central Limit Theorem one gets
\[
\xi^n_j \overset{\Dcal}{\to} \Ncal_d(0, \Sigma)
\]
where $\Sigma_{ij} = 1_{i=j} - \sqrt{p_0(i) p_0(j)}$

Consider
\[\eta_k = \epsilon_k - \langle \epsilon, q_0\rangle q_0\]
where $\epsilon \sim \Ncal_d(o, I_d)$ and $q_{0k} = \sqrt{p_0(k)}$ for $k=1,\ldots,d$.
For an orthonormal basis $q_k$ in $\Real^d$ with $q_0$ defined above, one get
\[
\sum_{k=1}^d
= \sum_{k=1}^{d-1} \langle \epsilon, q_k \rangle
\sim \chi^2_{d-1}
\]

Comparison criterion
Consider two samples $Y^n\sim \Ncal(\mu_Y,\Sigma^2_Y)$ and $Z^m\sim \Ncal(\mu_Z,\Sigma^2_Z)$.
Lets use maximum likelihood to test the hypotheses
\[H_0: \mu_Y = \mu_Z, \sigma^2_Y = \sigma^2_Z\text{ and } H_1:\text{ otherwise}\]
The log-likelihood for the null is
\[
L_0(Y^n,Z^m;\mu,\sigma^2)
= \prod_{i=1}^n p(Y_i;\mu,\sigma^2)\prod_{j=1}^m p(Z_i;\mu,\sigma^2)
\]
and under the alternative:
\[
L_1(Y^n,Z^m;\mu_Y,\sigma^2_Y,\mu_Z,\sigma^2_Z)
= \prod_{i=1}^n p(Y_i;\mu_Y,\sigma^2_Y) \prod_{j=1}^m p(Z_i;\mu_Z,\sigma^2_Z)
\]
Recalling the values of the log-likelihood in its maximum for both likelihoods
\[
\hat{L}_0(Y^n,Z^m)
= -\frac{n+m}{2} - \frac{n+m}{2} \log 2\pi
	\min_\mu \frac{m\sum_{i=1}^n (Y_i - \mu)^2 + n\sum_{i=1}^m (Z_i - \mu)^2}{n+m}
\]
whence
\[ \hat{\mu} = \frac{m\bar{Y}_n + n\bar{Z}_m}{n+m} \]

Thus
\[
T(Y^n,Z^m)
= -\frac{n+m}{2} - \frac{n+m}{2} \log 2\pi
  - \frac{n}{2} \log \frac{\sum_{i=1}^n (Y_i - \bar{Y}_n)^2}{n}
  - \frac{m}{2} \log \frac{\sum_{i=1}^m (Z_i - \bar{Z}_m)^2}{m}
- \frac{n+m}{2}\Bigl( 1
	+ \log 2\pi \frac{m\sum_{i=1}^n (Y_i - \hat{\mu})^2 + n\sum_{i=1}^m (Z_i - \hat{\mu})^2}{n+m}
\Bigr)
\]

% subsection discrete_distribution_case (end)

% section lecture_19 (end)

\end{document}
