\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Ntrl}{\mathbb{N}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\Lcal}{\mathcal{L}}

\newcommand{\argmax}{\mathop{\text{argmax}}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Structural analysis and visualization of networks}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}

\selectlanguage{english}

\maketitle

\begin{description}
	\item[Email] \hfill \\
	lzhukov@hse.ru
	\item[Course webpage] \hfill \\
	\url{http://www.leonidzhukov.net/hse/2015/networks/}
	\url{http://www.leonidzhukov.net/hse/2014/socialnetworks/}
\end{description}

Programming: iPython notebooks
Visualization: yEd, Gephi

Linear algebra prerequisites:
	Spares matrices
	Eigenanalysis


Graph $G=(V,E)$. The set $V$ is the set of vertices and $E$ is a subset of $V\times V$.
An element $(u,v)\in E$ is an edge starting at $u$ and ending in $v$.
The incidence matrix is defined as $a_{ij}=1_E\brac{(i,j)}$, so it denotes and edge $i\to j$.

Random graphs are pure mathematics theory created by Erd\"os and Renyi.
Statistical physics for analysis of complex networks.

Network, social network, complex network just another name for a graph.

Power law (scale free) few vertices with high degree, many nodes with few neighbours.

Complex means that reduction of a system actually destroys the systems.
Cannot predict the whole by the studying the parts.

Facebook network -- many worlds network -- typical for a power law random graph.

Complex networks usually have the following characteristics: \begin{enumerate}
	\item Power law distribution of the vertex degree;
	\item Small diameter and average path length;
	\item High propensity to cluster: the number of triangles in the network.
\end{enumerate}

Let's introduce the following local features of the graph: the vertex degree $\delta^+,\delta^-:V\to \Ntrl$
\begin{align*}
	\delta^+(v) &\defn \#\obj{\induc{u\in V}\, (u,v)\in E }\\
	\delta^-(v) &\defn \#\obj{\induc{u\in V}\, (v,u)\in E }
\end{align*}

``Any two people are on average separated no more than by six intermediate connections.''
\begin{itemize}
\item ``The small-world problem'', Stanley Milgram, 1967
\item ``An experimental study of the small-world problem'', Travers J., Milgram S., 1969
\end{itemize}

Bethe lattice $(V,E)$ is an infinite cycle-free graph with every node having the same number of neighbours $z > 1$.
Given an edge $(v,u)\in E$ the end vertex $u$ is connected to $z-1$ other neighbours, which means that $N_k = N_{k-1}\cdot (z-1)$ with $N_1 = z$, since the centre vertex is connected to $z$ other vertices.
Thus $N_k = z \brac{z-1}^{k-1}$.
And the total number of nodes is \[S_n \defn 1+\sum_{k = 1}^n N_k = 1 + z \frac{\brac{z-1}^n-1}{(z-1)-1}\] (check this!)

For any network $G=(V,E)$ its \textbf{order} is $\abs{V}$ and size is $\abs{E}$.

% As for the format of homework, python + \Latex is acceptable.

% Homework: complete the notebook.!!! -- Completed on 2014-12-17

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}

THe power law distribution is very natural in the study of networks:
\[p(x) \defn \frac{C}{x^\beta} 1_{\clop{x_0,+\infty}}(x)\]
where $\beta > 0$ is the power.

The indefinite integral of $p(x)$ is equal to
\[\int p(x) dx = \begin{cases}
    C \frac{x^{1-\beta}}{1-\beta},& \text{if } \beta\neq 1\\
    C \ln x, & \text{if } \beta = 1
\end{cases}\]

If $\beta > 1$ then $p(x)$ is a density function and the normalisation constant is determined by
\[\induc{C \frac{x^{1-\beta}}{1-\beta}}^\infty_{x_0} = C \brac{ - \frac{x_0^{1-\beta}}{1-\beta}} = 1\]
whence $C\defn (\beta - 1) x_0^{\beta-1}$. In its final form the density looks like:
\[p(x) \defn \frac{\beta - 1}{x_0} \brac{\frac{x}{x_0}}^{-\beta} 1_{\clop{x_0,+\infty}}(x)\]

The \textbf{survival} function $H(x) = 1 - F(x)$ is useful in network analysis. For the power law the survival function is \[H(x) \defn \brac{\frac{x_0}{x}}^{\beta-1}\]

Median is more suitable for estimating the power law distribution parameters. The quantile function is given by:
\[q_\alpha \defn \brac{1-\alpha}^{-\frac{1}{\beta - 1}} x_0 \]

Power law is scale invariant. Indeed 
\[H(s x) = \brac{\frac{x_0}{s x}}^{\beta - 1} = \brac{s}^{1-\beta} H(x)\]

Consider the node degree distribution of an undirected network.
There will be a majority of nodes with low degree,
and there will be very few vertices with extremely high degree.

Node degree -- the number of nearest neighbours
Degree distribution \[P(k) \defn \frac{n_k}{\sum_{k}n_k}\]
the model is $P(k) = C k^{-\gamma}$.
Normalization -- the Riemann Zeta function.
\[C \sum_{k\geq 1} k^{-\gamma} = 1 \Leftrightarrow C \defn \frac{1}{\zeta(\gamma)}\]

In maximum likelihood vary $x_0$ to get the idea of how $\alpha$ depends on it.

Use Kolmogorov-Smirnov test against the exponential distribution to find the ``best'' values of $x_0$.
Look for the minimal value of the K-S statistic and choose $x_0$.

% read the references

\subsubsection{The maximum likelihood estimation} % (fold)
\label{ssub:the_mle}

Suppose $\brac{x_i}_{i=1}^n$ is an iid sample from some random variable distributed according to the power law.
The log-likelihood is given by \[\log\mathcal{L} = \sum_{k=1}^n \beta \log\frac{x_0}{x_k} + n \log\frac{\beta - 1}{x_0}\]
where $x_0\leq \min_{k=1\ldots n}x_k$, since otherwise the log-likelihood would be $-\infty$.
The first-order conditions are given by \begin{align*}
	- \sum_{k=1}^n \log\frac{x_k}{x_0} + \frac{n}{\beta - 1} = 0\\
	n \brac{ \beta - 1 } \frac{1}{x_0} > 0
\end{align*}

Therefore $\hat{x}_0 \defn \min_{k=1\ldots n}x_k$ and the ML-optimal estimator of $\beta$ is
\[\hat{\beta} \defn 1 + \frac{n}{\sum_{k=1}^n ( \log x_k - \log x_0 )}\]

% subsubsection the_mle (end)

\subsubsection{The OLS estimation} % (fold)
\label{ssub:the_ols_estimation}

Another possibility is to estimate the parameters of the power law by means of a regression of the frequencies of binned sample on the ``typical'' values of the bins.

Suppose $\brac{f_i}_{i=1}^B$ and $\brac{b_i}_{i=1}^B$ are bin frequencies and centres respectively, constructed on data $\brac{x_k}_{k=1}^n$ which supposedly came from a power law distribution.

Then it is possible to fit the following regression model to the histogram data to get the parameters of the law:
since $f_x \sim p(x) \Delta x$
\[\log f_i \sim \brac{\log{(\beta-1)} + (\beta-1) \log x_0} + (-\beta) \log b_i\]
It is a fast but a rather crude way of estimating.

% subsubsection the_ols_estimation (end)

% section lecture_2 (end)

\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}

Graph models

Erd\"os-Rnyi model

A random graph is a element of the set $\prod_{\omega\in \Pwr_2(V)} \obj{0,1}$.
A large collection of Bernulli random variables, indicating whether an edge is present or not.

$G_{n,m}$ model -- a graph is randomly selected form a set  $C_N^m$ graphs, with $N\defn \frac{n(n-1)}{2}$ of with $n$ nodes and $m$ edges.

$G_{n,p}$ -- the model of a random graph in which an edge between
any two vertices is established with probability $p$. The total
number of edges is again $N$. The size of the graph is a thus a
random number.

The models are asymptotically equivalent.

The mean node degree is given by
\[\epsilon(G) = \frac{2 \ex(m)}{n} = p\frac{2 n(n-1)}{2 n} \approx p n\]
The graph density is the \[\rho \defn 2\frac{\ex(m)}{n(n-1)}\]

The degree distribution of a random graph.

The chance that a given node $v$ has $k$ neighbours is given by:
\[\pr\brac{\delta(v) = k} = C^k_{n-1} p^k \brac{1-p}^{n-k-1}\]

Since the binomial distribution is asymptotically poisson if $p n = \lambda$ and $n\to \infty$:
\[C^k_{n-1} p^k \brac{1-p}^{n-k-1} \to \]

Use these models as a benchmark (in the null hypothesis) to compare against the phenomena in the real life.

Treating the model parametrically to witness phase transitions.
\begin{description}
	\item[$p=0$] \hfill \\ the graph is a set of isolated vertices;
	\item[$p=p_c$] \hfill \\ the graph obtains a spanning tree;
	\item[$p=1$] \hfill \\ the graph is totally connected.
\end{description}

Suppose $p$ changes with $n$. 


The chance that a node does not belong the the Giant Connected Component,
in the case of Poisson node degree distribution is given by
\[ u = \sum_{k\geq 0} \pr(k) u^k = \sum_{k\geq 0} e^{-\lambda}\frac{\lambda^k}{k!}u^k = e^{\lambda (u-1)}\]
the necessary condition for a nonzero $u$ is that $\lambda>1$
(ensures that the graph of the exponent intersects the $45\,^{\circ}$ line at a nonzero point).

\begin{description}
	\item[$p\to p_c-$] \hfill \\ no components larger than $O(\log n)$;
	\item[$p=p_c$] \hfill \\ the largest component has $O(n^\frac{2}{3})$;
	\item[$p\to p_c+$] \hfill \\ the gigantic connected component is of the order $O(n)$.
\end{description}
The critical values is $p_c n = 1$.

Threshold values at which interesting phenomena appear -- subgraphs of order $g$:
\begin{description}
	\item[$p\sim n^\frac{-g}{g-1}$] there almost surely exists a tree of order $g$;
	\item[$p\sim n^{-1}$] there is a cyclce of order $g$;
	\item[$p\sim n^\frac{-2}{g-1}$] a clique of order $g$ (a complete subgraph).
\end{description}


The clustering coefficient: ratio of closed triangles to all possible triangles
\[C(k) = \frac{p k(k-1) 2 }{ k( k-1 ) 2} = p\]
The sparser the graph the more negligible is the clustering coefficient.

The average number of nodes $s$ hops away from the current node is $\lambda^s$.
In the critical regime of the edge probability, all nodes belong to the the GCC with high probability.
Thus $d \sim \frac{\log n}{\log \lambda}$.

Fixing the issues with degree distribution.
The configuration model.
Take $V$ vertices and a finite sequence of node degrees $\brac{d_v}_{v\in V}$ with $\sum_{v\in V} d_v$ -- even.

% Slide~17.
The constructed graph might be a lopped multigraph.
Solution: use special graphical degree sequences.
The probability that $v,u\in V$ are connected is given by
\[p_{uv}\frac{d_u d_v}{2 m - 1}\]

Does not allow for an asymptotically non-zero clustering coefficient.

Very different from the Erd\"os-Renyi model!

A random graphs does not have communities.

Non-zero clustering coefficient hint at non-random processes underlying the edge formation.

% section lecture_3 (end)

\section{Lecture \# 4} % (fold)
\label{sec:lecture_4}

Dynamical grove (?) model.

Small Worlds model [Watts and Strogatz; 1998]

Previously considered networks were static fixed number of vertices and edges.

Let's have look at evolving graphs.
Growing networks (evovling with time)
\begin{itemize}
	\item Citation networks;
	\item Collaboration networks;
	\item Web;
	\item Social networks.
\end{itemize}

Consider pure growth model of an network (random growing network)
\begin{itemize}
	\item $t=0$ there are $n_t$ unconnected nodes;
	\item growth . on every time step $t\geq 1$ add a node with $m\leq n_0$ edges $k_t(t) = m$ attached randomly to the existing nodes;
	\item from $m$ edges between (?) existing nodes uniformly at random \[\Pi(k_i) = \frac{1}{n_0+t-1}\] (? probability of an edge getting connected to some existing node)
\end{itemize}

The expected degree of any node $i$ (the time the node was introduced):
\[k_i(t) = m + \sum_{k=0}^k\frac{m}{n_0+i-1 + k}\]

for $t>>n_0$
\[k_i(t) \approx m \brac{1 + \log\frac{t}{i}}\]

The longer the node has been in the system, the more degree it has: ``the first move-in advantage''.

If interpreted as a function of the time of join $i$ for a fixed $t$ (a snapshot), then it can show that a node the more recent nodes have lesser chances of having a high degree.


Find the nodes that at time $t$ have degree less than $k$: $\obj{\induc{i}\,k_i(t) < k}$

\[k_i(t) = m\brac{1+\log\frac{t}{i}}<k\,\implies\, i . t e^\frac{m-k}{m}\]

Fraction of nodes with degrees $k_i(t)<k$:
\[F(k) = \Pr\brac{k_i(t)<k} = \frac{n_0+t-i}{n_0+1} \approx 1 - e^\frac{m-k}{m}\]
Thus the degree distribution is approximately exponential.


Another way of getting at the same result: use continuous approximation!

Since $\Pi(k_i) = \frac{1}{n_0+t-i} \approx \frac{1}{t}$, the following must be true:
\[k_i(t+\delta t) = k_i(t) - \frac{m}{t}\delta t\]
whence the following first order differential equation emerges:
\[\frac{d}{dt}k_i(t) = \frac{m}{t}\]
with initial conditions $k_i(t=i) = m$.
the solution id $k_i(t) = m\brac{1+\log\frac{t}{i}}$.

% [Barabasi and Albert; 1999]
\begin{description}
	\item[$t=0$] there are $n_0$ nodes;
	\item[growth] on ever step add a node with $m$ edges ($m\leq n_0$), $k_i(i)=m$;
	\item[Preferential attachment] probability of linking it to an existing node $i$ is proportional to the node degree $k_i$
	\[Pi(k_i) = \frac{k_i}{\sum_j k_j}\]
\end{description}

Preferential attachment represents the heuristic rule that ``similar attract''.

\[k_i(t+\delta t) = k_i(t) + m \Pi(k_i)\delta t\]
\[\frac{d}{dt}k_i(t) = m \Pi(k_i) = m \frac{m k_i}{2 m t}\]
which has the solution: time evolution of a node $i$ degree
\[k_i(t) = m\Bigl(\frac{t}{i}\Bigr)^\frac{1}{2}\]

Nodes with degree less than $k$: $\frac{m^2}{k^2} t<i$

The power law emerges:
\[F(k) = \Pr\brac{k_i(t)<k} = \frac{n_0+t-i}{n_0+1} \approx \frac{n_0+t-\frac{m^2}{k^2} t}{n_0+1} \approx 1 - \frac{m^2}{k^2} t\]


Barabasi model has heavier tails than the random graph: indeed it is obvious from the exponent versus hyperbola comparison.

Had such model been a reality there would be extremely many poor, and a few extremely rich, who get richer with time.

The average path length (analytical result):
\[\brkt{L}\approx \frac{\log N}{\log\log N}\]

Clustering coefficient (simulation result):
\[C\approx n^{-\frac{3}{4}}\]
$C$ is infinitesimal as $N\to \infty$.

Barabasi (1999) preferential attachment model has been rediscovered multiple times:
\begin{itemize}
	\item Yule process 1925;
	\item Polya's urn model;
	\item Herbert simon;
	\item Evolution of citation networks, cumulative advantage model; Derek de Solla Price, 1976;
\end{itemize}

\subsection{The small world model} % (fold)
\label{sub:the_small_world_model}

Devised by Watts and Strogatz in 1998.

Image a two-tier ring graph, with structural triangles.

% subsection the_small_world_model (end)
The motivation of the small world model: keep high clustering, get small diameter (the greatest distance between any two nodes).

Evolution is to construct shortcuts so as to decrease the average path length.

Strogatz proposed to re-wire the graph: pick any edge, sever it, and wire it elsewhere.

start with regular lattice with $n$ nodes, $k$ edges per vertex $k<<n$.
randomly connect with 

Small world region:
\begin{itemize}
	\item high clustering coefficient;
	\item small average path length.
\end{itemize}

% section lecture_4 (end)

\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

Centrality measures of graphs

Which vertices in the network are important?

Is it possible to order the nodes in the network, so that the first node is the mos important, and the last is the least important.

What is meant by importance?

let's have a look at graph theoretic understanding of the centrality.

\noindent\textbf{Definition}\hfill\\
THe \textbf{eccentricity} of vertex $u\in V$ is the maximum distance between the vertex $u\in V$ and any other $v\in V$ in the graph $G$ 
\[\text{ecc}(u) \defn \max_{v\in V}\induc{d}_G(u,v)\]

The diameter is really the largest eccentricity: the larges possible distance between any two nodes in a graph.
\[\text{diam}(G)\defn \max_{u\in V} \text{ecc}(u)\]

The radius of a graph is the minimum eccentricity
\[\text{rad}(G)\defn \min_{u\in V} \text{ecc}(u)\]

The \textbf{central point} of a graph $G$ is a vertex, with eccentricity equal to the radius of $G$:
\[\text{diam}(G)\defn \max_{u\in V} \text{ecc}(u)\]

A graph \textbf{centre} is the set of all central nodes of $G$. The \textbf{periphery} of $G$ is the set of nodes with eccentricities equal to the diameter of $G$.

The just defined notions are not robust with respect to ``small'' changes to the graph: the may undergo extreme changes when the graph is perturbed. But what change is ``small'' for a given graph? (adding a chain may actually change the topology quite significantly)


Periphery is a graph is used in the sense other than graph theoretic (may be with respect to the vertex degree distribution).

The notion of vertex importance (from Sociology): the location of the node in a social graph is correlated with the influence of that vertex in the network (the ``ties'').

Notions of centrality usually deal with undirected graphs, corresponding to symmetric relations. Well what about prestige network relations?


The degree centrality is just the number of nearest neighbours for a graph incidence matrix $A$:
\[C_D(u) = \delta_u = \sum_{v\in V} A_{uv}\]

Normalized graph centrality, defined as $C_D^*(u) \defn \frac{C_D(u)}{\abs{V}-1}$, is useful for comparison between graphs, not just within one graphs.

High centrality means direct contact with many other nodes. Low centrality degree -- an in active vertex, a peripheral vertex.

Calculate how close the node is to any other vertex in the system. This gives us the closeness centrality metric
\[C_C(u) \defn \frac{1}{\sum_{v\in V} d(u,v)}\]
It can be normalise by $\tfrac{1}{\abs{V}-1}$.

Nodes in the centre can quickly interact with all others, have short communication path to others, and minimal number of steps to reach others.

There are problems for disconnected graphs, where infinite distances emerge. In theses cases use the harmonic centrality
\[C_H(u)\defn \brac{\sum_{v\in V} \frac{1}{d(u,v)}}^{-1}\]

The most interesting definition of centrality is betweenness centrality.

Consider the network as a set of interconnected communication hubs. The most important nodes are those, which are indispensable for proper network operation. 

Consider a graph $G = \brac{V,E}$.
The betweenness centrality measure of $G$ is defined as follows:
\[C_B(u) = \sum_{s\neg t\neq u}\frac{\sigma_{st}(u)}{\sigma_{st}}\] 

The normalised version of betweenness is just scaled by
\[\frac{2}{(n-1)(n-2)}\]

It is possible to extend the betweenness centrality to edges.

Probability that a communication from $s$ to $t$ will go through $u$ (geodesics) is given by $\frac{\sigma_{st}(u)}{\sigma_{st}}$.

Usage of these measures greatly depends on what is understood by ``centrality''.

\subsection{Eigenvector centrality} % (fold)
\label{sub:eigenvector_centrality}

Consider a recursive definition of centrality: An important nodes have important neighbours and connections.

Let $\brac{I^{t+1}_u}_{u\in V}$ be the importance vector at some iteration $t\geq1$, with components corresponding to each node. Then define the importance as the limit of the following process
\begin{align*}
	\sum_{j\in V} A_{ij} I^t_j \to I^{t+1}_i \\
	\frac{1}{\lambda}\sum_{j\in V} A_{ij} I_j = I_i
\end{align*}

Thus if the iterations converge, then they converge to an eigenvector of the adjacency matrix.

A node with large centrality will be connected with nodes of large centrality. Thus highly important nodes are likely to have highly important neighbours.

It is a kind of \emph{self-reinforced centrality}.

% subsection eigenvector_centrality (end)

\subsection{Katz centrality} % (fold)
\label{sub:katz_centrality}

Weighted count of the path coming to the node (Katz; 1953). The weight of a path of length $n$ is counted with attenuation factor $\beta^n$.
\begin{align*}
	k_u &= \sum_{v\in V} \brac{E_{uv} + \beta A_{uv} + \beta^2 A_{uv} + \ldots}\\
	k = \brac{\sum_{k\geq0} \beta^k A^k} \mathbf{1} \\
	k = \brac{E - \beta A}^{-1} \mathbf{1} \\
	\brac{E - \beta A} k = \mathbf{1} 
\end{align*}

The parameter $\beta$ controls the measure, if it is $1$ then Katz centrality reduces to the ... NO IT DOES NOT!

Two-parametric centrality measure

Let $\beta$ be the degree to which node's importance is a function of it's neighbours' importance, and $\alpha$ be a normalization parameter.
\[c_u(\alpha,\beta) \defn \sum_{v\in V}\brac{\alpha+\beta c_v} A_{uv}\]
In the matrix form:$c = \alpha A\mathbf{1} + A\mathbf{1} c$.

% subsection katz_centrality (end)

\textbf{Centralization} of the whole network is measure of how ``central'' is the most central node in relation to all other vertices.

%% Slide p.~18
\[C_x(G) \defn \frac{\sum_{v\in V} C_x(\bar{u}_G) - C_x(v) }{\max_H \sum_{v\in V} C_x(\bar{u}_H) - C_x(v)}\]
where $C_x$ is any vertex centrality measure and $\bar{p}_G$ is the central node of the graph $G$. The maximum is take over the graph of the same order as $G$.

Prestige is a measure of centrality of directed graphs.
Degree prestige $\delta_{\text{in}}(u)$ is the vertex in-degree.

The main contribution of the Page Rank algorithm is in how to deal with ``directedness'' of the graph.

It is insightful to compare the centrality measures between each other and see how they are correlated.
\begin{itemize}
	\item Pearson's correlation measures strength of linear association;
	\item Spearman's rank correlation -- strength of monotone relationship;
	\item Kendall $\tau$, count the number of pairwise agreements and disagreements between a pair of ranked data series. 
	\[2\frac{n_c - n_d}{n(n-1)}\]
	$n_c$ is the number of concordant pairs, and $n_d$ -- discordant pairs (agreements $\tau=1$, disagreements $\tau=-1$).
\end{itemize}

% section lecture_5 (end)

\section{Lecture \# 6} % (fold)
\label{sec:lecture_6}

\subsection{WEB search algorithms} % (fold)
\label{sub:web_search_algorithms}



% subsection web_search_algorithms (end)

% section lecture_6 (end)

\section{Lecture \# 7} % (fold)
\label{sec:lecture_7}

Assortative mixing and structural equivalence.
\begin{description}
	\item[Global] average node degree, average clustering coefficient, average path length;
	\item[local] $\ldots$;
\end{description}

Nodes $u, v\in V$ are \textbf{strongly} structurally equivalent if they share neighbours: $N^+(u) = N^+(v)$ and $N^-(u) = N^-(v)$. The problem is the connectivity pattern (directions and number of edges) is almost identical, yet $u\in N^-(v)$ and $v\in N^+(u)$.

In adjacency matrices this means that the $u$ and $v$ columns and rows are identical. However this definition is too strict: it requires \textbf{identical} connectivity.

Structural similarity of nodes

\begin{description}
	\item[Jaccard] \hfill\\
	For $u,v/in V$ the metric is defined as \[J(v,u) = \frac{\big\lvert N(v) \cap N(v) \big\rvert}{\big\lvert N(v) \cup N(v) \big\rvert}\] It show how much overlap there is between two nodes.
	\item[Cosine] \hfill\\
	Defined as the cosine of the angle between the column-vectors $u,v$ of the adjacency matrix $A$:
	\[\sigma_{uv} = \frac{ A_v'A_u }{\sqrt{ A_v'A_v }\sqrt{ A_u'A_u }}\]
	\item[Pearson]\hfill\\
	The straightforward sample correlation of $A_v$ and $A_u$ in the adjacency matrix $A$.
	\item[Binary adjacency matrix] \hfill\\
	Then the degree $k_v = \sum_{u\in V} A_{uv} = \sum_{u\in V} A_{uv}^2$. Furthermore $n_{uv}$ -- the number of shared neighbours is given by 
	\[n_{uv}\sum_{w\in V} A_{uw}A_{vw}\]
\end{description}

\noindent\textbf{Regular equivalence}\hfill\\
Two vertices are regularly equivalent if they are equally related to equivalent others.

In terms of role assignment (colouring): nodes of the same colour have neighbours of similar colours. Only special colourings permit this (slide~10).


Requires similar patterns in the vertex connectivity .

Recursive definition:
\[\sigma_{uv} = \alpha \sum_{s,t\in V} A_{us} A_{vt} \sigma_{st} + \delta_{uv} = \alpha \sum_{s\in V} A_{us} \sum_{t\in V} \sigma_{st} A_{tv}' + \delta_{uv}\]
where $\sigma_{uv}$ is the similarity score. In the matrix notation $\Sigma = \alpha A\Sigma A' + I$.

Another similarity metric
\[\sigma_{uv} = \sum_{t\in V} A_{ut}\sigma_{tv} + \delta_{uv}\]
which in the matrix form is $\Sigma = A \Sigma + I$.

Similarity is extremely important!

\subsection{SimRank} % (fold)
\label{sub:simrank}
Let $s(u,v)$ be the similarity metric between two nodes, and $i(\cdot)$ be the set of incoming neighbours. Then $s()$ is defined as 
\[ s(u,v) = \frac{C}{\abs{i(u)}\abs{i(v)}} \sum_{s\in i(u)} \sum_{t\in i(v)} s(s,t)\]
with $s(u,u)=1$. In the matrix notation
\[ S = \brac{\frac{C}{k_u k_v}}_{u,v}\odot A' S A\]

Staring the random walks from nodes $u$ and $v$, which follow the edges in the opposite direction (not to each other!), then $S_{uv}$ is the expected number of steps before the walks meet.

% subsection simrank (end)

\subsection{Mixing} % (fold)
\label{sub:mixing}

Suppose nodes have certain properties, label, colours of whatever.
Mixing means patterns in connectivity
\begin{description}
	\item[Like-to-like] Assortiative mixing: attributes of connected nodes tend to be more similar rather than disconnected;
	\item[Opposite-to-opposite] Disassortative mixing: the presentce of an edge between vertices makes attributes more likely to have low similarity.
\end{description}

Mixing is allowed with respect to any attribute, be it observed or latent., or even vertex degree (as in preferential attachment).

How much more often do attributes match across edges than expected at random? The modularity metric
\[ Q = \frac{m_c-\brkt{m_c}}{m} = \frac{1}{2m}\sum_{u,v\in V} \Big( A_{u,v} - \frac{k_u k_v}{2m} \Big) \delta(c_u,c_v)\]
where $c:V\to \mathcal{C}$ is the vertex labelling, class or attributes, and $m_c$ -- number of edges with the same attributes, and $\brkt{m_c} =\ex(\#_c)$.

The term $\frac{k_u k_v}{2m}$ is responsible to capturing the expected number or connections had the graph been random.

\subsubsection{Mixing with scalars} % (fold)
\label{ssub:mixing_with_scalars}

Suppose $X:V\to \Real$ is a random variable on the vertices. Average and covariance over edges
\[\brkt{x} = \frac{\sum_{v\in V}k_v x_v}{\sum_{v\in V}k_v} = \frac{\sum_{v\in V}k_v x_v}{2m} = \frac{1}{2m}\sum_{v,u\in V}A_{uv} x_v\]

%%% Missed some slides~19
% subsubsection mixing_with_scalars (end)

Node degree as an attribute.

Disassortative mixing -- high degree vertices are connected to low degree nodes, yields a start like structure.
Assortative mixing -- interconnected high degree nodes -- the core, low degree vertices -- periphery,

% subsection mixing (end)

% section lecture_7 (end)

\section{Lecture \# 8} % (fold)
\label{sec:lecture_8}

Network communities, Graph cliques, k-plex and k-cores.

What might be a good definition of a ``community'' in a graph?

First run a connected component extraction procedure, and then work with each connected component separately.

Main properties of a community \begin{itemize}
	\item mutuality of connections (ties);
	\item compactness: reachability in few number of steps;
	\item High density of edges within the group;
	\item Separation of a group from other groups.
\end{itemize}

A clique a complete fully connected subgraph of $G = (V,E)$.
Every pair of node connected by an edge is a $2$-clique.

A \textbf{maximal} clique is a complete subgraph which cannot be expanded by adding any adjacent node. A \textbf{maximum} clique is the largest possible clique in a given graph. A clique number of a graph is the size of the maximal clique.

algorithms: \begin{itemize}
	\item finding a maximal clique of a given size $k$ is $O(n^kk^2)$;
	\item computing a maximum clique is $O(3^\frac{n}{2})$;
	\item but for sparse graphs finding a max-clique is easier.
\end{itemize}

Relaxing the definition of a clique, which does not require complete connectivity (like an $\epsilon$-clique?).

Maximum cliques might overlap, and yet might not provide insightful information on the community structure of the graph.

\subsection{Relaxation} % (fold)
\label{sub:relaxation}

A $k$-\textbf{plex} of size $n$ is a maximal subset of $n$ vertices such that each vertex is connected at least to $n-k$ other in the subset. The minimum degree is not less than $n-k$.

A $k$-\textbf{core} is a maximal subset of vertices such that each node is connected to at least $k$ other vertex in the subset.

% subsection relaxation (end)

A network community is a group of vertices similar to each other.

In community detection nodes are assigned to communities and the main requirement is assignment of a vertex to at most one community.

Similarity based vertex clustering:
\begin{itemize}
	\item Jaccard, cosine, Pearson, Euclidean distance (dissimilairty);
	\item group together vertices with high similarity.
\end{itemize}

Hierarchical clustering may be used to cluster graphs using the similarity-adjacency matrix.
\begin{itemize}
	\item Assign each node to an individual group;
	\item Find two groups with the highest similarity and merge them:
		in order to calculate the similarity (aggregate) between groups it is possible to use: \begin{itemize}
			\item single-linkage (the most similar);
			\item complete-linkage (the least similar);
			\item single-linkage (mean similarity between members).
		\end{itemize}
	\item Repeat until all nodes have been joined into a single group.
\end{itemize}
This results in a dendrogramm of hierarchical clustering.

With a threshold on the similarity, the hierarchical clustering would produce a bunch of non-overlapping clusters.

nodes $\to$ similarity matrix $\to$ data mining.

\subsection{Graph partitioning} % (fold)
\label{sub:graph_partitioning}

Edges needed to be removed in order to split the graph in to pieces with as tightly connected subsets as possible


Graph partitioning is an NP hard problem: \begin{itemize}
	\item number of way to divide a network of $n$ nodes into two groups: $C^2_n$;
	\item Partitioning the graph into $k$ non-empty subgroups: Stirling number of the second kind \[\big[\begin{matrix}n\\k\end{matrix}\big]\];
	\item the number of all possible partitions is given by the Bell number.
\end{itemize}

\subsubsection{Heuristic approach} % (fold)
\label{ssub:heuristic_approach}

Focus on edges that connect communities (briges): use edge betweenness defines as the number of shortest paths $\pi_{st}$ through the edge $\epsilon$
\[C_b(\epsilon) = \sum_{s\neq t} \frac{\pi_{st}(\epsilon)}{\pi_{st}}\]
Construct communities by progressively removing edges.

\begin{itemize}
	\item[input] graph $G=(V,E)$;
	\item[output] dendrogramm;
	\item[output] Repeat until the set of edge is nonenpty:
	for all $e\in E$ compute $C_B(e)$ and remove the edge with the largest $C_B(e)$.
\end{itemize}

Community quality. Compare the fraction of edges within the cluster to expected fraction had edges been distributed at random:
\[Q = \frac{1}{2m}\sum_{ij} \big(A_{ij} - \frac{k_ik_j}{2m} \delta_{c_i,c_j}\big)\]

% subsubsection heuristic_approach (end)

% subsection graph_partitioning (end)

% section lecture_8 (end)

\section{lecture \# 9} % (fold)
\label{sec:lecture_9}

Mid-term = analyse your own friend's network on the 24th of March.

\subsection{graph partitioning algorithms} % (fold)
\label{sub:graph_partitioning_algorithms}

A community is a group of nodes coupled between each other tighter that with the rest of the world.

We consider only the sparse graphs with $m<<n^2$ (ie. $m = O(n^\frac{1}{2})$). Community detection is NP-hard.
Properly designed and correct greedy algorithms approximate the true and correct solution. THe approach is recursive top-down ``divide-and-conquer''.

What is the criteria of the best partition?\begin{itemize}
	\item Minimize the ``communication'' between graphs partitions; Gives rise to the \textbf{Min}imum \textbf{cut}, which is not the best, since it could easily cut out singly connected vertices.
	\item 
\end{itemize}

Optimization criteria for a graph $G=(V,E)$ with $V = W\uplus U$:
\begin{description}
	\item[graph cut] \[\text{cut}(W, U) = \sum_{i\in U}{j \in W}e_{ij}\]
	\item[Ration cut] \[\frac{\text{cut}(W, U)}{|W|}  + \frac{\text{cut}(W, U)}{|U|} \]
	\item[Normalised cut] \[\frac{\text{cut}(W, U)}{\text{vol}(W)}  + \frac{\text{cut}(W, U)}{\text{vol}(U)} \]
	\item[Quotient cut] also known as \emph{Conductance}: \[\frac{\text{cut}(W, U)}{\min\big\{\text{vol}(W),\text{vol}(U)\big\}} \]
\end{description}
where the volume $\text{vol}(W) = \sum_{i\in W} \sum_{i\in V} e_{ij} = \sum_{i\in W} \delta_i$;

Using the linear algebra solve the mincut problem (remember Mirkin?).

Optimization methods:
\begin{itemize}
	\item Local search algorithms (greedy);
	\item Spectral graph partitioning;
	\item Randomized min cut;
\end{itemize}

% subsection graph_partitioning_algorithms (end)

\subsection{special clustering} % (fold)
\label{sub:special_clustering}

Suppose we have two sets of node $U$ and $W$ ($U\uplus W = V$) and let $s_v = 2\cdot1_{v\in W}-1$ the indicator vector of being a member of the either class.

The number of edges crossing the partition $U\vert W$:
\begin{align*}
	\text{cut}(U,W)
	&= \frac{1}{4}\sum_{i,j\in V} (s_i - s_j)^2 1_{(i,j)\in E} \\
	&= \text{summing each edge twice}
	&= \frac{1}{8}\sum_{i,j\in V} (s_i - s_j)^2 a_{ij} \\
	&= \frac{1}{4}\sum_{i,j\in V} (s_i^2 + s_j^2 - 2s_i s_j) a_{ij} \\
	&= \frac{1}{8}\sum_{i\in V} \sum_{j\in V} (s_i^2 + s_j^2) a_{ij} - \frac{1}{4}\sum_{i,j\in V} s_i s_j a_{ij} \\
	&= \frac{1}{8}\sum_{i\in V} s_i^2 \sum_{j\in V} a_{ij} + \frac{1}{8}\sum_{j\in V} \sum_{i\in V} s_j^2 a_{ij} - \frac{1}{4} \sum_{i,j\in V} s_i s_j a_{ij} \\ 
	&= \frac{1}{4}\sum_{i\in V} s_i^2 \delta_i - \frac{1}{4} \sum_{i,j\in V} s_i s_j a_{ij} \\
	&= \frac{1}{4} s'\Delta s - \frac{1}{4} s'As = \frac{1}{4} s'\big(\Delta - A \big)s
\end{align*}
where $\Delta$ is a diagonal matrix of vertex degrees: $\Delta_{ii} = \delta_i$ for $i\in V$.

The matrix $\Delta-A$ is called the Laplacian matrix, and it is the discrete version of the Laplacian operator -- second derivative operation. (The finite difference method).

The graph cut problem is to minimize $Q(s) = \frac{s'Ls}{4}$ with respect to $s\in \{-1,1\}^V$ and subject to $\big \lvert \sum_{i\in V}s_i \big\rvert < B$. Unfortunately this problem is that of integer minimization, which is $NP$-hard. With balancing constraint this requires $\frac{n!}{\big(\frac{n}{2}\big)!}$ (shouldn't this be the number of paired brackets?)

Let's solve an approximation -- a relaxed problem:
consider $s\in [-1,1]^V$ subject to an $L_2$ penalty, $\nrm{s}_2 = |V|$, and $\sum_{j\in V} s_j = 0$. Finally having some optimal $\hat{s}$ project in onto $\{-1,1\}^V$ with $\text{sign}(s_j)$.
In fact the original problem necessarily solves this one, but not the other way around.

The optimization problem:
\[\frac{1}{4} x'Lx - \frac{1}{4}\lambda(x'x - |V|) - \frac{1}{2}\mu x'1 \to \min_{x,\mu,\lambda}\]
The FOC is $\frac{1}{2}Lx - \frac{1}{2} \lambda x - \frac{1}{2}\mu 1 = 0$, $x'1 = 0$ and $x'x=|V|$, while the SOC is $\frac{1}{2}L - \frac{1}{2}\lambda I_V$.

Therefore $Lx = \lambda x + \mu 1$ and $x'Lx - \lambda x'x = 0$ whence $x'Lx = \lambda |V|$. Also $1'Lx = \lambda 1'x + \mu 1'1 = \mu |V|$. Thus
\begin{align*}
	Lx &= x \lambda + 1 \mu\\
	x'Lx &= x'x \lambda + x'1 \mu = x'x \lambda\\
	1'Lx &= 1'x \lambda + 1'1 \mu = 1'1 \mu
\end{align*}
and the $x=0$ does not work. Hence $\lambda = \frac{x'Lx}{x'x}$ and $\mu = \frac{1'Lx}{1'1} = \frac{x'L1}{1'1}$. Therefore the goal is to find $x$ such that 
\[Lx = x \frac{x'Lx}{x'x} + 1 \frac{1'Lx}{1'1}\]
since $x'x = |V|$ and $1'1=|V|$, this reduces to
\[(|V|I_V - x x' - 1 1' ) Lx = 0\]

% The fiddler vector.

The minimization of the Rayleigh-Ritz quotient
\[\min_{x\perp x_1}\frac{x'Lx}{x'x}\]
where $x_1$ is the eigenvector with the least eigenvalue $\lambda = 0$.

The number of zero eigenvalues is the number of connected components.

For a totally connected graph $\lambda_2$ -- the second smallest eigenvalue, is always $1$.

So the basic idea is to find the second least eigenvector of the graph laplacian.

For a min cut solve $Lx = \lambda x$, for the normalised cut solve $Lx=\lambda D x$.

To actually partition, use the zero-thresholding rule.

% subsection special_clustering (end)

another optimality criterion:
modularity
\[\text{mod} = \frac{1}{2m} \sum_{u,v\in V} \Big(A_{u,v} - \frac{k-vk_u}{2m}\Big)\delta_{c_u=c_v}\]
where $c_v$ is the class of the node $v$. In a two-cluster case 
\[\delta_{c_u=c_v} = \frac{1}{2}(s_u s_v + 1)\]
whence the modularity becomes
\[\text{mod} = \frac{1}{4m}s'Bs\]
where $s\in \{-1,1\}^V$.
Relaxation: $s's = |V|$ and therefore we get
\[\frac{1}{4m} x'Bx - \lambda(x'x - |V|)\]
with $B=A - K'(2m)^{-1}K$.

Since the modularity metric is maximized, we seek the eigenvector with the largest eigenvalue.

% Critiques of modularity: the 

In application do recursive splitting.


% section lecture_9 (end)

\section{Lecture \# 10} % (fold)
\label{sec:lecture_10}
\subsection{Community detection (continued)} % (fold)
\label{sub:community_detection_continued}

\begin{itemize}
	\item vertex clustering method (vertex similarity);
	\item graph partitioning (sparse cuts).
\end{itemize}

What about fuzzy clustering? Or overlapping communities (multiple allegiances)? In an egocentric graph, you is the point of community overlap.

One edge overlap, multi-node overlap.

The $k$-clique algorithm.

A $k$-clique is a clique with $\geq k$ nodes. A $k$-clique community is a union of all $k$-cliques which can be reached through a series adjacent $k$-cliques. $k$-cliques are adjacent iff they share $k-1$ nodes (the degree of overlap). Varying the overlap degree allows relaxation of the community detection.

Finding $k$-cliques is NP hard.
\begin{itemize}
	\item Find all maximal cliques;
	\item Create a new meta-graph made from cliques from the clique overlap matrix;
	\item set a threshold ($k-1$) and select the remaining overlaps.
\end{itemize}

\textbf{kNN} label propagation
Decide on the label of a node based on $k$ of its nearest neighbours.
\begin{itemize}
	\item Initialize the labels on all nodes randomly;
	\item Iterate in the order of a random permutation:
	\item For every node replace its label with the most frequent label among its neighbours with uniform tie-breaking;
	\item Stop when no relabelling takes place.
\end{itemize}

Fast community detection
\begin{itemize}
	\item Assign every node to its own community;
	\item Phase I: \begin{itemize}
		\item For every node evaluate modularity gain from removing this vertex from its community and placing it in the community of its neighbour;
		\item Place the node in the community with the higherst modularity gain;
		\item repeat until saturation (impossible to make a node transfer). 
	\end{itemize}
	\item Phase II: \begin{itemize}
		\item Nodes in communities are fused together into ``super-node'' prerequisites;
		\item
	\end{itemize}
	\item Repeat until convergence;
\end{itemize}

% subsection community_detection_continued (end)

\subsubsection{Random walk community detection} % (fold)
\label{ssub:random_walk_community_detection}

\noindent\textbf{Walktrap community}. Use the random walk to calculate the similarity between the vertices;

At each time step compute the stochastic transition kernel $P = D^{-1}A'$ with $D=\text{diag}\big(\delta_i\big)$ where $\delta_i = \sum_j A_{ij}$. The probability $P_{ij}$ is the chance of moving from $j$ to $i$. Then $P^t$ is the matrix showing the probability of moving from $i$ to $j$ in $t$ steps.

The main assumption is that for nodes $i$ and $j$ in the same community the probability $P_{ij}^t$ is high and $P_{ik}^t \approx P_{kj}^t$ for all $k$.  

The diffusion distance between the nodes $I$ and $j$ is 
\[ r_{ij}(t) = \sqrt{\sum_{k=1}^n\frac{(P_{ik}^t - P_{kj}^t)}{\delta_k} } = \nrm{ D^{-\frac{1}{2}}P^t - P^tD^{-\frac{1}{2}} }\]

between communities $A$ and $B$:
\[ r_{AB}(t) = \sqrt{\sum_{k=1}^n\frac{(P_{Ak}^t - P_{kB}^t)}{\delta_k} } = \nrm{ D^{-\frac{1}{2}} P_A^t  - P_B^t D^{-\frac{1}{2}} }\]

%% Missing 6 slides until the 24th

Local clustering algorithm
\begin{itemize}
	\item For $t=1\ldots t_M$ compute $q_t = M r_{t-1}$ and set
	$r_t(i) = q_t(i)$ if $q_t(i) > \epsilon \delta_i$, and zero otherwise.
	\item order the nodes according to the ratio $\frac{q_t(i)}{\delta_i}$.
	\item MISSING from slide 24.
\end{itemize}
No need to deal with the entire graph.

% subsubsection random_walk_community_detection (end)

% section lecture_10 (end)

\section{Lecture \# 11} % (fold)
\label{sec:lecture_11}

Two large home assignments, or, equivalently, two small projects.
Problems: network processes -- how to chose the nodes so as to maximize some
values functional.
All topics are active research.

\subsection{Diffusion and random walks on graphs} % (fold)
\label{sub:diffusion_and_random_walks_on_graphs}

Today's lecture is mostly technical, but it is

A random walk on a graph $G = (V,E)$ is a sequences of vertices with each one
being a neighbour of the other chosen according to some transition probability
\[\Pr(x_{t+1} = u \lvert x_t = v) = \Pr(v\to u)\]
given by a stochastic matrix $P_{uv}$ with $\sum_{v\in V} P_{uv} = 1$

Undirected connected unweighted graphs without loops: all vertices are
reachable from one another.

A simple stochastic is uniform transition from a node to its neighbours:
\[
\Pr(u\to v)
= P_{uv}
= \frac{1}{\delta_u} 1_{\{u,v\}\in E}
\]
there is no opportunity to linger at $u$. Such stochastic matrix can be easily
computed: for $A$ -- $V\times V$ adjacency matrix and $D = \text{diag}(\delta_u)_{u\in V}$
\[P = D^{-1} A\]

Let $\pi_u(t)$ be a probability of being at a node $u$ at time $t$. Then for
$\pi(t) = \bigl(\pi_u(t)\bigr)_{u\in V}$ -- a row vector, the probability at the next point in
time is (left action of the transition kernel $P$)
\[
\pi_v(t+1)
= \pi(t) P
= \pi(t) D^{-1} A
\]

The initial distribution can be anything $\pi(0)$, even a degenerate with a
singularity at some vertex. What then will be the long term probability of
ending up at some vertex? Does there exist $\pi = \lim_{t\to \infty}\pi(t)$?

Since the linearity of the transition operator and its boundedness imply continuity, if such limit exists then it should be equal to
\[
\pi
= \lim_{t\to\infty}\pi(t+1)
= \lim_{t\to\infty}(\pi(t)P)
= (\lim_{t\to\infty}\pi(t)) P
= \pi P
\]
Therefore the limiting distribution must be a left eigenvector of $P$
corresponding to a unit eigenvalue.

The conditions for existence are : the graph must be connected, not be a bipartite or cyclical, 

A random walk is reversible if
\[
\pi_i P_{ij} 
= \pi_i \Pr(i\to j)
= \pi_j \Pr(j\to i)
= \pi_j P_{ji}
\]
For a random walk on a undirected graph with the adjacency matrix $A$, which is symmetric, the reversibility condition is automatically satisfied:
\[\pi_i \frac{A_{ij}}{\delta_i} = \pi_j \frac{A_{ji}}{\delta_j}\]
whence $\pi_i\delta_j = \pi_j \delta_i$ for all $i,j\in V$.
Therefore $\frac{\pi_i}{\delta_i} = C$ and $\pi_i = C\delta_i$, whence
\[\pi_i = \frac{\delta_i}{\sum_i \delta_i}\]
because $\sum_i \pi_i = 1$.

A lazy random walk: for every node the walk is allowed to make no transition at all (stay at the node). THen the transition kernel is given by
\[I_V \beta + P (1-\beta) \]
however the stationary distribution is the same! Indeed
\[\]

The convergence rate: Let $\lambda_2$ be the second largest eigenvalue of the 
transition kernel $P$. The convergence rate to the stationary distribution from a node $i$ with initial distribution $\pi(t=0) = e_i$ is bounded by
\[\lvert p_j(t) - \pi_j \rvert \leq \sqrt{\frac{\delta_j}{\delta_i}} \frac{\lambda_2}{\lambda_1}\]
where $\frac{\lambda_2}{\lambda_1}$ is the spectral gap of $P$.

The more neighbours the starting vertex has the quicker the distribution
converges to the stationary. The spectral gap for the lazy random is wider,
since $\lambda_2' = \beta \lambda_2 + 1-\beta$.

\subsubsection{Diffusion} % (fold)
\label{ssub:diffusion}

Fik's law: if $\Phi(x,t)$ is the concentration, then
\[
J
= - C\frac{\partial}{\partial t}\Phi
= - C \nabla\Phi
\]

Numerical solution of diffusion equations: finite difference method.

\begin{description}
	\item[2D laplacian] 
	\[\delta = \nabla^2 f = \sum_i \frac{\partial}{\partial x_i} f\]
	\item[MISSING] slide 12
\end{description}

Let's model the diffusion of some substance on a graph. Let $\phi(t)$ be the
concentration at a node $i$ at time $t$ and suppose the diffusion is given by
\[
\phi_i(t+\delta t)
= \phi_i(t) + \text{inflow}_i(t) - \text{outflow}_i(t)
= \phi_i(t) + \sum_j A_{ij} \bigl(\phi_j(t) - \phi_i(t)\bigr) c \delta t
\]
The flow is proportional to the difference of concentrations. This equation
can be simplified down to (with $\delta t \to 0$)
\[
\frac{d}{dt}\phi_i(t+1)
= \sum_j A_{ij} \bigl(\phi_j(t) - \phi_i(t)\bigr) c
= c \sum_j A_{ij} \phi_j(t) - c \phi_i(t) \sum_j A_{ij}
= c \sum_j \bigl( A_{ij} - \delta_j 1{j=i} \bigr) \phi_j(t)
\]
Thus in the matrix form
\[
\frac{d}{dt}\phi
= c (A-D) \phi
= - c L \phi
\]

In spectral clustering did the second eigenvector represent the ``stationary'' concentration of mass at the nodes? No it determines the dynamics of the diffusion.

The diffusion equation is thus just a simple linear ODE :
\[\dot{\phi} = -c L \phi\]
The solution is the matrix exponential $\phi(t) = e^{-c L t}$

Since the laplacian matrix $L$ is symmetric, there necessarily exists an
orthogonal basis $(v_i)_i$ of eigenvectors of $L$. Since $\phi(t)$ is a vector
it is representable as a linear combination of the basis vector, whence:
\[
\dot{\phi}
= \sum_i \dot{a}_i v_i
= - c L \sum_i a_i v_i
= - c \sum_i a_i L v_i
= - c \sum_i a_i \lambda_i v_i
\]
Since $v_i$ a re linearly independent, for all $t\geq0$ it must be true that
\[\dot{a}_i = - c \lambda_i a_i\]
which implies that
\[a_i(t) = a_i(0) e^{-c \lambda_i t}\]
since the laplacian matrix is positive semidefinite and the graph is connected,
there is an eigenvalue $\lambda_k=0$. Therefore
\[
\phi(t)
= \sum_i a_i(t) v_i
= \sum_i a_i(0) e^{-c \lambda_i t} v_i
= a_k(0) v_k + \sum_{i\neq k} a_i(0) e^{-c \lambda_i t} v_i
\]

Why diffusion processes are needed? The smoothing effect! The idea of a
diffusion is to smooth the mass from over concentrated to under concentrated --
even out the concentration. Averaging images is an example of redistributing
the excess concentrations of the noise at pixels no denoise them.

% subsubsection diffusion (end)

There are different laplacians! The normalised laplacian matrix
\[\Lcal = D^{-\frac{1}{2}} L D^{-\frac{1}{2}} \]
it scales the laplacian but retains it symmetric properties. Has good bound on
eigenvalues, but still has an orthonormal eigenbasis. Seen this in normalised
cut clustering:
\[
(D-A) x
= \lambda D x 
\Leftrightarrow
D^{-\frac{1}{2}} L D^{-\frac{1}{2}} D^\frac{1}{2} x
= \lambda D^\frac{1}{2} x
\]
Furthermore (check this?)
\[
P
= D^{-1} A
= D^{-\frac{1}{2}}(I_n - \Lcal)D^\frac{1}{2}
\]

% subsection diffusion_and_random_walks_on_graphs (end)

% section lecture_11 (end)

\section{Lecture \# 12} % (fold)
\label{sec:lecture_12}

\subsection{Epidemic dynamic model} % (fold)
\label{sub:epidemic_dynamic_model}

The SIR models the dynamics of the size of three groups of people:\begin{itemize}
	\item Susceptible $S_t$;
	\item Infected $I_t$;
	\item Recovered $R_t$.
\end{itemize}

major assumptions \begin{itemize}
	\item Fully mixing model -- everyone is in contact with everyone else;
	\item The population is static.
\end{itemize}

\subsubsection{SI model} % (fold)
\label{ssub:si_model}

A simple \textbf{S}usceptible-\textbf{I}nfected model is given by the following dynamic and balance equations:
\begin{align*}
	I_{t+\delta t} &= I_t + \beta \frac{S_t}{N} I_t \delta t\\
	N &= S_t + I_t
\end{align*}
The factor $\frac{S_t}{N}$ can be interpreted as the probability of meeting a
susceptible agent by an infected agent.
Put $s_t = \frac{S_t}{N}$, $i_t = \frac{I_t}{N}$ and tend $\delta t \to 0$ effectively transforms the system into
\begin{align*}
	\frac{d}{dt}i_t = \beta (1 - i_t) i_t
\end{align*}
the general solution is given by
\[ -\int \frac{-1}{1-i} di + \int \frac{1}{i} di = \int \beta dt + C \]
whence
\[ \frac{i_t}{1-i_t} = Ce^{\beta t} \]
and the solution is a logistic curve
\[i_t = \frac{1}{1+Ce^{-\beta t}} \]

% subsubsection si_model (end)

\subsubsection{SIS model} % (fold)
\label{ssub:sis_model}

This is a basic extension of the the SI model to a situation, where there is a
back flow from $i_t$ to $s_t$ :
\begin{align*}
	\dot{s} &= \gamma i - \beta s i\\
	\dot{i} &= \beta s i - \gamma i\\
	1 &= s+i
\end{align*}
this non-linear system simplifies to
\[\dot{i} = \beta (1-i) i - \gamma i\]
whence
\[\dot{i} = (\beta - \gamma - \beta i ) i \]

% subsubsection sis_model (end)

\subsubsection{The SIR model} % (fold)
\label{ssub:the_sir_model}

It further modifies the SI model to include a separate group of agents, which
once recovered from the infection cannot get infected again. The dynamics is
given by
\begin{align*}
	\dot{s} &= - \beta s i\\
	\dot{i} &= \beta s i - \gamma i\\
	\dot{r} &= \gamma i\\
	1 &= s+i+r
\end{align*}
Substituting the $s$ into $i$ yields
\[\dot{s} = - \beta s \frac{1}{\gamma}\dot{r}\]
whence
\[\ln|s| = -\frac{\beta}{\gamma} r_t + C \]
and 
\[ s = s_0 e^{-\frac{\beta}{\gamma} r_t}\]

Thus
\[\dot{r} = \gamma (1 - r - s_0 e^{-\frac{\beta}{\gamma} r_t})\]

This non-linear system is unsolvable analytically. But it is still useful. If 
$t\to \infty$ then since $r_t$ is bounded (and if $r_t$ is not periodic) (?),
then $r_t$ tends to an equilibrium (stationary point), where $\dot{r} = 0$.
Thus
\[r_\infty = 1 - e^{-R_0 r_\infty}\]
with $R_0 = \frac{\beta}{\gamma}$. This looks very much like the probability
of a giant connected component occurring in a random Erd\"os-Renyi graph.

In order to establish stationarity of $r_t$ as $t\to\infty$ one might use
Lyapunov or other theorems.

The critical point is $R_0 = 1$. The total size of the outbreak $r_\infty$ and
$R_0$ is the basic reproduction number, which is the average number of agents
infected by another agent before its recovery (removal from the population).
\[
R_0
= \ex(\beta \tau)
= \int_0^\infty \gamma \tau e^{-\gamma \tau} d\tau
= \frac{\beta}{\gamma}
\]
The distribution of time being infected is $\text{Exp}(\gamma)$. The heuristic
derivation is to look at the outflow from the infected share: an agent ceases to be infected with rate $\gamma$
\[\dot{i} = - \gamma i\]

% subsubsection the_sir_model (end)

% subsection epidemic_dynamic_model (end)

\subsection{Contagion model} % (fold)
\label{sub:contagion_model}

A simple model of rumour propagation, disease contagion or any other
irreversible status transmission. \begin{description}
	\item[1st Wave] First infected agent occurs in the population and
	transmits the infection to each agent it meets with probability $p$.
	Suppose it meets $Z^1_1=k$ agents;
	\item[2nd Wave] Each infected agent $m=Z^1_1$ from the $1$-st wave
	re-transmits the disease to $Z^2_m=k$ agents to each with probability $p$;
	\item[So on] \hfill\\
\end{description}
This kind of dynamics is known as branching stochastic process, which was
first introduced by Francis Galton in 1889 (in a less general formulation).

\subsubsection{Random branching process} % (fold)
\label{ssub:random_branching_process}

Let $\xi^n_i$ be the number of transmitted infections by the $i$-th node on
the level $n$. Further, let $Z_n$ be the number of infected people at the
$n$-th level, $Z_0 = 1$. Then the number of infected people at the $n+1$-th
level (generation, iteration) is given by
\[Z_{n+1} = \sum_{m=1}^{Z_k} \xi^n_m\]
The main assumption, that $\xi^n_i$ are mutually independent, yields
\begin{multline*}
G_{Z_{n+1}}(s)
= \ex s^{Z_{n+1}}
= \ex \ex \bigl(s^{Z_{n+1}}\rvert Z_n\bigr)
= \ex \prod_{m=1}^{Z_n} \ex \bigl(s^{\xi^n_m}\rvert Z_n\bigr)\\
= \ex \bigl( \ex s^{\xi^n_m} \bigl)^Z_n
= \ex (G_\xi(s))^{Z_n}
= G_{Z_n}( G_\xi(s) )
\end{multline*}
whence
\[G_{Z_n}(s) = \underset{n\text{-times}}{(G_\xi\circ \ldots G_\xi)}(s)\]

In simpler language the probability $q_n$ of the infection never spreading
through a node at level $n$ is the chance that neither of $k$ potentially
infectable agents further propagates the infection. This results in the
following recursive equation for $q_n$:
\[(1-pq_{n+1})^k = 1-q_n\]

%% See the first homework on probabilistic methods

% subsubsection random_branching_process (end)

% subsection contagion_model (end)

% section lecture_#_12 (end)

\section{Lecture \# 13} % (fold)
\label{sec:lecture_13}

% Probabilistic models missed the first 5 slides

Probabilities of a node $i$ of being infected is $x_i(t)$
$\beta$ is the infection rate 
\[\frac{d}{dt} x_i(t) = \beta (1-x_i(t)) \sum_j A_{ij} x_j(t)\]

This is a non-linear equation, which is hard to solve. But it is possible to
consider some approximations: early-time and the late-time approximations.

\subsubsection{Early-time approximation} % (fold)
\label{ssub:early_time_approximation}

Consider the case $t\to 0$ and suppose $x_i(t) << 1$. Then the $(1-x_i(t))$ term
can be take to be equal to $1$, whence
\[\frac{d}{dt} x_i(t) \approx \beta A x(t)\]
This system's solution in the eigenbasis of $A$ is given by
\[x(t) = \sum_k c_k e^{\lambda_k \beta t} v_k\]
When $t\to 0$ then for $\lambda_\text{max} = \lambda_1> \lambda_k$, the solution
is approximated by
\[x(t) = c_1 v_1 e^{\lambda_1 \beta t}\]
indeed as $t\to 0$
\[
x(t) - c_j v_j e^{\lambda_j \beta t}
= \sum_{k\neq 1} c_k \bigl(e^{\beta\lambda_k t}-e^{\beta\lambda_j t}\bigr) v_k
= e^{\beta\lambda_j t} \sum_{k\neq 1} c_k \bigl(e^{-\beta(\lambda_j-\lambda_k) t}-1\bigr) v_k
\]
whence (?) this does not show any rate of convergence...

% subsubsection early_time_approximation (end)

\subsubsection{Late-time approximation} % (fold)
\label{ssub:late_time_approximation}

Now consider the limiting case when $t\to\infty$ and $x(t)\to x^*$. In this case it
necessarily must be true that $\dot{x} = 0$, which implies that 
\[\beta (1-x^*_i) \sum_j A_{ij} x^*_j = 0\]

% subsubsection late_time_approximation (end)

\subsection{The SIS model} % (fold)
\label{sub:the_sis_model}

Infection equations
\[\frac{d}{dt} s_i(t) = \beta s_i(t) \sum_j A_{ij} x_j(t) - \gamma x_i(t)\]
and the balance $s_i(t) = 1 - x_i(t)$.

Early-time approximation (linearization): if $x_i(t) << 1$ then
\[
\frac{d}{dt} s_i(t)
\approx \beta \sum_j A_{ij} x_j(t) - \gamma x_i(t)
= \beta \sum_j (A_{ij} - \frac{\gamma}{\beta} 1_{ij}) x_j(t)
\]
whence
\[\dot{x} = \beta (A - \frac{\gamma}{\beta} I) x\]
This problem has the same eigenbasis, as the previous one, but 
eigenvalues:
\[x(t) = \sum_k c_k v_k e^{(\beta \lambda_k - \gamma )t} \]

The relative magnitudes of $\lambda_1$ and $\frac{\gamma}{\beta}$ have a large
impact on the limiting behaviour of the infection spread.

In the previous lecture the critical value of $\frac{\gamma}{\beta}$ was $1$, while
here it is determined by some characteristic of the structure of the connectivity
graph. The larger the top eigenvalue, the more infection resilient the network is.

% subsection the_sis_model (end)

\subsection{SIR model as percolation} % (fold)
\label{sub:sir_model_as_percolation}

For every edge we flip a biased a coin to determine if the edge is active or not. The
paths of activated edges is a percolation path. \textbf{Bond percolation}: only those
nodes that are on a percolation path with an infected node will get infected.

It is reminiscent of the random Erd\"os-Renyi graph. Thus if the probability of being
activated is larger than some threshold, then the whole network becomes infected. This
threshold is determined by the largest eigenvalue (Keeling et al.;2005).

% subsection sir_model_as_percolation (end)

% section lecture_13 (end)

\section{Lecture \# 14} % (fold)
\label{sec:lecture_14}

A brief outline of the lecture. Diffusion of information and social contagion.
Rumour spreading models: mean field model and homogeneous approximations.
Propagation tree.

Social contagion is understood as processes describing individual propensity to
adopt and diffuse information. Such models are quite similar to epidemiological 
models:
\begin{description}
	\item [``susceptible''] an individual, who has not yet been exposed to new
	information;
	\item [``infected''] an agent, who carries and actively disseminates the
	rumour;
	\item [``recovered''] an individual, aware of the new gossip, but no longer
	spreading it.
\end{description}

Key parameters in social social diffusion are the density of connectivity and the
virulence of a rumour.

The key result is the criticality threshold on the ratio of virulence (the mean
number of infected individuals) and the connectivity of the network (the average
number of connected individuals) and which beyond which the whole population gets
infected, whereas below -- the infection dies out.

The main difference between physical and social diffusion is that no conservation
of mass is involved in the latter.

\subsection{Maki-Thomson model (1973)} % (fold)
\label{sub:maki_thomson_model_1973}

It is speculated that the rumour spreads via the process described below.
Compartmental fully mixed model. There are three categories of individuals
\begin{itemize}
	\item Ignorants ($I$) -- those who do not known the rumour;
	\item Spreaders ($S$) -- know the rumour and actively spreading it;
	\item Stiflers ($R$) -- know the information, but ceased to disseminate it;
\end{itemize}
The rumour spreads via pairwise contacts (meetings): \begin{itemize}
	\item $I-S \overset{\lambda}{\to} S-S$;
	\item $S-R \overset{\alpha}{\to} R-R$;
	\item $S-S \overset{\alpha}{\to} S-R$;
\end{itemize}
where $\lambda$ is the rate of ignorant-spreader contacts, and $\alpha$ -- is the 
rate of spreader-spreader and spreader-stifler contacts.

\subsubsection{Homogeneous approximation} % (fold)
\label{ssub:homogeneous_approximation}

Lattice or exponential network $G$ with $\bar{\delta}$ neighbours on average.
Consider a network of fixed size:
\begin{align*}
	\dot{i} &= - \lambda \bar{\delta} i s\\
	\dot{s} &= \lambda \bar{\delta} i s - \alpha \bar{\delta} s (s + r)\\
	\dot{r} &= \alpha \bar{\delta} s (s + r)
\end{align*}
with $s+i+r = 1$ and $i = \frac{I(t)}{N}$ -- the share of ignorants, and so forth.

As time goes on the process becomes saturated with ``stiflers''. Thus there exists
$r_\infty = \lim_{t\to \infty} r(t)$, with
\[r_\infty = 1 - e^{-(1+\frac{\lambda}{\alpha})r_\infty}\]
In order for there to exist a stable non-zero solution it must be so that
\[
\frac{d}{d r_\infty} \bigl(
	1 - e^{-(1+\frac{\lambda}{\alpha}) r_\infty}
\bigr)\rvert_{r_\infty = 0} > 0
\]
In this case the rumour epidemic becomes super-critical when $1+\frac{\lambda}{\alpha}>1$,
which means that any rumour, however weak and ridiculous, would still ``infect'' the whole
population.

% subsubsection homogeneous_approximation (end)

\subsubsection{Mean field approach} % (fold)
\label{ssub:mean_field_approach}

The idea came from the particle physicists, who though to replace the pairwise
interaction a particle with its neighbours by its interaction against the whole
ensemble. This ``mean field'' approximation is valid when the local density of 
particles in a system is high and the number of particles is high.

First, group the nodes of $G = (V,E)$ with respect to the node degree:
the topology of the network $G$ is static. The dynamics of the share of nodes with
the same-connectivity pattern in each of the categories is given by
\begin{align*}
	\dot{i}_k &= -\lambda \bar{\delta} i_k \sum_{ d } \frac{ d P(d) s_d}{\bar{\delta}}\\
	\dot{s}_k &= \lambda \bar{\delta} i_k \sum_{ d } \frac{ d P(d) s_d}{\bar{\delta}}
		- \alpha \bar{\delta} \sum_{ d } \frac{ d P(d) s_d (s_d + r_d) }{\bar{\delta}}\\
	\dot{r}_k &= \alpha \bar{\delta} \sum_{ d } \frac{ d P(d) s_d (s_d + r_d) }{\bar{\delta}}\\
\end{align*}
where $P(k)$ is the probability distribution of node degree:
\[P(k) = \frac{1}{|V|} \sum_{v\in V} 1_{k}(\delta_v)\]

% subsubsection mean_field_approach (end)

% subsection maki_thomson_model_1973 (end)

Rumour dynamics:
\begin{itemize}
	\item if a spreader meets an ignorant, then the ignorant becomes a spreader with rate $\lambda$;
	\item \textbf{Missing items}.
\end{itemize}

The critical threshold for homogeneous, scale-free, or even random graphs is the same
\[\frac{\alpha}{\lambda} \geq \frac{E \text{degree}}{E \text{degree}^2}\]

% Spread of a good idea: Luis M.A, Bettencourt et al. 2005

\subsubsection{Propagation tree} % (fold)
\label{ssub:propagation_tree}

Rumours usually spread in cascades, which look like trees. Recall the Galton-Watson
branching process: \begin{itemize}
	\item if $p \bar{\delta} < 1$ then rumour dies out exponentially;
	\item if $p \bar{\delta} = 1$ then the number of infected nodes is fixed;
	\item if $p \bar{\delta} > 1$ then rumour spreads throughout the whole network
	with exponential asymptotics.
\end{itemize}

% subsubsection propagation_tree (end)

% section lecture_14 (end)

\section{Lecture \# 15} % (fold)
\label{sec:lecture_15}

Consdier a local intercation game, which rewards synchoronisation. If $u$ and $v$
are adjacent nodes in a network $G = (V,E)$. If the strategies of $u$ and $v$ are
aligned then both get payoff of either $(a,a)$ or $(b,b)$, and zero if opposite
behaviour is adopted.

Network coordination game, direct benfit effect. Let $p$ be the portion of adjacent
nodes adopting a strategy $\pi_1$ then it is optimal to use $\sigma_1$ if
\[ d p a \geq d (1-p) b\]
whence the payoff treshold is $q=\frac{b}{a+b}$: if $p\geq q$ then choose $\sigma_1$.

``Cascade'' is sequence of changes of behaviour, a ``chain reaction''. The size of the
cascade is the number off nodes which changes the behaviour, and a cascade is complete
if every node is affected.

This activation cascade settles within a local cluster, since a cluster it densely
connected within, but has much less connection to the outside.
Mass media is a node, connected to almost any other vertex in the network.

Another assumption is that every person has the same adoption threshold.

Granovettor (1978): individual threshold model. The basic idea that each agent has
individual threshold dependent on the idiosyncratic preferences.

Let $\theta_i$ be the threshold of $u$-th agent, and $x$ is the number of participants.
Each one makes a binary decision: $x>\theta_i$ then $+1$ and $-1$ otherwise.
Let $f(x)$ be the density of people with threshold level $\theta=x$, and $F(x)$ be
$\int_0^x f(s) ds$ the share of people with threshold $\theta$ not greater than $x$.
\[F(x) = \sum_{x'\geq x} f(x')\]
Initially, $x_0$ have $+1$ (joined a protest). Thus after the first iteration, there
would be $x_1 = F(x_0)$ ratio, and so on.

...

Influence maximisation problem: activate certain nodes so as to maximise the size of
the resulting cascade. More formally, find a $k$-set $A_0$...

Properties of submodular set functions. A function $f:\mathcal{P}(V)\to \Real$
is submodular if for $S<T\subseteq V$ with $S\subseteq T$ for all $v\notin T$
\[
f(S\cup \{v\})-f(S)\geq f(T\cup \{v\})-f(T)
\]
The function $f$ has a kind of diminishing returns and is monotonic: $f(S\cup \{v\})\geq f(S)$.

Theorem. if $f$ is a submodular monotone function and $S$

\begin{itemize}
	\item $S=\emptyset$;
	\item for each $i=1,\ldots, k$: select $v\notin S$ such that 
		\[v = \argmax_{u\in V\setminus S} \sigma(S\cup\{u\}) - \sigma(S)\]
	\item $S = S\cup \{v\}$.
\end{itemize}
where $\sigma:\mathcal{P}(V)\to \Real^+$ which is calculated via simulation:
run the activation cascade from the set-argument of $\sigma$, to get the coverage.
To speed up the greedy algorithm< it is possible to consider more ``central'' nodes
first.

% section lecture_15 (end)

\section{Lecture \# 17} % (fold)
\label{sec:lecture_17}

\subsection{Label propagation} % (fold)
\label{sub:label_propagation}

The graph $G=(V,E)$ with edges representing similarity between vertices.
In social networks edges do represent closeness due to assortative mixing -- bias
in connectivity in favour of nodes with similar characteristics:
\begin{itemize}
	\item homophily ;
	\item influence.
\end{itemize}

Supervise learning approach. Suppose teh set of vertices is split into two subsets:
\begin{itemize}
	\item nodes $V_l$ with ``labels'' $Y_l$;
	\item nodes $V_u$ without ``labels''.
\end{itemize}
The goal is to infer the ``labels'' $Y_u$ for $V_u$. Features can be computed for each
vertex $v\in V$: \begin{itemize}
	\item local node features (intrinsic);
	\item topological, graph-based features: connectivity patterns, labels of neighbours
	neighbouring node degrees et c.).
\end{itemize}
However these features work if the nearest neighbours already have labels, which means
that nodes surrounded by unlabelled vertices would not be susceptible to machine learning.

The idea is to assume that unknown labels are latent random variables and either Bayesian
modelling or \textbf{EM}.

Iterative classification method: given a partitioned graph $G=(V,E)$ with $V = V_l \cup V_u$
\begin{itemize}
	\item compute $\Phi_l^0$ and train a relational classifier on $(\Phi_l^0, Y_l)$;
	\item predict labels $Y^0_u$ for vertices $V_u$;
	\begin{itemize}
		\item compute $\Phi_u^i$ and train a classifier on $(\Phi^i, Y)$;
		\item predict labels $Y^{i+1}_u$ for vertices $V_u$ from $\Phi^i$;
	\end{itemize}
	\item Output $Y^i_u$.
\end{itemize}

Relational classifier uses the nearest neighbours of a vertex to predict its label.
\begin{itemize}
	\item Weighet-vote classifier: 
	\[\pr(y_i = c) = \frac{1}{Z} \sum_{j\in N_i} A_{ij} \pr(y_j=c|N_j)\]
	\item Bayesian classifier: 
		\[\pr(y_i = c) = \frac{\pr(N_i|y_i=c) \pr(y_i=c)}{\pr(N_i)}\]
	\item and so on.
\end{itemize}

Semi-supervised learning: partially labelled dataset $X = X_l\cup X_u$. The labelled
set is small compared to the volume of unlabelled set of data. Construct a similarity
graph $G=(V,E)$ where every vertex corresponds to a data point. In our case the
similarity graph is already given.

\subsubsection{Random walk method} % (fold)
\label{ssub:random_walk_method}

Consider a random walk with absorbing states. Where labelled nodes are considered
sink nodes. The probability for a node $i\in V_u$ to have label $c$ is given by
\[\hat{y}_i(c) = \sum_{j\in V_l} p^\infty_{ij} y_j(c)\]
where $y_j(c)$ is the probability distribution over labels and $p_{uv} = \pr(u\to v)$
-- the transition kernel of the graph random walk. Assign the most probable label

The kernel with absorbing nodes' modification is given by
\[
  \begin{pmatrix} P_{ll} & P_{lu}\\ P_{ul} & P_{uu} \end{pmatrix}
= \begin{pmatrix} I & 0\\ P_{ul} & P_{uu} \end{pmatrix}
\]
based on $P = D^{-1} A$, $p_{ij} = \pr(i\to j)$ and $\sum_{j\in V} P_{ij} = 1$.

There is actually no need to compute the limit of the transition kernel: just use
iterative multiplication:
\begin{align*}
	Y^t &= P Y^{t-1}\\
	Y^t_u &= P_{ul} Y_l + P_{uu} Y^{t-1}_u\\
	Y^t_u &= \sum_{n=1}^t P_{uu}^{n-1} P_{ul} Y_l + P_{uu}^t Y_u\\
\end{align*}
and $Y^t_u$ converges to $(I-P_{uu})^{-1}P_{ul} Y_l$.

Another variation is
\[
Y^t = \alpha S Y^{t-1} + (1-\alpha)Y^0
\]
where $S = D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$ and it converges to
\[\hat{Y} = (1-\alpha) (I - \alpha S)^{-1} Y^0\]
and $Y^0 = \bigl(\begin{smallmatrix} Y_l\\0 \end{smallmatrix}\bigr)$.

% subsubsection random_walk_method (end)

\subsection{Regularization on graphs} % (fold)
\label{sub:regularization_on_graphs}

Find labels $\hat{Y} = (\hat{Y}_l, \hat{Y}_u)$ such that they are consistent
\[
\sum_{i\in V_l} (\hat{y}_i - y_i)^2 = \| \hat{Y}_l - Y_l\|^2
\]
honour the graph structure (locally smooth: does not vary too much in a neighbourhood)
\[
\frac{1}{2} \sum_{i,j\in V} A_{ij} (\hat{y}_i - \hat{y}_j)^2
= \hat{Y}'(D-A)\hat{Y}
= \hat{Y}' L \hat{Y}
\]
where $L$ is the graph Laplacian and (or) stable
\[
\epsilon \sum_{i\in V} \hat{y}_i^2 = \epsilon \|\hat{Y}\|^2
\]

Label propagation (random walk with absorption) is a special case of this regularization.
\begin{itemize}
	\item Label propagation: $ Q(Y) = \frac{1}{2} Y'L Y $;
	\item Label spread: $ Q(Y) = \frac{1}{2} Y'\Lcal Y $;
	\item Laplacian regularization: 
	\[ Q(Y) = \frac{1}{2} Y' L Y + \mu \|\hat{Y}_l - Y_l\|^2 \]
	find the eigenvectors $(e_i)$ of $L$ corresponding to $p$ smallest eigenvalues and then
	classify using the optimal learned $(a_j)_{j=1}^p$
	\[ \argmax_{a_j} \sum_{i\in V_l}^ (y_i - \sum_{j=1}^p a_j e_{ji})^2\]
	which gives $\hat{a} = (E'E)^{-1} E'Y_l$. Classify according to
	\[\hat{Y}_u = \text{sign} \sum_{j=1}^p a_j \]
	MISSING(?)
\end{itemize}

% subsection regularization_on_graphs (end)

% subsection label_propagation (end)


% section lecture_17 (end)

\end{document}
