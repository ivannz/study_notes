{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Structural Analysis and Visualization of Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Home Assignment #1: Power law</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Student: *Nazarov Ivan*</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <hr /> General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due Date:** 28.01.2015 23:59 <br \\>\n",
    "**Late submission policy:** -0.2 points per day <br \\>\n",
    "\n",
    "\n",
    "Please send your reports to <mailto:leonid.e.zhukov@gmail.com> and <mailto:shestakoffandrey@gmail.com> with message subject of the following structure:<br \\> **[HSE Networks 2015] *Nazarov* *Ivan* HA*1***\n",
    "\n",
    "Support your computations with figures and comments. <br \\>\n",
    "If you are using IPython Notebook you may use this file as a starting point of your report.<br \\>\n",
    "<br \\>\n",
    "<hr \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preabmle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining several routines, which would become helpuf later on in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as la\n",
    "from scipy.stats import rankdata\n",
    "%matplotlib inline\n",
    "\n",
    "## Construct a regression model\n",
    "def lm_model( X, Y, intercept = True ) :\n",
    "    T = np.array( Y, dtype = float )\n",
    "    M = np.array( X, dtype = float )\n",
    "    if intercept is True :\n",
    "        M = np.vstack( [ np.ones( len( Y ) ), M ] ).T\n",
    "\treturn (M,T, intercept)\n",
    "\n",
    "## Define the OLS regression routine:\n",
    "def lm_fit( model ) :\n",
    "\tM, T, intercept = model\n",
    "\tMMinv = la.inv( ## implement (X'X)^{-1} (X'Y)\n",
    "\t\tnp.dot( M.T, M ) ) \n",
    "\tcoef = np.dot( MMinv,\n",
    "\t\tnp.dot( M.T, T ) )\n",
    "## Estimate the residual standard deviation\n",
    "\tresid = T - np.dot(M, coef)\n",
    "\tdof = len( T ) - len( coef )\n",
    "\tRSS = np.dot( resid.T, resid )\n",
    "\treturn (coef, RSS, dof, MMinv )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continusous random variable $X$ is distributed accorind to the power law (also known as Pareto distibution) if it's probability density function is $$p(x) = \\frac{\\alpha-1}{u} {\\bigg (\\frac{x}{u} \\bigg )}^{-\\alpha} 1_{[u,+\\infty)} (x)$$\n",
    "\n",
    "The maximum likelihood esitimate of the exponent $\\alpha$ is given by $$\\hat{\\alpha} = 1 + {\\bigg( \\frac{\\sum_{k=1}^n \\ln x_k}{n} - \\ln u \\bigg) }^{-1}$$ provided all the observed sample values are not less than the threshold $u$. \n",
    "\n",
    "A random variable $N$ with discrete power law distribution exceeding a certain thershold $u$ has the following probabilities: $$\\mathbb{P}(N=k) = C \\frac{1}{{(k-u+1)}^{\\,\\gamma}}$$ where $k\\geq u$ is the value over the threshold $u$, $\\gamma > 1$ and the constant $C$ is given by the reciprocal of Riemann's zeta function $$\\zeta(\\gamma) = \\sum_{n\\geq 0} \\frac{1}{n^{\\,\\gamma}}$$.\n",
    "\n",
    "The MLE estimate of the power parameter of discrete power law involves the derivative of the Zeta function, which forbids  a closed agelbaric form of the solution to the first order conditions on the maximum of log-likelihhod: $$\\frac{\\partial }{\\partial \\gamma} \\mathcal{L}\\quad:\\quad \\frac{-\\zeta'(\\gamma)}{\\zeta(\\gamma)} = \\sum_{i=1}^n \\ln (k_i-u+1)$$ \n",
    "In practice it is necessary to resort to numerical optimization in order to finde the MLE under this distributional assumption.\n",
    "\n",
    "The routines below implements exactly the MLE of $\\alpha$ and $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ML estimator of the power law in the \"tail\" (x≥u):\n",
    "##  x_k \\sim C x^{-\\alpha} 1_{[u,+∞)}(x).\n",
    "def mle_alpha( data, threshold ) :\n",
    "## Keep the data observations, that we consider to be in the tail\n",
    "\ttail = np.array( [ v for v in data if v >= threshold ] )\n",
    "## Estimate the mean log of the peaks over threshold\n",
    "\tsum_log = np.sum( np.log( tail ) ) / ( len( tail ) + 0.0 )\n",
    "## Use the closed form expression for the value of the power at an optimum\n",
    "\talpha = 1.0 + 1.0 / ( sum_log - np.log( threshold ) )\n",
    "## Using the delta-method compute the s.e of the estimate.\n",
    "\treturn alpha, ( alpha - 1 ) / np.sqrt( len( tail ) )\n",
    "\n",
    "## The function below implements the same functionality as the previous one\n",
    "##  but instead of the continuous version it works with the discrete power law.\n",
    "from scipy.special import zeta\n",
    "from scipy.optimize import minimize\n",
    "## The discrete power law gives marginally different results\n",
    "##  \\Pr(N=n) \\defn \\frac{1}{\\zeta(\\gamma)} n^{-\\gamma}, n -- positive integer\n",
    "def mle_alpha_d( data, threshold ) :\n",
    "## Keep the data observations, that we consider to be in the tail\n",
    "\ttail = np.array( [ v for v in data if v >= threshold ] )\n",
    "## Estimate the mean log of the peaks over threshold\n",
    "\tsum_log = np.sum( np.log( tail - threshold + 1 ) ) / ( len( tail ) + 0.0 )\n",
    "## Define minus log-likelihood of the discrete power law\n",
    "\tloglik = lambda alpha : np.log( zeta( alpha ) ) + alpha * sum_log\n",
    "## Compute the ML estimate of the exponent, with a view to using it as the\n",
    "##  initial seed for the numerical minimizer for better convergence.\n",
    "\tres = minimize( loglik, ( 1.0 + 1.0 / sum_log, ), method = 'Nelder-Mead', options = { 'disp': False } )\n",
    "## Return the \"optimal\" argument, regardless of its quality. Potentially DANGEROUS!\n",
    "\treturn res.x[ 0 ], float( 'nan' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting an optimal threshold, beyond which the power-law like tail behaviour is expected, which adequately balances between the bias and the variance, is very important. As suggested in ... this task is preformed well by employing the statistic in the Kolmogorov-Smirnov goodness-of-fit test. The statistic itself is the $L^\\infty$ norm of the difference between the hypothesised distribution function and the observed (empirical) CDF.\n",
    "\n",
    "Routines below implement this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define a convenience function for estimating the power parameter\n",
    "##  of the continuous power law\n",
    "from scipy.stats import kstest\n",
    "def ks_dist( data, threshold ) :\n",
    "## Estimate the power given the current threshold\n",
    "\talpha, sd = mle_alpha( data, threshold )\n",
    "## Construct the CDF in the current environment\n",
    "\tcdf = lambda x : 1.0 - ( x / threshold ) ** ( 1.0 - alpha )\n",
    "## Return the output of the out-of-the box Kolmogorov-Smirnov test:\n",
    "##  the infinity norm of the difference between the distribution functions.\n",
    "\td, pv = kstest( [ v for v in data if v >= threshold ], cdf )\n",
    "\treturn (d, pv), (alpha, sd)\n",
    "\n",
    "def ks_dist_d( data, threshold ) :\n",
    "## Estimate the power given the current threshold\n",
    "\talpha, sd = mle_alpha_d( data, threshold )\n",
    "## Construct the CDF in the current environment\n",
    "\tcdf = lambda k : 1.0 - zeta( alpha, k-threshold+1 ) / zeta( alpha )\n",
    "## Return the output of the out-of-the box Kolmogorov-Smirnov test:\n",
    "##  the infinity norm of the difference between the distribution functions.\n",
    "\td, pv = kstest( [ v for v in data if v >= threshold ], cdf )\n",
    "\treturn (d, pv), (alpha, sd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper functions invert an array and count the number of occurrences of distinct values in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def values( data, frequency = False ) :\n",
    "\tbins = dict( )\n",
    "## For each value in the given array, add the index of each occurrence\n",
    "##  into the bin dedicated to the encountered value.\n",
    "\tfor i, x in enumerate( sorted( data ) ) :\n",
    "## Prepend the current occurrence of a value, unless it has never been\n",
    "##  seen before, in which case initialise the list of indices for it.\n",
    "\t\tbins[ x ] = bins.get( x, [] ) + [ i ]\n",
    "\treturn bins\n",
    "\n",
    "## It was brought to my attention, that numpy.unique() does the same trick... \n",
    "def counts( data ) :\n",
    "## Count the number of times a value occurs in the array.\n",
    "\tcounts = dict( )\n",
    "\tfor x in data :\n",
    "## If the values has not been seen yet, then initialize it to\n",
    "##  a single occurrence otherwise increment its counter.\n",
    "\t\tcounts[ x ] = counts.get( x, 0 ) + 1\n",
    "\treturn counts.items( )\n",
    "\n",
    "## Construct the complimentary cumulative distribution function for\n",
    "##  the data exceedig the given tail threshold.\n",
    "def ccdf( data, threshold ) :\n",
    "## Count the occurrences of values over some threshold in the array\n",
    "\tfreq = np.array( counts(\n",
    "\t\t\t[ v for v in data if v >= threshold ] ),\n",
    "\t\tdtype = float )\n",
    "## Sort the counts along the growing values they correspond to\n",
    "\tfreq = freq[ freq[ :, 0 ].argsort( ), : ]\n",
    "## ... and compute the fraction of data with values lower than the current\n",
    "\tfreq[:,1] = 1.0 - np.cumsum( freq[ :,1 ], dtype = float ) / sum( freq[ :,1 ] )\n",
    "\treturn freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean excess plot is a visual tool that helps determine the tail-type behavoiur from the sample data. Basically it is just the plot of the sample mean of values exceeding some threshold.\n",
    "\n",
    "Id $X$ is some random varaible with $\\mathbb{E}X^+ < +\\infty$, then the function $M(u)$, also known as the means residual lifetime, or mean excess over threshlod, is defined as\n",
    "$$ M(u) \\overset{\\Delta}{=} \\mathbb{E}{\\Big ( {\\big. X-u\\,\\big \\rvert}\\, X\\geq u \\Big)} = \\mathbb{E}{\\Big ( {\\big. X\\,\\big \\rvert}\\, X\\geq u \\Big)} - u$$\n",
    "Its empirical analog is providedn by the folowing expression:\n",
    "$$\\hat{M}(u) \\overset{\\Delta}{=} \\frac{1}{\\sum_{i=1}^n 1_{[u, \\infty)}(x_i)} \\sum_{i=1}^n (x_i - u)1_{[u, \\infty)}(x_i) $$\n",
    "\n",
    "Heavy-tailed behaviour reveals itself as an upwards trend in the graph above some threshold. A downward trend shows thin-tailed behaviour whereas an almost flat line shows an exponential tail. Mean excesses for higher thresholds are averages of a handful of extreme excesses, which implies that in this region the plot is unstable.\n",
    "\n",
    "Indeed, if $X\\sim \\text{Pwr}(\\alpha,x_0)$ then \n",
    "$$M(u) = \\frac{ (\\alpha-1)\\,x_0^{\\,\\alpha-1}\\,\\int\\limits_u^\\infty s^{1-\\alpha} ds }{{\\big (\\tfrac{u}{x_0}\\big )}^{1-\\alpha}} - u= \\frac{\\alpha-1}{\\alpha-2} u - u = \\frac{1}{\\alpha-2} u$$\n",
    "\n",
    "If, however, $X\\sim \\text{Exp}(\\lambda)$ then \n",
    "$$M(u) = \\frac{ \\int\\limits_u^\\infty s \\lambda e^{-\\lambda s} ds }{e^{-\\lambda u}} - u = \\frac{  u e^{-\\lambda u}+ e^{-\\lambda u}\\lambda^{-1} }{e^{-\\lambda u}} - u = \\frac{1}{\\lambda}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_excess( data ) :\n",
    "\tdata = np.array( sorted( data, reverse = True ) )\n",
    "## Compute the last positions in the sorted array of each repeated observation\n",
    "\tranks = rankdata( data, method = 'max' )\n",
    "## Since the array is sorted, the number of observation exceeding the current\n",
    "##  is givne by difference between the length of the array and the max-rank.\n",
    "\texcesses = np.array( np.unique( len( data ) - ranks ), dtype = np.int )\n",
    "## Get the last values in each group -- the thresholds\n",
    "\tthresholds = data[ excesses ]\n",
    "## Get the sum of all values greater than the current threshold \n",
    "\tmean_excess = np.cumsum( data )[ excesses ] / ( excesses + 0.0 ) - thresholds\n",
    "\treturn np.array( zip( thresholds, mean_excess ), dtype = np.float )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [wordcounts](http://www.leonidzhukov.net/hse/2015/networks/data/wordcounts.txt) dataset. \n",
    "1. Check that Zipf's Law holds\n",
    "2. Assuming that the data is distributed according to the Power Law, find\n",
    " * $\\alpha$ of the distribution\n",
    " * mean sample variance $\\sigma^2$\n",
    "3. Produce summary of the frequencies: min, max, mean, median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#+ 0. Load the data (yes, it is a milestone!)\n",
    "## Load the word count dataset\n",
    "wordcount = np.fromregex(\n",
    "\t'./data/wordcounts.txt', r\"(\\d+)\\s+(.{,32})\",\n",
    "\t[ ( 'freq', np.int64 ), ( 'word', 'S32' ) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "##+ 1. Check that Zipf's Law holds\n",
    "## Pre-sort the frequencies: in ascending order of frequencies\n",
    "wordcount.sort( order = 'freq' )\n",
    "freqs = wordcount[ 'freq' ]\n",
    "## PRoduce ranks: from 1 up to |W|\n",
    "ranks = np.arange( 1, len( wordcount ) + 1, dtype = float )[::-1]\n",
    "## The probability of a word frequency being not less than the \n",
    "##  frequency of a gien word w it exactly the ratio of the w's rank\n",
    "##  to the total number of words.\n",
    "probs = ranks / len( wordcount )\n",
    "\n",
    "## estimate f_k\\sim C k^{-\\gamma} model\n",
    "mdl = lm_model( np.log( ranks ), np.log( freqs ), True )\n",
    "coef, rss, dof, XX = lm_fit( mdl )\n",
    "\n",
    "## Define the fitted Zipf's law\n",
    "# zipf = lambda r : np.exp( coef.dot( ( 1, np.log( r ) ) ) )\n",
    "zipf = lambda r : np.exp( coef[0] + coef[1] * np.log( r ) )\n",
    "\n",
    "## Show how well is was estimated.\n",
    "plt.loglog( freqs, probs, \"xr\" )\n",
    "plt.plot( zipf( ranks ), probs, \"-b\" )\n",
    "plt.xlabel( \"frequency\" ) ; plt.ylabel( \"ranks\" )\n",
    "plt.title( \"Wordcount data\" )\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "##+ 2. Assuming that the data is distributed according to the Power Law, find\n",
    "##  * $\\alpha$ of the distribution\n",
    "##  * mean sample variance $\\sigma^2$\n",
    "\n",
    "## Get the ML estimate\n",
    "alpha_ml, alpha_ml_sd = mle_alpha( freqs, freqs.min( ) )\n",
    "\n",
    "## Let's suppose that the rank is proportional to the complementary CDF\n",
    "##  of a power law: $\\bar{F}(x) = {\\left(\\frac{x}{u}\\right)}^{1-\\alpha}$\n",
    "##  Thus the following econometric model is to be estimated:\n",
    "##  $\\log \\text{rank} \\sim C + (1-\\alpha) \\log \\text{freq} + \\epsilon$\n",
    "mdl = lm_model( np.log( freqs ), np.log( ranks ), True )\n",
    "beta, rss, dof, XX = lm_fit( mdl )\n",
    "## Transform the coefficient\n",
    "alpha_ls = 1 - beta[ 1 ]\n",
    "\n",
    "## The regression estimate of the power should be close\n",
    "##  to the ML estimate\n",
    "print \"the OLS estimate of alpha is %f\\n\" % alpha_ls\n",
    "print \"Whereas the ML estimate is %f (%f) \\n\" % ( alpha_ml, alpha_ml_sd )\n",
    "print \"Since ML is more theoretically sound, the relative error is %f%%\\n\" % (\n",
    "\t100 * np.abs( 1.0 - alpha_ls / alpha_ml ), )\n",
    "\n",
    "## The mean and the sample variance of the sample\n",
    "##  frequency distribution:\n",
    "print \"The average frequency over the sample is \", freqs.mean(), \"\\n\"\n",
    "print \"The sample variance is \", freqs.var(), \"\\n\"\n",
    "\n",
    "## Theoretical mean and variance of the power law distribution\n",
    "##  significantly depend on the power parameter.\n",
    "## Indeed for $x\\sim \\frac{\\alpha-1}{u} {\\left( \\frac{x}{u} \\right)}^\\alpha$ one has the following:\n",
    "##   $E(x) = \\frac{\\alpha-1}{\\alpha-2} u$ if $\\alpha>2$\n",
    "##   $E(x^2) = \\frac{\\alpha-1}{\\alpha-3} u^2$ if $\\alpha>3$\n",
    "## The estimated parameter is less than 2, implying that the frequency\n",
    "##  distribution is unlikely to have even a finite mean under the\n",
    "##  assumed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "##+ 3. Produce summary of the frequencies: min, max, mean, median\n",
    "## Does it make sense to compute these summaries? What does the mean frequency tell us?\n",
    "print \"The minimum frequency is \", freqs.min(), \"\\n\"\n",
    "print \"The mean frequency is \", freqs.mean(), \"\\n\"\n",
    "print \"The median frequency is \", np.median( freqs ), \"\\n\"\n",
    "print \"The maximum frequency is \", freqs.max(), \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <hr /> Task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find and plot PDF and CDF for the following networks:\n",
    "* [Routing network](http://www.leonidzhukov.net/hse/2015/networks/data/network.txt)\n",
    "* [Web graph](http://www.leonidzhukov.net/hse/2015/networks/data/web_Stanford.txt)\n",
    "* [Facebook network](http://www.leonidzhukov.net/hse/2015/networks/data/fb_Princeton.txt)\n",
    "\n",
    "\n",
    "1. Are they correspondent to power law?\n",
    "2. Find max and mean values of incoming and outcoming node degrees\n",
    "3. Find $\\alpha$ via Maximum Likelihood and calculate $\\sigma^2$\n",
    "4. Determine $x_{min}$ via Kolmogorov-Smirnov test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The routing network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "## + 0. Read the graph\n",
    "## Load the network routing graph first as it is the smallest. It is\n",
    "##  an undirected graph.\n",
    "import networkx as nx\n",
    "G = nx.read_edgelist( \"./data/network.txt\", create_using = nx.Graph( ) );\n",
    "\n",
    "node_degree = G.degree( )\n",
    "deg = np.array( node_degree.values( ), dtype = np.int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "##+ 1. Are they correspondent to power law?\n",
    "## First let's draw the frequency plot of the node degree distribution.\n",
    "degree_freq = np.array( counts( deg ) )\n",
    "deg_me = mean_excess( deg )\n",
    "\n",
    "plt.figure( 1, figsize = ( 10, 5 ) )\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title( \"Node degree frequency\" )\n",
    "plt.loglog( degree_freq[:,0], degree_freq[:,1], \"bo\" )\n",
    "plt.xlabel( \"degree\" ) ; plt.ylabel( \"frequency\" )\n",
    "\n",
    "plt.subplot(122)\n",
    "## An upward trend in plot shows heavy-tailed behaviour, but the\n",
    "##  values for high thresholds are unreliably estimated.\n",
    "plt.title( \"Mean excess plot\" )\n",
    "plt.loglog( deg_me[:,0], deg_me[:,1], \"bo-\", linewidth = 2 )\n",
    "plt.ylabel( \"mean excess\" ) ; plt.xlabel( \"threshold\" )\n",
    "\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean excess plot has an unmistakeable upward trend throughout the whole set of thresholds. This is strong heuristic evidence for a heavy tail in the node degree distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The empirical degree distribution may not correspond to a power\n",
    "##  law per se, but it definitely has some heavy tailed behaviour,\n",
    "##  which exhibits itself, when the only data exceeding same truncated\n",
    "##  is considered.\n",
    "cc = ccdf( deg, 0 )\n",
    "\n",
    "plt.title( \"Degree cCDF\" )\n",
    "plt.xlabel( \"degree\" ) ; plt.ylabel( \"probability\" )\n",
    "plt.loglog( cc[:,0], cc[:,1], \"bo-\", linewidth = 2 )\n",
    "plt.show( )\n",
    "\n",
    "## Clearly the chances of an extremely high node degree decay proportional\n",
    "##  to the value of the degree on a log-log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "##+ 2. Find max and mean values of incoming and outcoming node degrees\n",
    "## Since the network graph is undirected it does not make sense to\n",
    "##  distinguish in- and out- nodes. Thus let's check the range of the\n",
    "##  general (two-way) degree.\n",
    "\n",
    "print \"The degrees range from %d to %d\" % ( min( deg ), max( deg ) ) #, \"\\n\"\n",
    "print \"The average degree over the sample is %.3f\" % ( G.size( ) / G.order( ) ) #, \"\\n\"\n",
    "print \"The degree standard deviation is %.3f\" % ( np.sqrt( np.var( deg ) ) ) #, \"\\n\"\n",
    "print \"The median degree is %d\" % ( np.median( deg ) ) #, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "##+ 3. Find $\\alpha$ via Maximum Likelihood and calculate $\\sigma^2$\n",
    "##+ 4. Determine $x_{min}$ via Kolmogorov-Smirnov test\n",
    "\n",
    "## We have reasons to believe there are some power law-like effects in\n",
    "##  the behaviour of the node degree (treated as a random variable).\n",
    "##  Let's pursue this lead and estimate the exponent in the power law\n",
    "##  and select the most likely breakpoint, beyond which the degree\n",
    "##  is heavy tailed.\n",
    "\n",
    "#####################################################################\n",
    "## Get the ML estimate of the exponent parameter.\n",
    "alpha_ml, alpha_ml_se = mle_alpha( deg, min( deg ) )\n",
    "print \"The Maximum likelihood estimate of the exponent of the node degree distribution is %.3f (%.4f)\\n\" % ( alpha_ml, alpha_ml_se )\n",
    "\n",
    "#####################################################################\n",
    "## Run the KS threshold selection routine\n",
    "thresholds = np.unique( deg )\n",
    "## The ks_dist() function returns a tuple of the following parameters:\n",
    "##  * ( KS-distance, PV of the KS-test ), ( MLE of alpha, the standard error of the MLE )\n",
    "ks_min = np.array( [ ks_dist( deg, u ) for u in thresholds ] )\n",
    "\n",
    "## Select the x_min that brings the KS metric to its minimum on the given\n",
    "##  degree data. Note the first threshold is removed, since it is likely\n",
    "##  to yield very biased estimate.\n",
    "i_min = np.argmin( ks_min[1:,0,0] )+1\n",
    "x_min = thresholds[ i_min ]\n",
    "alpha_ml, alpha_ml_se = ks_min[ i_min, 1, : ]\n",
    "\n",
    "## Produce a dataset for cCDF plotting.\n",
    "x = np.arange( x_min, 2 * np.max( deg ) )\n",
    "deg_ccdf = ccdf( deg, x_min )\n",
    "pwr_ccdf = lambda x : ( x / ( x_min + 0.0 ) ) ** ( 1.0 - alpha_ml )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the dependence of $\\alpha$ and the KS statistic on the threshold $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Produce the hill plot: the correspondence between the threshold\n",
    "##  and the estimated exponent.\n",
    "\n",
    "plt.figure( 1, figsize = ( 10, 5 ) )\n",
    "\n",
    "plt.subplot( 121 )\n",
    "plt.title( 'The Hill plot of the degree distribution' )\n",
    "plt.ylabel( 'alpha' ) ; plt.ylabel( 'threshold' )\n",
    "plt.axhline( y = alpha_ml, linewidth = 1, color = 'b' )\n",
    "plt.axvline( x = x_min, linewidth = 1, color = 'b', linestyle = '--' )\n",
    "plt.loglog( thresholds, ks_min[:,1,0], \"r-\")\n",
    "\n",
    "## In fact the KS-metric is the $L^\\infty$ norm on the set of distribution\n",
    "##  functions.\n",
    "plt.subplot( 122 )\n",
    "plt.title( 'KS metric distance' )\n",
    "plt.ylabel( 'max distance' ) ; plt.ylabel( 'threshold' )\n",
    "plt.axhline( y = ks_min[ i_min, 0, 0 ], linewidth = 1, color = 'b' )\n",
    "plt.axvline( x = x_min, linewidth = 1, color = 'b', linestyle = '--' )\n",
    "plt.loglog( thresholds, ks_min[:,0,0], \"r-\")\n",
    "\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated theoretical and the empirical CDFs are quite well aligned with each other for the chosen threshold. The tail of the hypothesised node degree law appears to have a higher tail decay rate, but that is due to the common severe undersampling of the tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The Kolmogorov-Smirnov metric yielded %.1f as the optimal threshold\\n\" % ( x_min)\n",
    "print \"'Optimal' exponent is %.3f (%.3f)\\n\" % ( alpha_ml, alpha_ml_se )\n",
    "\n",
    "plt.title( \"Degree cCDF\" )\n",
    "plt.xlabel( \"degree\" ) ; plt.ylabel( \"probability\" )\n",
    "plt.plot( x, pwr_ccdf( x ), \"b-\", linewidth = 2 )\n",
    "plt.plot( deg_ccdf[:,0], deg_ccdf[:,1], \"r-\", linewidth = 2 )\n",
    "plt.axvline( x = x_min, linewidth = 2, color = 'k', linestyle = '-' )\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "## + 0. Read the graph\n",
    "## Load the network routing graph first as it is the smallest. It is\n",
    "##  an undirected graph.\n",
    "import networkx as nx\n",
    "G = nx.read_edgelist( \"./data/fb_Princeton.txt\", create_using = nx.DiGraph( ) );\n",
    "\n",
    "node_in_degree = G.in_degree( )\n",
    "node_out_degree = G.out_degree( )\n",
    "\n",
    "in_deg = np.array( node_in_degree.values( ), dtype = np.int )\n",
    "out_deg = np.array( node_out_degree.values( ), dtype = np.int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "##+ 1. Are they correspondent to power law?\n",
    "## First let's draw the frequency plot of the node degree distribution.\n",
    "degree_in_freq = np.array( counts( in_deg ) )\n",
    "degree_out_freq = np.array( counts( out_deg ) )\n",
    "\n",
    "plt.title( \"Node degree frequency\" )\n",
    "plt.xlabel( \"degree\" ) ; plt.ylabel( \"frequency\" )\n",
    "plt.loglog( degree_out_freq[:,0], degree_out_freq[:,1], \"bo\" )\n",
    "plt.loglog( degree_in_freq[:,0], degree_in_freq[:,1], \"r<\" )\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure( 1, figsize = ( 10, 5 ) )\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title( \"Degree cCDF-loglog\" )\n",
    "out_cc = ccdf( out_deg, 0 )\n",
    "plt.loglog( out_cc[:,0], out_cc[:,1], \"bo-\", linewidth = 2 )\n",
    "in_cc = ccdf( in_deg, 0 )\n",
    "plt.loglog( in_cc[:,0], in_cc[:,1], \"r<-\", linewidth = 2 )\n",
    "plt.xlabel( \"degree\" ) ; plt.ylabel( \"probability\" )\n",
    "\n",
    "plt.subplot(122)\n",
    "## An upward trend in plot shows heavy-tailed behaviour, but the\n",
    "##  values for high thresholds are unreliably estimated.\n",
    "plt.title( \"Mean excess plot\" )\n",
    "out_me = mean_excess( out_deg )\n",
    "plt.loglog( out_me[:,0], out_me[:,1], \"bo-\", linewidth = 2 )\n",
    "in_me = mean_excess( in_deg )\n",
    "plt.loglog( in_me[:,0], in_me[:,1], \"r<-\", linewidth = 2 )\n",
    "plt.ylabel( \"mean excess\" ) ; plt.xlabel( \"threshold\" )\n",
    "\n",
    "plt.show( )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ME plots for both the in- and the out-degrees possess significantly long flat regions, and only at the high end of the thresholds do they \"explode\" into a singularity. Such behaviour hints at the possibilty of an exponential tail in the distributions of both inward and outward vertex degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "##+ 2. Find max and mean values of incoming and outcoming node degrees\n",
    "print \"The degrees range from %d to %d for inward direction and from %d to %d for outward edges\" % ( min( in_deg ), max( in_deg ), min( out_deg ), max( out_deg ) ) #, \"\\n\"\n",
    "print \"The average degree over the sample is %.3f (IN) and %.3f (OUT)\" % ( np.sum( in_deg ) / ( G.order( ) + 0.0 ), np.sum( out_deg ) / ( G.order( ) + 0.0 ) ) #, \"\\n\"\n",
    "print \"The degree standard deviation is %.3f for the in-degree and %.3f -- out-degree\" % ( np.sqrt( np.var( in_deg ) ), np.sqrt( np.var( out_deg ) ) ) #, \"\\n\"\n",
    "print \"The median in- and out-degree is %d and %d respectively\" % ( np.median( in_deg ), np.median( out_deg ) ) #, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "##+ 3. Find $\\alpha$ via Maximum Likelihood and calculate $\\sigma^2$\n",
    "##+ 4. Determine $x_{min}$ via Kolmogorov-Smirnov test\n",
    "\n",
    "#####################################################################\n",
    "## Get the ML estimate of the exponent parameter. There are some isolated\n",
    "##  nodes in the provided graph, which means that it is necessary\n",
    "##  to omit these nodes from the analysis using a simple power law.\n",
    "## One of course could try to fit a model with an explicit atom at zero,\n",
    "##  but that should wait for a better time.\n",
    "\n",
    "in_alpha_ml, in_alpha_ml_se = mle_alpha( in_deg, min( in_deg )+1 )\n",
    "out_alpha_ml, out_alpha_ml_se = mle_alpha( out_deg, min( out_deg )+1 )\n",
    "\n",
    "#####################################################################\n",
    "in_thresholds = np.unique( in_deg )\n",
    "out_thresholds = np.unique( out_deg )\n",
    "\n",
    "## Run the KS threshold selection routine\n",
    "in_ks_min = np.array( [ ks_dist( in_deg, u ) for u in in_thresholds ] )\n",
    "out_ks_min = np.array( [ ks_dist( out_deg, u ) for u in out_thresholds ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Select the x_min that brings the KS metric to its minimum on the given\n",
    "##  degree data. Note the first threshold is removed, since it is likely\n",
    "##  to yield very biased estimate.\n",
    "in_i_min = np.argmin( in_ks_min[1:,0,0] )+1\n",
    "out_i_min = np.argmin( out_ks_min[1:,0,0] )+1\n",
    "\n",
    "## Produce a dataset for cCDF plotting.\n",
    "in_x = np.arange( in_thresholds[ in_i_min ], 2 * np.max( in_deg ) )\n",
    "out_x = np.arange( out_thresholds[ out_i_min ], 2 * np.max( out_deg ) )\n",
    "\n",
    "## Get the empirical complementary distribution fuction.\n",
    "in_deg_ccdf = ccdf( in_deg, in_thresholds[ in_i_min ] )\n",
    "out_deg_ccdf = ccdf( out_deg, out_thresholds[ out_i_min ] )\n",
    "\n",
    "## ... and the fitted power law.\n",
    "in_pwr_ccdf = lambda x : ( x / ( in_thresholds[ in_i_min ] + 0.0 ) ) ** ( 1.0 - in_ks_min[ in_i_min, 1, 0 ] )\n",
    "out_pwr_ccdf = lambda x : ( x / ( out_thresholds[ out_i_min ] + 0.0 ) ) ** ( 1.0 - out_ks_min[ out_i_min, 1, 0 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The MLE of the exponent of the inward and outward degree distribution is %.3f (%.4f) and %.3f (%.4f) respectively\\n\" % ( in_alpha_ml, in_alpha_ml_se, out_alpha_ml, out_alpha_ml_se )\n",
    "\n",
    "## Produce the hill plot: the correspondence between the threshold\n",
    "##  and the estimated exponent.\n",
    "plt.figure( 1, figsize = ( 10, 5 ) )\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title( 'The Hill plot of the degree distribution' )\n",
    "\n",
    "plt.axhline( y = in_ks_min[ in_i_min, 1, 0 ], linewidth = 1, color = 'r' )\n",
    "plt.axvline( x = in_thresholds[ in_i_min ], linewidth = 1, color = 'r', linestyle = '--' )\n",
    "plt.loglog( in_thresholds, in_ks_min[:,1,0], \"r<-\")\n",
    "\n",
    "plt.axhline( y = out_ks_min[ out_i_min, 1, 0 ], linewidth = 1, color = 'b' )\n",
    "plt.axvline( x = out_thresholds[ out_i_min ], linewidth = 1, color = 'b', linestyle = '--' )\n",
    "plt.loglog( out_thresholds, out_ks_min[:,1,0], \"bo-\")\n",
    "\n",
    "plt.ylabel( 'alpha' ) ; plt.ylabel( 'threshold' )\n",
    "\n",
    "\n",
    "## In fact the KS-metric is the $L^\\infty$ norm on the set of distribution\n",
    "##  functions.\n",
    "plt.subplot(122)\n",
    "plt.title( 'The KS metric distance' )\n",
    "\n",
    "plt.axhline( y = in_ks_min[ in_i_min,0, 0 ], linewidth = 1, color = 'r' )\n",
    "plt.axvline( x = in_thresholds[ in_i_min ], linewidth = 1, color = 'r', linestyle = '--' )\n",
    "plt.loglog( in_thresholds, in_ks_min[:,0,0], \"r<-\")\n",
    "\n",
    "plt.axhline( y = out_ks_min[ out_i_min,0, 0 ], linewidth = 1, color = 'b' )\n",
    "plt.axvline( x = out_thresholds[ out_i_min ], linewidth = 1, color = 'b', linestyle = '--' )\n",
    "plt.loglog( out_thresholds, out_ks_min[:,0,0], \"bo-\")\n",
    "\n",
    "plt.ylabel( 'max distance' ) ; plt.ylabel( 'threshold' )\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hill plots (the estimated exponent $\\hat{\\alpha}_u$ against the employed threshold $u$) have a distinct upward curving trend, which can only be a result of an exponential behaviour in the tail of both the in- and the out-degreee distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"OUT-degree: The Kolmogorov-Smirnov metric yielded %.1f as the optimal threshold and %.3f (%.3f) as 'optimal' exponent\\n\" % ( out_thresholds[ out_i_min ], out_ks_min[ out_i_min, 1, 0 ], out_ks_min[ out_i_min, 1, 1 ] )\n",
    "print \"IN-degree: The Kolmogorov-Smirnov metric yielded %.1f as the optimal threshold and %.3f (%.3f) as 'optimal' exponent\\n\" % ( out_thresholds[ in_i_min ], in_ks_min[ in_i_min, 1, 0 ], in_ks_min[ in_i_min, 1, 1 ] )\n",
    "\n",
    "plt.figure( 3, figsize = ( 10, 5 ) )\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title( \"Out degree cCDF\" )\n",
    "plt.plot( out_x, out_pwr_ccdf( out_x ), \"k-\", linewidth = 2 )\n",
    "plt.plot( out_deg_ccdf[:,0], out_deg_ccdf[:,1], \"bo-\", linewidth = 2 )\n",
    "plt.xlabel( \"degree\" ) ; plt.ylabel( \"probability\" )\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title( \"In degree cCDF\" )\n",
    "plt.plot( in_x, in_pwr_ccdf( in_x ), \"k-\", linewidth = 2 )\n",
    "plt.plot( in_deg_ccdf[:,0], in_deg_ccdf[:,1], \"r<-\", linewidth = 2 )\n",
    "plt.xlabel( \"degree\" ) ; plt.ylabel( \"probability\" )\n",
    "\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, both complimetary CDFs show decay rates faster than the power law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEB Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "## + 0. Read the graph\n",
    "import networkx as nx\n",
    "G = nx.read_edgelist( \"./data/web_Stanford.txt\", create_using = nx.DiGraph( ) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node_in_degree = G.in_degree( )\n",
    "node_out_degree = G.out_degree( )\n",
    "\n",
    "in_deg = np.array( node_in_degree.values( ), dtype = np.int )\n",
    "out_deg = np.array( node_out_degree.values( ), dtype = np.int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "##+ 1. Are they correspondent to power law?\n",
    "degree_in_freq = np.array( counts( in_deg ) )\n",
    "degree_out_freq = np.array( counts( out_deg ) )\n",
    "\n",
    "plt.title( \"Node degree frequency\" )\n",
    "plt.xlabel( \"degree\" ) ; plt.ylabel( \"frequency\" )\n",
    "plt.loglog( degree_out_freq[:,0], degree_out_freq[:,1], \"bo\" )\n",
    "plt.loglog( degree_in_freq[:,0], degree_in_freq[:,1], \"r<\" )\n",
    "plt.show( )\n",
    "\n",
    "plt.figure( 1, figsize = ( 10, 5 ) )\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title( \"Degree cCDF-loglog\" )\n",
    "out_cc = ccdf( out_deg, 0 )\n",
    "plt.loglog( out_cc[:,0], out_cc[:,1], \"bo-\", linewidth = 2 )\n",
    "in_cc = ccdf( in_deg, 0 )\n",
    "plt.loglog( in_cc[:,0], in_cc[:,1], \"r<-\", linewidth = 2 )\n",
    "plt.xlabel( \"degree\" ) ; plt.ylabel( \"probability\" )\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.title( \"Mean excess plot\" )\n",
    "out_me = mean_excess( out_deg )\n",
    "plt.loglog( out_me[:,0], out_me[:,1], \"bo-\", linewidth = 2 )\n",
    "in_me = mean_excess( in_deg )\n",
    "plt.loglog( in_me[:,0], in_me[:,1], \"r<-\", linewidth = 2 )\n",
    "plt.ylabel( \"mean excess\" ) ; plt.xlabel( \"threshold\" )\n",
    "\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ME plot a very common case, when it is not quite clear whether mean excesses have an upward trand or not. Neglecting the upper thresholds, both distributions behave concictently with a heavy tailed distiribution. However, the tail of a distribution by definition the asymptotic behaviour for increasing threshold. This means that one must needs look at the unstable estimates of the conditional mean at the right end of the threshold range.\n",
    "\n",
    "In the case of the WEB graph the $\\hat{M}(u)$ for extremely high thresholds shows clear oscillations around some constant level. This sugests an exponential tail, but still further investigation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "##+ 2. Find max and mean values of incoming and outcoming node degrees\n",
    "print \"The degrees range from %d to %d for inward direction and from %d to %d for outward edges\" % ( min( in_deg ), max( in_deg ), min( out_deg ), max( out_deg ) ) #, \"\\n\"\n",
    "print \"The average degree over the sample is %.3f (IN) and %.3f (OUT)\" % ( np.sum( in_deg ) / ( G.order( ) + 0.0 ), np.sum( out_deg ) / ( G.order( ) + 0.0 ) ) #, \"\\n\"\n",
    "print \"The degree standard deviation is %.3f for the in-degree and %.3f -- out-degree\" % ( np.sqrt( np.var( in_deg ) ), np.sqrt( np.var( out_deg ) ) ) #, \"\\n\"\n",
    "print \"The median in- and out-degree is %d and %d respectively\" % ( np.median( in_deg ), np.median( out_deg ) ) #, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "##+ 3. Find $\\alpha$ via Maximum Likelihood and calculate $\\sigma^2$\n",
    "##+ 4. Determine $x_{min}$ via Kolmogorov-Smirnov test\n",
    "\n",
    "in_alpha_ml, in_alpha_ml_se = mle_alpha( in_deg, min( in_deg )+1 )\n",
    "out_alpha_ml, out_alpha_ml_se = mle_alpha( out_deg, min( out_deg )+1 )\n",
    "\n",
    "in_thresholds = np.unique( in_deg )\n",
    "out_thresholds = np.unique( out_deg )\n",
    "\n",
    "## Run the KS threshold selection routine\n",
    "in_ks_min = np.array( [ ks_dist( in_deg, u ) for u in in_thresholds ] )\n",
    "out_ks_min = np.array( [ ks_dist( out_deg, u ) for u in out_thresholds ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Select the x_min that brings the KS metric to its minimum on the given\n",
    "##  degree data.\n",
    "in_i_min = np.argmin( in_ks_min[1:,0,0] )+1\n",
    "out_i_min = np.argmin( out_ks_min[1:,0,0] )+1\n",
    "\n",
    "## Produce a dataset for cCDF plotting.\n",
    "in_x = np.arange( in_thresholds[ in_i_min ], 2 * np.max( in_deg ) )\n",
    "out_x = np.arange( out_thresholds[ out_i_min ], 2 * np.max( out_deg ) )\n",
    "\n",
    "## Get the empirical complementary distribution fuction.\n",
    "in_deg_ccdf = ccdf( in_deg, in_thresholds[ in_i_min ] )\n",
    "out_deg_ccdf = ccdf( out_deg, out_thresholds[ out_i_min ] )\n",
    "\n",
    "## ... and the fitted power law.\n",
    "in_pwr_ccdf = lambda x : ( x / ( in_thresholds[ in_i_min ] + 0.0 ) ) ** ( 1.0 - in_ks_min[ in_i_min, 1, 0 ] )\n",
    "out_pwr_ccdf = lambda x : ( x / ( out_thresholds[ out_i_min ] + 0.0 ) ) ** ( 1.0 - out_ks_min[ out_i_min, 1, 0 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The MLE of the exponent of the inward and outward degree distribution is %.3f (%.4f) and %.3f (%.4f) respectively\\n\" % ( in_alpha_ml, in_alpha_ml_se, out_alpha_ml, out_alpha_ml_se )\n",
    "\n",
    "## Produce the hill plot: the correspondence between the threshold\n",
    "##  and the estimated exponent.\n",
    "plt.figure( 1, figsize = ( 10, 5 ) )\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title( 'The Hill plot of the degree distribution' )\n",
    "\n",
    "plt.axhline( y = in_ks_min[ in_i_min, 1, 0 ], linewidth = 1, color = 'r' )\n",
    "plt.axvline( x = in_thresholds[ in_i_min ], linewidth = 1, color = 'r', linestyle = '--' )\n",
    "plt.loglog( in_thresholds, in_ks_min[:,1,0], \"r<-\")\n",
    "\n",
    "plt.axhline( y = out_ks_min[ out_i_min, 1, 0 ], linewidth = 1, color = 'b' )\n",
    "plt.axvline( x = out_thresholds[ out_i_min ], linewidth = 1, color = 'b', linestyle = '--' )\n",
    "plt.loglog( out_thresholds, out_ks_min[:,1,0], \"bo-\")\n",
    "\n",
    "plt.ylabel( 'alpha' ) ; plt.ylabel( 'threshold' )\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title( 'The KS metric distance' )\n",
    "\n",
    "plt.axhline( y = in_ks_min[ in_i_min,0, 0 ], linewidth = 1, color = 'r' )\n",
    "plt.axvline( x = in_thresholds[ in_i_min ], linewidth = 1, color = 'r', linestyle = '--' )\n",
    "plt.loglog( in_thresholds, in_ks_min[:,0,0], \"r<-\")\n",
    "\n",
    "plt.axhline( y = out_ks_min[ out_i_min,0, 0 ], linewidth = 1, color = 'b' )\n",
    "plt.axvline( x = out_thresholds[ out_i_min ], linewidth = 1, color = 'b', linestyle = '--' )\n",
    "plt.loglog( out_thresholds, out_ks_min[:,0,0], \"bo-\")\n",
    "\n",
    "plt.ylabel( 'max distance' ) ; plt.ylabel( 'threshold' )\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"OUT-degree: The Kolmogorov-Smirnov metric yielded %.1f as the optimal threshold and %.3f (%.3f) as 'optimal' exponent\\n\" % ( out_thresholds[ out_i_min ], out_ks_min[ out_i_min, 1, 0 ], out_ks_min[ out_i_min, 1, 1 ] )\n",
    "print \"IN-degree: The Kolmogorov-Smirnov metric yielded %.1f as the optimal threshold and %.3f (%.3f) as 'optimal' exponent\\n\" % ( in_thresholds[ in_i_min ], in_ks_min[ in_i_min, 1, 0 ], in_ks_min[ in_i_min, 1, 1 ] )\n",
    "\n",
    "plt.figure( 3, figsize = ( 10, 5 ) )\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title( \"Out degree cCDF\" )\n",
    "plt.plot( out_x, out_pwr_ccdf( out_x ), \"k-\", linewidth = 2 )\n",
    "plt.plot( out_deg_ccdf[:,0], out_deg_ccdf[:,1], \"bo-\", linewidth = 2 )\n",
    "plt.xlabel( \"degree\" ) ; plt.ylabel( \"probability\" )\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title( \"In degree cCDF\" )\n",
    "plt.plot( in_x, in_pwr_ccdf( in_x ), \"k-\", linewidth = 2 )\n",
    "plt.plot( in_deg_ccdf[:,0], in_deg_ccdf[:,1], \"r<-\", linewidth = 2 )\n",
    "plt.xlabel( \"degree\" ) ; plt.ylabel( \"probability\" )\n",
    "\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the fit on the power law is quite poor for the WEB graph, and at the same time its degree distribution show weak signs of existence of extremely high node degrees. This means that it is impossible to definitely determine the tail behaviour of this graph and in fact, a much richer sample of the WEB graph is needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
