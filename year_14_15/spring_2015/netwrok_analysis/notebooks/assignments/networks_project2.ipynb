{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Structural Analysis and Visualization of Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center/>Course Project #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Student: *Nazarov Ivan*</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <hr /> General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hard Deadline:** 21.06.2015 23:59 <br \\>\n",
    "\n",
    "Please send your reports to <mailto:leonid.e.zhukov@gmail.com> and <mailto:shestakoffandrey@gmail.com> with message subject of the following structure:<br \\> **[HSE Networks 2015] *{LastName}* *{First Name}* Project*{Number}***\n",
    "\n",
    "Support your computations with figures and comments. <br \\>\n",
    "If you are using IPython Notebook you may use this file as a starting point of your report.<br \\>\n",
    "<br \\>\n",
    "<hr \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with the [DBLP dataset](https://www.dropbox.com/s/ft4ekv2f3r43u7b/dblp_2000.csv.gz?dl=0) (warning, raw data!). It contains coauthorships that were revealed during $2000$-$2014$. Particularly, the file contains $3$ colomns: first two for authors' names and the third for the year of publication. This data can be naturally mapped to undirected graph structure.\n",
    "\n",
    "Your task is construct supervised link prediction scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Use *pandas* module to load and manipulate the dataset in Python\n",
    "1. Initiallize your classification set as follows:\n",
    "    * Determine training and testing intervals on your time domain (for instance, in DBLP dataset take a period $2000$-$2010$ as training period and $2011$-$2014$ as testing period)\n",
    "    * Pick pairs of authors that **have appeared during training interval** but **have not published together** during it\n",
    "    * These pairs form **positive** or **negative** examples depending on whether they have formed coauthorships **during the testing interval**\n",
    "    * You have arrived to binary classification problem. PROFIT!\n",
    "2. Construct feature space:\n",
    "    * Most of our features tend to be topological. Examples of the features can be: (weighted) sum of neigbours, shortest distance, etc\n",
    "3. Choose at least $4$ classification algorithms from [scikit module](http://scikit-learn.org/stable/) (goes with Anaconda) and compare them in terms of Accuracy, Precision, Recall, F-Score (for positive class) and Mean Squared Error. Use k-fold cross-validation and average your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd, networkx as nx, numpy as np, scipy.sparse as sp\n",
    "import os, regex as re, time as tm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DATADIR = os.path.realpath( os.path.join( \".\", \"data\", \"proj02\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pity there is no out-of-box solution for this in SKlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "class MultiColumnLabelEncoder :\n",
    "    def __init__( self, columns = None ):\n",
    "        self.columns = columns\n",
    "        self.__le = LabelEncoder(  )\n",
    "    def fit( self, X, y = None ) :\n",
    "        self.__columns = X.columns if self.columns is None else self.columns\n",
    "## Initialize the label encoder and make it assign labels to polled columns\n",
    "        self.__le.fit( pd.concat( X[ col ] for col in self.__columns ) )\n",
    "        self.classes_ = self.__le.classes_\n",
    "        return self\n",
    "    def transform( self, X, copy = True ) :\n",
    "## Copy the input dataframe and figure out what coluns to re-code\n",
    "        __output = X.copy( ) if copy else X\n",
    "## Iterate over the required columns\n",
    "        for col in self.__columns :\n",
    "            __output[ col ] = self.__le.transform( __output[ col ] )\n",
    "        return __output\n",
    "    def fit_transform( self, X, y = None ) :\n",
    "        return self.fit( X, y ).transform( X )\n",
    "    def inverse_transform( self, y ) :\n",
    "        return self.__le.inverse_transform( y )\n",
    "    def set_params( self, **params ) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the raw DBLP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBLP loaded in 5.721 sec.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists( os.path.join( DATADIR, \"dblp_dataframe.ppdf\" ) ) :\n",
    "## Start\n",
    "    tick = tm.time( )\n",
    "## Load the raw DBLP dataset\n",
    "    dblp_raw = pd.read_csv( os.path.join( DATADIR, \"dblp_2000.csv.gz\" ), # nrows = 100,\n",
    "## On-the-fly decompression\n",
    "                        compression = \"gzip\", header = None, quoting = 0,\n",
    "## Assign column headers\n",
    "                        names = [ 'author1', 'author2', 'year', ], encoding = \"utf-8\" )\n",
    "    tock = tm.time( )\n",
    "    print \"DBLP loaded in %.3f sec.\" % ( tock - tick, )\n",
    "## Pool the author columns together and let Pandas assign labels.\n",
    "    le = MultiColumnLabelEncoder( [ 'author1', 'author2', ] )\n",
    "    dblp = le.fit_transform( dblp_raw )\n",
    "    authors_index = le.classes_\n",
    "    del dblp_raw, le\n",
    "    tick = tm.time( )\n",
    "    print \"DBLP preprocessed in %.3f sec.\" % ( tick - tock, )\n",
    "## Cache\n",
    "    dblp.to_pickle( \"./data/proj02/dblp_dataframe.ppdf\" )\n",
    "    with open( \"./data/proj02/author_index.dic\", \"wb\" ) as out :\n",
    "        out.writelines( label + \"\\n\" for label in authors_index )\n",
    "## Report\n",
    "    tock = tm.time( )\n",
    "    print \"DBLP cached in %.3f sec.\" % ( tock - tick, )\n",
    "else :\n",
    "## Start\n",
    "    tick = tm.time( )\n",
    "## Load the database from pickled format\n",
    "    dblp = pd.read_pickle( \"./data/proj02/dblp_dataframe.ppdf\" )\n",
    "## Read the dictionary of authors\n",
    "    with open( \"./data/proj02/author_index.dic\", \"rb\" ) as out :\n",
    "        authors_index = out.readlines( )\n",
    "## Finish\n",
    "    tock = tm.time( )\n",
    "    print \"DBLP loaded in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It so happens that authors' names are quoted and their unicode letters escaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_author( txt ) :\n",
    "    return re.sub( r\"^\\s*\\\"(.*)\\\"\\s*$\", r\"\\1\", txt ).decode( \"unicode_escape\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data in two non-overlapping samples by year:\n",
    "* the coathorship data form 2000 till 2010 is the training data;\n",
    "* the collaboration since 2011 is the test sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the train and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dblp_X, dblp_y = dblp[ dblp.year <= 2010 ], dblp[ dblp.year >= 2011 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pick pairs of authors that **have appeared during training interval** but **have not published together** during it;\n",
    "* These pairs form **positive** or **negative** examples depending on whether they have formed coauthorships **during the testing interval**;\n",
    "* You have got yourself a binary classification problem.\n",
    "\n",
    "from \"M. Al Hasan, V. Chaoji, S. Salem, M. Zaki, Link prediction using supervised learning. Proceedings of SDM workshop on link analysis, 2006\":\n",
    "\n",
    "Each article bears, at least, its author information and publication year. To predict a link, we partition the range of publication year into two non-overlapping subranges. The first sub-range is selected as train years and the later one as the test years. Then, we prepare the classification dataset, by choosing those author pairs, that appeared in the train years, but did not publish any papers together in those years. Each such pair either represent a positive example or a negative example, depending on whether those author pairs published at least one paper in the test years or not. Coauthoring a paper in test years by a pair of authors, establishes a link between them, which was not there in the train years. Classification model of link prediction problem needs to predict this link by successfully distinguishing the positive classes from the dataset. Thus, link prediction problem can be posed an a binary classification problem, that can be solved by employing effective features in a supervised learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_sparse( df, shape = ( len( authors_index ), len( authors_index ) ), dtype = np.bool ) :\n",
    "    return sp.coo_matrix( (\n",
    "            np.ones( 2 * len( df ), dtype = dtype ), (\n",
    "                np.concatenate( ( df[ \"author1\" ].values, df[ \"author2\" ].values ) ),\n",
    "                np.concatenate( ( df[ \"author2\" ].values, df[ \"author1\" ].values ) ) )\n",
    "        ), shape = shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A handy procedure to quickly find which elements of one array are in another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match( a, b ) :\n",
    "## Get insertion indices\n",
    "    indices = np.searchsorted( a, b )\n",
    "## Truncate the indices by the length of a\n",
    "    mask = indices < len( a )\n",
    "    result = np.zeros( len( b ), dtype = np.bool )\n",
    "    result[ mask ] = a[ indices[ mask ] ] == b[ mask ]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to predict the edges in $G_1 = G[2011,2014]$ using the data in $G_0 = G[2000, 2010]$. The trainig sample is $\\bigl( (u,v), t_{uv} \\bigr)_{u,v\\in G_0}$ where $t_{uv}$ indicates whether the edge $\\langle u,v \\rangle$ is in $G_1$ but not in $G_0$.\n",
    "\n",
    "It is like predicting $y_{t+1}$ given $y_t$ in time-series, but instead of series one has graphs. And we want to predict **new** edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Compactify the perdictors' data\n",
    "le = MultiColumnLabelEncoder( [ 'author1', 'author2', ] )\n",
    "dblp_X = le.fit_transform( dblp_X )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ditch all yet unseen vertices from the graph with edges to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Consider the coauthorship that has taken place since 2010 \n",
    "dblp_y = dblp[ dblp.year >= 2011 ]\n",
    "\n",
    "## Keep all paris with both vertices found among the predictors\n",
    "dblp_y_classes_ = np.unique( np.concatenate( ( dblp_y[ \"author1\" ].values, dblp_y[ \"author2\" ].values ) ) )\n",
    "common_classes_ = np.intersect1d( le.classes_, dblp_y_classes_ )\n",
    "dblp_y_mask = match( common_classes_, dblp_y[ \"author1\" ].values ) & match( common_classes_, dblp_y[ \"author2\" ].values )\n",
    "\n",
    "## Re-label the target data\n",
    "dblp_y = le.transform( dblp_y[ dblp_y_mask ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split into sub-samples: lil matrices support advanced indexing\n",
    "Adj_train, Adj_test = to_sparse( dblp_X ).tocsr( ), to_sparse( dblp_y ).tolil( )\n",
    "## Remove edges present in the predictor graph\n",
    "nnz_index = Adj_train.nonzero( )\n",
    "## Remove teh edges and \n",
    "Adj_test[ nnz_index[ 0 ], nnz_index[ 1 ] ] = False\n",
    "Adj_test = Adj_test.tocsr( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behold in awe the sheer magnitude of the dataset!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1259124x1259124 sparse matrix of type '<type 'numpy.bool_'>'\n",
       " \twith 6518435 stored elements in Compressed Sparse Row format>,\n",
       " <1259124x1259124 sparse matrix of type '<type 'numpy.bool_'>'\n",
       " \twith 1500844 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Adj_train, Adj_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_degree = Adj_train.sum( axis = 1 ).getA1( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any( train_degree == 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PageRank_iter( A, beta = 0.85, x0 = None, rel_eps = 1.0E-8, niter = 10000 ) :\n",
    "## Create a teleporation vector\n",
    "    E = np.full( A.shape[ 0 ], 1.0 / A.shape[ 0 ], np.float )\n",
    "## If the initial ranking is not provided us the uniform distribution over the nodes\n",
    "    x0 = np.copy( E ) if x0 is None else x0\n",
    "## Find the normalising constants for each row\n",
    "    out = np.array( A.sum( axis = 1 ), np.int ).flatten( )\n",
    "## Locate the dangling vertices\n",
    "    dan = np.array( out == 0 )\n",
    "##  ... and reset their normalising constant to avoid NANs\n",
    "    out[ dan ] = 1.0\n",
    "## The resulting status of the convergence procedure:\n",
    "##  0 -- convergence within the set relative tolerance\n",
    "##  1 -- exceeded the number of iterations.\n",
    "    status = 1 ; i = 0\n",
    "## First stopping rule: within the specified number of iterations\n",
    "    while i < niter :\n",
    "## The main computational step\n",
    "        x1 = beta * ( x0 / out ) * A + ( beta * np.sum( x0 * dan ) + 1 - beta ) * E\n",
    "## Second stopping rule: within the required tolerance. Correction for \n",
    "##  possible machine zeros in the denominator.\n",
    "        if np.sum( np.abs( x1 - x0 ) / ( np.abs( x0 ) + rel_eps ) ) < rel_eps :\n",
    "            status = 0\n",
    "            break\n",
    "## Proceed to the next iteration\n",
    "        x0 = x1 ; i += 1\n",
    "## return the stationary distribution and the convergence information\n",
    "    return ( x1, { 'convergence': status, 'iterations' : i } )\n",
    "## Some small test cases\n",
    "# T = spma.csc_matrix( [ [ 0,1,1,0], [0,0,1,0], [1,0,0,1], [0,0,0,0] ] )\n",
    "# T = spma.csc_matrix( [ [ 0,1,1,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1], [1,0,0,0,0] ] )\n",
    "# print PageRank_iter( T, .9, rel_eps = 1e-10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct topological features:\n",
    "* vertex's pagerank score;\n",
    "* degree;\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_degree = Adj_train.sum( axis = 1 ).getA1( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_common_neighbours = Adj_train.dot( Adj_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_common_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_prank = PageRank_iter( Adj_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.set_yscale( 'log' ) ; ax.set_xscale( 'log' )\n",
    "d,f = np.unique( degree, return_counts = True )\n",
    "ax.plot( d, f, \"b.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the [flickr dataset](https://www.dropbox.com/s/srsib3hq863drtp/flickr_data.tar.gz?dl=0) (warning, raw data!). <br/>\n",
    "File ''*users.txt*'' provides a table of form *userID*, *enterTimeStamp*, *additionalInfo*... <br/>\n",
    "File \"*contacts.txt*\" consists of pairs of *userID*'s and link establishment timestamp <br/>\n",
    "\n",
    "Recall *scoring functions* for link prediction. Your task is to compare the performance of each scoring function as follows:\n",
    "1. TOP-$n$ accuracy\n",
    "    * Denote the number of links $E_\\text{new}$ appeared during testing period as $n$\n",
    "    * Denote the ranked list of node pairs provided by score $s$ as $\\hat{E}_s$\n",
    "    * Take top-$n$ pairs from $\\hat{E}_s$ and intersect it with $E_\\text{new}$. Performance is measured as the size of resulted set\n",
    "2. ROC and AUC ('star' subtask)\n",
    "\n",
    "Essentially, for this task you also have to follow the guideline points $1$ and $2$ above. The only thing you have to keep in mind is that flickr dataset is growing dataset. Since then, consider nodes that are significantly represented both in training and testing intervals (for instance, have at least $5$ adjacent edges in training and testing intervals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
