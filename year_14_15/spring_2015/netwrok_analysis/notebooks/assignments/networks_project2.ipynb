{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Structural Analysis and Visualization of Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center/>Course Project #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Student: *Nazarov Ivan*</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <hr /> General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hard Deadline:** 21.06.2015 23:59 <br \\>\n",
    "\n",
    "Please send your reports to <mailto:leonid.e.zhukov@gmail.com> and <mailto:shestakoffandrey@gmail.com> with message subject of the following structure:<br \\> **[HSE Networks 2015] *Nazarov* *Ivan* Project*2***\n",
    "\n",
    "Support your computations with figures and comments. <br \\>\n",
    "If you are using IPython Notebook you may use this file as a starting point of your report.<br \\>\n",
    "<br \\>\n",
    "<hr \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with the [DBLP dataset](https://www.dropbox.com/s/ft4ekv2f3r43u7b/dblp_2000.csv.gz?dl=0) (warning, raw data!). It contains coauthorships that were revealed during $2000$-$2014$. Particularly, the file contains $3$ colomns: first two for authors' names and the third for the year of publication. This data can be naturally mapped to undirected graph structure.\n",
    "\n",
    "Your task is construct supervised link prediction scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Use *pandas* module to load and manipulate the dataset in Python\n",
    "1. Initiallize your classification set as follows:\n",
    "    * Determine training and testing intervals on your time domain (for instance, in DBLP dataset take a period $2000$-$2010$ as training period and $2011$-$2014$ as testing period)\n",
    "    * Pick pairs of authors that **have appeared during training interval** but **have not published together** during it\n",
    "    * These pairs form **positive** or **negative** examples depending on whether they have formed coauthorships **during the testing interval**\n",
    "    * You have arrived to binary classification problem. PROFIT!\n",
    "2. Construct feature space:\n",
    "    * Most of our features tend to be topological. Examples of the features can be: (weighted) sum of neigbours, shortest distance, etc\n",
    "3. Choose at least $4$ classification algorithms from [scikit module](http://scikit-learn.org/stable/) (goes with Anaconda) and compare them in terms of Accuracy, Precision, Recall, F-Score (for positive class) and Mean Squared Error. Use k-fold cross-validation and average your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, scipy.sparse as sp\n",
    "import os, gc, regex as re, time as tm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DATADIR = os.path.realpath( os.path.join( \".\", \"data\", \"proj02\" ) )\n",
    "\n",
    "raw_dblp_file = os.path.join( DATADIR, \"dblp_2000.csv.gz\" )\n",
    "cached_dblp_file = os.path.join( DATADIR, \"dblp_2000.ppdf\" )\n",
    "cached_author_index = os.path.join( DATADIR, \"dblp_2000_authors.txt\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Return a mask of elements of b found in a: optimal for numeric arrays\n",
    "def match( a, v, return_indices = False ) :\n",
    "\tindex = np.argsort( a )\n",
    "## Get insertion indices\n",
    "\tsorted_index = np.searchsorted( a, v, sorter = index )\n",
    "## Truncate the indices by the length of a\n",
    "\tindex = np.take( index, sorted_index, mode = \"clip\" )\n",
    "\tmask = a[ index ] == v\n",
    "## return\n",
    "\tif return_indices :\n",
    "\t\treturn mask, index[ mask ]\n",
    "\treturn mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A handy procedure for converting an $(v_{ij})$ list into a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert the edgelist into sparse matrix\n",
    "def to_sparse_coo( u, v, shape, dtype = np.int32 ) :\n",
    "## Create a COOrdinate sparse matrix from the given ij-indices\n",
    "\tassert( len( u ) == len( v ) )\n",
    "\treturn sp.coo_matrix( (\n",
    "\t\t\tnp.ones( len( u ) + len( v ), dtype = dtype ), (\n",
    "\t\t\t\tnp.concatenate( ( u, v ) ), np.concatenate( ( v, u ) ) )\n",
    "\t\t), shape = shape )\n",
    "\n",
    "## Remeber: when converting COO to CSR/CSC the duplicate coordinate\n",
    "##  entries are summed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the DBLP dataset, making a cached copy if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBLP loaded in 15.734 sec.\n"
     ]
    }
   ],
   "source": [
    "## Create cache if necessary\n",
    "tick = tm.time( )\n",
    "if not os.path.exists( cached_dblp_file ) :\n",
    "\t## Load the csv file into a dataframe\n",
    "\tdblp = pd.read_csv( raw_dblp_file, # nrows = 10000,\n",
    "\t## On-the-fly decompression\n",
    "\t\t\tcompression = \"gzip\", header = None, quoting = 0,\n",
    "\t## Assign column headers\n",
    "\t\t\tnames = [ 'author1', 'author2', 'year', ], encoding = \"utf-8\" )\n",
    "\t## Finish\n",
    "\ttock = tm.time( )\n",
    "\tprint \"Raw DBLP read in %.3f sec.\" % ( tock - tick, )\n",
    "## Map author names to ids\n",
    "\tfrom sklearn.preprocessing import LabelEncoder\n",
    "\tle = LabelEncoder( ).fit( np.concatenate( (\n",
    "\t\tdblp[\"author1\"].values, dblp[\"author2\"].values, ) ) )\n",
    "\tdblp_author_index = le.classes_\n",
    "\tfor col in [ 'author1', 'author2', ] :\n",
    "\t\tdblp[ col ] = le.transform( dblp[ col ] )\n",
    "## Cache\n",
    "\tdblp.to_pickle( cached_dblp_file )\n",
    "\twith open( cached_author_index, \"w\" ) as out :\n",
    "\t\tfor label in le.classes_ :\n",
    "\t\t\tout.write( label.strip( ).encode( \"utf-8\" ) + \"\\n\" )\n",
    "\tdel dblp, le\n",
    "## Finish\n",
    "\ttick = tm.time( )\n",
    "\tprint \"Preprocessing took %.3f sec.\" % ( tick - tock, )\n",
    "else :\n",
    "## Load the database from pickled format\n",
    "\tdblp = pd.read_pickle( cached_dblp_file )\n",
    "## Read the dictionary of authors\n",
    "\twith open( cached_author_index, \"r\" ) as author_index :\n",
    "\t\tdblp_author_index = [ line.decode( \"utf-8\" ) for line in author_index ]\n",
    "## Report\n",
    "tock = tm.time( )\n",
    "print \"DBLP loaded in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the DBLP dataset in two periods: pre and post 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First preprocess the pre 2010 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre2010 = dblp[ dblp.year <= 2010 ].copy( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reencode the vertices of the pre-2010 graph in a less wasteful format. Use sklearn's LabelEncoder() to this end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder( ).fit( np.concatenate( ( pre2010[ \"author1\" ].values, pre2010[ \"author2\" ].values ) ) ) \n",
    "pre2010_values = le.classes_\n",
    "\n",
    "## Recode the edge data\n",
    "for col in [ 'author1', 'author2', ] :\n",
    "    pre2010[ col ] = le.transform( pre2010[ col ] )      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the edge list data into a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre2010_adj = to_sparse_coo(\n",
    "    pre2010[ \"author1\" ].values, pre2010[ \"author2\" ].values,\n",
    "    shape = 2 * [ len( le.classes_ ) ] )\n",
    "\n",
    "## Eliminate duplicates by converting them into ones\n",
    "pre2010_adj = pre2010_adj.tocsr( )\n",
    "pre2010_adj.data = np.ones_like( pre2010_adj.data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the vertices of the pre 2010 period that are in post-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post2010 = dblp[ dblp.year > 2010 ]\n",
    "common_vertices = np.intersect1d( pre2010_values,\n",
    "\tnp.union1d( post2010[ \"author1\" ].values, post2010[ \"author2\" ].values ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove completely new vertices from post 2010 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post2010 = post2010[ (\n",
    "    match( common_vertices, post2010[ \"author1\" ].values ) &\n",
    "    match( common_vertices, post2010[ \"author2\" ].values ) ) ]\n",
    "del common_vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the post 2010 vertices to pre 2010 vertices and construct the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in [ 'author1', 'author2', ] :\n",
    "    post2010[ col ] = le.transform( post2010[ col ] )\n",
    "\n",
    "## The adjacency matrix\n",
    "post2010_adj = sp.coo_matrix( ( np.ones( post2010.shape[0], dtype = np.bool ),\n",
    "        ( post2010[ \"author1\" ].values, post2010[ \"author2\" ].values )\n",
    "    ), shape = pre2010_adj.shape ).tolil( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave only those edges in the post 2010 dataset, which had not existed during 2000-2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post2010_adj[ pre2010_adj.nonzero( ) ] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eleminate duplicate edges and transform into a CSR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post2010_adj = post2010_adj.tocsr( )\n",
    "post2010_adj.data = np.ones_like( post2010_adj.data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two aligned symmetric adjacency matrices : for edges exisited before 2010 and new edges formed after 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<873881x873881 sparse matrix of type '<type 'numpy.bool_'>'\n",
      "\twith 805815 stored elements in Compressed Sparse Row format>\n",
      "<873881x873881 sparse matrix of type '<type 'numpy.int32'>'\n",
      "\twith 6518435 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "print post2010_adj.__repr__()\n",
    "print pre2010_adj.__repr__( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All edes of the post2010 graph are included and considered to be positive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive = np.append( *( c.reshape((-1, 1)) for c in post2010_adj.nonzero( ) ), axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a slightly harder part is to generate an adequate number of negative examples, so that the final training sample would be balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generate a sample of vertex pairs with no edge in both periods\n",
    "negative = np.random.choice( pre2010_adj.shape[ 0 ], size = ( 2 * positive.shape[0], positive.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compie the final training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E = np.vstack( ( positive, negative ) )\n",
    "y = np.vstack( ( np.ones( ( positive.shape[ 0 ], 1 ), dtype = np.float ),\n",
    "                np.zeros( ( negative.shape[ 0 ], 1 ), dtype = np.float ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, finally, we got ourselves a trainig set of edges with 2:1 negative-to-postive ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pair of features is the degrees on the edge endpoints: for $(i,j) \\in V\\times V$\n",
    "$$ \\phi^1_{ij} = |N_i|\\,\\text{ and }\\,\\phi^2_{ij} = |N_j|\\,,$$\n",
    "where $N_v$ is the set of adjacent vertices of a node $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phi_degree( edges, A ) :\n",
    "\tdeg = A.sum( axis = 1 ).astype( np.float )\n",
    "\treturn np.append( deg[ edges[ :, 0 ] ], deg[ edges[ :, 1 ] ], axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that at least two edge features can be constructed via a so called \"sandwich\" matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __sparse_sandwich( edges, A, W = None ) :\n",
    "    AA = A.dot( A.T ) if W is None else A.dot( W ).dot( A.T )\n",
    "    result = AA[ edges[:,0], edges[:,1] ]\n",
    "    del AA ; gc.collect( 0 ) ; gc.collect( 1 ) ; gc.collect( 2 )\n",
    "    return result.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next geature is the Adamic/adar score: for $(i,j)\\in V\\times V$\n",
    "$$ \\phi^3_{ij} = \\sum_{v\\in N_i \\cap N_j } \\frac{1}{\\log |N_v|}\\,.$$\n",
    "\n",
    "Another feature is the numberod neighbours shared by the endpoints:\n",
    "$$\\phi^4_{ij} = |N_i\\cap N_j|\\,.$$\n",
    "\n",
    "In fact both features are special cases of the same formula :\n",
    "$$ (\\phi_{ij}) = A W A'\\,, $$\n",
    "where $W$ is the weight matrix. In the case of common neigbours it is the unit matrix $I$, whereas for Adamic/Adar it is the diagonal matrix of reciprocal of degree logarithms :\n",
    "$$ W = \\text{diag}\\Bigl( \\frac{1}{\\log |N_i|} \\Bigr)_{i\\in V}\\,.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phi_adamic_adar( edges, A ) :\n",
    "    inv_log_deg = 1.0 / np.log( A.sum( axis = 1 ).getA1( ) )\n",
    "    inv_log_deg[ np.isinf( inv_log_deg ) ] = 0\n",
    "    result = __sparse_sandwich( edges, A, sp.diags( inv_log_deg, 0 ) )\n",
    "    del inv_log_deg ; gc.collect( 0 ) ; gc.collect( 1 ) ; gc.collect( 2 )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phi_common_neighbours( edges, A ) :\n",
    "    return __sparse_sandwich( edges, A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertex degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex degree computed in 0.561 sec.\n"
     ]
    }
   ],
   "source": [
    "tick = tm.time()\n",
    "phi_12 = phi_degree( E, pre2010_adj )\n",
    "tock = tm.time()\n",
    "print \"Vertex degree computed in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adamic/Adar metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adamic/adar computed in 169.297 sec.\n"
     ]
    }
   ],
   "source": [
    "tick = tm.time()\n",
    "phi_3 = phi_adamic_adar( E, pre2010_adj )\n",
    "tock = tm.time()\n",
    "print \"Adamic/adar computed in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common neighbours computed in 60.207 sec.\n"
     ]
    }
   ],
   "source": [
    "tick = tm.time()\n",
    "phi_4 = phi_common_neighbours( E, pre2010_adj )\n",
    "tock = tm.time()\n",
    "print \"Common neighbours computed in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute all-pairs shortest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tick = tm.time()\n",
    "# phi_5 = phi_shortest_paths( E, pre2010_adj )\n",
    "# tock = tm.time()\n",
    "# print \"Shortest paths computed in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all features into a numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.hstack( ( phi_12, phi_3, phi_4 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having computed all the features, lets make a subsample so that the classfification would run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_modelling, X_main, y_modelling, y_main = train_test_split( X, y.ravel( ), train_size = 0.20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach SciKit's grid search and x-validation modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to analyze many classifiers at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = list( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression for binary classification solves the following problem on the training dataset $(x_i,t_i)_{i=1}^n \\in \\mathbb{R}^{1+p}\\times \\{0,1\\}$:\n",
    "$$ \\sum_{i=1}^n t_i \\log \\sigma( \\beta'x_i ) + (1-t_i) \\log \\bigl( 1-\\sigma( \\beta'x_i ) \\bigr) \\to \\min_{\\beta, \\beta_0} \\,, $$\n",
    "where $\\sigma(z) = \\bigr(1+e^{-z}\\bigl)^{-1}$. The classification is done using the folowing rule :  \n",
    "$$ \\hat{t}(x) = \\mathop{\\text{argmax}}_{k=1,2}\\, \\mathbb{P}(T=k|X=x)\\,, $$\n",
    "where $\\mathbb{P}(T=1|X=x) = \\sigma(\\beta'x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  50 | elapsed:  3.0min remaining:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  3.1min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_grid = GridSearchCV( LogisticRegression( ), cv = 10, verbose = 1,\n",
    "        param_grid = { \"C\" : np.logspace( -2, 2, num = 5 ) }, n_jobs = -1\n",
    "    ).fit( X_modelling, y_modelling )\n",
    "\n",
    "classifiers.append( ( \"Logistic\", LR_grid.best_estimator_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear and Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a widely known fact that sometimes simple models beat more complicated ones in terms of their accuracy. Thus let's consdier LDA and QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "\n",
    "classifiers.append( ( \"LDA\", LDA( ) ) )\n",
    "classifiers.append( ( \"QDA\", QDA( ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's employ the classification tree model. On its own a decision tree is a volatile classifier, meaning that the addition of new data can drammatically alter its structure, that is why let's utilize boosted trees and randomized forests. These methods learn the intirinsic nonlinear features of the data by iterative construction of weak classifiers focusing on different aspects of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 19.9min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_grid = GridSearchCV( RandomForestClassifier( n_estimators = 50 ), cv = 10, verbose = 1,\n",
    "        param_grid = { \"max_depth\" : [ 3, 5, 15, 30 ] }, n_jobs = -1\n",
    "    ).fit( X_modelling, y_modelling )\n",
    "\n",
    "classifiers.append( ( \"RandomForest\", RF_grid.best_estimator_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosted tree (AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "classifiers.append( ( \"AdaBoost\", AdaBoostClassifier( n_estimators = 50 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One does not expect a simple tree to do comparably well against ensemble classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  40 | elapsed:  1.0min remaining:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  1.0min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_grid = GridSearchCV( DecisionTreeClassifier( criterion = \"gini\" ), cv = 10, verbose = 1,\n",
    "        param_grid = { \"max_depth\" : [ 3, 5, 15, 30 ] }, n_jobs = -1\n",
    "    ).fit( X_modelling, y_modelling )\n",
    "\n",
    "classifiers.append( ( \"Tree\", tree_grid.best_estimator_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Nearest Negihbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quite engineering approach to classification is to follow a simple rule : if the majority of a points $l$ nearest neighbours correspond to a class $c$ then this point is very likely to come from calss $c$ as well. Know them by their freinds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  40 | elapsed: 42.6min remaining:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 44.3min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_grid = GridSearchCV( KNeighborsClassifier(  ), cv = 10, verbose = 1,\n",
    "        param_grid = { \"n_neighbors\" : [ 3, 5, 15, 30 ] }, n_jobs = -1\n",
    "    ).fit( X_modelling, y_modelling )\n",
    "\n",
    "classifiers.append( ( \"k-NN\", knn_grid.best_estimator_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X_main, y_main, train_size = 0.20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsample the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsample = np.random.permutation( X_train.shape[ 0 ] )#[ : 50000 ]\n",
    "X_train_subsample, y_train_subsample = X_train[ subsample ], y_train[ subsample ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:   29.1s remaining:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   34.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold crossvalidation for Logistic took 36.091 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:    7.3s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold crossvalidation for LDA took 10.003 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:    6.2s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold crossvalidation for QDA took 8.770 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:   42.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:  2.7min remaining:   39.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold crossvalidation for RandomForest took 197.491 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:  2.4min remaining:   35.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold crossvalidation for AdaBoost took 177.608 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:   11.2s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   13.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold crossvalidation for Tree took 14.085 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:  7.3min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  8.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold crossvalidation for k-NN took 526.373 sec.\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "for name, clf in classifiers :\n",
    "    tick = tm.time( )\n",
    "    results[name] = cross_val_score( clf, X_train_subsample, y_train_subsample, n_jobs = -1, verbose = 1, cv = 10 )\n",
    "    tock = tm.time( )\n",
    "    print \"k-fold crossvalidation for %s took %.3f sec.\" % ( name, tock - tick, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold_frame = pd.DataFrame( results )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoost        0.834854\n",
       "LDA             0.751858\n",
       "Logistic        0.823119\n",
       "QDA             0.788113\n",
       "RandomForest    0.835156\n",
       "Tree            0.832196\n",
       "k-NN            0.832491\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k_fold_frame.append( k_fold_frame.apply( np.average ), ignore_index = True )\n",
    "k_fold_frame.apply( np.average )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the [flickr dataset](https://www.dropbox.com/s/srsib3hq863drtp/flickr_data.tar.gz?dl=0) (warning, raw data!). <br/>\n",
    "File ''*users.txt*'' provides a table of form *userID*, *enterTimeStamp*, *additionalInfo*... <br/>\n",
    "File \"*contacts.txt*\" consists of pairs of *userID*'s and link establishment timestamp <br/>\n",
    "\n",
    "Recall *scoring functions* for link prediction. Your task is to compare the performance of each scoring function as follows:\n",
    "1. TOP-$n$ accuracy\n",
    "    * Denote the number of links $E_\\text{new}$ appeared during testing period as $n$\n",
    "    * Denote the ranked list of node pairs provided by score $s$ as $\\hat{E}_s$\n",
    "    * Take top-$n$ pairs from $\\hat{E}_s$ and intersect it with $E_\\text{new}$. Performance is measured as the size of resulted set\n",
    "2. ROC and AUC ('star' subtask)\n",
    "\n",
    "Essentially, for this task you also have to follow the guideline points $1$ and $2$ above. The only thing you have to keep in mind is that flickr dataset is growing dataset. Since then, consider nodes that are significantly represented both in training and testing intervals (for instance, have at least $5$ adjacent edges in training and testing intervals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
