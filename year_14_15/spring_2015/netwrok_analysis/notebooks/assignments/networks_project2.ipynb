{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Structural Analysis and Visualization of Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center/>Course Project #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Student: *Nazarov Ivan*</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <hr /> General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hard Deadline:** 21.06.2015 23:59 <br \\>\n",
    "\n",
    "Please send your reports to <mailto:leonid.e.zhukov@gmail.com> and <mailto:shestakoffandrey@gmail.com> with message subject of the following structure:<br \\> **[HSE Networks 2015] *{LastName}* *{First Name}* Project*{Number}***\n",
    "\n",
    "Support your computations with figures and comments. <br \\>\n",
    "If you are using IPython Notebook you may use this file as a starting point of your report.<br \\>\n",
    "<br \\>\n",
    "<hr \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with the [DBLP dataset](https://www.dropbox.com/s/ft4ekv2f3r43u7b/dblp_2000.csv.gz?dl=0) (warning, raw data!). It contains coauthorships that were revealed during $2000$-$2014$. Particularly, the file contains $3$ colomns: first two for authors' names and the third for the year of publication. This data can be naturally mapped to undirected graph structure.\n",
    "\n",
    "Your task is construct supervised link prediction scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Use *pandas* module to load and manipulate the dataset in Python\n",
    "1. Initiallize your classification set as follows:\n",
    "    * Determine training and testing intervals on your time domain (for instance, in DBLP dataset take a period $2000$-$2010$ as training period and $2011$-$2014$ as testing period)\n",
    "    * Pick pairs of authors that **have appeared during training interval** but **have not published together** during it\n",
    "    * These pairs form **positive** or **negative** examples depending on whether they have formed coauthorships **during the testing interval**\n",
    "    * You have arrived to binary classification problem. PROFIT!\n",
    "2. Construct feature space:\n",
    "    * Most of our features tend to be topological. Examples of the features can be: (weighted) sum of neigbours, shortest distance, etc\n",
    "3. Choose at least $4$ classification algorithms from [scikit module](http://scikit-learn.org/stable/) (goes with Anaconda) and compare them in terms of Accuracy, Precision, Recall, F-Score (for positive class) and Mean Squared Error. Use k-fold cross-validation and average your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd, networkx as nx, numpy as np, scipy.sparse as sp\n",
    "import os, regex as re, time as tm\n",
    "\n",
    "DATADIR = os.path.realpath( os.path.join( \".\", \"data\", \"proj02\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pity there is no out-of-box solution for this in SKlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "class MultiColumnLabelEncoder :\n",
    "    def __init__( self, columns = None ):\n",
    "        self.columns = columns\n",
    "        self.__le = LabelEncoder(  )\n",
    "    def fit( self, X, y = None ) :\n",
    "        self.__columns = X.columns if self.columns is None else self.columns\n",
    "## Initialize the label encoder and make it assign labels to polled columns\n",
    "        self.__le.fit( pd.concat( X[ col ] for col in self.__columns ) )\n",
    "        self.classes_ = self.__le.classes_\n",
    "        return self\n",
    "    def transform( self, X, copy = True ) :\n",
    "## Copy the input dataframe and figure out what coluns to re-code\n",
    "        __output = X.copy( ) if copy else X\n",
    "## Iterate over the required columns\n",
    "        for col in self.__columns :\n",
    "            __output[ col ] = self.__le.transform( __output[ col ] )\n",
    "        return __output\n",
    "    def fit_transform( self, X, y = None ) :\n",
    "        return self.fit( X, y ).transform( X )\n",
    "    def inverse_transform( self, y ) :\n",
    "        return self.__le.inverse_transform( y )\n",
    "    def set_params( self, **params ) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the raw DBLP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists( os.path.join( DATADIR, \"dblp_dataframe.ppdf\" ) ) :\n",
    "## Start\n",
    "    tick = tm.time( )\n",
    "## Load the raw DBLP dataset\n",
    "    dblp_raw = pd.read_csv( os.path.join( DATADIR, \"dblp_2000.csv.gz\" ), # nrows = 100,\n",
    "## On-the-fly decompression\n",
    "                        compression = \"gzip\", header = None, quoting = 0,\n",
    "## Assign column headers\n",
    "                        names = [ 'author1', 'author2', 'year', ], encoding = \"utf-8\" )\n",
    "    tock = tm.time( )\n",
    "    print \"DBLP loaded in %.3f sec.\" % ( tock - tick, )\n",
    "## Pool the author columns together and let Pandas assign labels.\n",
    "    le = MultiColumnLabelEncoder( [ 'author1', 'author2', ] )\n",
    "    dblp = le.fit_transform( dblp_raw )\n",
    "    authors_index = le.classes_\n",
    "    del dblp_raw, le\n",
    "    tick = tm.time( )\n",
    "    print \"DBLP preprocessed in %.3f sec.\" % ( tick - tock, )\n",
    "## Cache\n",
    "    dblp.to_pickle( \"./data/proj02/dblp_dataframe.ppdf\" )\n",
    "    with open( \"./data/proj02/author_index.dic\", \"wb\" ) as out :\n",
    "        out.writelines( label + \"\\n\" for label in authors_index )\n",
    "## Report\n",
    "    tock = tm.time( )\n",
    "    print \"DBLP cached in %.3f sec.\" % ( tock - tick, )\n",
    "else :\n",
    "## Start\n",
    "    tick = tm.time( )\n",
    "## Load the database from pickled format\n",
    "    dblp = pd.read_pickle( \"./data/proj02/dblp_dataframe.ppdf\" )\n",
    "## Read the dictionary of authors\n",
    "    with open( \"./data/proj02/author_index.dic\", \"rb\" ) as out :\n",
    "        authors_index = out.readlines( )\n",
    "## Finish\n",
    "    tock = tm.time( )\n",
    "    print \"DBLP loaded in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It so happens that authors' names are quoted and their unicode letters escaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_author( txt ) :\n",
    "    return re.sub( r\"^\\s*\\\"(.*)\\\"\\s*$\", r\"\\1\", txt ).decode( \"unicode_escape\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data in two non-overlapping samples by year:\n",
    "* the coathorship data form 2000 till 2010 is the training data;\n",
    "* the collaboration since 2011 is the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dblp_train, dblp_test = dblp[ dblp.year <= 2010 ], dblp[ dblp.year >= 2011 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_coo( df, shape = ( len( authors_index ), len( authors_index ) ) ) :\n",
    "    return sp.coo_matrix( (\n",
    "            np.ones( 2 * len( df ), dtype = np.bool ), (\n",
    "                np.concatenate( ( df[ \"author1\" ].values, df[ \"author2\" ].values ) ),\n",
    "                np.concatenate( ( df[ \"author2\" ].values, df[ \"author1\" ].values ) ) )\n",
    "        ), shape = shape ).tocsr( )\n",
    "\n",
    "## Split into sub-samples\n",
    "Adj_train, Adj_test = make_coo( dblp_train ), make_coo( dblp_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct topological features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_degree = Adj_train.sum( axis = 1 ).getA1( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.set_yscale( 'log' ) ; ax.set_xscale( 'log' )\n",
    "d,f = np.unique( degree, return_counts = True )\n",
    "ax.plot( d, f, \"b.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the [flickr dataset](https://www.dropbox.com/s/srsib3hq863drtp/flickr_data.tar.gz?dl=0) (warning, raw data!). <br/>\n",
    "File ''*users.txt*'' provides a table of form *userID*, *enterTimeStamp*, *additionalInfo*... <br/>\n",
    "File \"*contacts.txt*\" consists of pairs of *userID*'s and link establishment timestamp <br/>\n",
    "\n",
    "Recall *scoring functions* for link prediction. Your task is to compare the performance of each scoring function as follows:\n",
    "1. TOP-$n$ accuracy\n",
    "    * Denote the number of links $E_\\text{new}$ appeared during testing period as $n$\n",
    "    * Denote the ranked list of node pairs provided by score $s$ as $\\hat{E}_s$\n",
    "    * Take top-$n$ pairs from $\\hat{E}_s$ and intersect it with $E_\\text{new}$. Performance is measured as the size of resulted set\n",
    "2. ROC and AUC ('star' subtask)\n",
    "\n",
    "Essentially, for this task you also have to follow the guideline points $1$ and $2$ above. The only thing you have to keep in mind is that flickr dataset is growing dataset. Since then, consider nodes that are significantly represented both in training and testing intervals (for instance, have at least $5$ adjacent edges in training and testing intervals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
