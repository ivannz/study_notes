{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Structural Analysis and Visualization of Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center/>Course Project #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Student: *Nazarov Ivan*</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <hr /> General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hard Deadline:** 21.06.2015 23:59 <br \\>\n",
    "\n",
    "Please send your reports to <mailto:leonid.e.zhukov@gmail.com> and <mailto:shestakoffandrey@gmail.com> with message subject of the following structure:<br \\> **[HSE Networks 2015] *Nazarov* *Ivan* Project*2***\n",
    "\n",
    "Support your computations with figures and comments. <br \\>\n",
    "If you are using IPython Notebook you may use this file as a starting point of your report.<br \\>\n",
    "<br \\>\n",
    "<hr \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with the [DBLP dataset](https://www.dropbox.com/s/ft4ekv2f3r43u7b/dblp_2000.csv.gz?dl=0) (warning, raw data!). It contains coauthorships that were revealed during $2000$-$2014$. Particularly, the file contains $3$ colomns: first two for authors' names and the third for the year of publication. This data can be naturally mapped to undirected graph structure.\n",
    "\n",
    "Your task is construct supervised link prediction scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Use *pandas* module to load and manipulate the dataset in Python\n",
    "1. Initiallize your classification set as follows:\n",
    "    * Determine training and testing intervals on your time domain (for instance, in DBLP dataset take a period $2000$-$2010$ as training period and $2011$-$2014$ as testing period)\n",
    "    * Pick pairs of authors that **have appeared during training interval** but **have not published together** during it\n",
    "    * These pairs form **positive** or **negative** examples depending on whether they have formed coauthorships **during the testing interval**\n",
    "    * You have arrived to binary classification problem. PROFIT!\n",
    "2. Construct feature space:\n",
    "    * Most of our features tend to be topological. Examples of the features can be: (weighted) sum of neigbours, shortest distance, etc\n",
    "3. Choose at least $4$ classification algorithms from [scikit module](http://scikit-learn.org/stable/) (goes with Anaconda) and compare them in terms of Accuracy, Precision, Recall, F-Score (for positive class) and Mean Squared Error. Use k-fold cross-validation and average your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, scipy.sparse as sp\n",
    "import os, gc, regex as re, time as tm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DATADIR = os.path.realpath( os.path.join( \".\", \"data\", \"proj02\" ) )\n",
    "\n",
    "raw_dblp_file = os.path.join( DATADIR, \"dblp_2000.csv.gz\" )\n",
    "cached_dblp_file = os.path.join( DATADIR, \"dblp_2000.ppdf\" )\n",
    "cached_author_index = os.path.join( DATADIR, \"dblp_2000_authors.txt\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Return a mask of elements of b found in a: optimal for numeric arrays\n",
    "def match( a, v, return_indices = False ) :\n",
    "\tindex = np.argsort( a )\n",
    "## Get insertion indices\n",
    "\tsorted_index = np.searchsorted( a, v, sorter = index )\n",
    "## Truncate the indices by the length of a\n",
    "\tindex = np.take( index, sorted_index, mode = \"clip\" )\n",
    "\tmask = a[ index ] == v\n",
    "## return\n",
    "\tif return_indices :\n",
    "\t\treturn mask, index[ mask ]\n",
    "\treturn mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A handy procedure for converting an $(v_{ij})$ list into a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert the edgelist into sparse matrix\n",
    "def to_sparse_coo( u, v, shape, dtype = np.int32 ) :\n",
    "## Create a COOrdinate sparse matrix from the given ij-indices\n",
    "\tassert( len( u ) == len( v ) )\n",
    "\treturn sp.coo_matrix( (\n",
    "\t\t\tnp.ones( len( u ) + len( v ), dtype = dtype ), (\n",
    "\t\t\t\tnp.concatenate( ( u, v ) ), np.concatenate( ( v, u ) ) )\n",
    "\t\t), shape = shape )\n",
    "\n",
    "## Remeber: when converting COO to CSR/CSC the duplicate coordinate\n",
    "##  entries are summed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the DBLP dataset, making a cached copy if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create cache if necessary\n",
    "tick = tm.time( )\n",
    "if not os.path.exists( cached_dblp_file ) :\n",
    "\t## Load the csv file into a dataframe\n",
    "\tdblp = pd.read_csv( raw_dblp_file, # nrows = 10000,\n",
    "\t## On-the-fly decompression\n",
    "\t\t\tcompression = \"gzip\", header = None, quoting = 0,\n",
    "\t## Assign column headers\n",
    "\t\t\tnames = [ 'author1', 'author2', 'year', ], encoding = \"utf-8\" )\n",
    "\t## Finish\n",
    "\ttock = tm.time( )\n",
    "\tprint \"Raw DBLP read in %.3f sec.\" % ( tock - tick, )\n",
    "## Map author names to ids\n",
    "\tfrom sklearn.preprocessing import LabelEncoder\n",
    "\tle = LabelEncoder( ).fit( np.concatenate( (\n",
    "\t\tdblp[\"author1\"].values, dblp[\"author2\"].values, ) ) )\n",
    "\tdblp_author_index = le.classes_\n",
    "\tfor col in [ 'author1', 'author2', ] :\n",
    "\t\tdblp[ col ] = le.transform( dblp[ col ] )\n",
    "## Cache\n",
    "\tdblp.to_pickle( cached_dblp_file )\n",
    "\twith open( cached_author_index, \"w\" ) as out :\n",
    "\t\tfor label in le.classes_ :\n",
    "\t\t\tout.write( label.strip( ).encode( \"utf-8\" ) + \"\\n\" )\n",
    "\tdel dblp, le\n",
    "## Finish\n",
    "\ttick = tm.time( )\n",
    "\tprint \"Preprocessing took %.3f sec.\" % ( tick - tock, )\n",
    "else :\n",
    "## Load the database from pickled format\n",
    "\tdblp = pd.read_pickle( cached_dblp_file )\n",
    "## Read the dictionary of authors\n",
    "\twith open( cached_author_index, \"r\" ) as author_index :\n",
    "\t\tdblp_author_index = [ line.decode( \"utf-8\" ) for line in author_index ]\n",
    "## Report\n",
    "tock = tm.time( )\n",
    "print \"DBLP loaded in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the DBLP dataset in two periods: pre and post 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First preprocess the pre 2010 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre2010 = dblp[ dblp.year <= 2010 ].copy( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reencode the vertices of the pre-2010 graph in a less wasteful format. Use sklearn's LabelEncoder() to this end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder( ).fit( np.concatenate( ( pre2010[ \"author1\" ].values, pre2010[ \"author2\" ].values ) ) ) \n",
    "pre2010_values = le.classes_\n",
    "\n",
    "## Recode the edge data\n",
    "for col in [ 'author1', 'author2', ] :\n",
    "    pre2010[ col ] = le.transform( pre2010[ col ] )      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the edge list data into a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre2010_adj = to_sparse_coo(\n",
    "    pre2010[ \"author1\" ].values, pre2010[ \"author2\" ].values,\n",
    "    shape = 2 * [ len( le.classes_ ) ] )\n",
    "\n",
    "## Eliminate duplicates by converting them into ones\n",
    "pre2010_adj = pre2010_adj.tocsr( )\n",
    "pre2010_adj.data = np.ones_like( pre2010_adj.data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the vertices of the pre 2010 period that are in post-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post2010 = dblp[ dblp.year > 2010 ]\n",
    "common_vertices = np.intersect1d( pre2010_values,\n",
    "\tnp.union1d( post2010[ \"author1\" ].values, post2010[ \"author2\" ].values ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove completely new vertices from post 2010 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post2010 = post2010[ (\n",
    "    match( common_vertices, post2010[ \"author1\" ].values ) &\n",
    "    match( common_vertices, post2010[ \"author2\" ].values ) ) ]\n",
    "del common_vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the post 2010 vertices to pre 2010 vertices and construct the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in [ 'author1', 'author2', ] :\n",
    "    post2010[ col ] = le.transform( post2010[ col ] )\n",
    "\n",
    "## The adjacency matrix\n",
    "post2010_adj = sp.coo_matrix( ( np.ones( post2010.shape[0], dtype = np.bool ),\n",
    "        ( post2010[ \"author1\" ].values, post2010[ \"author2\" ].values )\n",
    "    ), shape = pre2010_adj.shape ).tolil( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave only those edges in the post 2010 dataset, which had not existed during 2000-2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post2010_adj[ pre2010_adj.nonzero( ) ] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminate duplicate edges and transform into a CSR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post2010_adj = post2010_adj.tocsr( )\n",
    "post2010_adj.data = np.ones_like( post2010_adj.data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two aligned symmetric adjacency matrices : for edges exisited before 2010 and new edges formed after 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print post2010_adj.__repr__()\n",
    "print pre2010_adj.__repr__( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All edes of the post2010 graph are included and considered to be positive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive = np.append( *( c.reshape((-1, 1)) for c in post2010_adj.nonzero( ) ), axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a slightly harder part is to generate an adequate number of negative examples, so that the final training sample would be balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generate a sample of vertex pairs with no edge in both periods\n",
    "negative = np.random.choice( pre2010_adj.shape[ 0 ], size = ( 2 * positive.shape[0], positive.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compie the final training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E = np.vstack( ( positive, negative ) )\n",
    "y = np.vstack( ( np.ones( ( positive.shape[ 0 ], 1 ), dtype = np.float ),\n",
    "                np.zeros( ( negative.shape[ 0 ], 1 ), dtype = np.float ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, finally, we got ourselves a trainig set of edges with 2:1 negative-to-postive ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pair of features is the degrees on the edge endpoints: for $(i,j) \\in V\\times V$\n",
    "$$ \\phi^1_{ij} = |N_i|\\,\\text{ and }\\,\\phi^2_{ij} = |N_j|\\,,$$\n",
    "where $N_v$ is the set of adjacent vertices of a node $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phi_degree( edges, A ) :\n",
    "\tdeg = A.sum( axis = 1 ).astype( np.float )\n",
    "\treturn np.append( deg[ edges[ :, 0 ] ], deg[ edges[ :, 1 ] ], axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that at least two edge features can be constructed via a so called \"sandwich\" matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __sparse_sandwich( edges, A, W = None ) :\n",
    "    AA = A.dot( A.T ) if W is None else A.dot( W ).dot( A.T )\n",
    "    result = AA[ edges[:,0], edges[:,1] ]\n",
    "    del AA ; gc.collect( 0 ) ; gc.collect( 1 ) ; gc.collect( 2 )\n",
    "    return result.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next feature is the Adamic/adar score: for $(i,j)\\in V\\times V$\n",
    "$$ \\phi^3_{ij} = \\sum_{v\\in N_i \\cap N_j } \\frac{1}{\\log |N_v|}\\,.$$\n",
    "\n",
    "Another feature is the numberod neighbours shared by the endpoints:\n",
    "$$\\phi^4_{ij} = |N_i\\cap N_j|\\,.$$\n",
    "\n",
    "In fact both features are special cases of the same formula :\n",
    "$$ (\\phi_{ij}) = A W A'\\,, $$\n",
    "where $W$ is the weight matrix. In the case of common neigbours it is the unit matrix $I$, whereas for Adamic/Adar it is the diagonal matrix of reciprocal of degree logarithms :\n",
    "$$ W = \\text{diag}\\Bigl( \\frac{1}{\\log |N_i|} \\Bigr)_{i\\in V}\\,.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phi_adamic_adar( edges, A ) :\n",
    "    inv_log_deg = 1.0 / np.log( A.sum( axis = 1 ).getA1( ) )\n",
    "    inv_log_deg[ np.isinf( inv_log_deg ) ] = 0\n",
    "    result = __sparse_sandwich( edges, A, sp.diags( inv_log_deg, 0 ) )\n",
    "    del inv_log_deg ; gc.collect( 0 ) ; gc.collect( 1 ) ; gc.collect( 2 )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phi_common_neighbours( edges, A ) :\n",
    "    return __sparse_sandwich( edges, A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another potential feature is the so-called personalized page rank. Basically it is the same page Rank score, but with the ability to teleport only to a single node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, the global Pagerank is the stationary distribution of the markov chain with this transition kernel:\n",
    "$$ M = \\beta P + (1-\\beta) \\frac{1}{|V|}\\mathbb{1} \\mathbb{1}'\\,, $$\n",
    "where $\\mathbb{1} = (1)_{v\\in V}$ and $ P = \\bigl(D_{uu}^{-1} A_{uv} + \\frac{1}{|V|} 1_{\\delta^+_v=0} \\bigr)_{u,v\\in V}$ -- the transition probability matrix $u\\leadsto v$, which removes the sink (dangling) nodes, by connecting them to  every other vertex. The normalizing matrix\n",
    "$$D=\\text{diag}\\bigl( \\delta^+_v + 1_{\\delta^+_v=0} \\bigr)_{v\\in V}\\,,$$\n",
    "where the out-degree $\\delta^+_v = \\sum_{j\\in V} A_{uj}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The personalised pagerank for some node $w\\in V$ is only slightly different: the random walk still is forced ot teleport away from a dangling vertex to any other, but the general teleportation probability is altered so that the walk restarts from node $w\\in V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $R\\in \\{0,1\\}^{1\\times V}$ be such that $R = e_w$. Then the personalized pagerank is the statrionary distribution of a random walk with this transition matrix:\n",
    "$$ M_w = \\beta P + (1-\\beta) \\frac{1}{\\|R\\|_0}\\mathbb{1} R'\\,, $$\n",
    "where $\\|R\\|_0$ denotes the number of nonzero elements in $R$.\n",
    "\n",
    "The stationary distribution is in fact the left-eigenvector of the transition kernel with eigenvalue $1$ : $ \\pi = \\pi M$, $\\pi\\in [0,1]^{1\\times V}$ and $\\pi\\mathbb{1} = 1$. It is computed using hte power iterations methods, which basically gets the eigenvector with dominating eigenvalue. In the case of aperiodic, irreducible stochastic matrices the Perro-Frobenius theorem states that such eigenvector exists and the eigenvalues of $M_w$ are within $[-1,1]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic iteration, with dangling vertex elimination, is\n",
    "$$ \\pi_1 = \\beta \\pi_0 D^{-1} A + \\bigl( \\beta \\pi_0 R \\frac{1}{|V|} + (1-\\beta) \\pi_0 \\mathbb{1} \\frac{1}{\\|R\\|_0} \\bigr) e_w'\\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __sparse_pagerank( A, beta = 0.85, one = None, niter = 1000, rel_eps = 1e-6, verbose = True ) :\n",
    "## Initialize the iterations\n",
    "\tone = one if one is not None else np.ones( ( 1, A.shape[ 0 ] ), dtype = np.float )\n",
    "\tone = sp.csr_matrix( one / one.sum( axis = 1 ) )\n",
    "## Get the out-degree\n",
    "\tout = np.asarray( A.sum( axis = 1 ).getA1( ), dtype = np.float )\n",
    "## Obtain the mask of dangling vertices\n",
    "\tdangling = np.where( out == 0.0 )[ 0 ]\n",
    "## Correct the out-degree for sink nodes\n",
    "\tout[ dangling ] = 1.0\n",
    "## Just one iteration: all dangling nodes add to the importance of all vertices.\n",
    "\tpi = np.full( ( one.shape[0], A.shape[0] ), 1.0 / A.shape[ 0 ], dtype = np.float )\n",
    "## If there are no dangling vertices then use simple iterations\n",
    "\tkiter, status = 0, -1\n",
    "## Make a stochastic matrix\n",
    "\tP = sp.diags( 1.0 / out, 0, dtype = np.float ).dot( A ).tocsc( )\n",
    "\twhile kiter < niter :\n",
    "## make a copy of hte current ranking estimates\n",
    "\t\tpi_last = pi.copy( )\n",
    "## Use sparse inplace operations for speed. Firstt the random walk part\n",
    "\t\tpi *= beta ; pi *= P\n",
    "## Now the teleportaiton ...\n",
    "\t\tpi += ( 1 - beta ) * one\n",
    "##  ... and dangling vertices part\n",
    "\t\tif len( dangling ) > 0 :\n",
    "\t\t\tpi += beta * one.multiply( np.sum( pi_last[ :, dangling ], axis = 1 ).reshape( ( -1, 1 ) ) )\n",
    "## Normalize\n",
    "\t\tpi /= np.sum( pi, axis = 1 )\n",
    "\t\tif np.sum( np.abs( pi - pi_last ) ) <= one.shape[0] * rel_eps * np.sum( np.abs( pi_last ) ) :\n",
    "\t\t\tstatus = 0\n",
    "\t\t\tbreak\n",
    "## Next iteration\n",
    "\t\tkiter += 1\n",
    "\t\tif kiter % 10 == 0 :\n",
    "\t\t\tprint kiter\n",
    "\treturn pi, status, kiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the feature extractors themselves: the global pagerank and the presonalized (rooted) pagerank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The global pagerank score\n",
    "def phi_gpr( edges, A, verbose = True ) :\n",
    "\tpi, s, k = __sparse_pagerank( A, one = None, verbose = verbose )\n",
    "\treturn np.concatenate( ( pi[ :, edges[ :, 0 ] ], pi[ :,edges[ :, 1 ] ] ), axis = 0 ).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phi_ppr( edges, A ) :\n",
    "\tresult = np.empty( edges.shape, dtype = np.float )\n",
    "\n",
    "    return __sparse_sandwich( edges, A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertex degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tick = tm.time()\n",
    "phi_12 = phi_degree( E, pre2010_adj )\n",
    "tock = tm.time()\n",
    "print \"Vertex degree computed in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adamic/Adar metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tick = tm.time()\n",
    "phi_3 = phi_adamic_adar( E, pre2010_adj )\n",
    "tock = tm.time()\n",
    "print \"Adamic/adar computed in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tick = tm.time()\n",
    "phi_4 = phi_common_neighbours( E, pre2010_adj )\n",
    "tock = tm.time()\n",
    "print \"Common neighbours computed in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tick = tm.time()\n",
    "phi_56 = phi_gpr( E, pre2010_adj, verbose = False )\n",
    "tock = tm.time()\n",
    "print \"Global pagerank computed in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rooted (personalized) pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tick = tm.time()\n",
    "phi_78 = phi_ppr( E, pre2010_adj, verbose = False )\n",
    "tock = tm.time()\n",
    "print \"Personalized pagerank computed in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute all-pairs shortest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tick = tm.time()\n",
    "# phi_5 = phi_shortest_paths( E, pre2010_adj )\n",
    "# tock = tm.time()\n",
    "# print \"Shortest paths computed in %.3f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all features into a numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.hstack( ( phi_12, phi_3, phi_4, phi_56, phi_78 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having computed all the features, lets make a subsample so that the classfification would run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_modelling, X_main, y_modelling, y_main = train_test_split( X, y.ravel( ), train_size = 0.20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach SciKit's grid search and x-validation modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to analyze many classifiers at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = list( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression for binary classification solves the following problem on the training dataset $(x_i,t_i)_{i=1}^n \\in \\mathbb{R}^{1+p}\\times \\{0,1\\}$:\n",
    "$$ \\sum_{i=1}^n t_i \\log \\sigma( \\beta'x_i ) + (1-t_i) \\log \\bigl( 1-\\sigma( \\beta'x_i ) \\bigr) \\to \\min_{\\beta, \\beta_0} \\,, $$\n",
    "where $\\sigma(z) = \\bigr(1+e^{-z}\\bigl)^{-1}$. The classification is done using the folowing rule :  \n",
    "$$ \\hat{t}(x) = \\mathop{\\text{argmax}}_{k=1,2}\\, \\mathbb{P}(T=k|X=x)\\,, $$\n",
    "where $\\mathbb{P}(T=1|X=x) = \\sigma(\\beta'x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_grid = GridSearchCV( LogisticRegression( ), cv = 10, verbose = 1,\n",
    "        param_grid = { \"C\" : np.logspace( -2, 2, num = 5 ) }, n_jobs = -1\n",
    "    ).fit( X_modelling, y_modelling )\n",
    "\n",
    "classifiers.append( ( \"Logistic\", LR_grid.best_estimator_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear and Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a widely known fact that sometimes simple models beat more complicated ones in terms of their accuracy. Thus let's consdier LDA and QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "\n",
    "classifiers.append( ( \"LDA\", LDA( ) ) )\n",
    "classifiers.append( ( \"QDA\", QDA( ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's employ the classification tree model. On its own a decision tree is a volatile classifier, meaning that the addition of new data can drammatically alter its structure, that is why let's utilize boosted trees and randomized forests. These methods learn the intirinsic nonlinear features of the data by iterative construction of weak classifiers focusing on different aspects of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_grid = GridSearchCV( RandomForestClassifier( n_estimators = 50 ), cv = 10, verbose = 1,\n",
    "        param_grid = { \"max_depth\" : [ 3, 5, 15, 30 ] }, n_jobs = -1\n",
    "    ).fit( X_modelling, y_modelling )\n",
    "\n",
    "classifiers.append( ( \"RandomForest\", RF_grid.best_estimator_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosted tree (AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "classifiers.append( ( \"AdaBoost\", AdaBoostClassifier( n_estimators = 50 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One does not expect a simple tree to do comparably well against ensemble classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_grid = GridSearchCV( DecisionTreeClassifier( criterion = \"gini\" ), cv = 10, verbose = 1,\n",
    "        param_grid = { \"max_depth\" : [ 3, 5, 15, 30 ] }, n_jobs = -1\n",
    "    ).fit( X_modelling, y_modelling )\n",
    "\n",
    "classifiers.append( ( \"Tree\", tree_grid.best_estimator_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Nearest Negihbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quite engineering approach to classification is to follow a simple rule : if the majority of a points $l$ nearest neighbours correspond to a class $c$ then this point is very likely to come from calss $c$ as well. Know them by their freinds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_grid = GridSearchCV( KNeighborsClassifier(  ), cv = 10, verbose = 1,\n",
    "        param_grid = { \"n_neighbors\" : [ 3, 5, 15, 30 ] }, n_jobs = -1\n",
    "    ).fit( X_modelling, y_modelling )\n",
    "\n",
    "classifiers.append( ( \"k-NN\", knn_grid.best_estimator_ ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X_main, y_main, train_size = 0.20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsample the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsample = np.random.permutation( X_train.shape[ 0 ] )#[ : 50000 ]\n",
    "X_train_subsample, y_train_subsample = X_train[ subsample ], y_train[ subsample ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for name, clf in classifiers :\n",
    "    tick = tm.time( )\n",
    "    results[name] = cross_val_score( clf, X_train_subsample, y_train_subsample, n_jobs = -1, verbose = 1, cv = 10 )\n",
    "    tock = tm.time( )\n",
    "    print \"k-fold crossvalidation for %s took %.3f sec.\" % ( name, tock - tick, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold_frame = pd.DataFrame( results )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k_fold_frame.append( k_fold_frame.apply( np.average ), ignore_index = True )\n",
    "k_fold_frame.apply( np.average )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the [flickr dataset](https://www.dropbox.com/s/srsib3hq863drtp/flickr_data.tar.gz?dl=0) (warning, raw data!). <br/>\n",
    "File ''*users.txt*'' provides a table of form *userID*, *enterTimeStamp*, *additionalInfo*... <br/>\n",
    "File \"*contacts.txt*\" consists of pairs of *userID*'s and link establishment timestamp <br/>\n",
    "\n",
    "Recall *scoring functions* for link prediction. Your task is to compare the performance of each scoring function as follows:\n",
    "1. TOP-$n$ accuracy\n",
    "    * Denote the number of links $E_\\text{new}$ appeared during testing period as $n$\n",
    "    * Denote the ranked list of node pairs provided by score $s$ as $\\hat{E}_s$\n",
    "    * Take top-$n$ pairs from $\\hat{E}_s$ and intersect it with $E_\\text{new}$. Performance is measured as the size of resulted set\n",
    "2. ROC and AUC ('star' subtask)\n",
    "\n",
    "Essentially, for this task you also have to follow the guideline points $1$ and $2$ above. The only thing you have to keep in mind is that flickr dataset is growing dataset. Since then, consider nodes that are significantly represented both in training and testing intervals (for instance, have at least $5$ adjacent edges in training and testing intervals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
