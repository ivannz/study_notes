{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Structural Analysis and Visualization of Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Home Assignment #4: Community Detection Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Student: *Nazarov Ivan*</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <hr /> General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due Date:** 19.03.2015 23:59 <br \\>\n",
    "**Late submission policy:** -0.2 points per day <br \\>\n",
    "\n",
    "\n",
    "Please send your reports to <mailto:leonid.e.zhukov@gmail.com> and <mailto:shestakoffandrey@gmail.com> with message subject of the following structure:<br \\> **[HSE Networks 2015] *Nazarov* *Ivan* HA*4***\n",
    "\n",
    "Support your computations with figures and comments. <br \\>\n",
    "If you are using IPython Notebook you may use this file as a starting point of your report.<br \\>\n",
    "<br \\>\n",
    "<hr \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toolbox includes Numpy for computation, NetworkX for graph manipulation and algorithms, matplotlib for visualization and Scipy.IO for processing matlab objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.io import loadmat\n",
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1* (For those who have not done that during the seminar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this seminar your are asked to implement simple community detection algorightm. It is called [Markov Cluster Algorithm](http://micans.org/mcl/) (MCL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markor Clustering Algorithm  \n",
    "**Input:** Transition matrix $T = D^{-1}A$  \n",
    "**Output:** Adjacency matrix $M^*$  \n",
    "1. Set $M = T$\n",
    "2. **repeat:**\n",
    "    3. *Expansion Step:* $M = M^p$ (usually $p=2$)\n",
    "    4. *Inflation Step:* Raise every entry of $M$ to the power $\\alpha$ (usualy $\\alpha=2$)\n",
    "    5. *Renormalize:* Normalize each row by its sum\n",
    "    6. *Prunning:* Replace entries that are close to $0$ by pure $0$\n",
    "7. **until** $M$ converges\n",
    "8. $M^* = M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result you should get a cluster matrix s.t. elements of the cluster correspont to nonzero elements of the columns of the matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run this method for network [1](https://www.dropbox.com/s/so0ly2ozh2pzxp6/network1.mat?dl=0), [2](https://www.dropbox.com/s/xcswyhoeehq95v2/network2.mat?dl=0) and [3](https://www.dropbox.com/s/cwshsfr2d8fn470/network3.mat?dl=0).\n",
    "* Play with the parameters ($p$, $\\alpha$, zero tolerance), analyse the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the specification above clearly states the order of operations within each iteration, the algorithm implemented below mpved \"pruning\" to before \"inflation\". First of all this uncovered more straightforward connection of the pruning parameter with transition probability. Secondly, as a side effect the final step within each iteration became \"renormalization\", which produces a row-stocastic matrix for the next iteration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mcl_iter( A, p = 2, alpha = 2, theta = 1e-8, rel_eps = 1e-4, niter = 10000 ) :\n",
    "## Convert A into a transition kernel: M_{ij} is the probability of making a transition from i to j.\n",
    "    M = np.multiply( 1.0 / A.sum( axis = 1, dtype = np.float64 ).reshape(-1,1), A )\n",
    "    i = 0 ; status = -1\n",
    "    while i < niter :\n",
    "        M_prime = M.copy( )\n",
    "## Expansion step: M_{ij} is the probability of reaching a vertex j from i in p hops.\n",
    "        M = np.linalg.matrix_power( M, p )\n",
    "## Pruning: make paths with low transition probability into almost surely unused.\n",
    "        M[ np.abs( M ) < theta ] = 0\n",
    "## Inflation step: dampen the probabilites\n",
    "        M = np.power( M, alpha )\n",
    "## Renormalisation step: make the matrix into a stochastic transition kernel\n",
    "        N = M.sum( axis = 1, dtype = np.float64 )\n",
    "## If a nan is encountered, then abort\n",
    "        if np.any( np.isnan( N ) ) :\n",
    "            status = -2\n",
    "            break\n",
    "        M = np.multiply( 1.0 / N.reshape(-1,1), M )\n",
    "## Convergence criterion is the L1 norm of relative divergence of transition probabilities\n",
    "        if np.sum( np.abs( M - M_prime ) / ( np.abs( M_prime ) + rel_eps ) ) < rel_eps :\n",
    "            status = 0\n",
    "            break\n",
    "## Advance to the next iteration\n",
    "        i += 1\n",
    "    return ( M, (status, i) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agorithm is desinged to transform the graph connectivity in such a way as to disconnect different communities and concentrate connectivity within one.\n",
    "\n",
    "At each iteration the algorithm cacluates a new transition kernel, based on the **$p$-hop importance score** $u_{ij}$ of connectivity of nodes $i$ and $j$ in the original kernel.\n",
    "\n",
    "The score is a nonlinear transformation of the $p$-hop connectivity probability $\\pi^p_{sd} = \\mathbb{P}( s\\rightarrow_p d )$, which reflects the chances of reaching any node $d$ from a vertex $s$ in $p$-hops. \n",
    "\n",
    "The procedure consists of \"pruning\" and \"inflation\" steps. At the pruning step, the algorithm zeroes the $p$-hop transition prbabilities lower that a $\\theta$. The rationale is, that if the random walk from $i$ is unlikely to end up in vertex $j$ in $p$ hops, then the $i\\sim j$ path is not likely to connect vertices within one community. Thus the pairs of nodes $(i,j)$ with $p$-hop transition probability $\\pi_{ij}^p$ lower than the threshold are forcefully disconnected. However, if the nodes $i$ and $j$ belong to the same community, the higher is the chance that a random walk reaches $j$ from $i$. Therefore zeroing negligible in effect severs weak between community connections.\n",
    "\n",
    "At next step the remaining non-zero $\\pi_{ij}^p$ are damped by a power transformation with parameter $\\alpha>1$ to get the **importance score** $u_{ij} = \\big(\\pi^p_{ij}  \\big)^\\alpha 1_{\\pi_{ij}^p\\geq \\theta}$. Finally the renormalisation step, makes the scores into one-hop transition probabilities for the next iteration. The a new transition kernel is thus $\\pi_{ij}' \\propto u_{ij}$ with the constraint $\\sum_j \\pi_{ij}' = 1$. The matrix $M$ in the algorithm is a transposed stochastic transition matrix: each entry is the probability $\\pi_{ij} = \\mathbb{P}( i\\rightarrow j )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this description the following hypothese can be formulated with respect to the effects of the parameters $\\alpha$, $p$ and $\\theta$:\n",
    "* $\\theta\\in(0,1)$ -- determines the level beyond which the path transition probabilities are considered important enough to contibute to a community. A high $\\theta$ makes the detection less sensitive, and beyond some threshold may cause the algorithm to fail to detect anything. Lower significance thresholdmakes the MCL more likely to find larger communities;\n",
    "* $p\\geq1$ integer: determines how far the periphery of a community may spread. The higher the $p$ the the less sensitive is community detection to the separation of nodes, and the more likely it is to find larger communities;\n",
    "* $\\alpha>1$ real: governs the damping. With high $\\alpha$ highly probable transitions stay probable in the final transition kernel $\\pi_{ij}'$, while moderate become negligible. That is why with low $\\alpha$ larger communities are detected, whereas high damping exponent tends to detect more concentrated, tighter communities, is any at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_communities( M, lengths = True ) :\n",
    "## It is extected that the MCL matrix detects communities in columns\n",
    "    C = list( ) ; i0 = 0\n",
    "    if np.any( np.isnan( M ) ) :\n",
    "        return C\n",
    "## Find all indices of nonzero elements\n",
    "    r, c = np.where( M )\n",
    "## Sort them by the column index and find the community sizes\n",
    "    r = r[ np.argsort( c ) ]\n",
    "    u = np.unique( c, return_counts = True )\n",
    "    if np.sum( u[ 1 ] ) > M.shape[ 1 ] :\n",
    "        return C\n",
    "    if lengths :\n",
    "        return u[ 1 ]\n",
    "## Columns indices of nonzero entries are ordered, so we just need to\n",
    "##  sweep across the sizes\n",
    "    for s in u[ 1 ] :\n",
    "## Row indices for a column with a nonzero element are the indices of\n",
    "##  nodes in the community.\n",
    "        list.append( C, r[ i0:i0+s ] )\n",
    "        i0 += s\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are procedure for running the test, collecting the results and visualizing and printing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mcl_test( A, a_grid, p_grid, theta = 1e-8 ) :\n",
    "## Run the grid test\n",
    "    res = [ mcl_iter( A, p = p, alpha = a, theta = theta )\n",
    "        for p in p_grid for a in a_grid ]\n",
    "## Extract the results\n",
    "## Get the number of communities\n",
    "    NC = np.array( [ len( extract_communities( C ) )\n",
    "        for C,(s,i) in res ], dtype = np.int ).reshape( len( p_grid ), -1 )\n",
    "## Extract the number of iterations\n",
    "    NI = np.array( [ i for C,(s,i) in res ], dtype = np.int ).reshape( len( p_grid ), -1 )\n",
    "    return NI, NC\n",
    "\n",
    "## Not a good way of printing tables \n",
    "def show_table( S, r, c ): \n",
    "    print \"    p\\\\a\\t\", \"\\t\".join( len( c )*[ \"%#4.3g\" ] ) % tuple( c )\n",
    "    for j in xrange( S.shape[0] ) :\n",
    "        if np.all( S[j,:] == 0 ) :\n",
    "            break\n",
    "        row = [ \"%#2d\"%(v) if v > 0 else \"  \" for v in S[j,:] ]\n",
    "        print \"%#6d\\t\"%( r[ j ] ),  \"\\t\".join( len( c )*[ \"%s\" ] ) % tuple( row )\n",
    "    \n",
    "## Produce a visually appealling picture of the adjacency\n",
    "##  matrix and community detection results \n",
    "def show_network( A, C, title = \"\" ) :\n",
    "    plt.spy( A, color = \"gray\", markersize = .5 )\n",
    "    plt.spy( C, color = \"magenta\", markersize = 5 )\n",
    "    if title : plt.title( title )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create $10\\times 7$ grid of $(\\alpha, p)$ pairs, with $\\alpha$ spaced evenly apart from $1.1$ to $10$ on the $\\log$-scale, and $p$ running from $1$ to $20$ with step $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_grid, p_grid = np.logspace( 4.5e-3, 1, num = 10, dtype = np.float ), np.arange( 2, 21, dtype = np.int )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grid will be used throughout this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the first netork and run the MCL community detection procedure with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = [ 'network1.mat', 'network2.mat', 'network3.mat' ]\n",
    "titles = [ \"Noiseless network: \"+files[0], \"Noisier network: \"+files[1], \"Noisiest network: \"+files[2] ]\n",
    "matrices = [ np.array( loadmat( './data/hw4/'+f )[ 'A' ], np.float ) for f in files ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the communities detected by the MCL with default parameters $\\alpha=2$, $p=2$ and $\\theta=10^{-8}$ for various leves of noise in the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize=(18,6) )\n",
    "plt.subplot( 131 )\n",
    "C0, _ = mcl_iter( matrices[ 0 ] )\n",
    "show_network( matrices[ 0 ], C0, titles[0] )\n",
    "plt.subplot( 132 )\n",
    "C1, _ = mcl_iter( matrices[ 1 ] )\n",
    "show_network( matrices[ 1 ], C1, titles[1] )\n",
    "plt.subplot( 133 )\n",
    "C2, _ = mcl_iter( matrices[ 2 ] )\n",
    "show_network( matrices[ 2 ], C2, titles[2] )\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case: $\\theta = 10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's being with the prunnig threshold set to almost neglibile value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = 1.0E-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the mcl for the first network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I11, S11 = mcl_test( matrices[ 0 ], alpha_grid, p_grid, theta = theta )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the mcl for the second network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I12, S12 = mcl_test( matrices[ 1 ], alpha_grid, p_grid, theta = theta )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally for the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I13, S13 = mcl_test( matrices[ 2 ], alpha_grid, p_grid, theta = theta )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the number of communities detected (empty cells represent failure to detect anything, empty rows are omitted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print files[ 0 ]\n",
    "show_table( S11, p_grid, alpha_grid )\n",
    "print files[ 1 ]\n",
    "show_table( S12, p_grid, alpha_grid )\n",
    "print files[ 2 ]\n",
    "show_table( S13, p_grid, alpha_grid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the **higher** the $\\alpha$ the **less** perceptive the alogrithm is to the macrostructure. In contrast, **high** scouting range $p$ **permits longer paths within** a community, thereby making the iterative procedure **absorb** smaller communities into larger ones.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case: $\\theta = 10^{-4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set a higher threshold: $\\theta = 10^{-4}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = 1.0E-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I21, S21 = mcl_test( matrices[ 0 ], alpha_grid, p_grid, theta = theta )\n",
    "I22, S22 = mcl_test( matrices[ 1 ], alpha_grid, p_grid, theta = theta )\n",
    "I23, S23 = mcl_test( matrices[ 2 ], alpha_grid, p_grid, theta = theta )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the resulting tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print files[ 0 ]\n",
    "show_table( S21, p_grid, alpha_grid )\n",
    "print files[ 1 ]\n",
    "show_table( S22, p_grid, alpha_grid )\n",
    "print files[ 2 ]\n",
    "show_table( S23, p_grid, alpha_grid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusions about the effects of $\\alpha$ and $p$ match the $\\theta=10^{-8}$ case. Higher $\\theta$ narrowed the area, where the algorithm was able to detect any community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case: $\\theta = 10^{-2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally consider the case of $1\\%$-probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = 0.01\n",
    "I31, S31 = mcl_test( matrices[ 0 ], alpha_grid, p_grid, theta = theta )\n",
    "I32, S32 = mcl_test( matrices[ 1 ], alpha_grid, p_grid, theta = theta )\n",
    "I33, S33 = mcl_test( matrices[ 2 ], alpha_grid, p_grid, theta = theta )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print files[ 0 ]\n",
    "show_table( S31, p_grid, alpha_grid )\n",
    "print files[ 1 ]\n",
    "show_table( S32, p_grid, alpha_grid )\n",
    "print files[ 2 ]\n",
    "show_table( S33, p_grid, alpha_grid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The significance threshold $\\theta$ severely **reduced the range** of $\\alpha$-$p$ pairs, for which MCT detects anything at all. Within that region, however, parameters affect the detection **similarly to the previous cases**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersting cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some \"intersting\" parameter pairs for the case of $\\theta = 10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cases( n, pairs, theta = 1.0E-8 ) :\n",
    "    plt.figure( figsize=(18,6) )\n",
    "    for i, (a, p) in enumerate( pairs, 1 ) :\n",
    "        plt.subplot( int( \"1\" + str( len( pairs ) ) + str( i ) ) )\n",
    "        C, _ = mcl_iter( matrices[n], alpha = alpha_grid[a], p = p_grid[p], theta = theta )\n",
    "        show_network( matrices[n], C, titles[n] + \": (a=%.2f, p=%d)\" % ( alpha_grid[a], p_grid[p] ) )\n",
    "    plt.show( )\n",
    "\n",
    "## The first network\n",
    "cases( 0, [ (1, 0), (9, 1), (9, 15) ] )\n",
    "## Second network\n",
    "cases( 1, [ (8, 1), (9, 1), (9, 2) ] )\n",
    "## Third network\n",
    "cases( 2, [ (6, 1), (9, 1), (8, 2) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion:\n",
    "* the **size** of communitites detected is **inversely** related to $\\alpha$;\n",
    "* $p$ regulates the scouting radius of the underlying random walk, and **higher values** make the procedure less sensitive to the micro-level topology of the conncetivity graph, which causes detection of **coarser communities**;\n",
    "* The significance level $\\theta$ is **inversely** proportional to the sensitivity of the procedure: **higher values** make MCL ignore more probable paths between nodes, which leads to **breaking up of large communities** into finer, more concentrated ones. The **major effect** of significance threshold is on the **size of the region** of $\\alpha$ and $p$, where the procedure detect anything at all;\n",
    "* the MCL is **extrmely sensitive to noise or missing links** in the connectivity data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a failed attempt to alter the MCL algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with the $p$-hop probability is that $\\pi^p_{ij}$ reflects the total probability of arbitrary $p$-hop $i\\rightarrow j$ paths, even those that use $j$ as a transitory vertex.\n",
    "\n",
    "However one could also propose the following heuristic: if a randomly wandering agent, who sets off to purposefully scout the graph for the member $j$, reaches its goal within $m$ hops, then $i-j$ connection must be strong within the graph. Thus the air $i$ and $j$ should lie withi the same community.\n",
    "\n",
    "Such scouting probability is computed recursively via the following formula:  \n",
    "\n",
    "$$\\sigma^p_{ij} = \\sum_{k_1\\neq j}\\ldots \\sum_{k_{p-1}\\neq j} \\pi_{ik_1}\\pi_{k_1k_2}\\ldots \\pi_{k_{p-2}k_{p-1}} \\pi_{k_{p-1}j} = \\sum_{k\\neq j} \\pi_{ik} \\sigma^{p-1}_{kj} = \\big[\\pi \\sigma^{p-1}\\big]_{ij} - \\pi_{ij}\\sigma^{p-1}_{jj}$$\n",
    "which implies that in matrix notation:\n",
    "$$\\sigma^p = \\pi \\sigma^{p-1} - \\pi \\odot \\bigg(\\begin{smallmatrix}(\\sigma^{p-1}_{ii})_i\\\\\\ldots\\\\(\\sigma^{p-1}_{ii})_i\\end{smallmatrix}\\bigg)$$\n",
    "where $\\odot$ is the element-wise matrix product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute the scout probabilites: an m-scout probability is the\n",
    "##  chance of the first ever visit to j of a random walk from i\n",
    "##  taking place no later than m-th step.\n",
    "def scout( P, n = 3, cumulative = True ) :\n",
    "    pi = P.copy() ; Sc = pi.copy() ; i = 1\n",
    "    while i < n :\n",
    "## Get the probability of a j-j loop with i hops\n",
    "        pi_jj = pi.diagonal( ).copy( )\n",
    "## The probability of the first ever visit to j from i being\n",
    "##  on the m-th step: V^m_{ij} = \\sum_{k\\neq j} P_{ik} V^{m-1}_{kj}\n",
    "        pi = P.dot( pi ) - np.multiply( P, pi_jj )\n",
    "## The scout probability: the chance that the first visit is earlier\n",
    "##  or exactly at the m-th step.\n",
    "        Sc += pi\n",
    "        i += 1\n",
    "    return SC if cumulative else pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual code from the failed attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False :\n",
    "    A = matrices[ 0 ]\n",
    "    P = np.multiply( 1.0 / A.sum( axis = 1, dtype = np.float64 ).reshape(-1,1), A )\n",
    "    S = scout(P, 100, False)\n",
    "    l, v = np.linalg.eig( S )\n",
    "    from scipy.cluster.vq import kmeans, vq\n",
    "    K=4\n",
    "    V = v[:,np.argsort( l )[ -K: ] ]\n",
    "    O = vq(V,kmeans( V, K )[ 0 ])\n",
    "    i = np.argsort( O[ 0 ] )\n",
    "    plt.imshow( np.log(S[np.ix_(i,i)]))\n",
    "    plt.colorbar( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the all-pars shortest path matrix could yield some insight into the effects of the different parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floyd_warshall( A ) :\n",
    "## Create a matrix object (not just a 2D array -- different broadcasting properties)\n",
    "    pi = np.matrix( A, dtype = np.float, copy = True )\n",
    "## Fill as of yet unreachable vertices\n",
    "    pi[ pi != 1 ] = np.inf\n",
    "## And show that the shortest path to oneself is staying.\n",
    "    np.fill_diagonal( pi, 0 )\n",
    "## For each transitory vertex\n",
    "    for v in xrange( pi.shape[ 0 ] ) :\n",
    "## Decide which is faster: to use a path without it, or to pass through it.\n",
    "        np.minimum( pi, pi[:,v] + pi[v,:], pi )\n",
    "## Return the shortest path matrix\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criterion for partitioning\n",
    "Lets consider a problem of partitioning a  similarity matrix $A=\\big(a_{ij}\\big)_{i,j\\in I}$ in two clusters non overlapping clsuters $A, B\\subseteq I$ with $A\\uplus B = I$. The matrix $A$ is a symmetric matrix of nonnegative elements.\n",
    "\n",
    "The **cut**, or in other words, the between-cluster similarity, is defined as the sum of all edges across the boundary of the partition in a weighted graph with adjacency matrix $A$:\n",
    "$$\\text{cut}(A, B) = \\sum_{i\\in A}\\sum_{j\\in B} a_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $u$ be a partition vector with $u_i = +1$ if $i\\in A$ and $-1$ otherwise (if $i\\notin A$, or $i\\in B$). Then the cut can be represented as  \n",
    "$$\\text{cut}(A,B) = \\frac{1}{8} \\sum_{i,j} a_{ij}(u_i-u_j)^2$$\n",
    "since edges are counted twice, and $(u_i-u_j)^2 = 4$ whenever $i,j\\in A$ or $i,j\\in B$, and $0$ otherwise.\n",
    "Furthermore one has\n",
    "\n",
    "$$\\sum_{i,j} a_{ij}(u_i-u_j)^2 = \\sum_{i,j}a_{ij}(u_i^2+u_j^2) - 2\\sum_{i,j}a_{ij}u_iu_j\n",
    "= 2\\Big( \\sum_i u_i^2 \\sum_j a_{ij} - u'Au \\Big)\n",
    "= 2\\Big( \\sum_i u_i \\delta_i u_i - u'Au \\Big) = 2 u'\\big(D-A\\big)u$$\n",
    "\n",
    "where $D=\\text{diag}\\big(\\delta_i\\big)$ and $\\delta_i = \\sum_j a_{ij}$.\n",
    "Now since $u'u = n$, the cut is equivalently given by  \n",
    "$$\\text{cut}(A, B) = \\frac{n}{4} \\frac{u'Lu}{u'u}$$  \n",
    "where the matrix $L = D-A$ is also known as the Laplacian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that $L\\mathbf{1} = 0$, and the number of zero eigenvalues of $L$ is equal to the number of connected components in a graph represented by $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a weighted cut criterion:\n",
    "$$\\mathcal{Q}(A,B) = \\frac{\\text{cut}(A,B)}{W_A} + \\frac{\\text{cut}(A,B)}{W_B} = \\frac{W_A + W_B}{W_A W_B}\\text{cut}(A,B)$$\n",
    "where $W_A = \\sum_{i\\in A} \\omega_i$ and $W = \\text{diag}\\big(\\omega_i\\big)$ -- a full rank diagonal matrix. In the case of normalised cut, the weighting matrix $W$ equal to $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new partition vector $q$ by $q_i = \\sqrt{\\frac{W_B}{W_A}}$ for $i\\in A$ and $q_i = - \\sqrt{\\frac{W_A}{W_B}}$ otherwise. Then\n",
    "$$q'W\\mathbf{1} = \\sum_i q_i \\omega_i = \\sqrt{\\frac{W_B}{W_A}}\\sum_{i\\in A} \\omega_i - \\sqrt{\\frac{W_A}{W_B}}\\sum_j \\omega_j = \\sqrt{W_AW_B}-\\sqrt{W_AW_B}=0$$\n",
    "and\n",
    "$$q'Wq = \\sum_i q_i^2 \\omega_i = \\frac{W_B}{W_A}\\sum_{i\\in A} \\omega_i^2 + \\frac{W_A}{W_B}\\sum_j \\omega_j^2 = W_A + W_B$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's express $q$ in terms of $u$:\n",
    "$$q_i = \\frac{W_B}{\\sqrt{W_A W_B}} \\text{ if } i\\in A \\text{ and } q_i = - \\frac{W_A}{\\sqrt{W_A W_B}} \\text{ otherwise}$$\n",
    "Whence the following expression follows:\n",
    "$$q = \\frac{1}{2 \\sqrt{W_A W_B}} \\Big( W_B (u+\\mathbf{1}) + W_A (\\mathbf{1}-u) \\Big)\n",
    "= \\frac{1}{2 \\sqrt{W_A W_B}} \\Big( (W_B+W_A) u + (W_B-W_A) \\mathbf{1} \\Big)$$\n",
    "because  $u_i=+1$ if $i\\in A$ and $u_i=-1$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $L\\mathbf{1} = D\\mathbf{1} - A\\mathbf{1} = 0$, one the following expresssion is true:\n",
    "$$q'Lq\n",
    "= \\frac{1}{4 W_A W_B} \\Big( (W_B+W_A) u +(W_B-W_A) \\mathbf{1} \\Big)' L \\Big( (W_B+W_A) u +(W_B-W_A) \\mathbf{1} \\Big)\n",
    "= \\frac{(W_B+W_A)^2}{4 W_A W_B} u'L u$$\n",
    "\n",
    "Rearranging:\n",
    "$$\\frac{q'Lq}{q'Wq} = \\frac{W_B+W_A}{W_A W_B} \\frac{u'L u}{4} = \\frac{W_A + W_B}{W_A W_B}\\text{cut}(A,B) = \\mathcal{Q}(A,B)$$\n",
    "\n",
    "This demonstrates that the problem of finding $A,B\\subseteq I$ such that $\\mathcal{Q}(A, B)$ is minimized is equivalent to finding such a partition vector $q$ that the ratio $\\frac{q'Lq}{q'Wq}$ is minimal subject to $q'W\\mathbf{1}=0$ constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:  \n",
    "**Dhillon, I. S.** (2001, August). \"Co-clustering documents and words using bipartite spectral graph partitioning\". In _Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining_ (pp. 269-274). ACM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimial solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem\n",
    "\n",
    "$$\\frac{q'Lq}{q'Wq}\\rightarrow \\min_{q\\neq 0} \\text{ subject to } q'W\\mathbf{1} = 0$$\n",
    "\n",
    "for a real valued vector $q$ can easily be solved. Indeed, the Lagrangian of the problem is\n",
    "\n",
    "$$\\mathcal{L} = \\frac{q'Lq}{q'Wq} - 2q'W\\mathbf{1}\\mu \\rightarrow \\min$$\n",
    "\n",
    "yields the **F**irst **O**rder **C**onditions \n",
    "\n",
    "$$\\frac{\\partial}{\\partial q} \\mathcal{L} = \\frac{ 2Lq (q'Wq) - 2Wq (q'Lq) }{(q'Wq)^2} - W'\\mathbf{1} (2\\mu) = 0$$\n",
    "and\n",
    "$$\\frac{\\partial}{\\partial \\mu} \\mathcal{L} = 2q'W\\mathbf{1} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FOC for $q$ simplifies to $Lq (q'Wq) - Wq (q'Lq) = W'\\mathbf{1} \\mu (q'Wq)^2$. Left multiplying by $\\mathbf{1}'$ results in\n",
    "\n",
    "$$\\mathbf{1}'Lq (q'Wq) - \\mathbf{1}'Wq (q'Lq) = \\mathbf{1}'W\\mathbf{1} \\mu (q'Wq)^2$$\n",
    "\n",
    "However $\\mathbf{1}'L = 0'$, whence $-\\mathbf{1}'Wq (q'Lq) = \\mathbf{1}'W\\mathbf{1} \\mu (q'Wq)^2$ and the FOC for $\\mu$ finally gives $\\mathbf{1}'W\\mathbf{1} \\mu (q'Wq)^2 = 0$. This implies $\\mu=0$, as $W$ is diagonal with positive values, whence the first order conditions simplify to $Lq = Wq \\lambda$ for $\\lambda = \\frac{q'Lq}{q'Wq}$.\n",
    "\n",
    "Thus solution of the relaxed problem is equivalent to finding a generalsed eigenvector $q$ of $Lq=Wq\\lambda$ corresponfing to the smallest generalised eigenvalue subject to the orthogonality constraint $q'W\\mathbf{1} = 0$. \n",
    "\n",
    "Therefore the relaxed weighted cut minimization problem is equivalent to finding the smallest eigenvalues corresponding to an eigenvector in the subspace orthogonal to $W\\mathbf{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $x$ is some solution to the generalised eigenvalue problem $Lx=Wx\\lambda$. Since $L$ and $W$ are positive-semi deifinite, $x'Lx,x'Wx\\geq0$, $x'Lx = x'Wx\\lambda$ implies that $\\lambda\\geq0$.\n",
    "Note that $q=\\mathbf{1}$ is not a solution, even thought it is a generalised eigenvalue of $L$ and $W$ pair. Indeed, since $W$ is a sitriclty positive definite matrix, $\\mathbf{1}'W\\mathbf{1} \\neq 0$ violating the orthogonality constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact the problem needs to be reformulated so that the taks would become finding the largest or the second larges eigenvalue, because almsot all numerical methods for finding eigenvalue-egienvector pairs are optimised for computing the largest eigenvalues.\n",
    "\n",
    "For the case of normalised cut this reformulation is trivial. Since the equation $Lx = Dx\\lambda$ is eqivalent to\n",
    "$Dx - Ax = Dx\\lambda$, whence $Ax=Dx(1-\\lambda)=Ax$. Thue the eqivalent formulation of the problem is to find the largest eienvalues and the corresponding generalised eigenvector of the pair $(A, D)$ orthogonal to $D\\mathbf{1}$:\n",
    "$$\\text{ find largest } \\mu \\text{ with } Ax=Dx\\mu \\text{ subject to } x'D\\mathbf{1} = 0 \\text{ and } x\\neq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The egienvalues of the reformulated problem are nonegative, since the both matrices are positive semidefinite. Therfore $\\mu\\geq 0$ and $\\lambda\\geq0$ with the ovious relationship $\\mu = 1-\\lambda$, imply that $\\mu,\\lambda\\in [0,1]$.\n",
    "\n",
    "Finally, since $D$ is invertible, the problem simplifies to finding the ordinary eigenvector of $D^{-1}A$ corresponding to the second largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.io import loadmat\n",
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "\n",
    "import scipy.io\n",
    "import scipy.sparse as spma\n",
    "import scipy.sparse.linalg as spla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [Yahoo Music network](https://www.dropbox.com/s/o3x14v4rznrh555/music_data.mat?dl=0). Edges in this network appear if enough number of users have given ratings to both music bands. Note, that edges are weighted with similarity of the ratings.\n",
    "\n",
    "* Implement *multilevel spectral recursive partitioning* algorithm that was described during the lecture\n",
    "* Visualize community structure of the network and output some of the dense clusters (with interpretation, if you can)\n",
    "\n",
    "You can load .mat files with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('./data/hw4/music_data.mat')\n",
    "A = spma.csc_matrix( data[ 'A' ], dtype = np.float )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a graph $G=(V,E)$ given by a similarity matrix $A$. The basic idea of **recursive spectral clsutering** is to continue splitting $V$ in two clusters until either a cluster is depleted or a clique is detected. To cut a long story short, using the equivalent reformulation of the spectral clustering problem, the goal is to find an eigenvector corresponding to the second largest eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster( A, T = 100, Q = None, _index = None, mincut = False, depth = float( \"inf\" ), density_threshold = .05 ) :\n",
    "## If the recursion depth is exceeded return\n",
    "    if depth <= 0 :\n",
    "        return np.arange( A.shape[ 0 ] )\n",
    "## Create master indices if necessary\n",
    "    if _index is None :\n",
    "        _index = np.arange( A.shape[ 0 ] )\n",
    "## Compute the global similarity of each vertex/element\n",
    "    deg = A.sum( axis = 1 ).getA1( )\n",
    "## Detect non-isolated items\n",
    "    nz, zz = np.where( deg != 0 )[ 0 ], np.where( deg == 0 )[ 0 ]\n",
    "    if len( nz ) < T :\n",
    "        return np.arange( len( deg ) )\n",
    "## This fiddling with tocsr() and tocsc() makes slicing faster, since format\n",
    "##  conversions are extremely fast.\n",
    "    S = A[:,nz].tocsr()[nz,:].tocsc()\n",
    "    try :\n",
    "        if mincut :\n",
    "## Compute the unnormalised laplacian\n",
    "            L = spma.diags( deg[ nz ], offsets = 0 ) - S\n",
    "## Get the eigenvector of the second least eigenvalue\n",
    "            l, e = spma.linalg.eigs( L, k = 2, which = 'SM',\n",
    "                v0 = np.ones( L.shape[ 1 ], np.float ) )\n",
    "            e = e[ :, np.argmax( l ) ].real\n",
    "        else :\n",
    "## Compute the stochastic transition kernel of non-isolated vertices for the\n",
    "##  normalised cut problem.\n",
    "            L = spma.diags( 1.0 / deg[ nz ], offsets = 0 ).dot( S )\n",
    "## Find the eigenvector corresponding to the 2nd largest eigenvalue\n",
    "            l, e = spma.linalg.eigs( L, k = 2, v0 = np.ones( L.shape[ 1 ], np.float ) )\n",
    "## Get the real part of the second largest eigenvector\n",
    "            e = e[ :, np.argmin( l ) ].real\n",
    "    except Exception, e:\n",
    "#             print \"size = %d : \\t%s\" %( A.shape[ 0 ], e.message )\n",
    "## Return the global cluster if the eigenvalue computation failed to converge\n",
    "        return np.arange( len( deg ) )\n",
    "## Set the threshold: use the zero threshold\n",
    "    t = 0\n",
    "## Separate the items in two sets: left(n) and right (p)\n",
    "    n, p = np.where( e <= t )[ 0 ], np.where( e > t )[ 0 ]\n",
    "    N, P = S[:,n].tocsr()[n,:].tocsc( ), S[:,p].tocsr()[p,:].tocsc( )\n",
    "## Compute the densities of the halves\n",
    "    nd, pd = N.nnz, P.nnz\n",
    "    nw, pw = len( n ) * ( len( n ) - 1.0 ), len( p ) * ( len( p ) - 1.0 )\n",
    "## If there is enough elements in a set, split it.\n",
    "    if ( len( p ) > T ) and ( density_threshold * pw > pd ) :\n",
    "        p = p[ cluster( P, T = T, Q = Q, _index = _index[ nz[ p ] ],\n",
    "            mincut = mincut, depth = depth - 1, density_threshold = density_threshold ) ]\n",
    "    if ( len( n ) > T ) and ( density_threshold * nw > nd ) :\n",
    "        n = n[ cluster( N, T = T, Q = Q, _index = _index[ nz[ n ] ],\n",
    "            mincut = mincut, depth = depth - 1, density_threshold = density_threshold ) ]\n",
    "## Update the queue of clusters\n",
    "    if Q is not None :\n",
    "        Q.append( _index[ nz[ p ] ] )\n",
    "        Q.append( _index[ nz[ n ] ] )\n",
    "        Q.append( _index[ zz ] )\n",
    "#         Q.append( _index[ np.concatenate( ( zz, nz[ n ], nz[ p ] ) ) ] )\n",
    "## Reorder the clusters so that the denser groups are shifted to the left\n",
    "    if nd*pw > pd*nw : p, n = n, p\n",
    "    return np.concatenate( ( zz, nz[ n ], nz[ p ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the recursive spectral clustering on the Yahoo music similarity data to uncover the hidden cluster strucutre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure( figsize = ( 16, 16 ) )\n",
    "axs = fig.add_subplot(1, 1, 1, axisbg = 'black')\n",
    "I = cluster( A, T = 5, depth = 3, density_threshold = .2 )\n",
    "axs.spy( A[:,I].tocsr( )[I,:], marker = '.', markersize = 3, precision = 0, alpha = .35, color = 'magenta' )\n",
    "I = cluster( A, T = 5, depth = 5, density_threshold = .2 )\n",
    "axs.spy( A[:,I].tocsr( )[I,:], marker = '.', markersize = 2, precision = 0, alpha = .25, color = 'green' )\n",
    "I = cluster( A, T = 5, depth = 7, density_threshold = .2 )\n",
    "axs.spy( A[:,I].tocsr( )[I,:], marker = '.', markersize = 1, precision = 0, alpha = .25, color = 'cyan' )\n",
    "I = cluster( A, T = 5, depth = np.inf, density_threshold = .2 )\n",
    "axs.spy( A[:,I].tocsr( )[I,:], marker = '.', markersize = 1, precision = 0, alpha = .25, color = 'gold' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive spectral clustering does indeed reveal certain connectivity structure in the network even for a restriction on depth of the recusion tree to at most seven levels ($\\leq$128 clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the density of each cluster in the full recursive tree of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_tree = list( )\n",
    "I = cluster( A, T = 5, Q = cluster_tree, depth = np.inf, density_threshold = .2 )\n",
    "## Compute the density\n",
    "den = np.zeros( len( cluster_tree ), np.float )\n",
    "dia = A.diagonal( )\n",
    "for i, c in enumerate( cluster_tree ) :\n",
    "## Omit residual clusters\n",
    "    if len( c ) < 2 : continue\n",
    "    within = A[:,c].tocsr()[c,:].nnz # sum( ) - dia[c].sum( )\n",
    "    weight = len( c ) * ( len( c ) - 1.0 )\n",
    "    den[ i ] = within / weight\n",
    "## Reorder\n",
    "order = np.argsort( den )\n",
    "top = order[ ~np.isnan( den[ order ] ) ][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oviously denser cluster have less nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.loglog( den[ top ], [ len( cluster_tree[ i ] ) for i in top ] )\n",
    "plt.xlabel( 'Density (log)' ) ; plt.ylabel( 'Size (log)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the densest clusters: the final spectral image is in vyan, clusters with density $\\delta$ between $0.05$ and $0.2$ are in magenta, and the densest clusters ($\\delta\\geq0.3$) are in gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure( figsize = ( 16, 16 ) )\n",
    "axs = fig.add_subplot(1, 1, 1, axisbg = 'black')\n",
    "\n",
    "axs.spy( A[:,I].tocsr( )[I,:], marker = '.', markersize = 3, precision = 0, alpha = .50, color = 'cyan' )\n",
    "\n",
    "H = spma.csc_matrix( A.shape, dtype = np.int8 )\n",
    "for f in top[ np.where( np.logical_and( den[ top ] > .05, den[ top ] <= .2 ) ) ] :\n",
    "    C = cluster_tree[ f ]\n",
    "    H[ np.ix_( C, C ) ] = 1\n",
    "axs.spy( H[:,I].tocsr( )[I,:], marker = '.', markersize = 1, precision = 0, alpha = .25, color = 'magenta' )\n",
    "\n",
    "H = spma.csc_matrix( A.shape, dtype = np.int8 )\n",
    "for f in top[ np.where( den[ top ] > .30 ) ] :\n",
    "    C = cluster_tree[ f ]\n",
    "    H[ np.ix_( C, C ) ] = 1\n",
    "axs.spy( H[:,I].tocsr( )[I,:], marker = '.', markersize = 1, precision = 0, alpha = .15, color = 'yellow' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actaully quite difficult to analyse how well the clustering preformed, since we were not given the information on the artist's genre of music. However moderate googling revealed the genres of some of the densest clusters (#rank):\n",
    "*  2 -- country;\n",
    "* 48 -- Heavy metal cl&uuml;ster (strangely it includes the Kings of Metal Manowar, but not Iron Maiden);\n",
    "*  3 -- metal/hard rock;\n",
    "*  6 -- jazz;\n",
    "* 10 -- blues;\n",
    "* 42 -- rap;\n",
    "*  7 -- R&B;\n",
    "* 51 -- christian pop/rock/rap music.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n, i in enumerate( top, 1 ) :\n",
    "    c = cluster_tree[ i ]\n",
    "    if len( c ) and den[ i ] > .3 :\n",
    "        print \"%#4d (%#4d, %0.3f)\\t\"%(n,len( c ), den[ i ]), \", \".join(sorted( [ s.strip() for s in data['artists'][ c ] ] ) ), \"\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
