{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add JS-based table of contents\n",
    "from IPython.display import HTML as add_TOC\n",
    "add_TOC( u\"\"\"<h1 id=\"tocheading\">Table of Contents</h1></br><div id=\"toc\"></div>\n",
    "<script src=\"https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js\"></script></br></hr></br>\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, time as tm, warnings\n",
    "warnings.filterwarnings( \"ignore\" )\n",
    "# from IPython.core.display import HTML\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed( 1234567 )\n",
    "## This is the correct way to use the random number generator,\n",
    "##  since it allows finer control.\n",
    "rand = np.random.RandomState( np.random.randint( 0x7FFFFFFF ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _EM_ and MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\TeX$ markup used here uses the \"align*\" environment and thus should not be viewed though nbViewer.\n",
    "\n",
    "Before proceeding, it seems pedagogically necessary (at least for myself) to revise the **EM**-slgorithm and show its \"correctness\", so to say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief description of the **EM** algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EM algorithm seeks to maximize the likelihood by means of successive application of two steps: the E-step and the M-step.\n",
    "\n",
    "For any probability measure $Q$ on the space of latent variables $Z$ with density $q$ the following holds:  \n",
    "\\begin{align*}\n",
    "\\log p(X|\\Theta)\n",
    "    &= \\int q(Z) \\log p(X|\\Theta) dZ\n",
    "     = \\mathbb{E}_q \\log p(X|\\Theta) \\\\\n",
    "   %% &= \\Bigl[p(X,Z|\\Theta) = p(Z|X,\\Theta) p(X|\\Theta) \\Bigr] \\\\\n",
    "    &= \\mathbb{E}_{Z\\sim q} \\log \\frac{p(X,Z|\\Theta)}{p(Z|X\\Theta)}\n",
    "     = \\mathbb{E}_{Z\\sim q} \\log \\frac{q(Z)}{p(Z|X,\\Theta)}\n",
    "     + \\mathbb{E}_{Z\\sim q} \\log \\frac{p(X,Z|\\Theta)}{q(Z)} \\\\ \n",
    "    &= KL\\bigl(q\\|p(\\cdot|X,\\Theta)\\bigr) + \\mathcal{L}\\bigl(q, \\Theta\\bigr)\\,,\n",
    "\\end{align*}  \n",
    "\n",
    "since the Bayes theorem posits that $p(X,Z|\\Theta) = p(Z|X,\\Theta) p(X|\\Theta)$. Call this equiation the **\"master equation\"**.\n",
    "\n",
    "Now note that since the Kullback-Leibler divergence is **always** non-negative, one has the following inequality:\n",
    "$$\\log p(X|\\Theta) \\geq \\mathcal{L}\\bigl(q, \\Theta\\bigr) \\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to make the lower bound as large as possible by changing $\\Theta$ and varying $q$. But first note that the \n",
    "left-hand side of the **master equation** is independent of $q$, whence maximization of $\\mathcal{L}$ with respect to $q$ (with $\\Theta$ fixed) is equivalent to minimization of $KL\\bigl(q\\|p(\\cdot|X,\\Theta)\\bigr)$ with respect to $q$ taking $\\Theta$ fixed. Since $q$ is arbitrary, the optimal minimizer $q^*_\\Theta$ is $q^*(Z|\\Theta) = p(Z|X,\\Theta)$ for all $Z$.\n",
    "\n",
    "Now at the optimal distributuion $q^*_\\Theta$ the **master equation** becomes\n",
    "$$ \\log p(X|\\Theta)\n",
    "= \\mathcal{L}\\bigl(q^*_\\Theta, \\Theta\\bigr)\n",
    "= \\mathbb{E}_{Z\\sim q^*_\\Theta} \\log \\frac{p(X,Z|\\Theta)}{q^*(Z|\\Theta)}\n",
    "= \\mathbb{E}_{Z\\sim q^*_\\Theta} \\log p(X,Z|\\Theta) - \\mathbb{E}_{Z\\sim q^*_\\Theta} \\log q^*(Z|\\Theta) \\,,\n",
    "$$\n",
    "for any $\\Theta$. Thus the problem of log-likelihood maximization reduces to that of maximizing the sum of expectations on the right-hand side.\n",
    "\n",
    "This new problem does not seem to be tractable in general since the optimization paramters $\\Theta$ affect both the expected log-likelihood $\\log p(X,Z|\\Theta)$ under $Z\\sim q^*_\\Theta$ and the entropy of the optimal distribution of the latent variables $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully using an iterative procedure which switches between the computation of $q^*_\\Theta$ and the maximization of $\\Theta$ might be effective. Consider the folowing :\n",
    "* **E**-step: considering $\\Theta_i$ as given and fixed find $q^*_{\\Theta_i} = \\mathop{\\text{argmin}}_q\\,\\, KL\\bigl(q\\|p(\\cdot|X,\\Theta_i)\\bigr)$ and set $q_{i+1} = q^*_{\\Theta_i}$;\n",
    "* **M**-step: considering $q_{i+1}$ as given, solve $\\mathcal{L}(q_{i+1},\\Theta) \\to \\mathop{\\text{max}}_\\Theta$, where \n",
    "$$ \\mathcal{L}(q,\\Theta) = \\mathbb{E}_{Z\\sim q} \\log p(X,Z|\\Theta) - \\mathbb{E}_{Z\\sim q} \\log q(Z) \\,.$$\n",
    "\n",
    "The fact that $q_i$ is considered fixed makes the optimization of $\\mathcal{L}(q_i,\\Theta)$ equivalent to maximization of the expected log-likelihood, since the entropy term is fixed. Therefore the **M**-step becomes:\n",
    "* given $q_{i+1}$ find $\\Theta^*_{i+1} = \\mathop{\\text{argmax}}_\\Theta\\,\\, \\mathbb{E}_{Z\\sim q_{i+1}} \\log p(X,Z|\\Theta)$ and put $\\Theta_{i+1} = \\Theta^*_{i+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if the latent variables are mutually independent, then the optimal $q$ must be factorizable into marginal densities and:\n",
    "\n",
    "\\begin{align*}\n",
    "    KL\\bigl(q\\|p(\\cdot|X,\\Theta)\\bigr)\n",
    "    &= \\mathbb{E}_{Z\\sim q} \\log q(Z) - \\sum_j \\mathbb{E}_{z_j\\sim q_j} \\log p(z_j|X,\\Theta)\\\\\n",
    "    &= \\sum_j \\mathbb{E}_{z_j\\sim q_j} \\log q_j(z_j) - \\sum_j \\mathbb{E}_{z_j\\sim q_j} \\log p(z_j|X,\\Theta)\n",
    "     = \\sum_j KL\\bigl(q_j\\|p_j(|X,\\Theta)\\bigr) \\,,\n",
    "\\end{align*}\n",
    "where $q_j$ is the marginal desity of $z_j$ in $q(Z)$ (the last term in the first line comes from the Fubini theorem).\n",
    "\n",
    "Therefore the **E**-step could be reduced to a set of minimization problems with respect to one-dimensional density functions:\n",
    "$$ q_j^* = \\mathop{\\text{argmin}}_{q_j}\\,\\, KL\\bigl(q_j\\|p_j(\\cdot|X,\\Theta)\\bigr) \\,, $$\n",
    "since the Kulback-Leibler divergence in this case in additively separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **master equation** is an identity: for all densities $q$ on $Z$ and for all admissible parameters $\\Theta$\n",
    "$$ \\log p(X|\\Theta) = KL\\bigl(q\\|p(\\cdot|X,\\Theta)\\bigr) + \\mathcal{L}\\bigl(q, \\Theta\\bigr) \\,.$$\n",
    "\n",
    "Hence if after the **E**-step the Kulback-Leibler divergence is reduced:\n",
    "$$ KL\\bigl(q'\\|p(\\cdot|X,\\Theta)\\bigr) \\leq KL\\bigl(q\\|p(\\cdot|X,\\Theta)\\bigr) \\,,$$\n",
    "then for the same set of parameters $\\Theta$ one has\n",
    "$$ \\mathcal{L}(q,\\Theta) \\leq \\mathcal{L}(q',\\Theta) \\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just after the **E**-step one has $q_{i+1} = p(Z|X,\\Theta_i)$, whence $KL\\bigl(q_{i+1}\\|p(\\cdot|X,\\Theta_i)\\bigr) = 0$. In turn, this implies via the **master equation** that the following equality holds:\n",
    "$$ \\log p(X|\\Theta_i) = \\mathcal{L}(q_{i+1},\\Theta_i) \\,.$$\n",
    "\n",
    "After the **M**-step, since $\\Theta_{i+1}$ is a maximizer, or at least an \"_improver_\" of $\\mathcal{L}(q_{i+1},\\Theta)$ compared to its value at $(q_i,\\Theta_i)$, one has\n",
    "$$ \\mathcal{L}(q_{i+1},\\Theta_i) \\leq \\mathcal{L}(q_{i+1},\\Theta_{i+1}) \\,.$$\n",
    "\n",
    "Threfore the effect of a single complete round of **EM** on the log-likelihood itself is:\n",
    "$$ \\log p(X|\\Theta_i) = \\mathcal{L}(q_{i+1},\\Theta_i) \\leq \\mathcal{L}(q_{i+1},\\Theta_{i+1}) \\leq \\mathcal{L}(q_{i+2},\\Theta_{i+1}) = \\log p(X|\\Theta_{i+1}) \\,,$$\n",
    "where the equality is achieved between the **E** and the **M** step within one round. This implies that **EM** indeed iteratively improves the log-likihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that in the general case, without attaining zero Kulback-Leibler divergence at the $E$-step, one cannot be sure that the real log-likelihood is improved by each iteration and one can just say that\n",
    "$$ \\mathcal{L}(q_{i+1},\\Theta_i) \\leq \\log p(X|\\Theta_i) \\,,$$\n",
    "which does not uncover a relationship with $\\log p(X|\\Theta_{i+1})$. And without the guarantee that **EM** improves the log-likelihood to the maximum one cannot be sure about the consistency of the estimators. The key question is whether the lower bound $\\mathcal{L}(q,\\Theta)$ is any good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of the EM to MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is a random element in a discrete probability space $\\Omega = \\{0,1\\}^{N\\times M}$ with product-measure\n",
    "$$ \\mathbb{P}(\\omega) = \\prod_{i=1}^N\\prod_{j=1}^M \\theta_{ij}^{\\omega_{ij}} (1-\\theta_{ij})^{1-\\omega_{ij}} \\,,$$\n",
    "for any $\\omega\\in \\Omega$. In particular $M=N=28$. Basically each bit of the image is independent of any other bit and each one is a Bernoulli random variable with parameter $\\theta_{ij}$: $\\omega_{ij}\\sim \\text{Bern}(\\theta_{ij})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the EM algorithm to this dataset. The proposed model is the following.\n",
    "\n",
    "Consider a mixture model of discrete probability spaces. Suppose there are $K$ componets in the mixture. Then each image is distributed according to the following law:\n",
    "$$p(\\omega|\\Theta)\n",
    "= \\sum_{k=1}^K \\pi_k p_k(\\omega|\\theta_k)\n",
    "= \\sum_{k=1}^K \\pi_k \\prod_{i=1}^N \\prod_{j=1}^M \\theta_{kij}^{\\omega_{ij}} (1-\\theta_{kij})^{1-\\omega_{ij}}$$\n",
    "where $\\theta_{kij}$ is the paramter of the probability distribution of the $(i,j)$-th random variable (pixel) in the $k$-th class, and $\\pi_k$ is the (prior) porbability of the $k$-th mixutre to generate a random element, $\\sum_{k=1}^K \\pi_k= 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $X=(x_i)_{i=1}^n \\in \\Omega^n$ is the dataset. The log-likelihood is given by\n",
    "$$ \\log p(X|\\Theta) = \\sum_{s=1}^n \\log \\sum_{k=1}^K \\pi_k\n",
    "\\prod_{i=1}^N \\prod_{j=1}^M \\theta_{kij}^{x_{sij}} (1-\\theta_{kij})^{1-x_{sij}} \\,,$$\n",
    "where $x_{sij}\\in\\{0,1\\}$ -- is the value of the the $(i,j)$-th pixel at the $s$-th observation.\n",
    "\n",
    "If the source $Z=(z_i)_{i=1}^n$ components of the mixture at each datapoint were known, then the log-likelihood would have been\n",
    "$$ \\log p(X,Z|\\Theta) = \\sum_{s=1}^n \\log \\prod_{k=1}^K \\Bigl[ \\pi_k \n",
    "\\prod_{i=1}^N \\prod_{j=1}^M \\theta_{kij}^{x_{sij}} (1-\\theta_{kij})^{1-x_{sij}} \\Bigr]^{1_{z_s = k}} \\,,$$\n",
    "where $1_{z_s = k}$ is the indicator and take the value $1$ if $\\{z_s = k\\}$ and $0$ otherwise ($1_{\\{k\\}}(z_s)$ is another notation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood simplifies to\n",
    "$$ \\log p(X,Z|\\Theta) = \\sum_{s=1}^n \\sum_{k=1}^K 1_{z_s = k} \\Bigl( \\log \\pi_k + \n",
    "\\sum_{i=1}^N \\sum_{j=1}^M \\bigl( x_{sij} \\log \\theta_{kij} + (1-x_{sij}) \\log (1-\\theta_{kij}) \\bigr) \\Bigr) \\,,$$\n",
    "and further into a more separable form\n",
    "$$ \\log p(X,Z|\\Theta)\n",
    "= \\sum_{s=1}^n \\sum_{k=1}^K 1_{z_s = k} \\log \\pi_k\n",
    "+ \\sum_{s=1}^n \\sum_{k=1}^K 1_{z_s = k} \\Bigl( \\sum_{i=1}^N \\sum_{j=1}^M x_{sij} \\log \\theta_{kij}\n",
    "+ \\sum_{i=1}^N \\sum_{j=1}^M (1-x_{sij}) \\log (1-\\theta_{kij}) \\Bigr) \\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected log-likelihood under $z_s\\sim q_s$ with $\\mathbb{P}(z_s=k|X) = q_{sk}$, is given by\n",
    "$$ \\mathbb{E}\\log p(X,Z|\\Theta)\n",
    "= \\sum_{s=1}^n \\sum_{k=1}^K q_{sk} \\log \\pi_k\n",
    "+ \\sum_{s=1}^n \\sum_{k=1}^K q_{sk} \\sum_{i=1}^N \\sum_{j=1}^M \\bigl( x_{sij} \\log \\theta_{kij} + (1-x_{sij}) \\log (1-\\theta_{kij}) \\bigr) \\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analytic solution: **E**-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the **E**-step one must compute $q^*(Z) = \\mathbb{P}(z_s=k|X) = \\hat{q}_{sk}$ based on the value of $\\Theta = ((\\pi_k), (\\theta_{kij}))$.\n",
    "$$\\hat{q}_{sk}\n",
    "= \\frac{p(x_s|z_s=k,\\Theta) p(z_s=k)}{\\sum_{l=1}^K p(x_s|z_s=l,\\Theta) p(z_s=l)}\n",
    "\\propto \\pi_k \\prod_{i=1}^N \\prod_{j=1}^M \\theta_{kij}^{x_{sij}} (1-\\theta_{kij})^{1-x_{sij}}\n",
    "$$\n",
    "and\n",
    "$$ q^*(Z) = \\prod_{s=1}^n q_{s z_s} $$\n",
    "Note that the denominator is actually the log-likelihood of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve numerical stability and avoid numerical underflow it is better to use the following procedure for computation of the conditional probability:\n",
    "$$ l_{sk} = \\sum_{i=1}^N \\sum_{j=1}^M \\log \\bigl( \\theta_{kij}^{x_{sij}} (1-\\theta_{kij})^{1-x_{sij}} \\bigr) \\,,$$\n",
    "set $l^*_s = \\max_k l_{sk}$ and compute the log-sum\n",
    "$$ \\hat{l}_s = \\log \\sum_{k=1}^K \\text{exp}\\Bigl\\{ ( l_{sk} - l^*_s ) + \\log \\pi_k \\Bigr\\} \\,,$$\n",
    "and then compute the consitional distribution:\n",
    "$$ \\hat{q}_{sk} = \\text{exp}\\Bigl\\{ l_{sk} + \\log \\pi_k - ( \\hat{l}_s + l^*_s ) \\Bigr\\} \\,.$$\n",
    "\n",
    "This seemingly redunant subctration and addition of $l^*_s$ helps avoid underflow during the numerical exponentiation. After this sanitization the **E**-step's optimal distribution would be numerically accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $l^*_s >> l_{sk}$ for all $k$ but such that $l^*_s = l_{sk}$ (let it be $k^*$), then an underflow occurs at the sum-exp step, whence for some very small $\\epsilon > 0$ one has\n",
    "$$ \\hat{l}_s = l^*_s + \\log (1+\\epsilon) \\,,$$\n",
    "whence\n",
    "$$ \\hat{q}_{sk} = \\text{exp}\\bigl\\{l_{sk} - \\hat{l}_s\\bigr\\} = (1+\\epsilon)^{-1} \\cdot \\text{exp}\\bigl\\{l_{sk} - l^*_s \\bigr\\} \\,.$$\n",
    "For $k=k^*$ on has $\\hat{q}_{sk} = \\frac{1}{1+\\epsilon}\\approx 1$, and for $k\\neq k^*$ -- $\\hat{q}_{sk} = \\frac{\\eta}{1+\\epsilon} \\approx 0$ for some extremely small $\\eta>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables in the code have the following dimensions:\n",
    "* $\\theta \\in [0,1]^{K\\times (N\\times M)}$;\n",
    "* $\\pi \\in [0,1]^{1\\times K}$;\n",
    "* $x \\in \\{0,1\\}^{n\\times (N\\times M)}$;\n",
    "* $z \\in [0,1]^{n\\times K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrappers required for the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## A bunch of wrappers to match the task specifications\n",
    "def posterior( x, clusters ) :\n",
    "    pi = np.ones( clusters.shape[ 0 ], dtype = np.float ) / clusters.shape[ 0 ]\n",
    "    q, ll = __posterior( x, theta = clusters, pi = pi )\n",
    "    return q\n",
    "\n",
    "## The likelihood is a byproduct of the E-step's minimization of Kullback-Leibler\n",
    "def likelihood( x, clusters ) :\n",
    "    pi = np.ones( clusters.shape[ 0 ], dtype = np.float ) / clusters.shape[ 0 ]\n",
    "    q, ll = __posterior( x, theta = clusters, pi = pi )\n",
    "    return np.sum( ll )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify using the maximum aposteriori rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Classifier\n",
    "def classify( x, theta, pi = None ) :\n",
    "    pi = pi if pi is not None else np.ones( theta.shape[ 0 ], dtype = np.float ) / theta.shape[ 0 ]\n",
    "## Compute the posterior probabilities of the data\n",
    "    q_sk, ll_s = __posterior( x, theta = theta, pi = pi )\n",
    "## Classify according to max pasterior:\n",
    "    c_s = np.argmax( q_sk, axis = 1 )\n",
    "    return c_s, q_sk, ll_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A procedure to compute the log-likelihood of each observaton with respect to each mixture component. Used in the posterior computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __component_likelihood( x, theta ) :\n",
    "## Unfortunately sometimes there can be negative machine zeros, which\n",
    "##  spoil the log-likelihood computation by poisoning with NANs.\n",
    "## That is why the theta array is restricted to [0,1].\n",
    "    theta_clipped = np.clip( theta, 0.0, 1.0 )\n",
    "## Iterate over classes\n",
    "    ll_sk = np.zeros( ( x.shape[ 0 ], theta.shape[ 0 ] ), dtype = np.float )\n",
    "## Make a binary mask of the data\n",
    "    mask = x > 0\n",
    "    for k in xrange( theta.shape[ 0 ] ) :\n",
    "## Note that the power formulation is just a mathematically convenient way of\n",
    "##  writing \\theta if x=1 or (1-\\theta) otherwise.\n",
    "        ll_sk[ :, k ] = np.sum( np.where( mask,\n",
    "            np.log( theta_clipped[ k ] ), np.log( 1 - theta_clipped[ k ] ) ), axis = ( 1, ) )\n",
    "    return ll_sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual procedure for computing the **E***-step: the conditional distribution and the log-likelihood scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The core procedure for computing the conditional density of classes\n",
    "def __posterior( x, theta, pi ) :\n",
    "## Get the log-likelihoods of each observation in each mixture component.\n",
    "    ll_sk = __component_likelihood( x, theta )\n",
    "## Find the largest unnormalized probability.\n",
    "    llstar_s = np.reshape( np.max( ll_sk, axis = ( 1, ) ), ( ll_sk.shape[ 0 ], 1 ) )\n",
    "## Subtract the largest exponent\n",
    "    ll_sk -= llstar_s\n",
    "## In the rare case when the largest exponent is -Inf, force the differences\n",
    "##  to zero. This effective treaks such observations as having unfiorm likelihood\n",
    "##  across classes. This way the priors don't get masked by really small numbers. \n",
    "##  I could've used ``np.nan_to_num( ll_sk - llstar_s )'' but it actually copies\n",
    "##  the ll_sk array.\n",
    "    ll_sk[ np.isnan( ll_sk ) ] = 0.0\n",
    "## Don't forget to add the log-prior probability (Numpy broadcasting applies!).\n",
    "##  Adding priors before dealing with infinities would mask then and yield\n",
    "##  incorrect estimates of the log-likelihoods!\n",
    "    ll_sk += np.log( np.reshape( pi, ( 1, ll_sk.shape[ 1 ] ) ) )\n",
    "## Compute the log-sum-exp of the individual log-likelihoods. Negative infinities\n",
    "##  resolve to 0.0 while the largest exponent resolves to a one. This step cannot\n",
    "##  produce NaNs\n",
    "    ll_s = np.reshape( np.log( np.sum( np.exp( ll_sk ), axis = ( 1, ) ) ), ( ll_sk.shape[ 0 ], 1 ) )\n",
    "## The sum-exp could never be anything lower than 1, since at least one\n",
    "##  element of each row of ll_sk has to be lstar_s, whence the respective\n",
    "##  difference should be zero and the exponent -- 1. Thus even if the\n",
    "##  rest of the sum is close to machine zero, the logarithm would still\n",
    "##  return 0.\n",
    "## Normalise the likelihoods to get conditional probability, and compute\n",
    "##  the sum of the log-denominator, which is the log-likelihood.\n",
    "    return np.exp( ll_sk - ll_s ), ll_s + llstar_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analytic solution: **M**-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the **M**-step for some fixed $q(Z)$ one solves $\\mathbb{E}\\log p(X,Z|\\Theta)\\to \\max_\\Theta$ subject to $\\sum_{k=1}^K \\pi_k = 1$ which is a convex optimization problem with respect to $\\Theta$, since the log-likelihood as a linear combination of convex functions is convex. The first order condition is $\\sum_{s=1}^n \\frac{q_{sk}}{\\pi_k} - \\lambda = 0$ for all $k=1,\\ldots,K$, whence $ \\lambda = \\sum_{s=1}^n \\sum_{l=1}^K q_{sl} = n $ and finally\n",
    "$$ \\hat{\\pi}_k = \\frac{\\sum_{s=1}^n q_{sk}}{n} \\,.$$\n",
    "\n",
    "For $\\theta_{kij}$, $i=1,\\ldots,N$, $j=1,\\ldots,M$ and $k=1,\\ldots,K$ the FOC is\n",
    "$$ \\sum_{s=1}^n q_{sk} \\frac{x_{sij}}{\\theta_{kij}} - \\sum_{s=1}^n q_{sk} \\frac{1-x_{sij}}{1-\\theta_{kij}} = 0 \\,,$$\n",
    "whence\n",
    "$$ \\hat{\\theta}_{kij} =  \\frac{\\sum_{s=1}^n q_{sk} x_{sij}}{ \\sum_{s=1}^n q_{sk} } = \\frac{\\sum_{s=1}^n q_{sk} x_{sij}}{ n \\hat{\\pi}_k } \\,.$$\n",
    "\n",
    "This **M**-step procedure is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The E-step is simple: just compute the optimal parameters under\n",
    "##  the current conditional distribution of the latent variables.\n",
    "def __learn_clusters( x, z ) :\n",
    "## The prior class probabilities\n",
    "    pi = z.sum( axis = ( 0, ) )\n",
    "## Pixel probabilities conditional on the calss\n",
    "    theta = np.tensordot( z, x, ( 0, 0 ) ) / pi.reshape( ( pi.shape[ 0 ], 1 ) )\n",
    "## Return: regularization should be done at **E**-step!\n",
    "#     return np.clip( theta, 1.0/784, 1.0 - 1.0/784 ), pi / x.shape[ 0 ]\n",
    "    return theta, pi / x.shape[ 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wrapper to match the assignment specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## A wrapper for the above function\n",
    "def learn_clusters( x, z ) :\n",
    "    theta, pi = __learn_clusters( x, z )\n",
    "## Just return theta: in the condtional model the pi are fixed.\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A it has been mentioned eariler, the EM algorithm switches between **E** and **M** steps until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## A wrapper for the core em algorithm below\n",
    "def em_algorithm( x, K, maxiter, verbose = True, rel_eps = 1e-4, full = False ) :\n",
    "## Initialize the model parameters with uniform [0.25,0.75] random numbers\n",
    "    theta_1 = rand.uniform( size = ( K, x.shape[ 1 ] ) ) * 0.5 + 0.25\n",
    "    pi_1 = None if not full else np.ones( K, dtype = np.float ) / K\n",
    "## Run the em algorithm\n",
    "    tick = tm.time( )\n",
    "    ll, theta, pi, status = __em_algorithm( x, theta_1 = theta_1,\n",
    "        pi_1 = pi_1, niter = maxiter, rel_eps = rel_eps, verbose = verbose )\n",
    "    tock = tm.time( )\n",
    "    print( \"total %.3f, %.3f/iter\" % ( ( tock - tick ), ( tock - tick ) / len( ll ), ) )\n",
    "## Return the history of theta and the final log liklihood\n",
    "    if verbose :\n",
    "        if status[ 'status' ] != 0 :\n",
    "            print \"Convergence not achieved. %d\" % ( status[ 'status' ], )\n",
    "    if full :\n",
    "        return ( theta, pi ), ll\n",
    "    return theta, ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure above actually invokes the true EM core, defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The core of the EM algorithm\n",
    "def __em_algorithm( x, theta_1, pi_1 = None, niter = 1000, rel_eps = 1e-4, verbose = True ) :\n",
    "## If we were supplied with an initial estimate of the prior distribution,\n",
    "##  then assume the full model is needed.\n",
    "    full_model = pi_1 is not None\n",
    "## If the prior cluster probabilities are not supplied, assume uniform distribution.\n",
    "    pi_1 = pi_1 if full_model else np.ones( theta_1.shape[ 0 ], dtype = np.float ) / theta_1.shape[ 0 ]\n",
    "## Allocate the necessary space for the history of model estimates\n",
    "    theta_hist, pi_hist = theta_1[ np.newaxis ].copy( ), pi_1[ np.newaxis ].copy( )\n",
    "    ll_hist = np.asarray( [ -np.inf ], dtype = np.float )\n",
    "## Set \"old\" estimated to zero. At this line the current estimates are in fact\n",
    "##  the initially provided ones.\n",
    "    theta_0, pi_0 = np.zeros_like( theta_1 ), np.zeros_like( pi_1 )\n",
    "## Initialize the loop\n",
    "    status, kiter, rel_theta, rel_pi, ll = -1, 0, np.nan, np.nan, -np.inf\n",
    "    while kiter < niter :\n",
    "## Dump the current estimators and other information.\n",
    "        if verbose :\n",
    "            print( \"Iteration %d: avg. log-lik: %.3f, $\\\\Theta$ div. %.3f, $\\\\Pi$ div. %.3f\" % (\n",
    "                kiter, ll / x.shape[ 0 ], rel_theta, rel_pi ) )\n",
    "            show_data( theta_1 - theta_0 if True else theta_1, n = theta_0.shape[ 0 ],\n",
    "                n_col = min( 10, theta_0.shape[ 0 ] ), cmap = plt.cm.hot, interpolation = 'nearest' )\n",
    "## The convergence criterion is the L^∞ norm of relative L^1 errors\n",
    "        if max( rel_pi, rel_theta ) < rel_eps :\n",
    "            status = 0\n",
    "            break ;\n",
    "## Overwrite the initial estimates\n",
    "        theta_0, pi_0 = theta_1, pi_1\n",
    "## E-step: call the core posterior function to get both the log-likelihood\n",
    "##  and the estimate of the conditional distribution.\n",
    "        z_1, ll_s = __posterior( x, theta_0, pi_0 )\n",
    "## Sum the individual log-likelihoods of observations\n",
    "        ll = ll_s.sum()\n",
    "## M-step: compute the optimal parameters under the current estimate of the posterior\n",
    "        theta_1, pi_1 = __learn_clusters( x, z_1 )\n",
    "## Discard the computed estimate of pi if the model is discriminative (conditional likelihood).\n",
    "        if not full_model :\n",
    "            pi_1 = pi_0\n",
    "## Record the current estimates to the history\n",
    "        theta_hist = np.vstack( ( theta_hist, theta_1[np.newaxis] ) )\n",
    "        pi_hist = np.vstack( ( pi_hist, pi_1[np.newaxis] ) )\n",
    "        ll_hist = np.append( ll_hist, ll )\n",
    "## Check for bad float numbers\n",
    "        if not ( np.all( np.isfinite( theta_1 ) ) and np.all( np.isfinite( pi_1 ) ) ) :\n",
    "            status= -2\n",
    "            break ;\n",
    "## Check convergence: L^1 relative error. If the relative margin is exactly\n",
    "##  zero, then return NaNs. This makes teh loop exhaust all iterations, since\n",
    "##  any comprison agains a NaN returns False.\n",
    "        rel_theta = np.sum( np.abs( theta_1 - theta_0 ) / ( np.abs( theta_0 ) + rel_eps ) ) if rel_eps > 0 else np.nan\n",
    "        rel_pi = np.sum( np.abs( pi_1 - pi_0 ) / ( np.abs( pi_0 ) + rel_eps ) ) if rel_eps > 0 else np.nan\n",
    "## Next iteration\n",
    "        kiter += 1\n",
    "    return ll_hist, theta_hist, pi_hist, { 'status': status, 'iter': kiter }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a convenient procedure for running experiments. By setting relative error to zero the algorithm is forced to exhaust all the allocated iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment( data, K, maxiter, verbose = True, until_convergence = False, full = False ) :\n",
    "## Run the EM\n",
    "    return em_algorithm( data, K, maxiter, rel_eps = 1.0e-4 if until_convergence else 0.0, verbose = verbose, full = full )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellanea: visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to plot more flexibly, define another arranger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## A more flexible image arrangement\n",
    "def arrange_flex( images, n_row = 10, n_col = 10, N = 28, M = 28, fill_value = 0 ) :\n",
    "## Create the final grid of images row-by-row\n",
    "    im_grid = np.full( ( n_row * N, n_col * M ), fill_value, dtype = images.dtype )\n",
    "    for k in range( min( images.shape[ 0 ], n_col * n_row ) ) :\n",
    "## Get the grid cell at which to place the image\n",
    "        i, j = ( k // n_col ) * N, ( k % n_col ) * M\n",
    "## Just put the image in the cell\n",
    "        im_grid[ i:i+N, j:j+M ] = np.reshape( images[ k ], ( N, M, ) )\n",
    "    return im_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folowing pair of procedures are used to plot the digits in a clear manner. The first one just creates a canvas for the image: it sets up both axes properly and adds labels to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_canvas( axis, n_row, n_col, N = 28, M = 28 ) :\n",
    "## Setup major tick marks to the seam between images and disable their labels\n",
    "    axis.set_yticks( np.arange( 1, n_row + 1 ) * N, minor = False )\n",
    "    axis.set_xticks( np.arange( 1, n_col + 1 ) * M, minor = False )\n",
    "    axis.set_yticklabels( [ ], minor = False ) ; axis.set_xticklabels( [ ], minor = False )\n",
    "## Set minor ticks so that they are exactly between the major ones\n",
    "    axis.set_yticks( ( np.arange( n_row + 1 ) + 0.5 ) * N, minor = True )\n",
    "    axis.set_xticks( ( np.arange( n_col + 1 ) + 0.5 ) * M, minor = True )\n",
    "## Make their labels into cell x-y coordinates\n",
    "    axis.set_yticklabels( [ \"%d\" % (i,) for i in 1+np.arange( n_row + 1 ) ], minor = True )\n",
    "    axis.set_xticklabels( [ \"%d\" % (i,) for i in 1+np.arange( n_col + 1 ) ], minor = True )\n",
    "## Tick marks should be oriented outward\n",
    "    axis.tick_params( axis = 'both', which = 'both', direction = 'out' )\n",
    "## Return nothing!\n",
    "    axis.grid( color = 'white', linestyle = '--' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure displays the images on a nice plot. Used for one-line visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_data( data, n, n_col = 10, transpose = False, **kwargs ) :\n",
    "## Get the number of rows necessary to plot the needed number of images\n",
    "    n_row = ( n + n_col - 1 ) // n_col\n",
    "## Transpose if necessary\n",
    "    if transpose :\n",
    "        n_col, n_row = n_row, n_col\n",
    "## Set the dimensions of the figure\n",
    "    fig = plt.figure( figsize = ( n_col, n_row ) )\n",
    "    axis = fig.add_subplot( 111 )\n",
    "## Plot!\n",
    "    setup_canvas( axis, n_row, n_col )\n",
    "    axis.imshow( arrange_flex( data[:n], n_col = n_col, n_row = n_row ), **kwargs )\n",
    "## Plot\n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize( data, clusters, ll, n_col = 2, plot_ll = True ) :\n",
    "## Display the result\n",
    "    print \"Final conditional log-likelihood value per observation achieved %f in %d iteration(s)\" % (\n",
    "        ll[-1] / data.shape[ 0 ], len( ll ) )\n",
    "## Plot the first difference of average log-likelihood\n",
    "    if plot_ll :\n",
    "        plt.figure( figsize = ( 12, 7 ) )\n",
    "        ax = plt.subplot(111)\n",
    "        ax.set_title( r\"avg. log-likelihood change between successive iterations (log scale)\" )\n",
    "        ax.plot( np.diff( ll / data.shape[ 0 ]  ) )\n",
    "#         ax.set_ylabel( r\"$\\Delta_i \\frac{1}{n} \\sum_{s=1}^n \\mathbb{E}_{z_s\\sim q_i} \\log p(x_s,z_s|\\Theta_i)$\" )\n",
    "        ax.set_ylabel( r\"$\\Delta_i \\frac{1}{n} \\sum_{s=1}^n \\log p(x_s|\\Theta_i)$\" )\n",
    "        ax.set_yscale( 'log' )\n",
    "## Plot the final estimates\n",
    "    if n_col > 0 :\n",
    "        show_data( clusters[-1], n = clusters.shape[1], n_col = n_col, cmap = plt.cm.spectral, interpolation = 'nearest' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIscellanea: animating the **EM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates an animation of successive iterations of a run of the **EM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def animate( theta, ll, pi = None, n_col = 10, n_row = 10, interval = 1, **kwargs ) :\n",
    "## Create a background\n",
    "    bg = arrange_flex( np.zeros_like( theta[ 0 ] ), n_col = n_col, n_row = n_row )\n",
    "## Compute log-likelihood differences and sanitize them.\n",
    "    ll_diff = np.maximum( np.diff( ll ), np.finfo(np.float).eps )\n",
    "    ll_diff[ ~np.isfinite( ll_diff ) ] = np.nan\n",
    "## Set up the figure, the axis, and the plot elements we want to animate\n",
    "    fig = plt.figure( figsize = ( 12, 12 ) )\n",
    "## Create three subplots  and sposition them specifically\n",
    "    if pi is None :\n",
    "        ax1, ax3, ax2 = fig.add_subplot( 311 ), fig.add_subplot( 312 ), fig.add_subplot( 313 )\n",
    "    else :\n",
    "        ax1, ax4 = fig.add_subplot( 411 ), fig.add_subplot( 412 )\n",
    "        ax3, ax2 = fig.add_subplot( 413 ), fig.add_subplot( 414 )\n",
    "## Initialize different ranges for the image aritsts\n",
    "    setup_canvas( ax1, n_row = n_row, n_col = n_col )\n",
    "    ax1.set_title( r\"Current estimate of the mixture components\" )\n",
    "    setup_canvas( ax2, n_row = n_row, n_col = n_col )\n",
    "    ax2.set_title( r\"Change between successive iterations\" )\n",
    "## Initialize geomtery for the delta log-likelihood plot.\n",
    "    ax3.set_xlim( -0.1, ll.shape[ 0 ] + 0.1 )\n",
    "    ax3.set_yscale( 'log' ) #; ax3.set_yticklabels( [ ] )\n",
    "    ax3.set_title( r\"Change between successive iterations of EM (log scale)\" )\n",
    "    ax3.set_ylabel( r\"$\\Delta_i \\sum_{s=1}^n \\log p(x_s|\\Theta_i)$\" )\n",
    "    ax3.set_ylim( np.nanmin( ll_diff ) * 0.9, np.nanmax( ll_diff ) * 1.1 )\n",
    "    ax3.grid( )\n",
    "## Setup a plot for prior probabilites\n",
    "    if pi is not None :\n",
    "        classes = 1 + np.arange( len( pi[ 0 ] ) )\n",
    "        ax4.set_xticks( classes )\n",
    "        ax4.set_ylim( 0.0, 1.0 )\n",
    "        ax4.set_title( r\"Current estimate of the mixture weights\" )\n",
    "        ba1 = ax4.bar( classes, pi[ 0 ], align = \"center\" )\n",
    "## Setup the artists\n",
    "    im1 = ax1.imshow( bg, vmin = +0.0, vmax = +1.0, **kwargs )\n",
    "    im2 = ax2.imshow( bg, vmin = -1.0, vmax = +1.0, **kwargs )\n",
    "    line1, = ax3.plot( [ ], linestyle = \"-\", color = 'blue' )\n",
    "## Animation function.  This is called sequentially\n",
    "    def update( i ) :\n",
    "## Compute the frame\n",
    "        frame = theta[ i ] - theta[ i-1 ] if i > 0 else theta[ 0 ]\n",
    "        frame /= np.max( np.abs( frame ) )\n",
    "## Draw frames on the image artists\n",
    "        im1.set_data( arrange_flex( theta[ i ], n_col = n_col, n_row = n_row ) )\n",
    "        im2.set_data( arrange_flex( frame,      n_col = n_col, n_row = n_row ) )\n",
    "        if i > 0 :\n",
    "## Show history on the line artist\n",
    "            line1.set_data( np.arange( i ), ll_diff[ :i ] )\n",
    "        if pi is not None :\n",
    "            [ b.set_height( h ) for b, h in zip( ba1, pi[ i ] ) ]\n",
    "            if i > 0 :\n",
    "                [ b.set_color( 'green' if h > p else 'red' ) for b, h, p in zip( ba1, pi[ i ], pi[ i-1 ] ) ]\n",
    "## Return an iterator of artists in this frame\n",
    "            return ( im1, im2, line1, ) + ba1\n",
    "        return im1, im2, line1,\n",
    "## Call the animator.\n",
    "    return animation.FuncAnimation( fig, update, frames = theta.shape[ 0 ], interval = interval, blit = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that produces (using **ffmpeg**) and embeds a video in HTML into IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make simple animations of the EM estimatora\n",
    "## http://jakevdp.github.io/blog/2013/05/12/embedding-matplotlib-animations/\n",
    "## http://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "def embed_video( anim ) :\n",
    "    VIDEO_TAG = \"\"\"<video controls autoplay muted loop><source src=\"data:video/x-m4v;base64,{0}\"\n",
    "        type=\"video/mp4\">Your browser does not support the video tag.</video>\"\"\"\n",
    "    plt.close( anim._fig )\n",
    "    if not hasattr( anim, '_encoded_video' ) :\n",
    "        ffmpeg_writer = animation.FFMpegWriter( )\n",
    "        with NamedTemporaryFile( suffix = '.mp4' ) as f:\n",
    "            anim.save( 'myanim.mp4', fps = 12, extra_args = [ '-vcodec', 'libx264' ] )# , writer = ffmpeg_writer )\n",
    "            video = open( 'myanim.mp4', \"rb\" ).read( )\n",
    "        anim._encoded_video = video.encode( \"base64\" )\n",
    "    return HTML( VIDEO_TAG.format( anim._encoded_video ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellanea: obtaining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to download the MNIST data from the SciKit repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False :\n",
    "## Fetch MNIST dataset from SciKit and create a local copy.\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "    mnist = fetch_mldata( \"MNIST original\", data_home = './data/' )\n",
    "    np.savez_compressed('./data/mnist/mnist_scikit.npz', data = mnist.data, labels = mnist.target )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or obtain the data from the provided CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False :\n",
    "## The procedure below loads the MNIST data from a comma-separated text file.\n",
    "    def load_mnist_from_csv( filename ) :\n",
    "## Read the CSV file\n",
    "        data = np.loadtxt( open( filename, \"rb\" ), dtype = np.short, delimiter = \",\", skiprows = 0 )\n",
    "## Peel off the lables\n",
    "        return data[:,1:], data[:,0]\n",
    "## Fetch the data from the provided CSV (!) files and save as a compressed data blob\n",
    "    data, labels = load_mnist_from_csv( \"./data/mnist/mnist_train.csv\" )\n",
    "    np.savez_compressed( './data/mnist/mnist_train.npz', labels = labels, data = data )\n",
    "    data, labels = load_mnist_from_csv( \"./data/mnist/mnist_test.csv\" )\n",
    "    np.savez_compressed( './data/mnist/mnist_test.npz', labels = labels, data = data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all load and binarize the training data using the value 127 as the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert( os.path.exists( './data/mnist/mnist_train.npz' ) )\n",
    "with np.load( './data/mnist/mnist_train.npz', 'r' ) as npz :\n",
    "    mnist_labels, mnist_data = npz[ 'labels' ], np.array( npz[ 'data' ] > 127, np.int )\n",
    "    \n",
    "assert( os.path.exists( './data/mnist/mnist_test.npz' ) )\n",
    "with np.load( './data/mnist/mnist_test.npz', 'r' ) as npz :\n",
    "    test_labels, test_data = npz[ 'labels' ], np.array( npz[ 'data' ] > 127, np.int )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Case : $K=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some 6s and 9s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Mask\n",
    "inx_sixes, inx_nines = np.where( mnist_labels == 6 )[ 0 ], np.where( mnist_labels == 9 )[ 0 ]\n",
    "## Extract\n",
    "sixes = mnist_data[ rand.choice( inx_sixes, 90, replace = False ) ]\n",
    "nines = mnist_data[ rand.choice( inx_nines, 90, replace = False ) ]\n",
    "## Show\n",
    "show_data( sixes, n = 45, n_col = 15, cmap = plt.cm.gray, interpolation = 'nearest' )\n",
    "show_data( nines, n = 45, n_col = 15, cmap = plt.cm.gray, interpolation = 'nearest' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They do indeed look quite distinct. Now collect them into a single dataset and estimate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = mnist_data[ np.append( inx_sixes, inx_nines ) ]\n",
    "clusters, ll = experiment( data, 2, 30 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimate deltas show that the **EM** algorithm's **E**-step actually transfers the unlikely observations between classes, as is expected by constructon of the algorithm.\n",
    "\n",
    "Judging by the plot below, it turns out that 30 iterations is more than enough for the **EM** to get meaninful estimates the class ideals, represented by the probability porduct-measure on $\\Omega^{28\\times 28}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize( data, clusters, ll )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well the **EM** algorithm performs on a model with more classes. But before that let's have a look at a random sample of the handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = np.arange( mnist_data.shape[ 0 ] )\n",
    "rand.shuffle( indices )\n",
    "show_data( mnist_data[ indices[:100] ] , n = 100, n_col = 10, cmap = plt.cm.gray, interpolation = 'nearest' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case : $K=10, 15, 20, 30, 60$ and $90$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original size of the trainig sample is too large to fit in these RAM banks :) That is why I had to limit the sample to a random subset of 2000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_sample = np.concatenate( tuple( [ rand.choice( np.where( mnist_labels == i )[ 0 ], size = 500 ) for i in range( 10 ) ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, train_labels = mnist_data[ sub_sample ], mnist_labels[ sub_sample ]\n",
    "# train_data, train_labels = mnist_data, mnist_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the procedure that perfoems **EM** algorithm and return the history of the parameter estimates as well as the dynamics of the log-likelihood lower bonud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters_10, ll_10 = experiment( train_data, 10, 50 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can clearly see, that $50$ iterations were not enough for the alogirithm to converge: though the changes are tiny, even on the log-scale, they are still unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize( train_data, clusters_10, ll_10, n_col = 10, plot_ll = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if changing $K$ does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_15, ll_15 = experiment( train_data, 15, 50, verbose = False, until_convergence = False )\n",
    "clusters_20, ll_20 = experiment( train_data, 20, 50, verbose = False, until_convergence = False )\n",
    "clusters_30, ll_30 = experiment( train_data, 30, 50, verbose = False, until_convergence = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For what values of $K$ was it possible to infer the templates of all digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize( train_data, clusters_15, ll_15, n_col = 10, plot_ll = False )\n",
    "visualize( train_data, clusters_20, ll_20, n_col = 10, plot_ll = False )\n",
    "visualize( train_data, clusters_30, ll_30, n_col = 10, plot_ll = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the model with more mixture components is more likely to produce \"templates\" for all digits. For larger $K$ this is indeed the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having run this algorithm for many times we are able to say that the digits $3$ and $8$, $4$ and $9$ and sometimes $5$ tend to be poorly separated. Furthermore due to there being many different handwritten variations of the same digit one should estimate a model with more classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned templates of the mixture components are clearly suboptimal: the procedure seems to get stuck at individual examples. This may happen for any $K$ and allowing for more iterations does not remedy this.\n",
    "Some possibilities do exist: add regularizers to the **E** step, that tie neighbouring pixel distributions together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clusters_60, ll_60 = experiment( train_data, 60, 450, verbose = False, until_convergence = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, increasing the number of iterations does not necessarily improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize( train_data, clusters_60, ll_60, n_col = 15 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by the plot of the log-likelihood, the fact that the **EM** is guaranteed to converge to local maxima and does so extremely fast, there was no need for more than 120-130 iterations. The chages in the log-likelihood around that number of iterations are of the order $10^{-4}$. Since we are working in finite precision arithmetic (double), the smallest precision is $\\approx 10^{-14}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the dynamics of the estimated of the **EM** iterations. You will have to ensure that **ffMPEG** is installed (**Windows**: and is on the PATH environment variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## If you want to see this animation ensure that ffmpeg is installed and uncomment the following lines.\n",
    "anim_60 = animate( clusters_60, ll_60, n_col = 15, n_row = 6,\n",
    "    interval = 1, cmap = plt.cm.hot, interpolation = 'nearest' )\n",
    "embed_video( anim_60 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter estimates of the **EM** stabilize pretty quickly. In fact most templates stabilize by iterations 100-120."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing $K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among many methods of model selection, let's use simple training sample fittness score, givne by the value of the log-likelihood. Becasue the models are nested with respect to the number of mixture components, one should expect the likelihood to be a non decreasing function of $K$ (on average dut to randomization of the initial parameter values).\n",
    "For large enough $K$ this method may lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Test model with K from 12 up to 42 with a step of 4\n",
    "classes = 12 + np.arange( 11, dtype = np.int ) * 3\n",
    "ll_hist = np.full( len( classes ), -np.inf, dtype = np.float )\n",
    "## Store parameters\n",
    "parameter_hist = list( )\n",
    "for i, K in enumerate( classes ) :\n",
    "## Run the experiment\n",
    "    c, l = experiment( train_data, K, 50, verbose = False, until_convergence = False )\n",
    "    ll_hist[ i ] = l[ -1 ]\n",
    "    parameter_hist.append( c[ -1 ] )\n",
    "## Visualize the final parameters\n",
    "    show_data( c[-1], n = K, n_col = 13, cmap = plt.cm.hot, interpolation = 'nearest' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the log-likelihood does not decrease with $K$ on average. Nevertheless the model with the highes likelihood turs out to have this many mixture components: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print classes[ np.argmax( ll_hist ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice, yet expected coincidence :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification: label assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a model amd run get the posterior mixture component probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clusters = parameter_hist[ np.argmax( ll_hist ) ] * 0.999 + 0.0005\n",
    "clusters = clusters_60[-1]\n",
    "c_s, q_sk, ll_s = classify( train_data, clusters )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a simple majority rule to automatically assign lables to templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "template_x_label_maj = np.empty( clusters.shape[ 0 ], np.int )\n",
    "for t in range( clusters.shape[ 0 ] ) :\n",
    "    l, f = np.unique( train_labels[ c_s == t ], return_counts = True)\n",
    "## This is too blunt an approach: it does not guarantee surjectivity of the mapping.\n",
    "    template_x_label_maj[ t ] = l[ np.argmax( f ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the labels $l$ to templates $t$ according to its score, based on the average of the top-$5$ log-likelihoods of observations with label $l$ and classfified with template $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## there are 10 labels and K templates\n",
    "label_cluster_score = np.full( ( clusters.shape[ 0 ], 10 ), -np.inf, np.float )\n",
    "## Loop over each template\n",
    "for t in range( clusters.shape[ 0 ] ) :\n",
    "## The selected templates are chosen according to max-aposteriori rule.\n",
    "    inx = np.where( c_s == t )[ 0 ]\n",
    "## Get the assigned lables and their frequencies\n",
    "    actual_labels = train_labels[ inx ]\n",
    "    l, f = np.unique( actual_labels, return_counts = True )\n",
    "## For each template and each associated label in the training set the\n",
    "##  score is average of the top-5 highest log-likelihoods.\n",
    "    label_cluster_score[ t, l ] = [ np.average( sorted(\n",
    "        ll_s[ inx[ actual_labels == a ] ].flatten( ), reverse = True )[ : 5 ] ) for a in l ]\n",
    "\n",
    "template_x_label_lik = np.argmax( label_cluster_score, axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the label assignments. Here are the templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_data( clusters, clusters.shape[ 0 ], 15,\n",
    "          cmap = plt.cm.spectral, interpolation = 'nearest' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the template-label mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print template_x_label_lik #.reshape((2,-1))\n",
    "print template_x_label_maj #.reshape((2,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the pictures of templates ordered according to their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_data( clusters[ np.argsort( template_x_label_lik ) ], clusters.shape[ 0 ],\n",
    "          15, cmap = plt.cm.spectral, interpolation = 'nearest' ) \n",
    "show_data( clusters[ np.argsort( template_x_label_maj ) ], clusters.shape[ 0 ],\n",
    "          15, cmap = plt.cm.spectral, interpolation = 'nearest' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification: test sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shall we try running the classifier on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run the classifier on the test data\n",
    "c_s, q_sk, ll_s = classify( test_data, clusters )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the best template for each test observation in some sub-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Show a sample of images and their templates\n",
    "sample = np.random.permutation( test_data.shape[ 0 ] )[:64]\n",
    "## Stack image and tis best template atop one another\n",
    "display_stack = np.empty( ( 2 * len( sample ), test_data.shape[ 1 ] ), dtype = np.float )\n",
    "display_stack[0::2] = test_data[ sample ] * q_sk[ sample, c_s[ sample ], np.newaxis ]\n",
    "display_stack[1::2] = clusters[ c_s[ sample ] ]\n",
    "## Display\n",
    "show_data( display_stack, n = display_stack.shape[ 0 ], n_col = 16,\n",
    "    transpose = False, cmap = plt.cm.spectral, interpolation = 'nearest' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digits are shown in pairs: each first digit is the test observation (colour is determined by the confidence of the classifier -- the whiter the higher), and each second -- is the best template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how accurate the classification was. Recall that the component assignment was done using \n",
    "$$ \\hat{t}_s = \\mathop{\\text{argmax}}_k p\\bigl( C_s = k \\, \\big\\vert\\, X = x_s\\bigr) \\,, $$\n",
    "i.e. maximum aposteriori rule.\n",
    "\n",
    "Then, with the classes assigned, labels were deduced based on either :\n",
    "* simple majority;\n",
    "* observations with top largest likelihoods in each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By accuracy I understand the following socre:\n",
    "$$ \\alpha = 1 - \\frac{1}{|\\text{TEST}|} \\sum_{s\\,\\in\\,\\text{TEST}} \\mathbf{1}\\{ l_s \\neq L(\\hat{t}_s)\\}\\,, $$\n",
    "where $l_s$ -- is the actual lablesof an observation $s$, $\\hat{t}_s$ -- is the inferred mixture component (class) of that observation, and $k\\mapsto L(k)$ is the component to label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Accuracy of likelihood based labelling: %.2f\" % (\n",
    "    100 * np.average( template_x_label_lik[ c_s ] == test_labels ), )\n",
    "print \"Accuracy of simple majority labelling: %.2f\" % (\n",
    "    100 * np.average( template_x_label_maj[ c_s ] == test_labels ), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, majority- and likelihood-based classification accuracies are close.\n",
    "\n",
    "Let's see which test observations the model considers an artefact and for which it cannot reliably assign a template: i.e. the posterior class probablity for these cases coincides with the prior. This happens when the likelihood of an observation is identical within each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Now display the test observations, which the model cold nor classify at all.\n",
    "bad_tests = np.where( np.isinf( ll_s ) )[ 0 ]\n",
    "show_data( test_data[ bad_tests ], n = max( len( bad_tests ), 10 ), n_col = 15, cmap = plt.cm.gray, interpolation = 'nearest' )\n",
    "# print q_sk[ bad_tests ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore everything below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let digress for a moment and consider the full model and create a video to see how the estimates are refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "( clusters_full, pi_full ), ll_full = experiment( data, 30, 1000, False, True, True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anim_full = animate( clusters_full, ll_full, pi = pi_full, n_col = 15, n_row = 2, interval = 1, cmap = plt.cm.hot, interpolation = 'nearest' )\n",
    "embed_video( anim_full )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random variable $X\\sim \\text{Beta}(\\alpha,\\beta)$ if the law of $X$ has density\n",
    "$$p(u) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} u^{\\alpha-1}(1-u)^{\\beta-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log p(X,Z|\\Theta) = \\sum_{s=1}^n \\log \\prod_{k=1}^K \\Bigl[ \\pi_k \n",
    "\\prod_{i=1}^N \\prod_{j=1}^M \n",
    "\\frac{\\Gamma(\\alpha_{kij}+\\beta_{kij})}{\\Gamma(\\alpha_{kij})\\Gamma(\\beta_{kij})} x_{sij}^{\\alpha_{kij}-1}(1-x_{sij})^{\\beta_{kij}-1} \\Bigr]^{1_{z_s = k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{E}_q \\log p(X,Z|\\Theta)\n",
    "&= \\sum_{k=1}^K \\sum_{s=1}^n q_{sk} \\log \\pi_k \\\\\n",
    "&+ \\sum_{k=1}^K \\sum_{i=1}^N \\sum_{j=1}^M \\sum_{s=1}^n q_{sk} \\bigl(\n",
    "    \\log \\Gamma(\\alpha_{kij}+\\beta_{kij}) - \\log \\Gamma(\\alpha_{kij}) - \\log \\Gamma(\\beta_{kij}) \\bigr) \\\\\n",
    "&+ \\sum_{k=1}^K \\sum_{i=1}^N \\sum_{j=1}^M \\sum_{s=1}^n q_{sk} \\bigl(\n",
    "    (\\alpha_{kij}-1) \\log x_{sij} + (\\beta_{kij}-1) \\log(1-x_{sij}) \\bigr) \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Derivative of a Gamma function does not seem to yeild analytically tracktable solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonparametric approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA( n_components = 50 )\n",
    "X_train_pca = pca.fit_transform( X_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = { 'bandwidth' : np.logspace( -1, 1, 20 ) }\n",
    "grid = GridSearchCV( KernelDensity( ), params )\n",
    "grid.fit( X_train_pca )\n",
    "print(\"best bandwidth: {0}\".format( grid.best_estimator_.bandwidth ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params\n",
    "kde = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = kde.sample( 100 )\n",
    "new_data = pca.inverse_transform( new_data )\n",
    "print new_data.shape\n",
    "\n",
    "plt.figure( figsize = ( 9, 9 ) )\n",
    "plt.imshow( arrange_flex( new_data ), cmap = plt.cm.gray, interpolation = 'nearest' )\n",
    "plt.show( )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
