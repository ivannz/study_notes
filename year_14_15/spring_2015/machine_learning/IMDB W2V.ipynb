{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, sklearn as sk, pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import matplotlib.pyplot as plt\n",
    "import time as tm, os, re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a preprocessing procedure that strips review of HTML tags and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __wordlist( text, stops = None ) :\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", bs( text ).get_text( ) )\n",
    "    words = letters_only.lower( ).split()\n",
    "    if stops is not None :\n",
    "        return [ w for w in words if not w in stops ]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the review text into a list of sentences, represented by lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __sentences( text, tokenizer = None, stops = None ):\n",
    "    raw_sentences = tokenizer.tokenize( text.strip( ) )\n",
    "    return [ __wordlist( s, stops = stops )\n",
    "         for s in raw_sentences if len( s ) > 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip the review of all HTML tags and punctuation to get just a sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text( text, stops = None ) :\n",
    "    words = __wordlist( text, stops = stops )\n",
    "    return( \" \".join( [ w for w in words ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load NL data, necessary for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nl_sw\n",
    "import nltk.data\n",
    "\n",
    "english_stopwords = set( nl_sw.words( \"english\" ) )\n",
    "english_tokenizer = nltk.data.load( \"tokenizers/punkt/english.pickle\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv( './data/imdb/labeledTrainData.tsv',\n",
    "    sep = \"\\t\", header = 0, quoting = 3, encoding=\"utf-8\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_reviews = [ clean_text( r, english_stopwords ) for r in train_data.review ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct document features unsing the bag-of-words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Use the Bag of Words model to extract features.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## SciKit vectorizer does exactly that: it constructs the set of most\n",
    "##  frequently used words in the entire collection (fit), and then it\n",
    "##  transforms each document into a frequency vector.\n",
    "vectorizer = CountVectorizer( analyzer = \"word\", tokenizer = None,\n",
    "    preprocessor = None, stop_words = None, max_features = 5000 )\n",
    "\n",
    "## The returned feature matrix is sparse. \n",
    "train_data_features = vectorizer.fit_transform( clean_train_reviews )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a RandomForest ensemble classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## Random forest constructs a tree classifiers on random (smaller) samples\n",
    "##  of the data. This create a sort of bootstrapped distributio of trees\n",
    "##  which are then used in ensemble, to make a classifier with less variance.\n",
    "forest = RandomForestClassifier( n_estimators = 10, verbose = 0, max_depth = None )\n",
    "forest = forest.fit( train_data_features, train_data.sentiment )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_train_sentiment = forest.predict( train_data_features )\n",
    "\n",
    "np.where( predicted_train_sentiment != train_data.sentiment )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the classifier on the left-out unlabelled test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv( './data/imdb/testData.tsv',\n",
    "    sep = \"\\t\", header = 0, quoting = 3, encoding=\"utf-8\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the reviews, convert them to features and apply the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_test_reviews = [ clean_text( r, english_stopwords ) for r in test_data.review ]\n",
    "test_data_features = vectorizer.transform( clean_test_reviews )\n",
    "predicted_test_sentiment = forest.predict( test_data_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_output = pd.DataFrame( data={ \"id\": test_data.id,\n",
    "    \"sentiment\": predicted_test_sentiment, \"review\": test_data.review } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, sklearn as sk, pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import matplotlib.pyplot as plt\n",
    "import time as tm, os, re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATAPATH = os.path.realpath( os.path.join( \".\", \"data\", \"imdb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for Natural Language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nl_sw\n",
    "import nltk.data\n",
    "\n",
    "english_stopwords = set( nl_sw.words( \"english\" ) )\n",
    "english_tokenizer = nltk.data.load( \"tokenizers/punkt/english.pickle\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load both labelled and unlabelled train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data from files \n",
    "unlabelled_train_data = pd.read_csv( os.path.join( DATAPATH, 'unlabeledTrainData.tsv' ), sep = \"\\t\", header = 0, quoting = 3, encoding=\"utf-8\" )\n",
    "labelled_train_data = pd.read_csv( os.path.join( DATAPATH, 'labeledTrainData.tsv' ), sep = \"\\t\", header = 0, quoting = 3, encoding=\"utf-8\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __wordlist( text, stops = None ) :\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", bs( text ).get_text( ) )\n",
    "    words = letters_only.lower( ).split()\n",
    "    if stops is not None :\n",
    "        return [ w for w in words if not w in stops ]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut reviews into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __sentences( text, tokenizer = None, stops = None ):\n",
    "    raw_sentences = tokenizer.tokenize( text.strip( ) )\n",
    "    return [ __wordlist( s, stops = stops )\n",
    "         for s in raw_sentences if len( s ) > 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut each review into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sentences = list( )\n",
    "if not os.path.exists( os.path.join( DATAPATH, 'imdb_review_train_sentences.txt' ) ) :\n",
    "    print \"Cutting reviews into sentences.\"\n",
    "## Begin time\n",
    "    tock = tm.time( )\n",
    "## Convert reviews into sentences\n",
    "    print \"Labelled train dataset...\"\n",
    "    for r in labelled_train_data.review :\n",
    "        train_sentences.extend( __sentences( r, english_tokenizer, stops = None ) )\n",
    "    print \"Unabelled train dataset...\"\n",
    "    for r in unlabelled_train_data.review :\n",
    "        train_sentences.extend( __sentences( r, english_tokenizer, stops = None ) )\n",
    "## End time\n",
    "    tick = tm.time( )\n",
    "## Report\n",
    "    print \"Preprocessing took %.1f sec.\" % ( tick - tock, )\n",
    "    print \"Caching...\"\n",
    "## Store the processed sentences in a UTF-8 text file\n",
    "    with open( os.path.join( DATAPATH, 'imdb_review_train_sentences.txt' ), 'wb' ) as cache :\n",
    "        cache.writelines( \"\\t\".join( s ).encode( 'utf8' ) + \"\\n\" for s in train_sentences )\n",
    "## Final time\n",
    "    tock = tm.time( )\n",
    "else :\n",
    "    print \"Loading cached sentences...\"\n",
    "## Begin time\n",
    "    tick = tm.time( )\n",
    "    with open( os.path.join( DATAPATH, 'imdb_review_train_sentences.txt' ), 'rb' ) as cache :\n",
    "        train_sentences.extend( l.decode( 'utf8' ).strip( ).split( '\\t' ) for l in cache.readlines( ) )\n",
    "## End time\n",
    "    tock = tm.time( )\n",
    "## Report\n",
    "print \"Loaded sentences in %.1f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vectore representation of words using word2vec in gensim module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim.models, time as tm\n",
    "# Initialize the model\n",
    "model = gensim.models.Word2Vec(\n",
    "    workers = 4,           # Number of threads to run in parallel\n",
    "    size = 400,            # Word vector dimensionality\n",
    "    min_count = 30,        # Minimum word count for pruning the internal dictionary\n",
    "    window = 12,           # Context sindow size\n",
    "    sample = 1e-3 )        # Downsample setting for frequent words\n",
    "\n",
    "model_cache_name = \"W2V_%d-%d-%d.mdl\" % ( model.layer1_size, model.min_count, model.window , )\n",
    "if not os.path.exists( os.path.join( DATAPATH, model_cache_name ) ) :\n",
    "## Begin time\n",
    "    tock = tm.time( )\n",
    "## First pass -- building the vocabulary\n",
    "    model.build_vocab( train_sentences )\n",
    "## Second pass --  training the neural net\n",
    "    model.train( train_sentences )\n",
    "## End time\n",
    "    tick = tm.time( )\n",
    "## Report\n",
    "    print \"Training word2vec took %.1f sec.\" % ( tick - tock, )\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims( replace = True )\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model.save( os.path.join( DATAPATH, model_cache_name ) )\n",
    "## End time\n",
    "    tock = tm.time( )\n",
    "else :\n",
    "## Begin time\n",
    "    tick = tm.time( )\n",
    "## Load the model from the blob\n",
    "    model = gensim.models.Word2Vec.load( os.path.join( DATAPATH, model_cache_name ) )\n",
    "## End time\n",
    "    tock = tm.time( )\n",
    "## Report\n",
    "print \"Model loaded in %.1f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the trained model performs over the google analogical proportions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Testing Google's analogical proportions...\"\n",
    "tick = tm.time( )\n",
    "## test model accuracy against the Google dataset\n",
    "google_dataset_accuracy = model.accuracy( os.path.join( DATAPATH, 'questions-words.txt' ) )\n",
    "tock = tm.time( )\n",
    "print \"Completed in %.1f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"CORRECT\\tTOTAL\\tSECTION\"\n",
    "for s in google_dataset_accuracy :\n",
    "    total = len( s['correct'] ) + len( s['incorrect'] )\n",
    "    print \"%4d\\t%5d\\t%s.\" % ( len( s['correct'] ), total, s['section'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why thiese results are so poor is that the reviews database is not a language corpus, it is does not provide enough coverage of the natural language variety (English), it is topically biased, and, since it is mainly user generated content, it is stylistically more colloquial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print model.most_similar( \"queen\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.most_similar( positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
    "print model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = np.asarray( model.vocab.keys(), dtype = np.str)\n",
    "# vocab[ np.argmax( np.abs(model.syn0), axis = 0 ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
