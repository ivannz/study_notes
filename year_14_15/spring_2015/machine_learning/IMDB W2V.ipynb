{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, sklearn as sk, pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import matplotlib.pyplot as plt\n",
    "import time as tm, os, regex as re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATAPATH = os.path.realpath( os.path.join( \".\", \"data\", \"imdb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary NL modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] <urlopen error [Errno 8] nodename nor servname provided,\n",
      "[nltk_data]     or not known>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f4cbebde3b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m\"stopwords\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wordnet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wordnet_ic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"punkt\"\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "assert( nltk.download( [ \"stopwords\", \"wordnet\", \"wordnet_ic\", \"punkt\" ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for Natural Language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nl_sw\n",
    "import nltk.data\n",
    "\n",
    "english_stopwords = set( nl_sw.words( \"english\" ) )\n",
    "english_tokenizer = nltk.data.load( \"tokenizers/punkt/english.pickle\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load both labelled and unlabelled train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data from files \n",
    "unlabelled_train_data = pd.read_csv( os.path.join( DATAPATH, 'unlabeledTrainData.tsv' ),\n",
    "                                        sep = \"\\t\", header = 0, quoting = 3, encoding=\"utf-8\" )\n",
    "labelled_train_data = pd.read_csv( os.path.join( DATAPATH, 'labeledTrainData.tsv' ),\n",
    "                                        sep = \"\\t\", header = 0, quoting = 3, encoding=\"utf-8\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __wordlist( text, stops = None ) :\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", bs( text ).get_text( ) )\n",
    "    words = letters_only.lower( ).split()\n",
    "    if stops is not None :\n",
    "        return [ w for w in words if not w in stops ]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut reviews into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __sentences( text, tokenizer = None, stops = None ):\n",
    "    raw_sentences = tokenizer.tokenize( text.strip( ) )\n",
    "    return [ __wordlist( s, stops = stops )\n",
    "         for s in raw_sentences if len( s ) > 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut each review into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached sentences...\n",
      "Loaded sentences in 279.7 sec.\n"
     ]
    }
   ],
   "source": [
    "train_sentences = list( )\n",
    "if not os.path.exists( os.path.join( DATAPATH, 'imdb_review_train_sentences.txt' ) ) :\n",
    "    print \"Cutting reviews into sentences.\"\n",
    "## Begin time\n",
    "    tock = tm.time( )\n",
    "## Convert reviews into sentences\n",
    "    print \"Labelled train dataset...\"\n",
    "    for r in labelled_train_data.review :\n",
    "        train_sentences.extend( __sentences( r, english_tokenizer, stops = None ) )\n",
    "    print \"Unabelled train dataset...\"\n",
    "    for r in unlabelled_train_data.review :\n",
    "        train_sentences.extend( __sentences( r, english_tokenizer, stops = None ) )\n",
    "## End time\n",
    "    tick = tm.time( )\n",
    "## Report\n",
    "    print \"Preprocessing took %.1f sec.\" % ( tick - tock, )\n",
    "    print \"Caching...\"\n",
    "## Store the processed sentences in a UTF-8 text file\n",
    "    with open( os.path.join( DATAPATH, 'imdb_review_train_sentences.txt' ), 'wb' ) as cache :\n",
    "        cache.writelines( \"\\t\".join( s ).encode( 'utf8' ) + \"\\n\" for s in train_sentences )\n",
    "## Final time\n",
    "    tock = tm.time( )\n",
    "else :\n",
    "    print \"Loading cached sentences...\"\n",
    "## Begin time\n",
    "    tick = tm.time( )\n",
    "    with open( os.path.join( DATAPATH, 'imdb_review_train_sentences.txt' ), 'rb' ) as cache :\n",
    "        train_sentences.extend( l.decode( 'utf8' ).strip( ).split( '\\t' ) for l in cache.readlines( ) )\n",
    "## End time\n",
    "    tock = tm.time( )\n",
    "## Report\n",
    "print \"Loaded sentences in %.1f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vector representation of words using word2vec in gensim module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 9.2 sec.\n"
     ]
    }
   ],
   "source": [
    "import gensim.models, time as tm\n",
    "# Initialize the model\n",
    "model = gensim.models.Word2Vec(\n",
    "    workers = 7,           # Number of threads to run in parallel\n",
    "    size = 300,            # Word vector dimensionality\n",
    "    min_count = 40,        # Minimum word count for pruning the internal dictionary\n",
    "    window = 10,           # Context sindow size\n",
    "    sample = 1e-3 )        # Downsample setting for frequent words\n",
    "\n",
    "model_cache_name = \"W2V_%d-%d-%d.mdl\" % ( model.layer1_size, model.min_count, model.window , )\n",
    "if not os.path.exists( os.path.join( DATAPATH, model_cache_name ) ) :\n",
    "## Begin time\n",
    "    tock = tm.time( )\n",
    "## First pass -- building the vocabulary\n",
    "    model.build_vocab( train_sentences )\n",
    "## Second pass --  training the neural net\n",
    "    model.train( train_sentences )\n",
    "## End time\n",
    "    tick = tm.time( )\n",
    "## Report\n",
    "    print \"Training word2vec took %.1f sec.\" % ( tick - tock, )\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims( replace = True )\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model.save( os.path.join( DATAPATH, model_cache_name ) )\n",
    "## End time\n",
    "    tock = tm.time( )\n",
    "else :\n",
    "## Begin time\n",
    "    tick = tm.time( )\n",
    "## Load the model from the blob\n",
    "    model = gensim.models.Word2Vec.load( os.path.join( DATAPATH, model_cache_name ) )\n",
    "## End time\n",
    "    tock = tm.time( )\n",
    "## Report\n",
    "print \"Model loaded in %.1f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the trained model performs over the google analogical proportions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Testing Google's analogical proportions...\"\n",
    "tick = tm.time( )\n",
    "## test model accuracy against the Google dataset\n",
    "google_dataset_accuracy = model.accuracy( os.path.join( DATAPATH, 'questions-words.txt' ) )\n",
    "tock = tm.time( )\n",
    "print \"Completed in %.1f sec.\" % ( tock - tick, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"####\\tCORRECT\\tTOTAL\\tSECTION\"\n",
    "for i, s in enumerate( google_dataset_accuracy, 0 ) :\n",
    "    total = len( s['correct'] ) + len( s['incorrect'] )\n",
    "    print \"%4d\\t%4d\\t%5d\\t%s.\" % ( i, len( s['correct'] ), total, s['section'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why thiese results are so poor is that the reviews database is not a language corpus, it is does not provide enough coverage of the natural language variety (English), it is topically biased, and, since it is mainly user generated content, it is stylistically more colloquial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how exactly the IMDB reviews fails as a corpus for the Google's analogical proportion test :\n",
    "word **A** is to **B** as **C** is to **D** <!-- B / A = C / D -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for A, B, C, expected in google_dataset_accuracy[1][\"incorrect\"][:10] :\n",
    "    predictions = [ p for p, s in model.most_similar( positive=[ B, C ], negative=[ A ], topn = 5 ) ]\n",
    "    if expected not in predictions :\n",
    "        print \"%s - %s : %s - %s \" % ( A,B,C, expected, ) , predictions\n",
    "    else :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not unexpectedly, the reviews do not cover geographical terms relations well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar( \"king\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most similar terms to \"king\" are the name of the Dinsey animation \"Lion King\", a fictional beast \"King Kong\" and the author of many a horror and supertnatural fiction novel \"Stephen King\". This document set is no good for general language semantics testing. Aladdin is no king."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar( \"gothic\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One would expect to see at least one reference to architecutral style, but the reviews are mostly focused on genres and movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"west\\t - \", [ d for d, s in model.most_similar( [ \"south\", \"west\" ], [ \"north\" ], topn = 5 ) ]\n",
    "print \"east\\t - \", [ d for d, s in model.most_similar( [ \"south\", \"east\" ], [ \"north\" ], topn = 5 ) ]\n",
    "print \"north\\t - \", [ d for d, s in model.most_similar( [ \"west\", \"north\" ], [ \"east\" ], topn = 5 ) ]\n",
    "print \"south\\t - \", [ d for d, s in model.most_similar( [ \"west\", \"south\" ], [ \"east\" ], topn = 5 ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model, trained on IMDB reviews cannot correctly identify one cardinal direction out of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.doesnt_match(\"sea ocean lake river\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.doesnt_match( \"good bad ugly horrible\".split( ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.most_similar( positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
    "print model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = np.asarray( model.vocab.keys(), dtype = np.str)\n",
    "# vocab[ np.argmax( np.abs(model.syn0), axis = 0 ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a lemmatizer based on WordNet relationship data and sentences of reivews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer( )\n",
    "def __lemmatize( text, lemmatizer, tokenizer ) :\n",
    "    processed_text = re.sub( \"\\\"\", \"\", bs( text ).get_text( ) )\n",
    "    raw_sentences = tokenizer.tokenize( processed_text.strip( ).lower( ) )\n",
    "    return [ lemmatizer.lemmatize( w )\n",
    "        for s in raw_sentences for w in re.sub( r\"\\p{Punctuation}+\", \" \", s ).split( ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect lemmatized reviews into one \"corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatized_reviews = list( )\n",
    "print \"Cutting reviews into sentences.\"\n",
    "## Begin time\n",
    "tock = tm.time( )\n",
    "## Convert reviews into sentences\n",
    "print \"Labelled train dataset...\"\n",
    "for r in labelled_train_data.review :\n",
    "    lemmatized_reviews.append( __lemmatize( r, wnl, english_tokenizer ) )\n",
    "print \"Unabelled train dataset...\"\n",
    "for r in unlabelled_train_data.review :\n",
    "    lemmatized_reviews.append( __lemmatize( r, wnl, english_tokenizer ) )\n",
    "## End time\n",
    "tick = tm.time( )\n",
    "## Report\n",
    "print \"Preprocessing took %.1f sec.\" % ( tick - tock, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import gensim toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the term vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists( os.path.join( DATAPATH, 'LDA_vocabulary.dct' ) ) :\n",
    "    vocabulary = corpora.Dictionary( lemmatized_reviews )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ditch too frequent or too rare terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists( os.path.join( DATAPATH, 'LDA_vocabulary.dct' ) ) :\n",
    "    vocabulary.filter_extremes( no_below = 5, no_above = 0.5, keep_n = None )\n",
    "    vocabulary.save( os.path.join( DATAPATH, 'LDA_vocabulary.dct' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the document words into word ID vectors: bag-of-terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [ vocabulary.doc2bow( text ) for text in lemmatized_reviews ]\n",
    "corpora.MmCorpus.serialize( os.path.join( DATAPATH, 'LDA_bow.mm' ), corpus ) # store on disc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Latent Dirichlet Allocation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Begin time\n",
    "tick = tm.time( )\n",
    "## Fit the LDA model\n",
    "model = models.ldamodel.LdaModel(\n",
    "    corpus, id2word = vocabulary, num_topics = 100, chunksize = 50, update_every = 1, passes = 2 )\n",
    "## End time\n",
    "tock = tm.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Estimatiing LDA model took %.3f sec.\"%( tock - tick, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
