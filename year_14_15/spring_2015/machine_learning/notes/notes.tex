\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\pr}{\mathbb{P}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Machine Learning}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section{Lecture \# 4} % (fold)
\label{sec:lecture_4}

\rus{Дмитрий Игоревич Игнатов}

Descision tree as classification tools.

Is a tree with nodes, labelled by features, conditions or rules. The leaves are the classes.

The learning sample may be unbalanced.

There is no point in identical conditions within the same branch: at nodes $u,v$ in the tree-order.

Main definitions.
The connection of decision tree with boolean functions.

Can a tree be represented in a disjunctive normal form?

Conjuncts are identified with the root-leaf paths, with a \emph{True} leaf.

Decision tree construction.
\begin{itemize}
	\item Pick a feature $Q$.
	\item For each value $i$ of $Q$.
	\begin{itemize}
		\item recursively construct the decision tree base of the learning sub-sample with $Q=i$.
	\end{itemize}
\end{itemize}

The question is how the next attribute should be chosen.

Suppose the we need to define a measure $\nu$ of uncertainty of events:\begin{itemize}
	\item If $A$ is certain to occur ($\pr(A)=1$), then $\nu(A)=0$;
	\item If $A$ is more likely than $B$, then is reasonable to expect the uncertainty of $B$ be higher than that of $A$. Thus $\nu$ should be anti-monotonous;
	\item if $A\perp B$, then the uncertainty of one event is unaffected by the uncertainty of the other. Thus it is reasonable to expect that their simultaneous uncertainty must be pooled: $\nu(A\cap B)= \nu(A)+\nu(B)$.
\end{itemize}

It is convenient to define $\nu$ as a composition of some $h$ with $\pr$: $\nu = h\circ \pr$.

Thus we arrive at a concept of entropy: if $Q<<\mu$ and $Q=\int q d\mu$, then $H = \int \log \frac{1}{q} q d\mu = - \int \log q dQ$.

In the discrete case the integral degenerates into $H = \sum_k \frac{1}{p_k} p_k$.

For a binary variable:
\[H(\theta) = - \theta \log \theta - (1-\theta) \log (1-\theta)\]
The entropy as a function of $\theta$ resembles a bell with zeros at $0,1$. Use l'H\^opital's rule.

\textbf{Idea}: Select the successive feature $Q$ to be as informative as possible.

Suppose $A$ elements with a property $S$ are classified by a feature $Q$ with finite number of values then the \textbf{information gain} is
\[\text{Gain} = H(A,S) - \sum_{q\in Q} \frac{\abs{A_q}}{\abs{A}} H(A_q,S)\]
where $A_q$ is the partition by $Q$: $A_q = \obj{\induc{\omega \in A} Q(\omega)=q }$.

The entropy in this case is defined as 
\[H(X,S) = - \sum_{s\in S} \frac{\abs{X_s}}{\abs{X}} \log \frac{\abs{X_s}}{\abs{X}}\]

The higher the gain, the more informative an attribute is.

The impurity of a class might be defined as
\[\dot{p} = \frac{p_+}{p_++p_-} = \frac{N_+}{N_+ + N_-}\]
in the learning sample with $+$ and $-$ examples.

the impurity measure is expected to be symmetric with respect to the classes $+$ and $-$.

Consider using $p_+(1-p_-)$ and $p_-(1-p_+)$ pooled together for symmetry.

Ross Queenlain ID3 algorithm (C4.5).
\begin{itemize}
	\item Create a root;
	\item If $S$ hold for all $A$, then set the node to $1$ and leave;
	\item If $S$ fails to hold for all $A$, then set the node to $0$ and leave;
	\item If $\mathcal{Q} = \emptyset$ then \begin{itemize}
		\item if $S$ it true for a half or more examples in $A$, then set the root to $1$ and leave;
		\item if $S$ it true for a half or more examples in $A$, then set the root to $1$ and leave;
	\end{itemize}
	\item IF $\mathcal{Q}\neq\emptyset$ then choose feature $Q\in \mathcal{Q}$ with the highest information gain $\text{Gain}(A, Q)$.
	\item ... MISSING LOC;
\end{itemize}

Gini inequality coefficient.
Gini index

A feature with distinct value for each example has the greatest information gain. Indeed, the entropy of a set consisting of an individual example with respect to the target variable is zero since $0\log 0 = 1 \log 1 = 0$.
\[\text{SplitInfo}(A,S) = H(A,S)\]

Branch trimming:
\[\text{GainRatio}(A,S) = \frac{\text{Gain}(A,S)}{\text{SplitInfo}(A,S)}\]


% Если зенит просто как случайная величина выигрывает/проигрывает с вероятностью $p$ то можно и построить такой ```случайный классификатор''.

One-node (one-rule) degenerate decision trees are called \eng{decision stumps}.

\begin{align*}
	\text{Gini}(D,T) &= 1 - \sum_{k=1}^T \brac{\frac{D_k}{D}}^2 \\
	H(D,T) &= - \sum_{k=1}^T \frac{D_k}{D} \log \frac{D_k}{D} \\
	\text{Gain}(D,T,A) &= H(D,T) - \sum_{k=1}^T \frac{D_k}{D} H(D_k,T)
\end{align*}



% section lecture_4 (end)

\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

Associative rules mining.

A ocntext is an object-attribute table: $\brac{G,M,I}$, with $I\subseteq G\times M$.
An Associative rule is a a pair $A\to B$ with $A,B\subseteq M$ (and sometimes $A\cap B = \emptyset$).

\subsection{A brief history of FCA} % (fold)
\label{sub:a_brief_history_of_fca}

\begin{itemize}
	\item Aristotle. An extnet (the objects) and an intent (the set of common attributes);
	\item Galois. I his reserach he used the what now is know as a Galois connection: a pair of maps $\psi:P\to Q$ and $\phi:Q\to P$ between ordered sets $(P,\leq)$ and $(Q,\leq)$ with the \textbf{GALOIS} property: for any $a\in P$ and $b\in Q$ it
	\[a\leq \phi(b) \Leftrightarrow b\leq \psi(a)\]
	\item Lattice structure; Birkhgoff proposed to compare incomparable elements of a poset $(P,\leq)$ by using the least upper and the greatest lower bounds $a\vee b$ and $a\wedge b$;
\end{itemize}

% subsection a_brief_history_of_fca (end)

\subsection{A recap of the FCA} % (fold)
\label{sub:a_recap_of_the_fca}

The intent of $A\subseteq G$ is the set of attributes
\[A' \defn \obj{\induc{m\in M}\,\forall g\in A (g,m)\in I} = \bigcap_{a\in A} a'\]
Similarly the extent of $B\subseteq M$ is the collection of objects
\[B' \defn \obj{\induc{g\in G}\,\forall m\in B (g,m)\in I} = \bigcap_{b\in B} b'\]

Also the ${(\cdot)}'$ operator has the following property:
\[\brac{\bigcup_{j\in J}A_j}' = \bigcap_{j\in J}A_j'\]

A formal concept is a pair $(A,B)$ such that $A'=B$ and $B'=A$ -- stability. 

% subsection a_recap_of_the_fca (end)

An associative rule is an intent (a formal intent, itemset). The support of an associative rule $A\to B$ is
\[\text{supp}(A\to B) = \frac{\abs{\brac{A\cup B}'}}{\abs{G}}\]
The confidence of a rule is 
\[\text{conf}(A\to B) = \frac{\abs{\brac{A\cup B}'}}{\abs{A'}}\]

The setting of the problem
Find all associative rules of the context with support and confidence higher than some pre-set threshold value.

An implication of the context is an associative rule $A\to B$ with $A'\subseteq B'$, which implies that $\text{conf}(A\to B) = 1$.

The problem is that $A\to B$ is an implication even if $A'=B'=\emptyset$.
For example $bc\to ad$ in the context $\abs{\begin{matrix}\times & \times & \cdot & \cdot\\ \cdot & \cdot & \times & \times\end{matrix}}$

\subsection{The Apriori algorithm} % (fold)
\label{sub:the_apriori_algorithm}
The algorithm is based on the following property of support:
\noindent\textbf{anti-monotonicity}\hfill\\
If $A,B\subseteq M$ and $A\subseteq B$ then $\text{supp}(B) \leq \text{supp}(A)$

\textbf{Apriori}
\begin{description}
	\item[input] A context $\brac{G,M,I}$, the minimum confidence $\alpha$ and support $\beta$ thresholds;
	\item[output] All frequent intents (itemsets);
	\item $C_i\leftarrow \obj{\text{1-itemsets}}$;
	\item[while] $C_i\neq \emptyset$;\begin{itemize}
		\item SupportCount($C_i$);
		\item $F_i \leftarrow \obj{\induc{f\in C_i}\,\text{supp}(f)\geq \beta}$;
		\item $C_{i+1}\leftarrow \text{AprioriGen}\brac{F_i}$.
	\end{itemize}
	\item[Finalization] $I_F \leftarrow \bigcup_i F_i$;
\end{description}

Where the service procedure is given by:
\textbf{AprioriGen}
\begin{description}
	\item[input] $F$ -- frequent itemsets of size $i$;
	\item[output] $C_{i+1}$ -- the potential candidate itemsets;
	\item[SQL-step]\hfill\\
		insert into $C_{i+1}$ select $\brac{p_k}_{k=1}^i$, $q_i$ from $F_ip$, $F_iq$ where $\brac{p_k}_{k=1}^{i-1} = \brac{q_k}_{k=1}^{i-1}$ and $p_i\leq q_i$;
	\item[for each] $c\in C_{i+1}$;\begin{description}
		\item $S\leftarrow (i-1)\text{-element subsets of }c$;
		\item[for each] $s\in S$;
		\item if $s\notin F_i$, then $C_{i+1}\leftarrow C_{i+1}\setminus \obj{s}$.
	\end{description}
\end{description}

Let $F$ be a frequent itemset. A rule is satisfactory if 
\[\text{conf}(f\to F\setminus \obj{f}) = \frac{\text{supp}(F)}{\text{supp}(f)} \geq \alpha\]

The confidence of $f\to F\setminus f$ is minimal when $\text{supp}(f)$ is maximal.

The the confidence is minimal when the Antecedent has only one attribute. Supersets of this attribute have lesser support and thus higher confidence.

The associative rules are extracted a frequent itemset $F$ recursively:
\begin{itemize}
	\item starting from $1$-itemset of $F$ antecedent which satisfies the $\alpha$ and $\beta$ thresholds;
	\item check its supersets in $F$; If the confidence requirement is met, then the association rule $f\to F\setminus f$ is kept.
\end{itemize}

In data mining $A$ is closed if $\text{supp}(A)\leq \alpha$ and there exists no $A\subseteq B$ with $\text{supp}(B)=\text{supp}(A)$.


% subsection the_apriori_algorithm (end)

% Associative rule document mining.

% section lecture_5 (end)

\section{Lecture \# 6} % (fold)
\label{sec:lecture_6}

For a graph $G=(V,E)$ computing the sequence of $k$-cores enables detection of communities.

Graph \textbf{cut}:
\begin{align*}
	\text{cut}(A,B) &= \sum_{u\in A} \sum_{v\in B} \omega(u, v)\\
	\text{assoc}(A,V) &= \sum_{v\in A} \sum_{w\in V} \omega(v,w)
\end{align*}
Normalised cut criterion
\[\text{Ncut}(A,B) = \frac{\text{cut}(A,B)}{\text{assoc}(A,V)} + \frac{\text{cut}(A,B)}{\text{assoc}(B,V)}\]

Spectral cut of the graph

Suppose there the nodes are totally ordered $(V,\leq)$ (for convenience). Then the optimal cut criterion: 
\[ \text{cut} = \frac{1}{4}\sum_{j<i} {(p_i - p_j)}^2 \omega(i,j) \]
where $\brac{p_v}_{v\in V}\in \obj{-1,+1}^V$ is the vector of binary partition association.

Let's relax the problem by extending the space of the association vector $\brac{p_v}_{v\in V}\in \clo{-1,1}$
and reduce this problem a problem of quadratic optimisation:
\[E = \frac{1}{4}\sum_{i<j} {(x_i-x_j)}^2 \omega_{ij} = \frac{x'Lx}{4}\]
where $L$ is defined below.

Observe that
\begin{align*}
	2\sum_{i<j} {(x_i-x_j)}^2 \omega_{ij} &= \sum_{i\neq j} {(x_i-x_j)}^2 \omega_{ij} + \sum_{i=j} {(x_i-x_j)}^2 \omega_{ij} \\
	&= \sum_i\sum_j (x_i^2 + x_j^2 - 2x_ix_j) \omega_{ij} = 2 \sum_i\sum_j x_i^2 - 2\sum_i\sum_j x_i \omega_{ij} x_j \\
	&= 2\sum_i x_i^2 \sum_j \omega_{ij} - 2\sum_i\sum_j x_i \omega_{ij} x_j \\
	&= 2 x' D x - 2 x' W x
\end{align*}
where $D\defn \text{diag}\brac{{(\sum_j w_{ij})}_{i\in V}}$ and $W = \brac{\omega_{ij}}_{i,j\in V}$.

Thus the energy functional is reduced to $E = \frac{1}{4} x' ( D - W ) x $.

There are constraints on the optimal vectors: $x'x = N$ and $x'e = 0$. The last restriction means that the optimal distribution ``assigns'' at least one element of each cluster.

The Lagrangian is 
\[\mathcal{L} = x'Lx - \lambda ( x'x - N )\]
and the first order conditions are given by
\begin{align*}
	\frac{\partial L}{\partial x} \quod:\quod 2Lx - 2\lambda x = \mathbf{0}\\
	\frac{\partial L}{\partial \lambda} \quod:\quod x'x - N = 0
\end{align*} 

...

For a rectangular matrix use the following incidence matrix
\[A = \left(\begin{matrix}
	0&W\\
	W'&0\\
\end{matrix}\right)\]


% section lecture_6 (end)

\section{Lecture \# 7} % (fold)
\label{sec:lecture_7}

\subsection{Page rank} % (fold)
\label{sub:page_rank}

The main idea is to use links to determine the the importance.
To rig the search engine spammers used links farms and other techniques.

Assess the importance of a page based on links.
\noindent \textbf{Recursive definition}\hfill\\
Measure relative importance of a web page. A page is considered \textbf{important} of many other \textbf{important} pages link to it.

Each link's $\omega = (s,t)\in E$ vote $i_\omega \propto i_s$ -- is proportional to the importance of its source.

Flow model -- a matrix formulation

Suppose $M$ is a columns stochastic matrix $\brac{M_{ts}}_{t,s\in V}$ corresponds to edges $s\leadsto t$.
$\sum_{t\in V}M_{ts} = 1$.

The goal of the model is to find $r = Mr$ where $r=\brac{r_v}_{v\in V}$.

Power iterations method \begin{description}
	\item[Initialize] \hfill\\
		$r_0=\frac{1}{\abs{V}}$;
	\item[Iteration] \hfill\\
		$r_{k+1} = M r_k$;
	\item[Stopping] \hfill\\
		$\nrm{r_{k+1}-r_k}_1 < \eps$;
\end{description}

Interpretation
\begin{enumerate}
	\item at any time $t$ the surfer is in the node $v\in V$;
	\item at time $t+1$ the surfer follows an outlink $(v,u)\in E$ chosen uniformly at random;
	\item at time $t+1$ the surfer is at $u\in V$.
\end{enumerate}

Thus $p_{t+1} = M p_t$ when $p_{t+1} \approx p_t$ then the distribution is called stationary.

For graphs that satisfy the following conditions, the stationary distribution $\pi$ is unique and for any starting point $p_0$ on the $\abs{V}-1$ dimensional simplex the iterative process converges to $\pi$.
\begin{description}
	\item[Stochasticity] the matrix $M$ is stochastic;
	\item[Acyclicity] the matrix $M$ is acylic;
	\item[other] 
\end{description}

The google solution for the traps in cliques: at each time step the random surfer has two options
\begin{itemize}
	\item[carry on] continue its random walk with $\beta$;
	\item[teleport] restart the random walk from a randomly chosen node $V$ with probability $1-\beta$.
\end{itemize}

Therefore the transition probability matrix becomes $A = \beta M + (1-\beta)U$ where $U = \underset{V\times 1}{1}\frac{1}{1'1}\underset{1\times V}{1'}$ is the uniform transition matrix.
The matrix $A$ is still stochastic. Find the eigenvectors of $A$ with unit eigenvalues.

What to do with pure sink nodes?
\begin{itemize}
	\item immediately teleport from dead-ends;
	\item eliminate the dead ends: \begin{itemize}
		\item multiple passes;
		\item approximate the values for dead-ends by propagating values from the reduced graph.
	\end{itemize}
\end{itemize}

Computing the page rank matrix. Use sparsity of the matrix $M$:
\[r_{t+1} = A r_t = \beta M r_t + (1-\beta) U r_t = \beta M r_t + (1-\beta) \frac{1}{\abs{V}}\]
then load-multiply-store.

\subsection{Topic specific page rank} % (fold)
\label{sub:topic_specific_page_rank}

Modify the page rank so that the teleportation happens only to nodes in a specific subset.

% subsection topic_specific_page_rank (end)

% subsection page_rank (end)

\subsection{Hubs and authorities} % (fold)
\label{sub:hubs_and_authorities}

A bipartite graphs of hubs $H$ and authorities $A$.
\begin{itemize}
	\item a hub aggregates links to other nodes;
	\item 
\end{itemize}
A good hub links to many good authorities, while a good authority is linked from many good hubs.

There re two scores for each node the hub score $h$ and the authority score $a$.

We need two matrices $A^+$ and $A^-$ corresponding to inward and outward adjacency respectively.
In fact $A^+ = \big( A^- \big)'$.

Relationship between scores: \begin{description}
	\item[The hub score] \hfill\\
		the score is proportional to the sum of the authority scores of the nods linking to it:
		$h = \lambda A a$ with $\lambda$ -- hub - scaling factor;
	\item[The authority score] \hfill\\
		Similarly this score is derived from the sum of the scores of hubs linking to it:
		$a = \mu A' h$ with $\mu$ -- authority - scaling factor;
\end{description}

% subsection hubs_and_authorities (end)

Core-wise scoring: use min-cut to split.

HITS is prone to rigging, which is why PageRank superseded it.

% section lecture_7 (end)

\section{Lecture \# 8} % (fold)
\label{sec:lecture_8}

\subsection{Recommendation Systems} % (fold)
\label{sub:recommendation_systems}

The basic idea is that similar object share similar preferences.

User-based
Suppose we have a relationship $R\subseteq U\times I$ between the users $U$ and the items $I$.
Assume there is a ``measure'' $\sigma$ of similarity on the set of users $U$ (a pseudo metric on $U\times U$).

Define the set $N_\theta(u) \defn \obj{ u \neq u_0,\, \sigma(u_0, u) \geq \theta}$ -- the users 
similar to the given one $u_0$.

Item-based (a ``transposed'' problem)
 see the photos of the slides

Cosine similarity measure reflect the alignment of two vectors, which is good in recommendation systems.

% subsection recommendation_systems (end)

\subsection{Boolean matrix factorisation} % (fold)
\label{sub:boolean_matrix_factorisation}

Basic Matrix factorisation techniques.

\noindent\textbf{SVD}\hfill \\
Given a matrix $A\in \Cplx^{m\times n}$ ($m>n$). Then the SVD decomposition is 
\[A = U \bigg(\begin{matrix}\Sigma \\ 0\end{matrix}\bigg) V'\]
where $U\in \Real^{m\times m}$, $U'U = I_m$, and $V\in \Real^{n\times n}$, $V'V = I_n$
and $\Sigma$ -- is the diagonal matrix of non-zero singular values.

\noindent\textbf{NMF}\hfill \\
Given a matrix $A\in \Real^{m\times n}$ find matrices $W\in \Real^{m\times k}$ and
$H\in \Real^{k\times n}$ with positive elements such that $A\approx W H$.

\noindent\textbf{BMF}\hfill \\
Given a boolean matrix $B\in \obj{0,1}^{m\times n}$ find $P\in \obj{0,1}^{m\times k}$ and
$Q\in \obj{0,1}^{k\times n}$ such that $B = P\circ Q$, where 
\[(P \circ Q)_{ij} \defn \bigvee_{l=1}^k P_{il}\wedge Q_{lj} \]

% subsection boolean_matrix_factorisation (end)

\subsection{Formal concept analysis (again)} % (fold)
\label{sub:formal_concept_analysis}

The item set $G$ and the feature set $M$, with a relation $I\subseteq G\times M$
give a context $(G,M,I)$ with Galois connection $(\cdot)'$ between $(\mathcal{P}(G),\subseteq)$ and $(\mathcal{P}(M),\subseteq)$.
% \emptyset' = M -- Нитко любит всё, а ничто лиюбимо всеми (\emptyset' = G)

% slide with a theorem.


Projecting a new person with a given preferences $u_0\in \obj{0,1}^{1\times n}$ onto the computed factors $I=P\circ Q$ : 
use boolean multiplication $\underset{k\times 1}{f_0} = Q \circ u_0'$ to get the estimates of the values of the hidden
factors, and then compute the boolean product $i_0 = P\circ f_0$ of recommendations.


% subsection formal_concept_analysis (end)

% section lecture_8 (end)

\section{Lecture \# 9} % (fold)
\label{sec:lecture_9}

\textbf{S}upport \textbf{V}ector \textbf{M}achine.
The basic problem setting. Discriminative approach.
Two classes: positive and negative examples. The objective is to find a hyperplane, which separates the classes.
There might be many hyperplanes, which one to choose? Maximize the gap (margin) between classes!

SVM is a separating hyperplane problem.
\[\Lcal = \frac{1}{2}\nrm{\beta} + \sum_i \alpha_i \big[y_i(\beta_0 + X_i'\beta) - 1\big]\]

In the case of may classes, the schemes are:
\begin{itemize}
	\item one versus many (each classifier discriminates one class from all the rest),
	\item pairwise classification.
\end{itemize}

UNderfitting contributes to bias, overfitting to variance.

\subsection{Bagging} % (fold)
\label{sub:bagging}

This is an acronym of \textbf{B}ootstrap \textbf{agg}regat\textbf{ing}.
Use sampling with replacement to improve the prediction quality of a classifier.
Upon each bootstrap sample, train a classifier. But instead of averaging the classifiers,
use majority voting to classify a new observation. 

% subsection bagging (end)

% Committee machines
\subsection{Boosting} % (fold)
\label{sub:boosting}

Is based on the idea of amplifying weak classifiers (with prediction error $\leq\frac{1}{2}$).
Each classifier might specialize on something specific. Therefore combining them may yields better performance.

Classifier-object matrix of binary indicators whether a classifier makes an error or not on these objects.

\textbf{Ada}ptive \textbf{Boost} (M1 method).

% subsection boosting (end)

\subsection{Random forest} % (fold)
\label{sub:random_forest}

The idea is to supplement bagging with sub-samples of features.

% subsection random_forest (end)

% section lecture_9 (end)

\section{Lecture \# 10} % (fold)
\label{sec:lecture_#_10}

Ensemble clustering methods
The goal is to make the results robust by combining weak classifiers.
Build consensus clustering upon FCA.

We got data, which we feed into collection of models, and then aggregate the results.

Using simple majority rule for binary classification.

The significant problem is the synchronisation of the partitions, induced by different classifiers.
The solution is to represent the classification as a binary vector -- one for each cluster within 
each partition of each individual classifier. This way even incomplete classifications (a partition
but not exhaustive) can be incorporated.
\begin{tabular}{c|c|c||c|c|c||c|c|}
$x_1$ & $1$ & $0$ & $0$ & $1$ & $1$ \\
$x_2$ & $1$ & $0$ & $0$ & $1$ & $0$ \\
$x_3$ & $0$ & $1$ & $1$ & $0$ & $0$ \\
$x_4$ & $0$ & $1$ & $1$ & $0$ & $1$
\end{tabular}

Consensus matrix: count the number of partitions voting for each object.

Using FCA: mine for association rules with some lower bound on support, but on extents.
The context $(G,C,I)$ and find within cluster association rules: $A,B\subseteq G$ such
that their support is larger than a threshold.

Another idea is to use a partition lattice. Each classifier produces a partition, which
altogether form a basis of the lattice. 

% section lecture_#_10 (end)

\section{Lecture \# 11} % (fold)
\label{sec:lecture_11}

Combinatorial dealing with over-fitting. Test set and training set. 

Consider a collection $X\in \Real^{l\times n}$ of objects and $Y\in \Real^{l\times 1}$
-- responses, determined through $y^*(X)$. The goal is to find $a:\mathbb{X}\to \mathbb{Y}$, which
is able to approximate $y^*$ on new set of objects $\tilde{X}\in \Real^{k\times n}$.

ALgorithmic model
\[A = \bigl\{g(\cdot, \theta)\rvert \theta\in \Theta\bigr\}\]
where $g:X\times \Theta\to Y$.
A learning method
\[\mu:(\mathbb{X}\times \mathbb{Y})^l\to A\]

Empirical risk
\[Q(a,X) = \frac{1}{l} \sum_{i=1}^l 1_{a(x_i)\neq y_i}\]
minimization of empirical risk yields
\[
\mu(X) = \argmin_{a\in A} Q(a, X)\]

Does $a$ approximate $y^*$ on the entire $\mathbb{X}$ well enough? What happens:
a discovery of a true latent law of nature, or overfitting?

Case-by-case learning. Consider a learning method $\mu:\mathcal{P}(X)\to A$ which
for an arbitrary training sample $X$ returns some algorithm $a\in A$. The response
of $\mu$ is determined with respect to empirical risk minimization:
\[\mu X =\argmin_{a\in A} m(a, X)\]

Introduce the following ``operator''
\[\mathbb{E} = \frac{1}{C^l_L} \sum_{X\subset \mathbb{X}}\]

Let $\delta(\mu,X,\bar{X}) = \nu(\mu X,\bar{X})-\nu(\mu X, X)$. Then the abstraction
capability functional:
\begin{itemize}
	\item The expected overfitting: $\mathbb{E} \delta(\mu,X,\bar{X})$;
	\item $\ldota$
\end{itemize}

% section lecture_11 (end)

\section{Lecture \# 12} % (fold)
\label{sec:lecture_12}

Dimensionality reduction.
Consider two problems: \begin{itemize}
	\item Document-term matrix, latent semantic analysis;
	\item MISSED.
\end{itemize}

Goal compactify the data with tolerance towards small errors.
Discover hidden correlations or topics: commonly occurring words.
Remove redundant or noisy variables.

Consider a matrix $M\in \Real^{m\times n}$ with rank $r\leq m,n$. Then there exist
orthogonal matrices $U\in \Real^{m\times m}$, $V\in \Real^{n\times n}$ and
a diagonal matrix $\Sigma = \diag(\sigma_i)_{i=1}^r$ of non-negative and non-zero elements
such that 
\[ M = U \Biggl( \begin{smallmatrix} \Sigma & 0\\0 & 0 \end{smallmatrix} \Biggr) V' \]
the matrices $U,V$ are unique up to first $r$ columns.

Here $U$ is object-to-concept and $V$ is feature-to-concept matrix. $\Sigma$ reflects
the strength of concepts.

Dimensionality reduction is performed by eliminating the smallest non-zero singular
values. With respect to the Fro\"benius norm on $\Real^{m\times n}$ it gives the
closest approximation.

An important result is that $\|M\|_F = \sum_{i=1}^r \sigma_i$. Furthermore
\[
\|M\|
= \sum_{i=1}^m \sum_{j=1}^n m_{ij}^2
= \sum_{i=1}^m \sum_{j=1}^n \Bigl( \sum_{k=1}^r \sum_{k=1}^r p_{ik}\sigma_{kl}q_{lj} \Bigr)^2
\]

If a pair of object or features is highly correlated, then 

Querying the SVD. For a given feature vector $q$ it is possible to project it into
the space of concepts by right multiplying by $V'$.

% A singular vector is a linear combination of rows and columns.

% section lecture_12 (end)

\section{Lecture \# 14} % (fold)
\label{sec:lecture_14}

\subsection{EM algorithm and its applications} % (fold)
\label{sub:em_algorithm_and_its_applications}

Consider a model of a mixture of distributions given by
\[p(x) = \sum_k p_k(x|\theta_k) \pi_k \]
where $\pi_k$ are the mixture component probabilities with $\sum_k \pi_k = 1$.
Given $X = (x_i)\sim p(x)$ we are to find $\Theta = (\pi_k,\theta_k)$.
The log-likelihood is
\[
L(X, \Theta)
= \log \prod_{i=1}^n p( x_i | \Theta )
= \sum_{i=1}^n \log \sum_k p_k(x_i|\theta_k) \pi_k
\]
and we wish to maximize it with respect to $\Theta$. Due to the sum under the
logarithm this problem is analytically intractable.

Suppose the hidden (latent) variables were in fact observable, then the log-likelihood
would simplify. Indeed, consider a model where the source mixture components are known
and given by $Z=(z_i)\sim \pi$ and $p(x_i|z_i = k) = p_k(x_i|\theta_k)$. The log-likelihood
thus becomes
\[
L(X,Z,\Theta)
= \log \prod_{i=1}^n p( x_i, z_i| \Theta )
= \sum_{i=1}^n \sum_k 1_{z_i=k} \log \pi_k p_k(x_i|\theta_k) \pi_k
\]

The problem separates into
\[ \sum_{i=1}^n 1_{z_i=k} \log p_k(x_i|\theta_k) \pi_k \to \max_{\theta_k} \]
and 
\[ \pi_k = \frac{\sum_{i=1}^n 1_{z_i=k}}{n} \]

In order to infer the hidden component in practice let's use Bayes
\[
g_{ik}
= p(z_i = k| x_i)
= \frac{ p_k(x_i|\theta_k) \pi_k }{\sum_j p_j(x_i|\theta_j) \pi_j}
\]

The basic step of the algortim is:
\begin{itemize}
	\item[E] Compute $g_{ik}$ based on $\Theta$;
	\item[M] Maximize $\sum_{i=1}^n g_{ik} \log p_k(x_i|\theta_k) \pi_k \to \max_{\theta_k}$
	and update $\Theta$.
\end{itemize}

Recall the Kullback-Leibler divergence for densities $p$ and $q$
\[
KL(P\|Q)
= \ex_p \log \frac{p}{q}
= \int p(x) \log \frac{p(x)}{q(x)} dx
= \int \log \frac{\parital P}{\partial Q} dP
\]
It is non-negative and $KL(P\|Q) = 0$ if and only if $p = q$ almost surely.
Furthermore, if $KL(P\|Q)< KL(Q\|P)$ then $P$ is more embedden in $Q$ than $Q$ in $P$.
Minimization of $KL(P\|Q(\theta))$ is equivalent to maximization of the expected
likelihood $\ex_p \log q(\theta)$ wit hrespec to $\theta$.

% subsection em_algorithm_and_its_applications (end)

\subsection{Derivation of EM} % (fold)
\label{sub:derivation_of_em}

Reduce the problem 
\[L(X,\Theta) \to \max_\Theta\]
to
\[L(X,Z,\Theta) \to \max_\Theta\]
Suppose $Z\sim q(\cdot)$. Then the log-likelihood of the sample $X$ is
\[
L(X,\Theta)
= \log p(X|\Theta)
= \ex_q \log p(X|\Theta)
= \int q(Z) \log p(X|\Theta) dT
\]
Next using $p(X \Theta) = \frac{p(X,Z|\Theta) q(Z)}{ q(Z) p(Z|X,\Theta)}$ we get this
\[
\log p(X|\Theta)
= \int q(Z) \log p(X,Z|\Theta) dT - \int q(Z) \log q(Z) dT
+ \int q(Z) \log \frac{q(Z)}{p(Z|X,\Theta)} dT
\]
which reduces to
\[ \log p(X|\Theta) = G(q(\cdot),\Theta) + KL(q\|p(Z|X,\Theta)) \]

Since $KL(q\|p(Z|X,\Theta)) \geq 0$ we get
\[
\log p(X|\Theta) \geq G(q(\cdot),\Theta)
\]

Let's do the following: \begin{description}
	\item[E] $G(X,\Theta) \to \max_q$;
	\item[M] $G(X,\Theta) \to \max_{\Theta}$.
\end{description}

Now, the left hand side is independent of $q$, whence maximization of $L(q(\cdot),\Theta)$
with respect to $q$ is equivalent to minimization of $KL(q\|p(Z|X,\Theta))$ with respect
to $q$, whence $\hat{q}(Z) = p(Z|X,\Theta)$. Thus
\[
L(X,\Theta)
= \int p(Z|X,\Theta) \log \frac{p(X,Z|\Theta)}{p(Z|X,\Theta)} dT
= \ex_{p(Z|X,\Theta)} \log p(X,Z|\Theta) - \ex_{p(Z|X,\Theta)} \log p(Z|X,\Theta)
\]

Now maximization of $L$ with respec to $\Theta$ reduces to
\[ \ex_{p(Z|X,\Theta)} \log p(X,Z|\Theta) \to \max_\Theta \]
In our case of the mixture
\[
\log p(X,Z|\Theta)
= \sum_{i=1}^n \log p(x_i|z_i,\Theta) p(z_i|\Theta)
\]
whence
\[
\ex_{q(Z)} \log p(X,Z|\Theta)
= \sum_{i=1}^n \ex_{q(Z)} \log p(x_i|z_i,\Theta) p(z_i|\Theta)
= \sum_{i=1}^n \sum_k q(z_i=k) \log p(x_i|z_i=k,\Theta) p(z_i=k|\Theta)
\]

% subsection derivation_of_em (end)

\subsection{Modifications of EM} % (fold)
\label{sub:modifications_of_em}

\begin{itemize}
	\item How to choose the inital $\Theta$ values? 
\end{itemize}

Stocastic EM. On the \textbf{M}-step instead of 
\[ \theta_k \argmax_\theta \sum_{i=1}^n g_{ik} \log p_k(x_i|\theta) \]
maximize
\[ \theta_k \argmax_\theta \sum_{x\in X_k} g_{ik} \log p_k(x|\theta) \]
where $X_k$ is a stochstically generated sample: for each $x_i$ one
generates $k\sim p(z_i=k|x_i) = g_{ik}$ and assignes it to $X_k$.

% subsection modifications_of_em (end)

\subsubsection{EM in general} % (fold)
\label{ssub:em_in_general}

Suppose $X$ -- observed variables and $Z$ are latent, And $\Theta$ are parameters.
The EM can be reformulated as the problem of maximization of incomplete likelihood:
\[
\log p(X|\Theta)
= \log \int p(X,T|\Theta) dT
= J\{q,\Theta\} + KL(q(\cdot)\|p(\cdot|X,\Theta)) \to \max_\Theta
\]
where 
\[J\{q,\Theta\} = \int q(T) \log p(X,T|\Theta) dT - \int q(T) \log q(T) dT \]
is the variational lower bound. Then \begin{description}
	\item[E] minimize 
	\[KL(q(\cdot)\|p(\cdot|X,\Theta)) \to \min_{q(T)}\] which yeilds $q(T) = p(T|X,\Theta)$;
	\item[M] maximize $J\{q,\Theta\} \to \max_\Theta$ which is equivalent to
		\[\ex_{q(T)} \log p(X,T|\Theta) \to \max_\Theta\]
\end{description} 

% subsubsection em_in_general (end)

\subsubsection{Another application of the EM} % (fold)
\label{ssub:another_application_of_the_em}

Machine translation. Given a sentence $f$ find $e$ such that
\[ \hat{e} = \argmax_e p(e) p(f|e)\]
where the probability $p(f|e)$ is the translation likelihood and $p(e)$ is the 
prior probability. The data sample is $D = (e_i,f_i)_{i=1}^n$ and the latent variables
are alignment $A$ and et c.

The observed likelihood of the sentence pairs is
\[
p(D|\Theta)
= \prod_{i=1}^n p(f_i|e_i,\Theta) p(e_i|\Theta)
= \prod_{i=1}^n \sum_{a_i} p(f_i|e_i,a_i\Theta) p(a_i|e_i,\Theta) p(e_i|\Theta)
\]

In particular
\[
p(f|e) = \prod_{j=1}^{|f|} \sum_{a_j=1}^{|e|} p(a_j|e) p(f_j|e_{a_j})
\]
% subsubsection another_application_of_the_em (end)

% section lecture_14 (end)

\end{document}
