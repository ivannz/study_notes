\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\pr}{\mathbb{P}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Machine Learning}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section{Lecture \# 4} % (fold)
\label{sec:lecture_4}

\rus{Дмитрий Игоревич Игнатов}

Descision tree as classification tools.

Is a tree with nodes, labelled by features, conditions or rules. The leaves are the classes.

The learning sample may be unbalanced.

There is no point in identical conditions within the same branch: at nodes $u,v$ in the tree-order.

Main definitions.
The connection of decision tree with boolean functions.

Can a tree be represented in a disjunctive normal form?

Conjuncts are identified with the root-leaf paths, with a \emph{True} leaf.

Decision tree construction.
\begin{itemize}
	\item Pick a feature $Q$.
	\item For each value $i$ of $Q$.
	\begin{itemize}
		\item recursively construct the decision tree base of the learning sub-sample with $Q=i$.
	\end{itemize}
\end{itemize}

The question is how the next attribute should be chosen.

Suppose the we need to define a measure $\nu$ of uncertainty of events:\begin{itemize}
	\item If $A$ is certain to occur ($\pr(A)=1$), then $\nu(A)=0$;
	\item If $A$ is more likely than $B$, then is reasonable to expect the uncertainty of $B$ be higher than that of $A$. Thus $\nu$ should be anti-monotonous;
	\item if $A\perp B$, then the uncertainty of one event is unaffected by the uncertainty of the other. Thus it is reasonable to expect that their simultaneous uncertainty must be pooled: $\nu(A\cap B)= \nu(A)+\nu(B)$.
\end{itemize}

It is convenient to define $\nu$ as a composition of some $h$ with $\pr$: $\nu = h\circ \pr$.

Thus we arrive at a concept of entropy: if $Q<<\mu$ and $Q=\int q d\mu$, then $H = \int \log \frac{1}{q} q d\mu = - \int \log q dQ$.

In the discrete case the integral degenerates into $H = \sum_k \frac{1}{p_k} p_k$.

For a binary variable:
\[H(\theta) = - \theta \log \theta - (1-\theta) \log (1-\theta)\]
The entropy as a function of $\theta$ resembles a bell with zeros at $0,1$. Use l'H\^opital's rule.

\textbf{Idea}: Select the successive feature $Q$ to be as informative as possible.

Suppose $A$ elements with a property $S$ are classified by a feature $Q$ with finite number of values then the \textbf{information gain} is
\[\text{Gain} = H(A,S) - \sum_{q\in Q} \frac{\abs{A_q}}{\abs{A}} H(A_q,S)\]
where $A_q$ is the partition by $Q$: $A_q = \obj{\induc{\omega \in A} Q(\omega)=q }$.

The entropy in this case is defined as 
\[H(X,S) = - \sum_{s\in S} \frac{\abs{X_s}}{\abs{X}} \log \frac{\abs{X_s}}{\abs{X}}\]

The higher the gain, the more informative an attribute is.

The impurity of a class might be defined as
\[\dot{p} = \frac{p_+}{p_++p_-} = \frac{N_+}{N_+ + N_-}\]
in the learning sample with $+$ and $-$ examples.

the impurity measure is expected to be symmetric with respect to the classes $+$ and $-$.

Consider using $p_+(1-p_-)$ and $p_-(1-p_+)$ pooled together for symmetry.

Ross Queenlain ID3 algorithm (C4.5).
\begin{itemize}
	\item Create a root;
	\item If $S$ hold for all $A$, then set the node to $1$ and leave;
	\item If $S$ fails to hold for all $A$, then set the node to $0$ and leave;
	\item If $\mathcal{Q} = \emptyset$ then \begin{itemize}
		\item if $S$ it true for a half or more examples in $A$, then set the root to $1$ and leave;
		\item if $S$ it true for a half or more examples in $A$, then set the root to $1$ and leave;
	\end{itemize}
	\item IF $\mathcal{Q}\neq\emptyset$ then choose feature $Q\in \mathcal{Q}$ with the highest information gain $\text{Gain}(A, Q)$.
	\item ... MISSING LOC;
\end{itemize}

Gini inequality coefficient.
Gini index

A feature with distinct value for each example has the greatest information gain. Indeed, the entropy of a set consisting of an individual example with respect to the target variable is zero since $0\log 0 = 1 \log 1 = 0$.
\[\text{SplitInfo}(A,S) = H(A,S)\]

Branch trimming:
\[\text{GainRatio}(A,S) = \frac{\text{Gain}(A,S)}{\text{SplitInfo}(A,S)}\]


% Если зенит просто как случайная величина выигрывает/проигрывает с вероятностью $p$ то можно и построить такой ```случайный классификатор''.

One-node (one-rule) degenerate decision trees are called \eng{decision stumps}.

\begin{align*}
	\text{Gini}(D,T) &= 1 - \sum_{k=1}^T \brac{\frac{D_k}{D}}^2 \\
	H(D,T) &= - \sum_{k=1}^T \frac{D_k}{D} \log \frac{D_k}{D} \\
	\text{Gain}(D,T,A) &= H(D,T) - \sum_{k=1}^T \frac{D_k}{D} H(D_k,T)
\end{align*}



% section lecture_4 (end)

\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

Associative rules mining.

A ocntext is an object-attribute table: $\brac{G,M,I}$, with $I\subseteq G\times M$.
An Associative rule is a a pair $A\to B$ with $A,B\subseteq M$ (and sometimes $A\cap B = \emptyset$).

\subsection{A brief history of FCA} % (fold)
\label{sub:a_brief_history_of_fca}

\begin{itemize}
	\item Aristotle. An extnet (the objects) and an intent (the set of common attributes);
	\item Galois. I his reserach he used the what now is know as a Galois connection: a pair of maps $\psi:P\to Q$ and $\phi:Q\to P$ between ordered sets $(P,\leq)$ and $(Q,\leq)$ with the \textbf{GALOIS} property: for any $a\in P$ and $b\in Q$ it
	\[a\leq \phi(b) \Leftrightarrow b\leq \psi(a)\]
	\item Lattice structure; Birkhgoff proposed to compare incomparable elements of a poset $(P,\leq)$ by using the least upper and the greatest lower bounds $a\vee b$ and $a\wedge b$;
\end{itemize}

% subsection a_brief_history_of_fca (end)

\subsection{A recap of the FCA} % (fold)
\label{sub:a_recap_of_the_fca}

The intent of $A\subseteq G$ is the set of attributes
\[A' \defn \obj{\induc{m\in M}\,\forall g\in A (g,m)\in I} = \bigcap_{a\in A} a'\]
Similarly the extent of $B\subseteq M$ is the collection of objects
\[B' \defn \obj{\induc{g\in G}\,\forall m\in B (g,m)\in I} = \bigcap_{b\in B} b'\]

Also the ${(\cdot)}'$ operator has the following property:
\[\brac{\bigcup_{j\in J}A_j}' = \bigcap_{j\in J}A_j'\]

A formal concept is a pair $(A,B)$ such that $A'=B$ and $B'=A$ -- stability. 

% subsection a_recap_of_the_fca (end)

An associative rule is an intent (a formal intent, itemset). The support of an associative rule $A\to B$ is
\[\text{supp}(A\to B) = \frac{\abs{\brac{A\cup B}'}}{\abs{G}}\]
The confidence of a rule is 
\[\text{conf}(A\to B) = \frac{\abs{\brac{A\cup B}'}}{\abs{A'}}\]

The setting of the problem
Find all associative rules of the context with support and confidence higher than some pre-set threshold value.

An implication of the context is an associative rule $A\to B$ with $A'\subseteq B'$, which implies that $\text{conf}(A\to B) = 1$.

The problem is that $A\to B$ is an implication even if $A'=B'=\emptyset$.
For example $bc\to ad$ in the context $\abs{\begin{matrix}\times & \times & \cdot & \cdot\\ \cdot & \cdot & \times & \times\end{matrix}}$

\subsection{The Apriori algorithm} % (fold)
\label{sub:the_apriori_algorithm}
The algorithm is based on the following property of support:
\noindent\textbf{anti-monotonicity}\hfill\\
If $A,B\subseteq M$ and $A\subseteq B$ then $\text{supp}(B) \leq \text{supp}(A)$

\textbf{Apriori}
\begin{description}
	\item[input] A context $\brac{G,M,I}$, the minimum confidence $\alpha$ and support $\beta$ thresholds;
	\item[output] All frequent intents (itemsets);
	\item $C_i\leftarrow \obj{\text{1-itemsets}}$;
	\item[while] $C_i\neq \emptyset$;\begin{itemize}
		\item SupportCount($C_i$);
		\item $F_i \leftarrow \obj{\induc{f\in C_i}\,\text{supp}(f)\geq \beta}$;
		\item $C_{i+1}\leftarrow \text{AprioriGen}\brac{F_i}$.
	\end{itemize}
	\item[Finalization] $I_F \leftarrow \bigcup_i F_i$;
\end{description}

Where the service procedure is given by:
\textbf{AprioriGen}
\begin{description}
	\item[input] $F$ -- frequent itemsets of size $i$;
	\item[output] $C_{i+1}$ -- the potential candidate itemsets;
	\item[SQL-step]\hfill\\
		insert into $C_{i+1}$ select $\brac{p_k}_{k=1}^i$, $q_i$ from $F_ip$, $F_iq$ where $\brac{p_k}_{k=1}^{i-1} = \brac{q_k}_{k=1}^{i-1}$ and $p_i\leq q_i$;
	\item[for each] $c\in C_{i+1}$;\begin{description}
		\item $S\leftarrow (i-1)\text{-element subsets of }c$;
		\item[for each] $s\in S$;
		\item if $s\notin F_i$, then $C_{i+1}\leftarrow C_{i+1}\setminus \obj{s}$.
	\end{description}
\end{description}

Let $F$ be a frequent itemset. A rule is satisfactory if 
\[\text{conf}(f\to F\setminus \obj{f}) = \frac{\text{supp}(F)}{\text{supp}(f)} \geq \alpha\]

The confidence of $f\to F\setminus f$ is minimal when $\text{supp}(f)$ is maximal.

The the confidence is minimal when the Antecedent has only one attribute. Supersets of this attribute have lesser support and thus higher confidence.

The associative rules are extracted a frequent itemset $F$ recursively:
\begin{itemize}
	\item starting from $1$-itemset of $F$ antecedent which satisfies the $\alpha$ and $\beta$ thresholds;
	\item check its supersets in $F$; If the confidence requirement is met, then the association rule $f\to F\setminus f$ is kept.
\end{itemize}

In data mining $A$ is closed if $\text{supp}(A)\leq \alpha$ and there exists no $A\subseteq B$ with $\text{supp}(B)=\text{supp}(A)$.


% subsection the_apriori_algorithm (end)

% Associative rule document mining.

% section lecture_5 (end)

\section{Lecture \# 6} % (fold)
\label{sec:lecture_6}

For a graph $G=(V,E)$ computing the sequence of $k$-cores enables detection of communities.

Graph \textbf{cut}:
\begin{align*}
	\text{cut}(A,B) &= \sum_{u\in A} \sum_{v\in B} \omega(u, v)\\
	\text{assoc}(A,V) &= \sum_{v\in A} \sum_{w\in V} \omega(v,w)
\end{align*}
Normalised cut criterion
\[\text{Ncut}(A,B) = \frac{\text{cut}(A,B)}{\text{assoc}(A,V)} + \frac{\text{cut}(A,B)}{\text{assoc}(B,V)}\]

Spectral cut of the graph

Suppose there the nodes are totally ordered $(V,\leq)$ (for convenience). Then the optimal cut criterion: 
\[ \text{cut} = \frac{1}{4}\sum_{j<i} {(p_i - p_j)}^2 \omega(i,j) \]
where $\brac{p_v}_{v\in V}\in \obj{-1,+1}^V$ is the vector of binary partition association.

Let's relax the problem by extending the space of the association vector $\brac{p_v}_{v\in V}\in \clo{-1,1}$
and reduce this problem a problem of quadratic optimisation:
\[E = \frac{1}{4}\sum_{i<j} {(x_i-x_j)}^2 \omega_{ij} = \frac{x'Lx}{4}\]
where $L$ is defined below.

Observe that
\begin{align*}
	2\sum_{i<j} {(x_i-x_j)}^2 \omega_{ij} &= \sum_{i\neq j} {(x_i-x_j)}^2 \omega_{ij} + \sum_{i=j} {(x_i-x_j)}^2 \omega_{ij} \\
	&= \sum_i\sum_j (x_i^2 + x_j^2 - 2x_ix_j) \omega_{ij} = 2 \sum_i\sum_j x_i^2 - 2\sum_i\sum_j x_i \omega_{ij} x_j \\
	&= 2\sum_i x_i^2 \sum_j \omega_{ij} - 2\sum_i\sum_j x_i \omega_{ij} x_j \\
	&= 2 x' D x - 2 x' W x
\end{align*}
where $D\defn \text{diag}\brac{{(\sum_j w_{ij})}_{i\in V}}$ and $W = \brac{\omega_{ij}}_{i,j\in V}$.

Thus the energy functional is reduced to $E = \frac{1}{4} x' ( D - W ) x $.

There are constraints on the optimal vectors: $x'x = N$ and $x'e = 0$. The last restriction means that the optimal distribution ``assigns'' at least one element of each cluster.

The Lagrangian is 
\[\mathcal{L} = x'Lx - \lambda ( x'x - N )\]
and the first order conditions are given by
\begin{align*}
	\frac{\partial L}{\partial x} \quod:\quod 2Lx - 2\lambda x = \mathbf{0}\\
	\frac{\partial L}{\partial \lambda} \quod:\quod x'x - N = 0
\end{align*} 

...

For a rectangular matrix use the following incidence matrix
\[A = \left(\begin{matrix}
	0&W\\
	W'&0\\
\end{matrix}\right)\]


% section lecture_6 (end)

\section{Lecture \# 7} % (fold)
\label{sec:lecture_7}

\subsection{Page rank} % (fold)
\label{sub:page_rank}

The main idea is to use links to determine the the importance.
To rig the search engine spammers used links farms and other techniques.

Assess the importance of a page based on links.
\noindent \textbf{Recursive definition}\hfill\\
Measure relative importance of a web page. A page is considered \textbf{important} of many other \textbf{important} pages link to it.

Each link's $\omega = (s,t)\in E$ vote $i_\omega \propto i_s$ -- is proportional to the importance of its source.

Flow model -- a matrix formulation

Suppose $M$ is a columns stochastic matrix $\brac{M_{ts}}_{t,s\in V}$ corresponds to edges $s\leadsto t$.
$\sum_{t\in V}M_{ts} = 1$.

The goal of the model is to find $r = Mr$ where $r=\brac{r_v}_{v\in V}$.

Power iterations method \begin{description}
	\item[Initialize] \hfill\\
		$r_0=\frac{1}{\abs{V}}$;
	\item[Iteration] \hfill\\
		$r_{k+1} = M r_k$;
	\item[Stopping] \hfill\\
		$\nrm{r_{k+1}-r_k}_1 < \eps$;
\end{description}

Interpretation
\begin{enumerate}
	\item at any time $t$ the surfer is in the node $v\in V$;
	\item at time $t+1$ the surfer follows an outlink $(v,u)\in E$ chosen uniformly at random;
	\item at time $t+1$ the surfer is at $u\in V$.
\end{enumerate}

Thus $p_{t+1} = M p_t$ when $p_{t+1} \approx p_t$ then the distribution is called stationary.

For graphs that satisfy the following conditions, the stationary distribution $\pi$ is unique and for any starting point $p_0$ on the $\abs{V}-1$ dimensional simplex the iterative process converges to $\pi$.
\begin{description}
	\item[Stochasticity] the matrix $M$ is stochastic;
	\item[Acyclicity] the matrix $M$ is acylic;
	\item[other] 
\end{description}

The google solution for the traps in cliques: at each time step the random surfer has two options
\begin{itemize}
	\item[carry on] continue its random walk with $\beta$;
	\item[teleport] restart the random walk from a randomly chosen node $V$ with probability $1-\beta$.
\end{itemize}

Therefore the transition probability matrix becomes $A = \beta M + (1-\beta)U$ where $U = \underset{V\times 1}{1}\frac{1}{1'1}\underset{1\times V}{1'}$ is the uniform transition matrix.
The matrix $A$ is still stochastic. Find the eigenvectors of $A$ with unit eigenvalues.

What to do with pure sink nodes?
\begin{itemize}
	\item immediately teleport from dead-ends;
	\item eliminate the dead ends: \begin{itemize}
		\item multiple passes;
		\item approximate the values for dead-ends by propagating values from the reduced graph.
	\end{itemize}
\end{itemize}

Computing the page rank matrix. Use sparsity of the matrix $M$:
\[r_{t+1} = A r_t = \beta M r_t + (1-\beta) U r_t = \beta M r_t + (1-\beta) \frac{1}{\abs{V}}\]
then load-multiply-store.

\subsection{Topic specific page rank} % (fold)
\label{sub:topic_specific_page_rank}

Modify the page rank so that the teleportation happens only to nodes in a specific subset.

% subsection topic_specific_page_rank (end)

% subsection page_rank (end)

\subsection{Hubs and authorities} % (fold)
\label{sub:hubs_and_authorities}

A bipartite graphs of hubs $H$ and authorities $A$.
\begin{itemize}
	\item a hub aggregates links to other nodes;
	\item 
\end{itemize}
A good hub links to many good authorities, while a good authority is linked from many good hubs.

There re two scores for each node the hub score $h$ and the authority score $a$.

We need two matrices $A^+$ and $A^-$ corresponding to inward and outward adjacency respectively.
In fact $A^+ = \big( A^- \big)'$.

Relationship between scores: \begin{description}
	\item[The hub score] \hfill\\
		the score is proportional to the sum of the authority scores of the nods linking to it:
		$h = \lambda A a$ with $\lambda$ -- hub - scaling factor;
	\item[The authority score] \hfill\\
		Similarly this score is derived from the sum of the scores of hubs linking to it:
		$a = \mu A' h$ with $\mu$ -- authority - scaling factor;
\end{description}

% subsection hubs_and_authorities (end)

Core-wise scoring: use min-cut to split.

HITS is prone to rigging, which is why PageRank superseded it.

% section lecture_7 (end)

\section{Lecture \# 8} % (fold)
\label{sec:lecture_8}

\subsection{Recommendation Systems} % (fold)
\label{sub:recommendation_systems}

The basic idea is that similar object share similar preferences.

User-based
Suppose we have a relationship $R\subseteq U\times I$ between the users $U$ and the items $I$.
Assume there is a ``measure'' $\sigma$ of similarity on the set of users $U$ (a pseudo metric on $U\times U$).

Define the set $N_\theta(u) \defn \obj{ u \neq u_0,\, \sigma(u_0, u) \geq \theta}$ -- the users 
similar to the given one $u_0$.

Item-based (a ``transposed'' problem)
 see the photos of the slides

Cosine similarity measure reflect the alignment of two vectors, which is good in recommendation systems.

% subsection recommendation_systems (end)

\subsection{Boolean matrix factorisation} % (fold)
\label{sub:boolean_matrix_factorisation}

Basic Matrix factorisation techniques.

\noindent\textbf{SVD}\hfill \\
Given a matrix $A\in \Cplx^{m\times n}$ ($m>n$). Then the SVD decomposition is 
\[A = U \bigg(\begin{matrix}\Sigma \\ 0\end{matrix}\bigg) V'\]
where $U\in \Real^{m\times m}$, $U'U = I_m$, and $V\in \Real^{n\times n}$, $V'V = I_n$
and $\Sigma$ -- is the diagonal matrix of non-zero singular values.

\noindent\textbf{NMF}\hfill \\
Given a matrix $A\in \Real^{m\times n}$ find matrices $W\in \Real^{m\times k}$ and
$H\in \Real^{k\times n}$ with positive elements such that $A\approx W H$.

\noindent\textbf{BMF}\hfill \\
Given a boolean matrix $B\in \obj{0,1}^{m\times n}$ find $P\in \obj{0,1}^{m\times k}$ and
$Q\in \obj{0,1}^{k\times n}$ such that $B = P\circ Q$, where 
\[(P \circ Q)_{ij} \defn \bigvee_{l=1}^k P_{il}\wedge Q_{lj} \]

% subsection boolean_matrix_factorisation (end)

\subsection{Formal concept analysis (again)} % (fold)
\label{sub:formal_concept_analysis}

The item set $G$ and the feature set $M$, with a relation $I\subseteq G\times M$
give a context $(G,M,I)$ with Galois connection $(\cdot)'$ between $(\mathcal{P}(G),\subseteq)$ and $(\mathcal{P}(M),\subseteq)$.
% \emptyset' = M -- Нитко любит всё, а ничто лиюбимо всеми (\emptyset' = G)

% slide with a theorem.


Projecting a new person with a given preferences $u_0\in \obj{0,1}^{1\times n}$ onto the computed factors $I=P\circ Q$ : 
use boolean multiplication $\underset{k\times 1}{f_0} = Q \circ u_0'$ to get the estimates of the values of the hidden
factors, and then compute the boolean product $i_0 = P\circ f_0$ of recommendations.


% subsection formal_concept_analysis (end)

% section lecture_8 (end)

\section{Lecture \# 9} % (fold)
\label{sec:lecture_9}

\textbf{S}upport \textbf{V}ector \textbf{M}achine.
The basic problem setting. Discriminative approach.
Two classes: positive and negative examples. The objective is to find a hyperplane, which separates the classes.
There might be many hyperplanes, which one to choose? Maximize the gap (margin) between classes!

SVM is a separating hyperplane problem.
\[\Lcal = \frac{1}{2}\nrm{\beta} + \sum_i \alpha_i \big[y_i(\beta_0 + X_i'\beta) - 1\big]\]

In the case of may classes, the schemes are:
\begin{itemize}
	\item one versus many (each classifier discriminates one class from all the rest),
	\item pairwise classification.
\end{itemize}

UNderfitting contributes to bias, overfitting to variance.

\subsection{Bagging} % (fold)
\label{sub:bagging}

This is an acronym of \textbf{B}ootstrap \textbf{agg}regat\textbf{ing}.
Use sampling with replacement to improve the prediction quality of a classifier.
Upon each bootstrap sample, train a classifier. But instead of averaging the classifiers,
use majority voting to classify a new observation. 

% subsection bagging (end)

% Committee machines
\subsection{Boosting} % (fold)
\label{sub:boosting}

Is based on the idea of amplifying weak classifiers (with prediction error $\leq\frac{1}{2}$).
Each classifier might specialize on something specific. Therefore combining them may yields better performance.

Classifier-object matrix of binary indicators whether a classifier makes an error or not on these objects.

\textbf{Ada}ptive \textbf{Boost} (M1 method).


% subsection boosting (end)

% section lecture_9 (end)

\end{document}
