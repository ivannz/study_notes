\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}
\usepackage{mathtools}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\w}{\mathbf{w}}
\newcommand{\e}{\mathbf{1}}
\newcommand{\R}{\text{R}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Var}{\text{var}}
\newcommand{\Tr}{\mathop{\text{tr}}\nolimits}
\newcommand{\RSS}{\text{RSS}}


\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[russian, english]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Assignment \# 1}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\tableofcontents
\clearpage

\section{problem \# 1} % (fold)
\label{sec:problem_1}

\subsection{subproblem a} % (fold)
\label{sub:p1subproblem_a}

Suppose instead of a one new observation we are given $m$ observation
$\big(\w_k\big)_{k=0}^m$ with $\w_k\in \obj{1}\times\Real^p$
collected in a matrix $W_{m\times 1+p}$. The vector of predictions
is then $\hat{T}_W = W\hat{\beta}$, where $\hat{\beta}$ is the OLS estimate of
the model
\[ \underset{n\times 1}{T} = \underset{n\times p+1}{X} \underset{(1+p)\times 1}{\beta} + \underset{n\times 1}{\epsilon}\]
given by $\hat{\beta} = \big(X'X\big)^{-1} X'T$, where $X$ is the observed matrix of explanatory variables and $T$ is the vector of associated responses
all in the training sample.

Let the errors in new observations be given by a random vector $\eta_{m\times 1}$ with mean $\Ex \eta = 0$ and variance $\Var \eta = \sigma^2 I_m$,
which is independent from $\epsilon$.

The expectation of the error vector of the joint prediction $\hat{\eta} = T_W - \hat{T}_W$ is given by
\begin{align*}
	\Ex \hat{\eta} &= \Ex\Big( T_W - \hat{T}_W \Big) \\
	&= \Ex\Big( W\beta + \eta - W\hat{\beta} \Big) \\
	&= \Ex\Big( W (\beta - \hat{\beta}) + \eta \Big) \\
	&= W \Ex(\beta - \hat{\beta}) + \Ex(\eta) = W 0 + 0 = 0
\end{align*}
because $\beta$ is an unbiased estimator of the true vector of parameters
(provided the model is specified correctly).

Note that since it is assumed the the linear model is correct, and the errors
$\epsilon$ and $\eta$ are independent, we have
\begin{align*}
	\underset{(1+p)\times 1}{\big(\beta-\hat{\beta}\big)}\underset{1\times m}{\eta'}
	&= \beta\eta' - \big(X'X\big)^{-1} X'\big(X\beta + \underset{n\times 1}{\epsilon}) \eta' \\
	&= \beta\eta' - \big(X'X\big)^{-1} X'X\beta\eta' - \big(X'X\big)^{-1} X'\epsilon \eta' \\
	&=  - \big(X'X\big)^{-1} X' \big( \epsilon \eta' \big)
\end{align*}
whence $\Ex \big(\beta-\hat{\beta}\big)\eta' = \mathbf{0}_{(1+p)\times m}$.

Thus the covariance matrix of the prediction error vector is
\begin{align*}
	\Var \hat{\eta} &= \Ex \hat{\eta}\hat{\eta}'
	= \Ex\big(W (\beta - \hat{\beta}) + \eta\big)\big(W (\beta - \hat{\beta}) + \eta\big)'\\
	&= \Ex \big( W (\beta - \hat{\beta})(\beta - \hat{\beta})'W' + \eta (\beta - \hat{\beta})'W' + W (\beta - \hat{\beta}) \eta' + \eta \eta' \big)\\
	&= \Big[ \text{linearity of expectation} \Big] = \\
	&= W \big( \Ex (\beta - \hat{\beta})(\beta - \hat{\beta})' \big) W' + \big( \Ex \eta(\beta - \hat{\beta}) \big)'W \\
		& \quad + W \Ex(\beta - \hat{\beta}) \eta' + \Ex \eta \eta' \\
	&= W \big( \Ex (\beta - \hat{\beta})(\beta - \hat{\beta})' \big) W' + \Ex\eta\eta' \\
	&= W \big( \sigma^2 (X'X)^{-1} \big) W' + \sigma^2 I_m \\
	&= \sigma^2 \Big( I_m + W \big( \sigma^2 (X'X)^{-1} \big) W' \Big)
\end{align*}

Now the mean square error of the prediction is given by
\begin{align*}
	\Ex \frac{1}{m}\hat{\eta}' \hat{\eta}
	&= \Ex\, \Tr \frac{1}{m} \hat{\eta}'\hat{\eta} 
	 = \frac{1}{m} \Tr \Ex \hat{\eta} \hat{\eta}' \\
	&= \frac{1}{m} \sigma^2 \Tr \big( I_m + W (X'X)^{-1} W' ) \\
	&= \sigma^2 + \frac{\sigma^2}{m} \Tr W (X'X)^{-1} W' \\
	&= \sigma^2 + \frac{\sigma^2}{m} \sum_{k=1}^m \w_k (X'X)^{-1} \w_k'
\end{align*}
where $\e\in \Real^{m\times 1}$ is a vector of ones. When $m=1$ this is equal to
\[ \Ex \hat{\eta}^2 = \sigma^2( 1 + \w_k (X'X)^{-1} \w_k' ) \]

% subsection p1subproblem_a (end)

\subsection{subproblem b} % (fold)
\label{sub:p1subproblem_b}

In the case of a simple linear regression the model matrix $X$ is given by
\[X = \big( \underset{n\times 1}{\e} \underset{n\times 1}{\mathbf{x}} \big)\]
which is an $n\times p+1$ matrix for $p=1$.

The inverse of $X'X$ is given by
\begin{align*}
 	(X'X)^{-1}
 	&= \bigg(\begin{matrix}\e'\e & \e'\mathbf{x} \\ \mathbf{x}'\e & \mathbf{x}'\mathbf{x}\end{matrix}\bigg)^{-1}\\
 	&= \bigg(\begin{matrix}n & n\bar{x} \\ n\bar{x} & n\overline{x^2}\end{matrix}\bigg)^{-1}
 \end{align*} 
where $\bar{x}\in \Real^{1\times 1}$ and $\overline{x^2}\in \Real^{1\times 1}$
are the sample mean and the mean square of the observations $\mathbf{x}\in \Real^{n\times 1}$ respectively.

The inverse of a $2\times 2$ matrix, provided it is non-singular, is given by
\[\bigg(\begin{matrix} a & b\\c & d\end{matrix}\bigg)^{-1} = \frac{1}{ad-bc}\bigg(\begin{matrix} d & -b\\-c & a\end{matrix}\bigg)\]

Therefore 
\begin{align*}
 	(X'X)^{-1}
 	&= \frac{1}{n^2 \big( \overline{x^2} - \bar{x}^2 \big)} \bigg(\begin{matrix} n\overline{x^2} & -n\bar{x} \\ -n\bar{x} & n \end{matrix}\bigg)\\
 	&= \frac{1}{n \big( \overline{x^2} - \bar{x}^2 \big)} \bigg(\begin{matrix} \overline{x^2} & -\bar{x} \\ -\bar{x} & 1 \end{matrix}\bigg)
\end{align*}

In the bivariate regression case $\w = (1\, x_{n+1})\in \Real^{1+p}$, and the
product $\w (X'X)^{-1}\w'$ reduces to
\[\frac{1}{n \big( \overline{x^2} - \bar{x}^2 \big)} \big(\begin{matrix}1 & x_{n+1}\end{matrix}\big) \bigg(\begin{matrix} \overline{x^2} & -\bar{x} \\ -\bar{x} & 1 \end{matrix}\bigg) \bigg(\begin{matrix}1 \\ x_{n+1}\end{matrix}\bigg) = \frac{1}{n}\frac{ \overline{x^2} - 2 x_{n+1}\bar{x} - x_{n+1}^2 }{ \overline{x^2} - \bar{x}^2} \]

After simplification this becomes
\begin{align*}
	W (X'X)^{-1} W'
	&= \frac{1}{n} \frac{ \overline{x^2} - \bar{x}^2 + \bar{x}^2 - 2 x_{n+1}\bar{x} - x_{n+1}^2 }{ \overline{x^2} - \bar{x}^2 } \\
	&= \frac{1}{n} + \frac{1}{n}\frac{ \bar{x}^2 - 2 x_{n+1}\bar{x} - x_{n+1}^2 }{ \overline{x^2} - \bar{x}^2} \\
	&= \frac{1}{n} + \frac{1}{n}\frac{ \big(\bar{x} - x_{n+1}\big)^2 }{ \overline{x^2} - \bar{x}^2 } \\
	&= \frac{1}{n} + \frac{ \big(\bar{x} - x_{n+1}\big)^2 }{ \sum_{i=1}^n (x_i - \bar{x})^2 }
\end{align*}
since $\overline{x^2} - \bar{x}^2$ is the sample variance of the observations
$\mathbf{x}$.

Therefore the expected squared error is 
\[
	\Ex \hat{\eta}^2
	= \sigma^2( 1 + \w_k (X'X)^{-1} \w_k' )
	= \sigma^2 \Big( 1 + \frac{1}{n} + \frac{ \big(\bar{x} - x_{n+1}\big)^2 }{ \sum_{i=1}^n (x_i - \bar{x})^2 } \Big)
\]

% subsection p1subproblem_b (end)

\subsection{subproblem c} % (fold)
\label{sub:p1subproblem_c}

In the bivariate case the expression for the square error of one-point
prediction is quadratic in $x_{n+1}$. Furthermore the squared term enters the
expression with a positive coefficient \[\frac{1}{\sum_{i=1}^n (x_i - \bar{x})^2}\geq 0\]

Therefore the term
\[\frac{ \big(\bar{x} - x_{n+1}\big)^2 }{ \sum_{i=1}^n (x_i - \bar{x})^2 } \geq 0 \]
whence its minimum is $x_{n+1} = \bar{x}$.

One could also compute the first order condition of a local maximum and then
the second order conditions to persuade oneself that this is indeed the case,
but this not needed here.

% subsection p1subproblem_c (end)

\subsection{subproblem d} % (fold)
\label{sub:p1subproblem_d}

Using the rules of matrix-matrix multiplication and the properties of the
transpose operator it is easy to see that
\[
X'X 
= \Bigg(\begin{matrix} \underset{1\times n}{\e'} \\ \underset{p\times n}{Z'} \end{matrix}\Bigg) \Big(\begin{matrix} \underset{n\times 1}{\e} & \underset{n\times p}{Z} \end{matrix}\Big)
= \bigg(\begin{matrix} \e'\e & \e'Z \\ Z\e' & Z'Z \end{matrix}\bigg) 
\]

% subsection p1subproblem_d (end)

\subsection{subproblem e} % (fold)
\label{sub:p1subproblem_e}

Notice that in 
\[ X'X = \bigg(\begin{matrix} T & U \\ V & W \end{matrix}\bigg) = \bigg(\begin{matrix} \e'\e & \e'Z \\ Z\e' & Z'Z \end{matrix}\bigg) \]
the block $T$ is given by $\e'\e$, which is a non-zero scalar and thus is an
invertible albeit degenerate $1\times 1$ matrix.

Let's employ the formula 
\[\bigg(\begin{matrix} T & U \\ V & W \end{matrix}\bigg)^{-1} = \bigg(\begin{matrix} T^{-1} + T^{-1} V Q^{-1} U T^{-1} & - T^{-1} U Q^{-1} \\ -Q^{-1} V T^{-1} & Q^{-1} \end{matrix}\bigg) \]
where $Q = W - VT^{-1}U$, which is valid when the left hand side matrix and
$T$ are invertible.

Thus one gets
\begin{align*}
	(X'X)^{-1}
	&= \bigg(\begin{matrix} (\e'\e)^{-1} + (\e'\e)^{-1} \e'Z Q^{-1} Z'\e (\e'\e)^{-1} & - (\e'\e)^{-1} \e'Z Q^{-1} \\ -Q^{-1} Z'\e (\e'\e)^{-1} & Q^{-1} \end{matrix}\bigg)
\end{align*}
where $Q = Z'Z - Z'\e (\e'\e)^{-1}\e'Z$.

First of all note the following:
\[Q = Z'Z - Z'\e (\e'\e)^{-1}\e'Z = Z'\big( I_n - \e (\e'\e)^{-1}\e' \big)Z\]

Second denote by $\bar{z}$ the vector of sample mean values of the explanatory
variables in $Z$: $\bar{z}_j = \frac{1}{n}\sum_{i=1}^n Z_{ij}$.

However in order to make further computations easier, note that the sample
mean vector is in fact given by the projection of the column vectors in $Z$
onto the space spanned by $\e$, denoted by $E = \clo{\e}$:
\[\underset{1\times p}{\bar{z}} = \underset{1\times 1}{(\e'\e)}^{-1}\, \underset{1\times n}{\e'}\,\underset{n\times p}{Z}\]

Note that $\e'\e \bar{z} = \e'Z$.

Third, in light of the second observation the $n\times n$ odd-looking matrix
$\e (\e'\e)^{-1}\e'$ is in fact a projector onto the mentioned linear one
dimensional subspace. Thus the operator $\pi$, identified by the matrix
$I_n - \e (\e'\e)^{-1}\e'$ is a projector onto the space orthogonal to $E$.

Now, let's simplify the expression of $(X'X)^{-1}$. Each element can be
simplified to
\begin{align*}
	(\e'\e)^{-1} \e'Z Q^{-1} Z'\e (\e'\e)^{-1} &= (\e'\e)^{-1} \e'\e \bar{z} Q^{-1} \bar{z}' \e'\e (\e'\e)^{-1} = \bar{z} Q^{-1} \bar{z}'\\
	(\e'\e)^{-1} \e'Z Q^{-1} &= (\e'\e)^{-1} \e'\e \bar{z} Q^{-1} = \bar{z} Q^{-1}\\
	Q^{-1} Z'\e (\e'\e)^{-1} &= Q^{-1} \bar{z}' \e'\e (\e'\e)^{-1} = Q^{-1} \bar{z}'
\end{align*}
whence
\begin{align*}
	(X'X)^{-1}
	&= \bigg(\begin{matrix} (\e'\e)^{-1} + \bar{z} Q^{-1} \bar{z}' & - \bar{z} Q^{-1} \\ - Q^{-1} \bar{z}' & Q^{-1} \end{matrix}\bigg)
	&= \bigg(\begin{matrix} \frac{1}{n} & 0 \\ 0 & 0 \end{matrix}\bigg)
	+ \bigg(\begin{matrix} \bar{z} Q^{-1} \bar{z}' & - \bar{z} Q^{-1} \\ - Q^{-1} \bar{z}' & Q^{-1} \end{matrix}\bigg)
\end{align*}
since matrices constitute a linear space and $(\e'\e)^{-1} = \frac{1}{n}$. The
zeroes in the first matrix in the right hand side expression denote
appropriately sized matrices of zeroes.

The block shapes of the matrix blocks of $(X'X)^{-1}$ are
\[\bigg(\begin{matrix} 1\times 1 & 1\times p \\ p\times 1 & p\times p \end{matrix}\bigg)\] 

% subsection p1subproblem_e (end)

\subsection{subproblem f} % (fold)
\label{sub:p1subproblem_f}

Suppose the new observation is $\w = ( 1\, z_{n+1} )\in \Real^{1+p}$.
Then the variance of the prediction error is 
\[\Ex \hat{\eta}^2 = \sigma^2( 1 + \w_k (X'X)^{-1} \w_k' )\]
the term $\w_k (X'X)^{-1} \w_k'$ is equal to
\begin{align*}
	\w_k (X'X)^{-1} \w_k'
	&= \big( \begin{matrix} 1 & z_{n+1} \end{matrix} \big) \big(X'X\big)^{-1}  \bigg(\begin{matrix}1 \\ z_{n+1}'\end{matrix}\bigg)\\
	&= \big( \begin{matrix} 1 & z_{n+1} \end{matrix} \big) \Bigg[ \bigg(\begin{matrix} \frac{1}{n} & 0 \\ 0 & 0 \end{matrix}\bigg) + \bigg(\begin{matrix} \bar{z} Q^{-1} \bar{z}' & - \bar{z} Q^{-1} \\ - Q^{-1} \bar{z}' & Q^{-1} \end{matrix}\bigg) \Bigg]  \bigg(\begin{matrix}1 \\ z_{n+1}'\end{matrix}\bigg)\\
	&= \frac{1}{n} + \big( \begin{matrix} 1 & z_{n+1} \end{matrix} \big) \bigg(\begin{matrix} \bar{z} Q^{-1} \bar{z}' & - \bar{z} Q^{-1} \\ - Q^{-1} \bar{z}' & Q^{-1} \end{matrix}\bigg) \bigg(\begin{matrix}1 \\ z_{n+1}'\end{matrix}\bigg)
\end{align*}
The last product in this expression can be simplified even further:
\begin{align*}
	\ldots
	&= \big( \begin{matrix} 1 & z_{n+1} \end{matrix} \big) \bigg(\begin{matrix} \bar{z} Q^{-1} \bar{z}' & - \bar{z} Q^{-1} \\ - Q^{-1} \bar{z}' & Q^{-1} \end{matrix}\bigg) \bigg(\begin{matrix}1 \\ z_{n+1}'\end{matrix}\bigg)\\
	& =  \big( \begin{matrix} \bar{z} Q^{-1} \bar{z}' - z_{n+1} Q^{-1} \bar{z}' & - \bar{z} Q^{-1} + z_{n+1} Q^{-1} \end{matrix} \big) \bigg(\begin{matrix}1 \\ z_{n+1}'\end{matrix}\bigg)\\
	&= \bar{z} Q^{-1} \bar{z}' - z_{n+1} Q^{-1} \bar{z}' - \bar{z} Q^{-1} z_{n+1}' + z_{n+1} Q^{-1} z_{n+1}'\\
	&= \big(\bar{z} - z_{n+1}\big)Q^{-1} \bar{z}' - \big(\bar{z} - z_{n+1}\big)Q^{-1} z_{n+1}'\\
	&= \big(\bar{z} - z_{n+1}\big)Q^{-1} \big(\bar{z}' - z_{n+1}'\big) = \big(z_{n+1} - \bar{z}\big)Q^{-1} \big(z_{n+1} - \bar{z}\big)'
\end{align*}
Hence
\[\w_k (X'X)^{-1} \w_k' = \frac{1}{n} + \big(z_{n+1} - \bar{z}\big)Q^{-1} \big(z_{n+1} - \bar{z}\big)'\]

Now note that
\begin{align*}
	Q
	&= Z'Z - Z'\e (\e'\e)^{-1}\e'Z \\
	&= Z'Z - \bar{z}'\e'\e (\e'\e)^{-1}\e'\e \bar{z} \\
	&= Z'Z - \bar{z}'\e'\e \bar{z} \\
	&= n \big( \frac{1}{n} Z'Z - \bar{z}'\bar{z} \big )
\end{align*}
looks similar to the expression in the denominator in the bivariate regression
case.

Next
\begin{align*}
	\w_k (X'X)^{-1} \w_k' & = \frac{1}{n} + \frac{1}{n} (z_{n+1} - \bar{z})\Gamma^{-1} (z_{n+1} - \bar{z})'
\end{align*}
where $\Gamma = \frac{1}{n} Z'Z - \bar{z}' \bar{z}$ is invertible since
$X'X$ is.

Finally the variance of the square prediction error is 
\[ \Ex \hat{\eta}^2 = \sigma^2\big( 1 + \frac{1}{n} + \frac{1}{n} (z_{n+1} - \bar{z})\Gamma^{-1} (z_{n+1} - \bar{z})' \big) \]

% subsection p1subproblem_f (end)

\subsection{subproblem h} % (fold)
\label{sub:p1subproblem_h}

Since $X'X$ is invertible, the matrix $Q = Z'Z - n \bar{z}'\bar{z}$ is
invertible. Furthermore $Q$ is invertible if and only if
$\Gamma = \frac{1}{n} Z'Z - \bar{z}'\bar{z}$ is non-singular.

It has been shown before that
\[Q = Z'Z - n \bar{z}'\bar{z} = Z'\big(I_n - \e(\e'\e)^{-1}\e'\big) Z = Z'\pi Z\]
and $\pi$ is a projection matrix onto a subspace orthogonal to $\clo{\e}$.
Therefore $\pi$ is idempotent and symmetric, whence $Z'\pi Z = Z'\pi \pi Z =
Z'\pi' \pi Z = \big(\pi Z\big)'\pi Z$. Therefore the matrix $Q$ is positive
semi-definite. It is in fact positive definite, because it is also invertible,
which means that it cannot have zero eigenvalues.

Since $\Gamma = n Q$ it must also be a positive definite matrix.

% subsection p1subproblem_h (end)

\subsection{subproblem g} % (fold)
\label{sub:p1subproblem_g}

Consider the following quadratic form
\[ f(\xi) =  \xi'A\xi - 2 b \xi + c\]
Using the very definition of a matrix vector product, it is possible to show
that its gradient with respect to $\xi$ is given by
\[\underset{n\times 1}{\nabla f(\xi)} = \underset{n\times n}{(A+A')} \underset{n\times 1}{\xi} - 2 \underset{n\times 1}{b}\]
The Hessian of $f$ is thus $A+A'$ -- an $n\times n$ matrix.

If $f$ is a positive definite quadratic form, then $A=A'$, whence 
\[\nabla f = 2 A\xi - 2 b \text{ and } \nabla^2 f = 2A\]

The extremum of $f$ is this a minimum and it is determined by the solution
\[ A\xi - b = \mathbf{0} \Leftrightarrow A\xi = b\]
Since $A$ a positive definite, the optimal solution exists and is unique.

Now the variance of the prediction error is a positive definite quadratic form
with respect to $\xi = z_{n+1} - \bar{z}$. Therefore its minimal value is
attained at $\xi$ satisfying
\[\Gamma^{-1} \xi = \mathbf{0}\]
Since $\Gamma$ is invertible, the optimal $\xi$ for the minimum of the
variance is thus $\mathbf{0}$, whence $z_{n+1} - \bar{z} = \mathbf{0}$. Thus
the prediction error variance attains its minimum at $z_{n+1} = \bar{z}$,
where its value is $\sigma^2(1+\frac{1}{n})$.

% subsection p1subproblem_g (end)

% section problem_1 (end)

\clearpage

\section{Problem 2} % (fold)
\label{sec:problem_2}

Consider a general problem of testing linear restrictions on coefficients in a linear regression model.

As usual the setting is as follows: \begin{itemize}
	\item An input set of variable represented by an $n\times p$ matrix $X$, $p<n$, with a technical condition that $X$ be full rank;
	\item A $n\times 1$ vector $T$ of responses associated with each particular observation (row) in $X$, which behaves according to the joint model
	\[T = X\beta + \epsilon\]
	with $\epsilon\sim\mathcal{N}_n(0,\sigma^2 I_n)$.
\end{itemize}

However this time it of interest to impose a set of linear constraints of the parameter $\beta$ summarized in this linear equation: $Q\beta = q$, where $Q\in \Real^{r\times p}$ is a full rank matrix, and $q\in \Real^{r\times 1}$.

In order to find the constrained lest square estimate of this model it is necessary to solve the following constrained quadratic optimization problem:
\[\frac{1}{2} (T - X\beta)'(T - X\beta) \to \min_\beta\]
subject to $Q\beta = q$. This model is called the \emph{restricted} model, whereas the model without the constraints is the \emph{unrestricted} model.


The Lagrangian of the problem is given by 
\[\mathcal{L} = \frac{1}{2} (T - X\beta)'(T - X\beta) + (Q\beta - q)'\lambda\]

which after minor simplification is given by
\[\mathcal{L} = \frac{1}{2}\big(\beta X'X\beta - T'X\beta - \beta'X'T + T'T\big) + (Q\beta - q)'\lambda\]

The first order conditions (\textbf{FOC}) are
\begin{align*}
	\frac{\partial}{D\beta} \mathcal{L} = X'X\beta - X'T + Q'\lambda = 0\\
	\frac{\partial}{D\lambda} Q\beta - q = 0
\end{align*}

It is easy to find $\hat{\lambda}$. First express $\beta$ from the first equation of the FOC:
\[\beta = (X'X)^{-1}(X'T - Q'\lambda) = \hat{\beta} - (X'X)^{-1}Q'\lambda\]
because the OLS estimate of $\beta$ is given by $\hat{\beta} = (X'X)^{-1}X'T$.
Left-multiply the expression by $Q$ and use the equation of the constraint to get
\[Q\beta = Q \hat{\beta} - Q(X'X)^{-1}Q'\lambda = q\]
Since matrices $Q$ and $X$ are full-rank the matrix $Q(X'X)^{-1}Q'$ is invertible, this implies
\[\hat{\lambda} = \big(Q(X'X)^{-1}Q'\big)^{-1}(Q\hat{\beta}-q)\]

Now lets express $\hat{\beta}_\R$ through $\hat{\beta}$.
\begin{align*}
	\hat{\beta}_\R
	&= \hat{\beta} - (X'X)^{-1}Q'\hat{\lambda} \\
	&= \hat{\beta} - (X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}(Q\hat{\beta}-q)
\end{align*}

Now let's derive the expression for the \textbf{R}esidual \textbf{S}um of \textbf{S}quares of the restricted model.
\begin{align*}
	\RSS_\R &= (T - X\hat{\beta}_\R)'(T - X\hat{\beta}_\R)\\
	&= \big(T - X\hat{\beta} + X (\hat{\beta} - \hat{\beta}_\R) \big)'\big(T - X\hat{\beta} + X (\hat{\beta} - \hat{\beta}_\R) \big)\\
	&= (T - X\hat{\beta})'(T - X\hat{\beta}) + (T - X\hat{\beta})'X (\hat{\beta} - \hat{\beta}_\R) + \\
	&+(\hat{\beta} - \hat{\beta}_\R)'X'(T - X\hat{\beta}) + (\hat{\beta} - \hat{\beta}_\R)'X'X(\hat{\beta} - \hat{\beta}_\R) \\
	&= \RSS + (\hat{\beta} - \hat{\beta}_\R)'X'X(\hat{\beta} - \hat{\beta}_\R)
\end{align*}
where $\RSS$ is the residual sum of square of the unrestricted model.

The last step requires elaboration:
\[(T - X\hat{\beta})'X (\hat{\beta} - \hat{\beta}_\R) = T\big(I_n - X(X'X)^{-1}X'\big)'X (\hat{\beta} - \hat{\beta}_\R) = 0\]
because $P_X = I_n - X(X'X)^{-1}X'$ is a projection matrix onto the space orthogonal to the column space of the matrix $X$ and $\clo{(X_i)_{i=1}^p} \perp \clo{(X_i)_{i=1}^p}^\perp$. Indeed direct calculations confirm this:
\[P_X X = X - X(X'X)^{-1}X'X = X - X = 0\]

Consider the second term in the right hand side of the final expression of $\RSS_\R$. Since 
\[(\hat{\beta} - \hat{\beta}_\R) = (X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}(Q\hat{\beta}-q)\]
one has the following
\begin{align*}
	(\hat{\beta} - \hat{\beta}_\R)'X'X(\hat{\beta} - \hat{\beta}_\R)
	&= (Q\hat{\beta}-q)'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1} (X'X) \\
	&\quad\times (X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}(Q\hat{\beta}-q)\\
	&= (Q\hat{\beta}-q)'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}Q' \\
	&\quad\times \big(Q(X'X)^{-1}Q'\big)^{-1}(Q\hat{\beta}-q)\\
	&= (Q\hat{\beta}-q)'\big(Q(X'X)^{-1}Q'\big)^{-1}(Q\hat{\beta}-q)
\end{align*}
therefore
\[\RSS_\R = \RSS + (Q\hat{\beta}-q)'\big(Q(X'X)^{-1}Q'\big)^{-1}(Q\hat{\beta}-q)\]
If the linear model is correct, the RSS of the unconstrained model is
\begin{align*}
	\RSS &= (T - X\hat{\beta})'(T - X\hat{\beta}) \\
	&= (\epsilon - X(X'X)^{-1}X'\epsilon)'(\epsilon - X(X'X)^{-1}X'\epsilon) \\
	&= \epsilon'(I_n - X(X'X)^{-1}X')'(I_n - X(X'X)^{-1}X')\epsilon = \epsilon'P_X \epsilon
\end{align*}
as $P_X$ is a projector.

Furthermore assuming that the linear restrictions are also correct in that for the true parameters $Q\beta = q$, one can simplify the second tern in the expression for $\RSS_\R$ as
\begin{align*}
	&(Q\hat{\beta}-q)'\big(Q(X'X)^{-1}Q'\big)^{-1}(Q\hat{\beta}-q) = \\
	&\quad= (\hat{\beta}-\beta)'Q'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(\hat{\beta}-\beta)\\
	&\quad= \epsilon'X(X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}X'\epsilon
\end{align*}
since $\hat{\beta}-\beta = (X'X)^{-1}X'(X\beta + \epsilon) - \beta = (X'X)^{-1}X'\epsilon$.

The matrix $P_{QX}$ defined as 
\[P_{QX} = X(X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}X'\]
is a projector. Indeed, it is symmetric and idempotent:
\begin{align*}
	P_{QX} P_{QX}
	&= X(X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}X'\\
	&\quad\times X(X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}X' \\ 
	&= X(X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}X' = P_{QX}
\end{align*}
Thus 
\[\RSS_\R = \RSS + \epsilon'X(X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}X'\epsilon = \RSS + \epsilon' P_{QX} \epsilon\]

\noindent Now it is necessary to state some results from the linear algebra and multivariate statistics.
\begin{description}
	\item[Spectral theorem] If $A\in \Real^{m\times m}$ is a symmetric matrix, then the there exists an orthogonal matrix $C$ of column eigenvectors of $A$ that constitute a basis and a diagonal matrix $\Lambda$ of associated eigenvalues such that $A = C\Lambda C'$;

	\item[Trace-rank] If $A\in \Real^{m\times m}$ is projection matrix, then its eigenvalues are either $0,$ or $1$. Indeed $A=A^2$ implies that $C\Lambda^2C' = C\Lambda C'$, whence $\Lambda^2 = \Lambda$ since $C$ is non-singular. Thus the elements of $\Lambda$ are either $0$ or $1$. A corollary is, that the trace of a projector matrix is equal to its rank, since the rank is preserved under non-singular linear transformations;

	\item[$\chi^2$-distribution] If $\epsilon\sim\mathcal{N}_m(0,I_m)$ and $A$ is a projector, then $\epsilon'A\epsilon \sim \chi^2_{\text{rank}(A)}$.
	Indeed,
	\[\epsilon'A\epsilon = \epsilon'C\Lambda C'\epsilon = (C'\epsilon) \Lambda (C'\epsilon) = \sum_{k=1}^m \lambda_k z_k^2\]
	where $Z = (z_k)_{k=1}^m = C'\epsilon$. Being a non-degenerate linear transformation of a normally distributed random vector, $Z\sim\mathcal{N}_m(0, C'I_m C)$. Thus $\epsilon'A\epsilon$ is a sum squares of $\text{rank}(A)$ independent normally distributed random variables.

	\item[Independence] If $\epsilon\sim\mathcal{N}_m(0,I_m)$ and $A,B$ are $m\times m$ projector matrices with $A B = 0$, then $\epsilon'A\epsilon$ and $\epsilon'B\epsilon$ are independent random variables.
	Indeed, due to idempotence $\epsilon'A\epsilon = \eta'\eta$ and $\epsilon'B\epsilon = \xi'\xi$ with $\eta = A'\epsilon$ and $\xi = B'\epsilon$.
	Now since $\epsilon$ is multivariate normal, both $\eta$ and $\xi$ are Gaussian. Thus $\eta$ and $\xi$ are independent if and only if they are uncorrelated. But $\eta'\xi = \epsilon'AB'\epsilon = \epsilon'AB\epsilon = 0$. Thus $\epsilon'A\epsilon\perp \epsilon'B\epsilon$
\end{description}

\noindent For the projection matrices in the problem it is true that
\begin{align*}
	\text{rank} ( X(X'X)^{-1}X' ) &= \Tr (X'X)^{-1}X'X = p \\
	\text{rank} (P_X) &= \Tr I_n - \Tr X(X'X)^{-1}X' = n-p \\
	\text{rank} (P_{QX})
	&= \Tr X(X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}X' \\
	&= \Tr \big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}X'X(X'X)^{-1}Q' \\
	&= \Tr \big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}Q' = \Tr I_r = r
\end{align*}

In the problem $\epsilon\sim\mathcal{N}_n(0,\sigma^2 I_n)$, which means that $\frac{1}{\sigma} \epsilon \sim \mathcal{N}_n(0,I_n)$. Thus we have the following results
\begin{align*}
	\frac{1}{\sigma^2}\RSS &= \frac{1}{\sigma^2}\epsilon'P_X \epsilon \sim \chi^2_{n-p} \\
	\frac{1}{\sigma^2}(\RSS_\R - \RSS) &= \frac{1}{\sigma^2}\epsilon'n P_{QX} \epsilon \sim \chi^2_r \\
	& \frac{1}{\sigma^2}\RSS \perp \frac{1}{\sigma^2}(\RSS_\R - \RSS)
\end{align*}
where the independence follows from $P_X P_{QX} = 0$:
\begin{align*}
	P_X P_{QX} &= \big(I_n - X(X'X)^{-1}X'\big) P_{QX} = P_{QX} - X(X'X)^{-1}X' P_{QX} \\
	&= P_{QX} - X(X'X)^{-1}X'X(X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}X' \\
	&= P_{QX} - X(X'X)^{-1}Q'\big(Q(X'X)^{-1}Q'\big)^{-1}Q(X'X)^{-1}X' \\
	&= P_{QX} - P_{QX} = 0
\end{align*}

Finally, the Fisher's distribution is a ratio of independent $\chi^2$ distributed random variables
\[F(m,n) \sim \frac{\chi^2_m}{m} \frac{n}{\chi^2_n}\]
Therefore under the null hypothesis $H_0: Q\beta = q$, nested under a more general hypothesis that the linear model is specified correctly, one has the following:
\[\frac{\RSS_\R - \RSS}{r \sigma^2}\frac{(n-p) \sigma^2}{\RSS} = \frac{\RSS_\R - \RSS}{\RSS}\frac{n-p}{r}\sim F(r,n-p)\]

A frequently tested linear restriction is that of the equality of a particular coefficient $\beta_k$, $k=1,\ldots,p$ of the model to a specific value -- zero. In terms of the restriction matrix $Q$ and vector $q$ this is equivalent to $Q_{1\times p} = e_k'$ and $q=0_{1\times 1}$ where $e_k$ is the $k$-th unit column vector (all zeros except for a 1 on the $k$-th coordinate). Thus
\begin{align*}
	\RSS_\R - \RSS
	&= (Q\hat{\beta}-q)'\big(Q(X'X)^{-1}Q'\big)^{-1}(Q\hat{\beta}-q) \\
	&= \hat{\beta}'e_k\big(e_k'(X'X)^{-1}e_k\big)^{-1}e_k'\hat{\beta} \\
	&= \hat{\beta}'e_k\big(v_{kk}\big)^{-1}e_k'\hat{\beta} = \frac{1}{v_{kk}}(e_k'\hat{\beta})' e_k'\hat{\beta} = \frac{1}{v_{kk}}\hat{\beta}_k^2
\end{align*}
where the scalar $v_{ij} = e_i'(X'X)^{-1}e_j$ -- the $i$-th row and $j$-th column entry of $(X'X)^{-1}$.

Therefore 
\[ \frac{\RSS_\R - \RSS}{\RSS}\frac{n-p}{r} = \frac{\RSS_\R - \RSS}{\frac{\RSS}{n-p}} = \frac{\hat{\beta}_k^2}{\frac{\RSS}{n-p}v_{kk}} \sim F(1,n-p)\]

However the two-sided test for significance of the $k$-th coefficient is performed by the t-test with the statistic
\[\frac{\lvert \hat{\beta}_k \rvert}{\hat{\sigma}\sqrt{v_{kk}}}\sim t_{n-p}\] where $\hat{\sigma}$ is given by the unbiased estimate $\sqrt{\frac{\RSS}{n-p}}$.

It is obvious that 
\[\bigg(\frac{\lvert \hat{\beta}_k \rvert}{\hat{\sigma}\sqrt{v_{kk}}}\bigg)^2 = \frac{\hat{\beta}_k^2}{\hat{\sigma}^2 v_{kk}} = \frac{\hat{\beta}_k^2}{\frac{\RSS}{n-p} v_{kk}} \]

% section problem_2 (end)
\clearpage

\section{Problem 3} % (fold)
\label{sec:problem_3}

Suppose $X\in \Real^{n\times p}$ is a full rank matrix of $p$ explanatory
variables (predictors) and $T\in \Real^{n\times 1}$ is the vector of
associated responses.

The elastic net optimisation problem, proposed by Zou and Hastie (2005), is
\[\min \Big\{ (T-X\beta)'(T-X\beta) + \lambda\big( \alpha \nrm{\beta}_2^2 + (1-\alpha) \nrm{\beta}_1 \big) \Big\}\]
for $\alpha\in [0,1]$ and $\nrm{\beta} = \sum_{k=1}^p \lvert \beta_k\rvert$.

Though it looks different, in fact it can be reduced to a lasso problem, albeit with transformed input data $X$ and response $T$. What follows is the detailed exposition of the steps required to do this.

The \textbf{S}ingular \textbf{V}alue \textbf{D}ecomposition theorem states
that for any matrix $A\in \Cplx^{n\times m}$ of rank $k\leq n,m$ there exist
unitary matrices $U\in \Cplx^{n\times n}$ and $V\in\Cplx^{m\times m}$
together with a diagonal matrix $\Sigma\in \Real^{k\times k}$ of non-negative
\emph{singular} values such that
\[X = U \Big(\begin{smallmatrix}\Sigma & 0 \\ 0 & 0\end{smallmatrix}\Big)V^*\]
where $\cdot^*$ is the matrix conjugate transpose. A unitary matrix
$U\in \Cplx^{n\times n}$ is such that $U^*U = UU^* = I_n$. If the matrix $A$ is real-valued, then $U$ and $V$ are real matrices as well.

Now, consider the matrix $X$. It is real and of full rank, whence by the SVD
theorem there exist an orthogonal $U\in \Real^{n\times n}$, a diagonal $\Sigma\in \Real^{p\times p}$ and another orthogonal $V\in \Real^{p\times p}$ such that 
\[X = U \Big(\begin{smallmatrix}\Sigma \\ 0\end{smallmatrix}\Big) V'\]
Since $X$ is full rank, the matrix $X'X$ must be non-singluar, whence $\Sigma$ must have strictly positive diagonal elements. Indeed, if there had existed such $z\neq 0$ that $X'Xz = 0$, then $z'X'Xz = 0$, whence $Xz = 0$ would have meant that $z$ are the coefficients of a non-trivial linear combination of columns of $X$, which are linearly independent by assumption of the full rank. Thus
\[\lvert X'X \rvert =
\lvert V(\begin{smallmatrix}\Sigma' & 0\end{smallmatrix})UU'\Big(\begin{smallmatrix}\Sigma \\ 0\end{smallmatrix}\Big)V'\rvert
= \lvert V(\Sigma'\Sigma + 0'0)' V'\rvert = \lvert V\Sigma'\Sigma V'\rvert = \lvert \Sigma\rvert^2 = \prod_{i=1}^p \sigma_i^2 \neq 0\]
where $\sigma_i$ is the $i$-th singular value in $\Sigma$.

Armed with this wonderful decomposition let's embark on a perilous
adventure, which if successful would yield a reduction of this problem to
that of lasso.

\subsection*{First} % (fold)
\label{sub:first}

Minor bracket expansion in the penalty term in the core expression
produces
\begin{align*}
 	\ldots + \lambda\big( \alpha \nrm{\beta}_2^2 + (1-\alpha) \nrm{\beta}_1 \big) &= \ldots + \lambda \alpha \beta'\beta + \lambda (1-\alpha) \nrm{\beta}_1
\end{align*}
this suggests that the reduced problem would have the lasso penalty term with the complexity coefficient given by $\lambda(1-\alpha)$ -- both known values.

Given this simplification, it seems logical to focus attention on this expression:
\[(T-X\beta)'(T-X\beta) + \lambda \alpha \beta'\beta\]
bearing in mind that $X = U \Big(\begin{smallmatrix}\Sigma \\ 0\end{smallmatrix}\Big) V'$.

% subsection* first (end)

\subsection*{Second} % (fold)
\label{sub:second}

The expression is equivalent to
\[T'T - \beta'X'T - T'X\beta + \beta'X'X\beta + \beta'\beta \alpha\lambda \]
Since $V$ is orthogonal, $V'=V^{-1}$ and $VV' = I_p$, and it can be transformed into
\[T'T - \beta'X'T - T'X\beta + \beta'X'X\beta + \beta'VV'\beta \alpha\lambda \]
If the following matrices $S$ and $\Delta$ are defined:
\[S = \bigg(\begin{matrix} \Sigma \\ 0 \end{matrix}\bigg)\quad\text{and}\quad \Delta = \bigg(\begin{matrix} \sqrt{\alpha \lambda}I_p \\ 0 \end{matrix}\bigg)\]
then the expression reduces to
\[T'T - \beta'X'T - T'X\beta + \beta'VS'SV'\beta + \beta'V \Delta' \Delta V'\beta \]
because $X'X = VS'U'USV' = VS'SV'$, since $U$ is orthogonal, and $\Delta\Delta' = \Delta'\Delta = \alpha\lambda I_p$.
Collecting the quadratic term results in
\[T'T - \beta'X'T - T'X\beta + \beta'V(S'S+\Delta'\Delta)V'\beta \]

% subsection* second (end)

\subsection*{Third} % (fold)
\label{sub:third}

Now, put $\Lambda = S'S+\Delta'\Delta$. Being a sum of non-negative diagonal matrices, $\Lambda$ is itself non-negative diagonal. In fact its $u$-th diagonal value is equal to $\Lambda_{ii} = \sigma_i^2 + \alpha \lambda$. Furthermore, begin a \textbf{diagonal} matrix, $\Lambda$ admits the square root $\sqrt{\Lambda}$ which is diagonal with elements equal to $\sqrt{\Lambda_{ii}}$. In addition, the root is invertible.

Define $L = \big(\begin{smallmatrix} \sqrt{\Lambda} \\ 0 \end{smallmatrix}\big)$ an $n\times p$ matrix, and observe that
\[L'L = (\begin{smallmatrix}\sqrt{\Lambda} & 0\end{smallmatrix})\Big(\begin{smallmatrix}\sqrt{\Lambda} \\ 0\end{smallmatrix}\Big) = \sqrt{\Lambda}\sqrt{\Lambda} + 0 = S'S + \Delta'\Delta = \Sigma'\Sigma + \alpha \lambda I_p\]
Furthermore, for
\[\tilde{L}_{n\times p} = \bigg(\begin{matrix}\Lambda^{-\sfrac{1}{2}} \\ 0\end{matrix}\bigg)\]
it is true that
\[\tilde{L}'L = L'\tilde{L} = \Lambda^{-\sfrac{1}{2}} \sqrt{\Lambda} + 0 = I_p\]
and
\[L\tilde{L}' = \tilde{L} L'= \bigg(\begin{matrix} \sqrt{\Lambda}\Lambda^{-\sfrac{1}{2}} & 0 \\ 0 & 0\end{matrix}\bigg) = \bigg(\begin{matrix} I_p & 0 \\ 0 & 0\end{matrix}\bigg)\]
Meanwhile $U'U = I_n$ implies
\[ \Sigma'\Sigma + \alpha \lambda I_p = S'S + \Delta'\Delta = L'L = L'U'UL \]
Thus the core expression becomes
\[T'T - \beta'X'T - T'X\beta + \beta'VL'U'ULV'\beta \]
Next one has $VL'U'ULV' = (ULV')'(ULV')$, which means that with $\tilde{X} = ULV'$ it is possible to express $X$ as
\[X = USV' = USI_pV' = US\tilde{L}'LV' = US\tilde{L}'I_nLV' = US\tilde{L}'U'ULV' = US\tilde{L}'U'\tilde{X}\]
The core expression is thus further ``simplified'' to
\[T'T - \beta'\tilde{X}'U\tilde{L}S'U'T - T'US\tilde{L}'U'\tilde{X}\beta + \beta'\tilde{X}'\tilde{X}\beta \]

% subsection* third (end)

\subsection*{Fourth} % (fold)
\label{sub:fourth}
If one puts $\tilde{T} = U\tilde{L}S'U'T$, then the expression finally becomes
\[T'T - \beta'\tilde{X}'\tilde{T} - \tilde{T}'\tilde{X}\beta + \beta'\tilde{X}'\tilde{X}\beta\]
whence it equals
\[ T'T - \tilde{T}'\tilde{T} + (\tilde{T} - \tilde{X}\beta)'(\tilde{T} - \tilde{X}\beta) \]

The term $T'T - \tilde{T}'\tilde{T}$ is essentially constant: it is determined by the sample data and the complexity parameters $\alpha$ and $\lambda$, while being entirely independent of $\beta$. With these transformations the elastic net problem becomes the lasso:
\[\min\Big\{ (\tilde{T} - \tilde{X}\beta)'(\tilde{T} - \tilde{X}\beta) + \lambda(1-\alpha)\nrm{\beta}_1 \Big\}\]
where
\begin{align*}
	\tilde{T} &= U\tilde{L}S'U'T = U\bigg(\begin{matrix} \sqrt{\Sigma'\Sigma+\alpha \lambda I_p}^{-1} \\ 0 \end{matrix}\bigg) V' X'T\\
	\tilde{X} &= U\bigg(\begin{matrix} \sqrt{\Sigma'\Sigma+\alpha \lambda I_p} \\ 0 \end{matrix}\bigg) V'
\end{align*}
This establishes the equivalence.

% subsection* fourth (end)

\subsection*{Final} % (fold)
\label{sub:final}

Observe that 
\[\tilde{L}S' = \bigg(\begin{matrix}\Lambda^{-\sfrac{1}{2}} \\ 0\end{matrix}\bigg) \bigg(\begin{matrix} \Sigma' & 0 \end{matrix}\bigg) = \bigg(\begin{matrix} \Lambda^{-\sfrac{1}{2}}\Sigma & 0 \\ 0 & 0\end{matrix}\bigg)\]
The diagonal matrix $W = \Lambda^{-\sfrac{1}{2}}\Sigma \in \Real^{p\times p}$ has especially interesting diagonal elements:
\[w_{ii} = \frac{\sigma_i}{\sqrt{ \sigma_i^2 + \alpha\lambda } }\]
Write the matrix $U$ as $\big(\begin{matrix}U_1 & U_2\end{matrix}\big)$, where $U_1\in \Real^{n\times p}$ and $U_2\in \Real^{(n-p)\times p}$.
Since $U$ is orthogonal, $U_1'U_1 = I_p$. With this decomposition of $U$ the modified response $\tilde{T}$ can be rewritten as
\[\tilde{T} = \big(\begin{matrix}U_1 & U_2\end{matrix}\big)\bigg(\begin{matrix} \Lambda^{-\sfrac{1}{2}}\Sigma & 0 \\ 0 & 0\end{matrix}\bigg)\bigg(\begin{matrix}U_1' \\ U_2'\end{matrix}\bigg)T = U_1 W U_1'T \]
which is essentially a projection of $T$ onto the subspace spanned by the columns of $U_1$ with each direction scaled by the appropriate element of the diagonal of $W$, though the matrix itself in not a projector. Therefore
\[\tilde{T} = \sum_{j=1}^p w_{jj} U_{1\cdot j} U_{1\cdot j}'T\]
where $U_{1\cdot j}$ is the $j$-th column of $U_1$.

Now, the squares of singular values are in fact the eigenvalues of the scaled sample covariance matrix $X'X = V\Sigma'\Sigma V'$ with associated eigenvectors give by the columns of $V$. In turn, the eigenvectors determine the principal components of the column space of $X$: the larger the eigenvalue, the more variance is that direction in the subspace.

This means that before running lasso, the elastic net actually amplifies the variance of all principal directions of the predictors $X$ with non-linear gain $\frac{\tilde{\sigma}_i}{\sigma_i} = \sqrt{1+\frac{\alpha\lambda}{\sigma_i}}$. At the same time, the coordinate of the response vector with respect to the $j$-th principal direction of the columnspace of $X$, $U_{1\cdot j}'T$, is shrunk by the reciprocal of the gain $\frac{1}{w_{jj}}$.

Therefore the optimal coefficients $\beta_{\text{el}}$ are shrunk.

The elastic net is equivalent to running lasso on $(\tilde{X}, \tilde{T})$:
\[\min\Big\{ (\tilde{T} - \tilde{X}\beta)'(\tilde{T} - \tilde{X}\beta) + \lambda(1-\alpha)\nrm{\beta}_1 \Big\}\]
It permits model selection due to the lasso term as well as coefficient shrinkage, due to the fact that the predictors and the response are amplified and shrunk respectively along the principal components of the predictors $X$.

% subsection* final (end)

% section problem_3 (end)
\clearpage

\section{Problem 4} % (fold)
\label{sec:problem_4}

\subsection{subproblem a} % (fold)
\label{sub:p4subproblem_a}

The penalized sum of squares
\[\RSS(\lambda,\Sigma) = (T-X\beta)'(T-X\beta) + \lambda \beta'\Sigma_1^{-1}\beta\]

Differentiating with respect to $\beta$ yields the first order conditions:
\[ 2 X'X\beta - 2 X'T + 2\lambda \Sigma_1^{-1}\beta = 0\]
and the second order conditions:
\[ X'X + \lambda \Sigma_1^{-1} \text{-- positive semi-definite}\]

since the matrix of predictors is assumed to be full rank, the $X'X$ is positive definite. The correlation matrix $\Sigma_1$, by definition, is positive semi-definite whence the inverse is positive semi-definite as well.
Therefore the sum $X'X + \Sigma_1^{-1}$ must be positive semi-definite too, meaning that the solution to the first order conditions is the global minimizer of the $\RSS(\lambda,\Sigma_1)$.

The optimal $\beta$ is given by
\[\hat{\beta} = \big(X'X + \lambda \Sigma_1^{-1}\big)^{-1}X'T\]

% subsection p4subproblem_a (end)

\subsection{subproblem b} % (fold)
\label{sub:p4subproblem_b}

To measure the effect of the prior correlation matrix on the shrinkage of the estimates $\hat{\beta}$ the credit scoring data\footnotemark is studied. In particular the dependence of the credit ``Rating'' on the some other variables is examined (see the listing below).
\footnotetext{ ``An Introduction to Statistical Learning, with applications in R'' (Springer, 2013) with permission from the authors: G. James, D. Witten, T. Hastie and R. Tibshirani;\url{http://www-bcf.usc.edu/~gareth/ISL/data.html}}

First, load the dataset and setup the necessary procedures.
\begin{Scode}{fig=FALSE,echo=TRUE}
## Use Sweave to compile this report
# % setwd( paste0(
# % 	"~/study_notes/year_14_15/spring_2015",
# % 	"/modern_decision_making/assignments/report" ) )
# % Sweave( file = "../Assign01.Stex", syntax = SweaveSyntaxLatex )
# % http://www-bcf.usc.edu/~gareth/ISL/Credit.csv
## Load the dataset and remove the first column
  Credit <- read.csv( "../Credit.csv" )[ , -1 ]

## Centre and scale the input and the response, since
##  penalized LS problems are not scale insensitive any more.
## Note that there is no constant term!
  X <- scale( model.matrix(
  	Rating ~ Income + Limit + Cards + Age + Education + Balance - 1,
  	Credit ) )
  Y <- scale( Credit$Rating ) #$

## A correlation matrices of an AR(1) process: the coefficients
##  are less correlated the farther they are apart in terms of their
##  indices.
  sigma_1 <- function( rho, p = 4 )
    rho ^ abs( outer( 1:p, 1:p,  `-` ) )

## Another matrix with constant correlation between the coefficient
  sigma_2 <- function( rho, p = 4 )
    rho ^ abs( outer( 1:p, 1:p, `!=` ) )
\end{Scode}

First, it should be noted, that $\Sigma_1$ prior correlation matrix actually assumes some structure in the predictors: indeed, the coefficient $\beta_i$ (of the predictor $x_i$) is more correlated with $\beta_{i+s}$ for $s=1$ rather than $s>1$.

Second, the matirx $\Sigma_2$ does not seem to be positive definite for all $\rho\leq 0$. Therefore only $\rho\in [0,1]$ are considered.

Below is the code for a generic estimator derived in the previous subsection.
\begin{Scode}
ridge <- function( X, T, rho = 0, lambda = 10, sigma = NULL ) {
  if( !is.function( sigma ) )
    sigma <- function( r, p ) diag( p )
## Precompute the matrix-matrix products
  XX <- crossprod( X ) ; XT <- crossprod( X, T )
## For each \rho
  sim <- lapply( rho, function( r ) {
##  compute the inverse of \Sigma
    sigma_inv <- solve( sigma( r, ncol( X ) ) )
##  and the ridge estimator.
    sapply( lambda, function( l )
      crossprod( solve( XX + l * sigma_inv ), XT ) )
  } )
## Present the results in by-coefficient format
  if( ncol( X ) > 1 ) {
    structure( names = colnames( X ), 
      lapply( 1 : ncol( X ), function( i ) aperm(
        sapply( sim, `[`, i, seq_along( lambda ) ),
        c( 2, 1 ) ) ) )
  } else {
    structure( names = colnames( X ), 
      list( aperm( simplify2array( sim ), c( 2, 1 ) ) ) )
  }
}
\end{Scode}

Define a couple of plotting procedures to facilitate the study the effects of the prior covariance matrix.
\begin{Scode}
## Study the absolute value of a coefficient by plotting it as
##  a heat map.
hmap <- function( res, i )
  image( rho, lambda, abs( res[[ i ]] ), col = heat.colors( 12 ),
    main = names( res )[ i ] )

## Plot the coefficients for a particular values of rho.
dync <- function( res, j, i ) {
  data <- lapply( j, function( x ) res[[x]][i,] )
## Setup the plotting area
  plot( range( lambda ), range( unlist( data ) ), type = "n",
    log = "x", xlab = "lambda (log scale)",
    ylab = "Bayes ridge regression coefficient estimates" )
## Pick some colours and plot everythin all at once
  colours <- topo.colors( n = length( j ) )
  invisible( lapply( seq_along( data ), function( x ) {
    lines( lambda, data[[x]], col = colours[ x ] )
  } ) )
## Never forget to annotate!
  legend( "bottomleft", names( res )[ j ], ncol = 2,
    lty = c( 1, 1, 1, 1 ), col = colours )
}
\end{Scode}

Having finished the setting up, run the experiment for both kinds of prior covariance matrices.
\begin{Scode}
## Initialize the grid
  rho <- seq( 0, .99, by = 0.01 )
  lambda <- 10 ^ seq( -2, 3.5, length = 100 )
## Run the experiment
  br1 <- ridge( X, Y, rho, lambda, sigma = sigma_1 )
  br2 <- ridge( X, Y, rho, lambda, sigma = sigma_2 )
\end{Scode}

Below the absolute magnitudes of the estimated coefficients are plotted against the $\lambda$-$\rho$ 100$\times$100-grid which is logarithmic in $\lambda$.

\begin{figure}[htb]\begin{center}
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br1, 1 )
		\end{Scode}
	\end{minipage}\hfill
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br1, 2 )
		\end{Scode}
	\end{minipage}
	\caption{Parameters ``\Sexpr{ colnames( X )[ 1 ] }'' and ``\Sexpr{ colnames( X )[ 2 ] }'' for $\Sigma_1$-type covariance matrix.}
	\label{fig:fig01}
\end{center}\end{figure}

\begin{figure}[htb]\begin{center}
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br1, 3 )
		\end{Scode}
	\end{minipage}\hfill
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br1, 4 )
		\end{Scode}
	\end{minipage}
	\caption{Parameters ``\Sexpr{ colnames( X )[ 3 ] }'' and ``\Sexpr{ colnames( X )[ 4 ] }'' for $\Sigma_1$-type covariance matrix.}
	\label{fig:fig02}
\end{center}\end{figure}

\begin{figure}[htb]\begin{center}
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br1, 5 )
		\end{Scode}
	\end{minipage}\hfill
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br1, 6 )
		\end{Scode}
	\end{minipage}
	\caption{Parameters ``\Sexpr{ colnames( X )[ 5 ] }'' and ``\Sexpr{ colnames( X )[ 6 ] }'' for $\Sigma_1$-type covariance matrix.}
	\label{fig:fig03}
\end{center}\end{figure}

The overall effect does not seem be to be uniform, but the higher the $\rho$ the more the complexity parameter seems to 

\begin{figure}[htb]\begin{center}
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br2, 1 )
		\end{Scode}
	\end{minipage}\hfill
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br2, 2 )
		\end{Scode}
	\end{minipage}
	\caption{Parameters ``\Sexpr{ colnames( X )[ 1 ] }'' and ``\Sexpr{ colnames( X )[ 2 ] }'' for $\Sigma_2$-type covariance matrix.}
	\label{fig:fig04}
\end{center}\end{figure}

\begin{figure}[htb]\begin{center}
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br2, 3 )
		\end{Scode}
	\end{minipage}\hfill
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br2, 4 )
		\end{Scode}
	\end{minipage}
	\caption{Parameters ``\Sexpr{ colnames( X )[ 3 ] }'' and ``\Sexpr{ colnames( X )[ 4 ] }'' for $\Sigma_2$-type covariance matrix.}
	\label{fig:fig05}
\end{center}\end{figure}

\begin{figure}[htb]\begin{center}
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br2, 5 )
		\end{Scode}
	\end{minipage}\hfill
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			hmap( br2, 6 )
		\end{Scode}
	\end{minipage}
	\caption{Parameters ``\Sexpr{ colnames( X )[ 5 ] }'' and ``\Sexpr{ colnames( X )[ 6 ] }'' for $\Sigma_2$-type covariance matrix.}
	\label{fig:fig06}
\end{center}\end{figure}

\clearpage
The effects of the $\Sigma_2$-type prior covariance seem to be similar to the $\Sigma_1$ case at least for the credit scoring data used. There also appears to be some dependence on the correlation structure of the predictors. Therefore in order to avoid this interference from the data and to control the analysis, let's simplify the problem and use $p$ orthogonal predictors with Gaussian noise. If the model is $T = X\beta+ \epsilon$ and $X'X = I_p$, then the expectation of the estimate is given by
\[\Ex\hat{\beta} = \Ex\big(X'X + \lambda \Sigma^{-1}\big)^{-1}X'T = \big(I_p + \lambda \Sigma^{-1}\big)^{-1}X'\Ex\big( X\beta+\epsilon\big) = \big(I_p + \lambda \Sigma^{-1}\big)^{-1}\beta\]
Since the predictors are orthogonal, there is no interference between $\hat{\beta}$ due to the data and the transformation is entirely due to the prior covariance matrix. The procedure below implements this test estimator:
\begin{Scode}{fig=FALSE,echo=TRUE}
coef_gain <- function( beta_0, lambda, rho, sigma = NULL ) {
  if( !is.function( sigma ) )
    sigma <- function( r, p ) diag( p )
  lapply( rho, function( r ) {
    sigma_inv <- solve( sigma( r, length( beta_0 ) ) )
    aperm( sapply( lambda, function( l )
      crossprod( solve( diag( length( beta_0 ) )
        + l * sigma_inv ), beta_0 ) ), c( 2, 1 ) )
  } )
}

## Gauge the effects of the prior covariance matrix
rho <- seq( 0, .99, by = 0.01 )
lambda <- 10 ^ seq( -2, 1, length = 100 )
beta <- seq( -3, 3, by = 0.1 )
gain1 <- coef_gain( beta, lambda, rho, sigma_1 )
gain2 <- coef_gain( beta, lambda, rho, sigma_2 )
\end{Scode}

\clearpage
The plots below depict the effect of the complexity parameter $\lambda$ on the magnitude of coefficients, for a given value of $\rho$ and for the prior covariance matrix $\Sigma_1$.
\begin{figure}[htb]\begin{center}
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			image( lambda, beta, abs( gain1[[ 1 ]] ), col = heat.colors( 12 ), ylab = "magnitude of beta", xlab = "complexity parameter" )
		\end{Scode}
	\end{minipage}\hfill
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			image( lambda, beta, abs( gain1[[ 34 ]] ), col = heat.colors( 12 ), ylab = "magnitude of beta", xlab = "complexity parameter" )
		\end{Scode}
	\end{minipage}
	\caption{The effect of $\rho=\Sexpr{ rho[ 1 ] }$ (left) and $\rho=\Sexpr{ rho[ 34 ] }$ (right) for $\Sigma_1$-type covariance matrix.}
	\label{fig:fig07}
\end{center}\end{figure}

\begin{figure}[htb]\begin{center}
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			image( lambda, beta, abs( gain1[[ 67 ]] ), col = heat.colors( 12 ), ylab = "magnitude of beta", xlab = "complexity parameter" )
		\end{Scode}
	\end{minipage}\hfill
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			image( lambda, beta, abs( gain1[[ 100 ]] ), col = heat.colors( 12 ), ylab = "magnitude of beta", xlab = "complexity parameter" )
		\end{Scode}
	\end{minipage}
	\caption{The effect of $\rho=\Sexpr{ rho[ 67 ] }$ (left) and $\rho=\Sexpr{ rho[ 100 ] }$ (right) for $\Sigma_1$-type covariance matrix.}
	\label{fig:fig08}
\end{center}\end{figure}

\noindent\textbf{Conclusion:} These plots clearly show, that higher values of the prior hyperparameter $\rho$ slow down the shrinkage done by the ridge regression penalty, though not linearly.

\clearpage
The plots below depict the effect of the complexity parameter $\lambda$ on the magnitude of coefficients, for a given value of $\rho$ and for the prior covariance matrix $\Sigma_2$.
\begin{figure}[htb]\begin{center}
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			image( lambda, beta, abs( gain2[[ 1 ]] ), col = heat.colors( 12 ), ylab = "magnitude of beta", xlab = "complexity parameter" )
		\end{Scode}
	\end{minipage}\hfill
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			image( lambda, beta, abs( gain2[[ 34 ]] ), col = heat.colors( 12 ), ylab = "magnitude of beta", xlab = "complexity parameter" )
		\end{Scode}
	\end{minipage}
	\caption{The effect of $\rho=\Sexpr{ rho[ 1 ] }$ (left) and $\rho=\Sexpr{ rho[ 34 ] }$ (right) for $\Sigma_2$-type covariance matrix.}
	\label{fig:fig09}
\end{center}\end{figure}

\begin{figure}[htb]\begin{center}
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			image( lambda, beta, abs( gain2[[ 68 ]] ), col = heat.colors( 12 ), ylab = "magnitude of beta", xlab = "complexity parameter" )
		\end{Scode}
	\end{minipage}\hfill
	\begin{minipage}{0.50\textwidth}
		\centering
		\begin{Scode}{fig=TRUE,echo=FALSE}
			image( lambda, beta, abs( gain2[[ 100 ]] ), col = heat.colors( 12 ), ylab = "magnitude of beta", xlab = "complexity parameter" )
		\end{Scode}
	\end{minipage}
	\caption{The effect of $\rho=\Sexpr{ rho[ 68 ] }$ (left) and $\rho=\Sexpr{ rho[ 100 ] }$ (right) for $\Sigma_2$-type covariance matrix.}
	\label{fig:fig10}
\end{center}\end{figure}

\noindent\textbf{Conclusion:} Higher values of the prior hyperparameter $\rho$ for the second type covariance matrix enable greater shrinkage for the smae complexity parameter $\lambda$.

% subsection p4subproblem_b (end)

% section problem_4 (end)
\clearpage

\section{Problem 5} % (fold)
\label{sec:problem_5}

This problem is mostly solved in the accompanying \textbf{R}-script by the name \emph{Problem5\_Nazarov.R}.

A brief summary of the main results:
\begin{itemize}
	\item a generic procedure for $k$-fold cross-validation of linear models has been implemented, see \emph{lm.kfold(...)}, which includes LOOCV;
	\item The $k$-fold cross validation was run with $5$-, $10$-, $15-$ and $20$-folds and the MSE were averaged across $20$ replications of each;
	\item According to the LOOCV the best polynomial model is of degree $3$, and the averaged $k$-fold cross validators seem to agree with LOOCV;
	\item The LOOCV selected the regression cubic spline with 10 degrees of freedom yielding $7$ knots and thus $8$ regions (again unilateral agreement from the $k$-fold validators as well). However it should be mentioned that the tested model was actually knot selection and the cubic spline, since the \emph{bs(...)} procedure does automatic knot selection based on empirical quantiles of the sample provided.
	\item The $R^2$ of the best polynomial and the best cubic spline is $0.715$ and $0.736$ respectively;
	\item However there seems to be a great amount of overfitting, since the overall shape on the nox$\sim$dis scatter plot suggests a multiplicative model along the line of 
	\[\log\text{nox}\propto \phi(\log\text{dis})\]
	with $\phi$ being a piecewise polynomial, for example.
\end{itemize}

% section problem_5 (end)

\end{document}
