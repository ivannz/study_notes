\documentclass[a4paper]{article}
\usepackage{geometry}

\usepackage[utf8]{inputenc}
\usepackage[russian, english]{babel}

% \usepackage{fullpage}
% \linespread{1.5}

\usepackage{graphicx, url}
\usepackage{amsmath, amsfonts}
\usepackage{mathtools}

\usepackage{multirow}

% \usepackage{natbib}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}

\newcommand{\pr}{\mathbb{P}}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\var}{\text{var}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}

\newcommand{\one}{\mathbf{1}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}
\newcommand{\lpto}{\mathop{\overset{L^p}{\to}}\nolimits}

\newcommand{\re}{\operatorname{Re}\nolimits}
\newcommand{\im}{\operatorname{Im}\nolimits}

\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}

\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}
% \selectlanguage{english}

\title{Assignment \# 2}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}

\begin{document}
\selectlanguage{english}
\maketitle

\tableofcontents
\clearpage

\section{Problem \# 1: part 1} % (fold)
\label{sec:problem_1}

Let $\bigl(x_i, t_i\bigr)_{i=1}^n\in X\times T$ be the training dataset
sampled form a multivariate distribution $(X,T)$ with $T = \{ l_k | k=1,\ldots,K\}$
where
\[
l_{km} = \begin{cases}
	1, &\text{ if } im = k\\
	- \frac{1}{K-1}, &\text{ otherwise}
\end{cases}  \,.
\]
Each $t_i$ denotes the class the data point $x_i$ is assigned to: $x_i\in \mathcal{C}_k$
whenever $t_i = l_k$. Let $\mathcal{L}: T\times T \to \Real$ be some loss function
given by
\[ \mathcal{L}(t,y) = \text{exp}\bigl\{ -\frac{1}{K} t'y \bigr\} \,. \]
A decision function $f:X\to T$ is called the population minimizer of $\mathcal{L}$ if
$\sum_{k=1}^K f_k(\cdot) = 0$ and it solves
\[ \ex_X \ex_{T|X}\mathcal{L}\bigl(T, f(X)\bigr) \to \min_{f(\cdot)} \,. \]

Conditional upon $X$ consider the following problem:
\[ \ex_{T|X} \Lcal\bigl( T, f(X) \bigr) \to \min_{f(\cdot)}\,, \]
subject to the constraint $\one'f(X) = 0$. The conditional expectation expands into
a sum of $K$ values :
\begin{align*}
	\ex_{T|X} \Lcal\bigl( T, f(X) \bigr)
	&= \sum_{k=1}^K \pr\bigl( T = l^k \big\vert X \bigr) \Lcal\bigl( l^k, f(X) \bigr)\\
	& = \ldots\,.
\end{align*}
The coding permits further simplification
\[
\ldots = \sum_{k=1}^K \pi_k \text{exp}\bigl\{ - K^{-1} f_k + (K-1)^{-1}K^{-1} \sum_{j=1,j\neq k}^K f_j \bigr\} \,,
\]
where fro all $k=1,\ldots, K$
\[ \pi_k = \pr\bigl( C = C_k \big\vert X \bigr) \,, \]
and
\[ f_k = f_k( X )\,. \]

The expression in the exponent can be further simplified :
\begin{align*}
	&  \text{exp}\Bigl\{ - K^{-1} f_k + (K-1)^{-1}K^{-1} \sum_{j=1,j\neq k}^K f_j \Bigr\}\\
	&= \text{exp}\Bigl\{ - K^{-1} (K-1)^{-1} f_k - K^{-1} f_k + (K-1)^{-1}K^{-1} \sum_{j=1}^K f_j \Bigr\}\\
	&= \text{exp}\Bigl\{ - K^{-1}\bigl( (K-1)^{-1} + 1 \bigr) f_k \Bigr\}\\
	&= \text{exp}\bigl\{ - (K-1)^{-1} f_k \bigr\} \,,
\end{align*}
where the requirement that $\sum_{j=1}^K f_j$ be $0$ was used. The problem thus
simplifies to finding the minimum
\[ \sum_{k=1}^K \pi_k \text{exp}\bigl\{ -(K-1)^{-1} f_k \bigr\} \to \min_{(f_k)_{k=1}^K} \,, \]
subject to $\sum_{j=1}^K f_j = 0$, which is in fact an ordinary constrained minimization
problem.

The Lagrangian is:
\[
L((f_k)_{k=1}^K, \lambda)
= \sum_{k=1}^K \pi_k \text{exp}\bigl\{ -(K-1)^{-1} f_k \bigr\} + \lambda \sum_{j=1}^K f_j\,.
\]

The KKT conditions for this problems are \begin{itemize}
	\item Primal feasibility: $\sum_{j=1}^K f_j = 0$;
	\item Dual feasibility: none;
	\item Complementary slackness: none;
	\item
	\[
	\frac{\partial L}{\partial f_k}
		= -(K-1)^{-1} \pi_k \text{exp}\bigl\{ -(K-1)^{-1} f_k \bigr\} + \lambda
	\,. \]
\end{itemize}
Now, for any $k,m=1,\ldots,K$ one has the following:
\[
(K-1)^{-1} \pi_k \text{exp}\bigl\{ -(K-1)^{-1} f_k \bigr\}
= \lambda
= (K-1)^{-1} \pi_m \text{exp}\bigl\{ -(K-1)^{-1} f_m \bigr\}\,,
\]
whence this relation for the optimal $f_k$'s follows
\[ f_k - f_m = (K-1)\log\frac{\pi_k}{\pi_m} \,, \]
of course still subject to $\sum_{j=1}^K f_j = 0$.

This relation enables two things: \begin{enumerate}
	\item express the minimizer $f^* = (f^*_k){k=1}_K$ through $(\pi_k)_{k=1}^K$;
	\item express the posterior probabilities of classes through the minimizer.
\end{enumerate}
Indeed, summing the relation across all $m=1,\ldots,K$ 
\[ \sum_{m=1}^K f^*_k - \sum_{m=1}^K f^*_m = (K-1)\sum_{m=1}^K \log \pi_k - \log \pi_m\,, \]
and using the constraint yields
\[ K f^*_k = (K-1) \Bigl( K \log \pi_k - \sum_{m=1}^K \log \pi_m \Bigr)\,, \]
whence
\[ f^*_k = (K-1) \Bigl( \log \pi_k - \frac{1}{K}\sum_{m=1}^K \log \pi_m \Bigr)\,. \]
In the original notation this looks like:
\[
f^*_k(x)
= (K-1) \Bigl( \log \pr\bigl( C = C_k \big\vert X \bigr)
	- \frac{1}{K}\sum_{m=1}^K \log \pr\bigl( C = C_m \big\vert X \bigr) \Bigr)
\,.\]

Now, the very same relation allows a symmetric expression of the probabilities
with respect to the optimal $f^*$. Indeed, it implies that 
\[ \text{exp}\Bigl\{ (K-1)^{-1}(f_k - f_m) \Bigr\}\pi_m = \pi_k \,, \]
which simplifies to
\[ \text{exp}\Bigl\{ -(K-1)^{-1}f_m \Bigr\} \pi_m \text{exp}\Bigl\{ (K-1)^{-1}f_k \Bigr\} = \pi_k \,, \]
whence using the fact that $\sum_{k=1}^K \pi_k = 1$ one gets:
\[
\text{exp}\Bigl\{ -(K-1)^{-1}f_m \Bigr\} \pi_m
	\sum_{k=1}^K \text{exp}\Bigl\{ (K-1)^{-1}f_k \Bigr\} = \sum_{k=1}^K \pi_k \,.
\]
Thus one gets this neat expression:
\[
\pi_m = \frac{\text{exp}\Bigl\{ (K-1)^{-1}f_m \Bigr\} }{
	\sum_{k=1}^K \text{exp}\Bigl\{ (K-1)^{-1}f_k \Bigr\}} \,.
\]

For a given $X$, finding the index of the largest component of the conditional population
minimizer $f^*(X)$ reduces to :
\begin{align*}
	\argmax_{k=1,\ldots,K} f^*_k(X)
	&= \argmax_{k=1,\ldots,K} (K-1) \Bigl( \log \pr\bigl( C = C_k \big\vert X \bigr)
		- \frac{1}{K}\sum_{m=1}^K \log \pr\bigl( C = C_m \big\vert X \bigr) \Bigr) \\
	&= \argmax_{k=1,\ldots,K} \log \pr\bigl( C = C_k \big\vert X \bigr) \,,
\end{align*}
which gives the maximum posterior rule for class assignment. This is actually the reason
for the population minimizer to be of interest in this problem.

In order to come up with a multi-class extension of the AdaBoost algorithm, let's begin
with reviewing AdaBoost for binary classification. The setting is as follows: given
some training data $\bigl(x_i,t_i\bigr)_{i=1}^n\in X\times \{-1,1\}$ AdaBoost
minimizes the exponential loss function by constructing an additive model (an ensemble)
\[ G(x) = \sum_{m=1}^M \beta_m f_m(x) \,,\]
where $(f_m)_{m=1}^M:X\to \{-1,1\}$ are binary classifiers, which are optimal in
a certain sense, and $(\beta_m)_{m=1}^M$ are their individual weights. Final classification
is performed by 
\[ G(x) = \text{sign}(F_M(x)) = \text{sign} \sum_{m=1}^M \beta_m f_m(x) \,. \]

It is possible to make the binary classification problem look similar to the multi-class
problem. Indeed, define a new classifier $h_m$ for each $f_M$ by embedding the output of
the latter in $\Real^2$ by setting:
\[ h_m(x) = \begin{pmatrix}f_m(x)\\-f_m(x)\end{pmatrix} \,,\]
and let
\[ H = \sum_{m=1}^M \beta_m h_m \,.\]
With this embedding the output of $H(x)$ is simply
\[ H(x) =\begin{pmatrix}F_M(x)\\ -F_M(x)\end{pmatrix} \,.\]
The classification rule, resulting from the majority vote, or the ``hard-max'' rule,
yields identical class assignments:
\[ \argmax_{k=1,2} H_k(x) = \begin{cases}
	1, &\text{ if } H_1(x) - H_2(x) \geq 0\\
	2, &\text{ if } H_2(x) - H_1(x) > 0
\end{cases} = \begin{cases}
	1, &\text{ if } 2 F_M(x) \geq 0\\
	2, &\text{ if } -2 F_M(x) > 0
\end{cases} = \begin{cases}
	1, &\text{ if } G(x) = +1\\
	2, &\text{ if } G(x) = -1
\end{cases} \,, \]
thus enabling generalisation of the binary classification to more than two classes.

In the heart of AdaBoost is the \textbf{F}orward \textbf{S}tagewise \textbf{A}dditive
\textbf{M}odelling approach which attempts to find an optimal solution to
\[ \sum_{i=1}^n \mathcal{L}\bigl(t_i, F(x_i)\bigr) \to \min_F \,,\]
for some loss function via the following greedy algorithm:
\begin{enumerate}
	\item set $F_0(x) = 0$;
	\item for each $m=1,\ldots,M$ do: \begin{enumerate}
		\item find \[
		(\beta_m,f_m) = \argmin_{\beta, f}
			\sum_{i=1}^n \mathcal{L}\bigl(t_i, F_{m-1}(x_i) + \beta f(x_i)\bigr)
		\,; \]
		\item set $F_m = F_{m-1} + \beta_m f_m$;
	\end{enumerate}
	\item yield $F_M$ as $F$.
\end{enumerate}

As in the original binary classification, in the multi-class setting each classifier
$(f_m)_{m=1}^M$ takes values in a finite set $T\subseteq \Real^K$, which consists of
labels $l_k$, $k=1,\ldots, K$ given by
\[ l_{kj} = 1_{k=j} - \frac{1}{K-1} 1_{k\neq j} \,. \]
Their weighted ensemble $F = \sum_{m=1}^M \beta_m f_m$ takes values in $\Real^K$.
As has been noted above, each class assignments are done via the ``hard-max'' rule
(1-vs-all):
\[
\argmax_{j=1,\ldots, K} F_j(x)
	= \argmax_{j=1,\ldots, K} \sum_{m=1}^M \beta_m f_{mj}(x)
\,. \]

Consider $m$-th iteration,$m = 1,\ldots, M$, of FSAM with multi-class exp-loss
\[ \mathcal{L}(t,y) = \text{exp}\bigl\{ -\frac{1}{K} t'y \bigr\} \,, \]
and let $F_{m-1}:X\to \Real^K$ be the function, obtained by this iteration. Then
at $m$-th iteration the algorithm tries to minimize 
\[ \sum_{i=1}^n \mathcal{L}\bigl(t_i,F_{m-1}(x_i) + \beta_m f_m(x_i) \bigr)\,, \]
with respect to $\beta_m$ and $f_m:X\to T$. The additive structure of the sought
solution combined with the exponential form of the loss function permits confining
the direct effect of prior iterations (through $F_m$) on this optimization problem
in re-weighing of individual observations. Indeed the objective function becomes
\begin{align*}
	\sum_{i=1}^n \mathcal{L}\bigl(t_i,F_{m-1}(x_i) + \beta_m f_m(x_i) \bigr)
	&= \sum_{i=1}^n \omega_{mi} \text{exp}\bigl\{ -\frac{\beta_m}{K} t_i' f_m(x_i) \bigr\}\,,
\end{align*}
where
\[ \omega_{mi} = \text{exp}\bigl\{ -\frac{1}{K} t_i'F_{m-1}(x_i) \bigr\} \,. \]
Therefore at $m$-th iteration FSAM in this instantiation seeks to solve
\[ 
\sum_{i=1}^n \omega_{mi} \text{exp}\bigl\{ -\frac{\beta_m}{K} t_i' f_m(x_i) \bigr\}
	\to \min_{\beta_m,f_m}
\,. \]

For a given classifier $f: X\to T$ the value of the dot product $t'f(x)$ in the loss
function can take only two values: if $f(x) = t$ then
\[ t'f(x) = 1 + \frac{K-1}{(K-1)^2} = \frac{K}{K-1}\,, \]
otherwise
\[ t'f(x) = -\frac{2}{K-1} + \frac{K-2}{(K-1)^2} = -\frac{K}{(K-1)^2} \,. \]
Therefore the expression in the exponent becomes
\[
-\frac{\beta_m}{K} t_i'f_m(x_i)
= -\frac{\beta_m}{K-1} 1_{t_i=f_m(x_i)}
	+ \frac{\beta_m}{(K-1)^2} 1_{t_i \neq f_m(x_i)} \,,
\]
and the objective function reduces to
\begin{align*}
	\sum_{i=1}^n \omega_{mi} \text{exp}\bigl\{ -\frac{\beta_m}{K} t_i' f_m(x_i) \bigr\}
	&= \sum_{i:t_i = f_m(x_i)} \omega_{mi} \text{exp}\bigl\{ -\frac{\beta_m}{K-1} \bigr\} \\
	&+ \sum_{i:t_i \neq f_m(x_i)} \omega_{mi} \text{exp}\bigl\{ \frac{\beta_m}{(K-1)^2} \bigr\} \,.
\end{align*}
The final minimization problem of the $m$-th iteration becomes
\[
e^{ -\frac{\beta_m}{K-1} }(1-\epsilon_m)
	+ e^{ \frac{\beta_m}{(K-1)^2} }\epsilon_m
\to \min_{\beta_m,f_m}
\,, \]
where $\epsilon_m$ is the weighted misclassification rate of the classifier $f_m$,
defined as
\[
\epsilon_m
= \frac{ \sum_{i=1}^n \omega_{mi}1_{t_i\neq f_m(x_i)} }{ \sum_{i=1}^n \omega_{mi} }
\,. \]
Hence the loss at $m$-th iteration depends on the classifier $f_m$ only through $\epsilon_m$.

Given a classifier $f_m$, the optimal weight $\beta_m$ is determined through the
first-order condition
\[
- \frac{1}{K-1} e^{ -\frac{\beta_m}{K-1} } (1-\epsilon_m)
+ \frac{1}{(K-1)^2} e^{ \frac{\beta_m}{(K-1)^2} } \epsilon_m = 0
\,, \]
whence
\[
\beta_m = \frac{(K-1)^2}{K} \biggl( \log \frac{1-\epsilon_m}{\epsilon_m} + \log (K-1) \biggr)
\,. \]
The second derivative of the loss function with respect to $\beta_m$ is obviously
non-negative:
\[
\frac{1}{(K-1)^2} e^{ -\frac{\beta_m}{K-1} } (1-\epsilon_m)
+ \frac{1}{(K-1)^4} e^{ \frac{\beta_m}{(K-1)^2} } \epsilon_m
\,,\text{--}\]
which implies that given $f_m$ this $\beta_m$ is the minimizer.

Plugging $\beta_m$ back into the objective function turns it into
\begin{multline*}
	e^{ -\frac{\beta_m}{K-1} } (1-\epsilon_m) + e^{ \frac{\beta_m}{(K-1)^2} } \epsilon_m = \\
	= \biggl( (K-1) \frac{1-\epsilon_m}{\epsilon_m} \biggr)^{ \frac{1}{K}-1 } (1-\epsilon_m)
		+ \biggl( (K-1) \frac{1-\epsilon_m}{\epsilon_m} \biggr)^{ \frac{1}{K} } \epsilon_m \,.
\end{multline*}
Denoting $\epsilon_m^{-1}$ by $x$, $x > 1$, simplifies it
\[
\bigl( (K-1) (x-1) \bigr)^{ \frac{1}{K}-1 } \frac{x-1}{x}
	+ \bigl( (K-1) (x-1) \bigr)^{ \frac{1}{K} } \frac{1}{x}
\,. \]

Therefore, the goal of the $m$-th iteration after substituting $\beta_m$ is
to find such classifier $f_m$ that the reciprocal of the weighted misclassification
rate $x$ of which solves
\[
L(x)
	= (x-1)^{ \frac{1}{K} } \frac{1}{x} (K-1)^{ \frac{1}{K} } \frac{K}{K-1}
	\to \min_{x > 1}
\,, \]
after minor simplification. Since the feasible set $x > 1$ of this problem is not
compact, there is no guarantee that there exist both its minimizer and maximizer. 

The derivative of this expression with respect to $x$ (up to a constant) is 
\[
\frac{ \frac{1}{K}(x-1)^{ \frac{1}{K}-1 }x - (x-1)^{ \frac{1}{K} } }{x^2}
= \frac{ x - K (x-1) }{x^2}\frac{1}{K} (x-1)^{ \frac{1}{K}-1 }
\,, \]
and its zero is achieved at $x_0 = \frac{K}{K-1}$. For $x\in[1,x_0)$ the derivative
is greater than zero, whereas for all $x>x_0$ is it strictly less. Therefore the
point $x_0$ is a local maximizer of the objective. However the behaviour of the first
derivative of $L$ implies that, since $K\geq 2$ the objective function monotonically
approaches zero as $x$ gets larger: $L(x)\downarrow 0$ as $x\to \infty$, -- and
the same time $L(x) \downarrow 0$ for $x \to 1$.

Thus there are two limiting solutions. The solution $x\to 1$ in fact corresponds to
finding a perfect multi-class ``misclassifier'' -- a classifier either so ``corrupt''
that it always assigns incorrect classes, almost on purpose. For obvious reasons of
parsimony and interpretability of the ensemble such ignorant classifiers are very
undesirable.

In contrast, the other solution, $x\to \infty$, corresponds to a classifier $f_m$,
the weighted misclassification rate $\epsilon_m$ of which tends to zero, meaning
that $f_m$ attempts to solve the classification problem correctly for the given
weights $(\omega_{mi})_{i=1}^n$.

In conclusion, the optimal $f_m$ and $\beta_m$ at the $m$-th iteration of FSAM are
given by
\[ f_m = \argmin_f \sum_{i=1}^n \omega_{mi} 1_{t_i \neq f(x_i)} \,, \]
and
\[
\beta_m
	= \frac{(K-1)^2}{K} \biggl( \log \frac{1-\epsilon_m}{\epsilon_m} + \log (K-1) \biggr)
\,, \]
respectively for
\[
\epsilon_m
	= \biggl( \sum_{i=1}^n \omega_{mi} \biggr)^{-1} \sum_{i=1}^n \omega_{mi} 1_{t_i \neq f(x_i)}
\,. \]

Now, let's see how the weight get updated after the $m$-th iteration. Since
\[ F_m = F_{m-1} + \beta_m f_m \,, \]
the new weights are
\begin{align*}
	\omega_{(m+1)i}
	&= \text{exp}\bigl\{ -\frac{1}{K} t_i'F_m(x_i) \bigr\} \\
	&= \text{exp}\bigl\{ -\frac{1}{K} t_i'F_{m-1}(x_i) \bigr\} \text{exp}\biggl\{ -\frac{\beta_m}{K} t_i'f_m(x_i) \biggr\}\\
% -\frac{\beta_m}{K-1} 1_{t_i = f_m(x_i)} + \frac{\beta_m}{(K-1)^2} 1_{t_i \neq f_m(x_i)}
% -\frac{\beta_m}{K-1} + \biggl( \frac{\beta_m}{(K-1)^2} + \frac{\beta_m}{K-1} \biggr) 1_{t_i \neq f_m(x_i)}
% -\frac{\beta_m}{K-1} + \frac{\beta_m}{K-1} \frac{K}{K-1} 1_{t_i \neq f_m(x_i)}
	&= \omega_{mi} e^{-\frac{\beta_m}{K-1}} \text{exp}\biggl\{ \beta_m \frac{K}{(K-1)^2} 1_{t_i \neq f_m(x_i)} \biggr\} \,.
\end{align*}
Note that only the value of the objective function depends on the total sum of
the weights and the optimal value of $\beta_m$ and the ``optimal'' classifier $f_m$
depend on 
\[ \epsilon_m = \frac{\sum_{i=1}^n \omega_{mi} 1_{t_i \neq f_m(x_i)} }{\sum_{i=1}^n\omega_{mi}} \,. \]
Therefore it is possible to ignore constant multipliers in the weight update expression.

The final multi-class extension of AdaBoost algorithm is \begin{enumerate}
	\item set $\omega_{0i} = \frac{1}{n}$ and $F_0 = 0$;
	\item for $m=1,\ldots, M$ do: \begin{enumerate}
		\item find a classifier solving
		\[ f_m = \argmin_f \sum_{i=1}^n \omega_{mi}  1_{t_i \neq f(x_i)} \,, \]
		and let $\epsilon_m$ be the achieved weighted misclassification rate;
		\item set the importance weight of the classifier $f_m$ to
		\[
		\beta_m =
			\frac{(K-1)^2}{K} \log (K-1) + \frac{(K-1)^2}{K} \log \frac{1-\epsilon_m}{\epsilon_m}
		\,; \]
		\item update the weights using the rule
		\[
		\omega_{(m+1)i} \propto
			\omega_{mi} \text{exp}\bigl\{ \frac{ \beta_m K}{(K-1)^2} 1_{t_i \neq f_m(x_i)} \bigr\}
		\,, \]
		and normalize them, so that they sum to $1$;
		\item set $F_m = F_{m-1} + \beta_m f_m$;
	\end{enumerate}
	\item return $F_M$.
\end{enumerate}
Another expression of the weight updating, which makes the effect on individual observations
more apparent, is to use
\[ \omega_{(m+1)i} \propto
	\omega_{mi} \biggl( \frac{1-\epsilon_m}{\epsilon_m} \biggr)^{ (K-1) 1_{t_i \neq f_m(x_i)} }
\,, \]
and normalize. If $f_m$ is a good classifier, meaning that $\epsilon_m$ is close
to $1$, then all misclassified observations would gain very weight. This elevated
importance would force the next classifier to focus on them. In contrast, a weak
classifier, with misclassification rate close to $\frac{1}{2}$ would almost negligibly
affect the weights of misclassified data points. Hence each weight is the accumulated
classification importance.

% section problem_1 (end)

\section{Problem \# 1: part 2} % (fold)
\label{sec:problem_1_part_2}

\subsection{subproblem v} % (fold)
\label{sub:subproblem_v}

In the notation and class coding given in the problem the multinomial log-likelihood
is given by
\[
L\bigl((x_i,t_i)_{i=1}^n\bigr) = \sum_{i=1}^n \log \prod_{k=1}^K p_k(x_i)^{t_{ik}} \,,
\]
which given the expression for the class probabilities educes to
\[
\ldots
= \sum_{i=1}^n \sum_{k=1}^K t_{ik} f_k(x_i)
- \sum_{i=1}^n \sum_{k=1}^K t_{ik} \log \sum_{j=1}^K e^{f_j(x_i)} \,,
\]
which simplifies to
\[
\ldots = \sum_{i=1}^n \sum_{k=1}^K t_{ik} f_k(x_i) - \log \sum_{j=1}^K e^{f_j(x_i)} \,,
\]
since $\sum_{k=1}^K t_{ik} = 1$ for any observation $i$ by definition.

% subsection subproblem_v (end)

\subsection{subproblem vi} % (fold)
\label{sub:subproblem_vi}

Let's compute the gradient and the Hessian with respect to $(f_k)_{k=1}^K$ of the
log-likelihood computed over the observations in region $R$:
\[ L = \sum_{i:x_i\in R} \sum_{k=1}^K t_{ik} f_k(x_i) - \log \sum_{j=1}^K e^{f_j(x_i)} \,. \]
However instead of functional variation of $f_k(\cdot)$ by an arbitrary function
of $x$, lets consider only ``uniform'' variation $\delta f_k(\cdot) = \gamma_k$ for
some constant $\gamma_k$, $k=1,\ldots, K-1$. The log-likelihood with this adjustment
looks like
\[
L = \sum_{i:x_i\in R} \sum_{k=1}^K t_{ik} f_k(x_i) + \gamma_k - \log \sum_{j=1}^K e^{f_j(x_i)}  e^{\gamma_j}\,.
\]

The derivative of the log-likelihood with respect to the adjustments is
\[
\frac{\partial L}{\partial \gamma_j}
= \sum_{i:x_i\in R} t_{ij} - \frac{e^{f_j(x_i)+\gamma_j}}{\sum_{m=1}^K e^{f_m(x_i)+\gamma_m}}\,,
\]
for $j=1,\ldots, K-1$, which simplifies at $\gamma_k = 0$ to 
\[ \frac{\partial L}{\partial \gamma_j} = \sum_{i:x_i\in R} t_{ij} - p_j(x_i)\,. \]
The second partial derivative of the log-likelihood for $j=l$, $j,l=1,\ldots, K-1$, is
\[
\frac{\partial^2 L}{\partial \gamma_j^2}
= - \sum_{i:x_i\in R} \frac{e^{f_j(x_i) + \gamma_j }\sum_{m=1}^K e^{f_m(x_i) + \gamma_m }
	- e^{2(f_j(x_i) + \gamma_j)} }{\bigl(\sum_{m=1}^K e^{f_m(x_i) - \gamma_j}\bigr)^2}\,,
\]
whence at $\gamma_k=0$
\[
\frac{\partial^2 L}{\partial f_j^2}
= - \sum_{i:x_i\in R} p_j(x_i) - p_j^2(x_i)
= - \sum_{i:x_i\in R} p_j(x_i) \bigl(1 - p_j(x_i)\bigr)\,.
\]
The second cross partial derivative for $j\neq l$, $j,l=1,\ldots, K-1$, is given by
\[
\frac{\partial^2 L}{\partial f_j \partial f_l}
= \sum_{i:x_i\in R} \frac{e^{f_j(x_i)+\gamma_j}e^{f_l(x_i)+\gamma_l}}{\bigl(\sum_{m=1}^K e^{f_m(x_i)+\gamma_m}\bigr)^2}\,,
\]
which at $\gamma_k=0$ reduces to
\[
\frac{\partial^2 L}{\partial f_j \partial f_l}
= \sum_{i:x_i\in R} p_j(x_i) p_l(x_i)\,.
\]

Recall the Newton-Raphson method. Suppose a function $f:\Real^d\to\Real$ is twice
differentiable and we want to find a root of its derivative. Given some point $a$
-- an estimate of the root of $\nabla f$, lets' approximate $f$ by a quadratic form
based on the $2$-nd order Taylor expansion of $f$ around $a$ : $f(x) \approx q(x)$
near $a$ and
\[
q(x) = f(a) + \nabla f(a)'(x-a) + \frac{1}{2}(x-a)'\nabla^2 f(a) (x-a)\,,
\]
where $\nabla f(a)$ is the gradient of $f$ evaluated at $a$ and $\nabla^2 f(a)$ is
the matrix partial second derivatives of $f$ at $a$ (the Hessian):
\[
H_{ij} = \biggl. \frac{\partial^2}{\partial x_i \partial x_j} f\biggr\rvert_{x=a}\,.
\]
The quadratic form is 
\[
q(x) = f(a) - \nabla f(a)'a + \frac{1}{2}x'\nabla^2 f(a) a
+ \bigl( \nabla f(a)' - a'\nabla^2 f(a) \bigr)x + \frac{1}{2}x'\nabla^2 f(a) x\,,
\]
and it is convex, since the Hessian matrix of $f$ at $a$ is positive definite (?).
Thus its minimum is given by
\[ \nabla^2 f(a) x + \nabla f(a) - \nabla^2 f(a) a = 0\,,\]
whence
\[ x^* = (\nabla^2 f(a))^{-1} \bigl( \nabla^2 f(a) a - \nabla f(a)\bigl)\,.\]
Thus
\[
x^* = a - (\nabla^2 f(a))^{-1}\nabla f(a)\,.
\]
This $x^*$ is guaranteed to yield a value of $f$ closer to an extremum $x_0$ up to a
second order approximation, where $\nabla f(x_0)=0$.

Therefore, we may use Newton-Raphson updating to iteratively approximate the root of
the first derivative of $L$ with respect to $(\gamma_k)_{k=1}^K$, which in the case of
convex log-likelihood would yield a minimum. However instead of computing the inverse
of the Hessian it is possible to use the reciprocal of its main diagonal of to the diagonal
of its inverse, provided, of course, the off-diagonal elements of the Hessian are
dominated by its diagonal. Indeed, $p_i p_j$ is usually an order of magnitude smaller
than $p_i (1-p_i)$.

Thus the update of $\gamma_k$ from $0$ for each $k=1,\ldots, K-1$ is
\[
\gamma_k^+ = - \frac{ \sum_{i:x_i\in R} t_{ij} - p_j(x_i) }{- \sum_{i:x_i\in R} p_j(x_i) \bigl(1 - p_j(x_i)\bigr)}\,,
\]
and for the sake of simplicity of notation put $\gamma_K^+ = 0$.

% subsection subproblem_vi (end)

\subsection{subproblem vii} % (fold)
\label{sub:subproblem_vii}

Now consider the proposed normalisation of the update. The intuition behind it is
that by subtracting the average it is possible to make any set of values sum to
zero. Indeed, consider the problem of regressing a set of values $(a_i)_{i=1}^q$
onto a subspace spanned by $\one$. The estimate of the coefficient using the OLS
is given by $\bar{a} = (\one'\one)^{-1} \one'a$ -- a scalar. The orthogonal component
to the subspace is given by $\hat{a} = a - \one \bar{a}$, and its values necessarily
sum to zero: $\one'\hat{a} = \one' a - \one'\one (\one'\one)^{-1} \one'a = 0$.
Thus in order to make $\gamma_k^+$ sum to zero, one just has to subtract their
average value.

Therefore the proposed update
\[
\hat{\gamma}_k = \frac{K-1}{K}\biggl\{ \gamma_k^+ - K^{-1} \sum_{l=1}^K \gamma_l^+ \biggr\} \,,
\]
sums to zero, since summing both sides by $k=1,\ldots, K$ yields
\[
\sum_{k=1}^K = \frac{K-1}{K} \biggl\{ \sum_{k=1}^K \gamma_k^+ - \sum_{l=1}^K \gamma_l^+ \biggr\} = 0\,.
\]

% subsection subproblem_vii (end)

\subsection{subproblem viii} % (fold)
\label{sub:subproblem_viii}

\subsubsection{item a} % (fold)
\label{ssub:item_a}

Consider the algorithm 1 on page~3 of the assignment. It implements the forward
stagewise adaptive modelling which attempts to minimize the loss function of the
model given by the following regression tree basis expansion:
\[
G_k(x) = \sum_{m=1}^M \beta_m \sum_{j=1}^{J_m} \gamma_{jk} 1_{x\in R_{jkm}}\,, \text{--}
\]
with respect the parameters of each regression tree $(\gamma_{jk}, R_{jkm})$ and
the weights $\beta_m$. The approximation is performed by sequentially adding new
basis functions to the expansion, while keeping the parameters and weights of those
bases that were already fit at a prior step.

The loss function in question if the negative of the log-likelihood given by
\[
\Lcal(t, f(x)) = - \sum_{k=1}^K t_k \bigl( f_k(x) - \log \sum_{l=1}^K e^{f_l(x)} \bigr)\,.
\]

% subsubsection item_a (end)

\subsubsection{item b} % (fold)
\label{ssub:item_b}

Consider the loss of an individual observation $i$ at step $m$ of the algorithm
with class probabilities $p_{km}(x)\propto f_{km}(x)$. It is given by:
\[
L_i = \Lcal(t_i, f_m(x_i)) 
= - \sum_{k=1}^K t_{ik} f_{km}(x_i)
+ \log \sum_{l=1}^K e^{f_{lm}(x_i)}\,,
\]
since $\sum_{k=1}^K t_{ik}=1$. Thus the step 2-b-i of algorithm~1 on page~3 can be
simplified to
\[
r_{ikm} = - \frac{\partial }{\partial f_{km}(x_i)} L_i = -( p_{km}(x_i) - t_{ik} )\,,
\]
using the derivative 
\[
\frac{\partial }{\partial f_{km}(x_i)} \Lcal(t_i, f_m(x_i))
= - t_{ik} + \frac{e^{f_{km}(x_i)}}{\sum_{l=1}^K e^{f_{lm}(x_i)}}\,,
\]
and the definition of $p_{km}(x)$.

% subsubsection item_b (end)

\subsubsection{item c} % (fold)
\label{ssub:item_c}

\textbf{No solution}.

% subsubsection item_c (end)

% subsection subproblem_viii (end)

% section problem_1_part_2 (end)

\section{Problem \# 2} % (fold)
\label{sec:problem_2}

The original \textbf{B}oolean \textbf{L}inear \textbf{P}rogram is : for some $c\neq 0$
minimize $c'x$ subject to $Ax \preceq b$ and $x_i\in\{0,1\}$. Denote its feasible set by
\[ \text{Feas}_{\text{BLP}} = \Bigl\{ x \in \{0,1\}^n \,\big|\, Ax \preceq b \Bigr\} \,. \]
The constraint matrix $A \in \Real^{m\times n}$ and $b \in \Real^{m\times 1}$.

The \textbf{r}elaxed BLP problem is the same except for the feasible set: the components
of $x$ are allowed to take any value between $0$ and $1$. Thus
\[ \text{Feas}_{\text{rBLP}} = \Bigl\{ x \in [0,1]^n \,\big|\, Ax \preceq b \Bigr\} \,. \]

Firstly, note that 
\[ \text{Feas}_{\text{BLP}} \subseteq \text{Feas}_{\text{rBLP}} \, \]
which means that if $x^*$ is the optimal solution to the BLP then it is necessarily
a solution to the rBLP. Therefore if $\hat{x}\neq x^*$ is a solution to the rBLP, then
\[ c'\hat{x} \leq c'x^* \,, \]
for otherwise a feasible $x^*$ should have been the solution of rBLP instead of $\hat{x}$.

Secondly, this set inclusion entails that if the rBLP is infeasible, then necessarily
the BLP must be infeasible as well, because infeasibility of rBLP means that there is
no $x\in \text{Feas}_{\text{rBLP}}$, and thus no $x$ in $\text{Feas}_{\text{BLP}}$.

Let's reformulate the BLP as:
\[ c'x \to \min_{x\in \Real^n}\,, \]
subject to 
\[ Ax \preceq b\,\text{ and }\, x_i(1-x_i) = 0\,,\, i=1\ldots, n \,. \]

The Lagrangian of the BLP thus becomes
\[
\Lcal( x, \lambda )
= c'x + \lambda' (A x - b) + \sum_{i=1}^n \mu_i x_i (1-x_i) \,,
\]
where $\lambda \in \Real^{m\times 1}$ and $\mu=(\mu_i)_{i=1}^n \in \Real^n$.

The KKT conditions for a candidate optimal triple $(x,\lambda,\mu)$ are
\begin{description}
	\item[Primal] $Ax \preceq b$ and $x_i(1-x_i) = 0$;
	\item[Dual] $\lambda_j\geq 0$, $j=1,\ldots, m$;
	\item[Complementary slackness] $\lambda_j (A_j' x - b)=0$, where $A_j$ is the $j$-th 
	row of $A$ ($A_i\in \Real^{n\times 1}$);
	\item[First-order] the gradient of the Lagrangian vanishes at $x$.
\end{description}
The gradient is given by $\nabla_x \Lcal = ( \tfrac{\partial\Lcal}{\partial x_i} )_{i=1}^n$:
\[
\frac{\partial\Lcal}{\partial x_i}
	= c_i + \sum_{j=1}^m \lambda_j A_{ji} + \mu_i ( 1 - 2 x_i ) \,.
\]
Denote by $a_i$ the $i$-th column of $A$: $a_i = (A_{ji})_{j=1}^m \in \Real^{m\times 1}$. 
Then the optimal $x_i$ solves
\[ c_i + \lambda' a_i + \mu_i ( 1-2 x_i)  = 0 \,, \]
whence
\[ x_i = \frac{1}{2} + \frac{1}{2\mu_i}\bigl( c_i + \lambda' a_i \bigr) \,. \]
If $x_i>0$, then the feasibility condition $x_i(1-x_i) = 0$ implies that $x_i$ must
be $1$, whence
\[ \frac{1}{2} = \frac{1}{2\mu_i}\bigl( c_i + \lambda' a_i \bigr)\,, \]
and
\[ \mu_i = c_i + \lambda' a_i \,. \]
Otherwise for $x_i=0$
\[ \mu_i = -\bigl( c_i + \lambda' a_i\bigr) \,. \]
Note that these conditions on $\mu_i$ cannot be satisfied simultaneously in the general
case. This means that the conditions on $x_i$ are equivalent to these conditions
on $\mu_i$.

Let's rewrite the Lagrangian
\begin{align*}
\Lcal( x, \lambda )
	&= - \lambda'b + \sum_{i=1}^n ( c_i + \lambda' a_i ) x_i + \sum_{i=1}^n \mu_i x_i (1 - x_i)\\
	&= - \lambda'b + \sum_{i=1}^n \bigl( c_i + \lambda' a_i + \mu_i( 1 - x_i ) \bigr) x_i \,.
\end{align*}
Note that the first-order conditions imply
\[ c_i + \lambda' a_i + \mu_i ( 1 - x_i ) = \mu_i x_i\,, \]
whence the Lagrangian reduces to
\begin{align*}
\Lcal( x, \lambda )
	&= - \lambda'b + \sum_{i=1}^n \bigl( c_i + \lambda' a_i + \mu_i( 1 - x_i ) \bigr) x_i \\
	&= - \lambda'b + \sum_{i=1}^n \mu_i x_i^2 \,.
\end{align*}
Now the feasibility condition $x_i(1-x_i) = 0$ implies that $x_i = x_i^2$, whence
\[
\Lcal( x, \lambda ) = - \lambda'b + \sum_{i=1}^n \mu_i x_i \,,
\]
and from the first-order conditions one gets
\[  
\sum_{i=1}^n \mu_i x_i
= \sum_{i=1}^n \frac{\mu_i}{2} + \frac{1}{2} \bigl( c_i + \lambda' a_i \bigr) \,.
\]
Consider a general $i=1,\ldots,n$. Then for some $s_i\in\{-1,+1\}$ the $\mu_i$ could
be re-expressed as 
\[ \mu_i = s_i \bigl(c_i + \lambda'a_i\bigr)\,, \]
whence the Lagrangian becomes
\[
\sum_{i=1}^n \mu_i x_i
= \sum_{i=1}^n \biggl(\frac{1}{2} s_i + \frac{1}{2}\biggr) \bigl( c_i + \lambda' a_i \bigr) \,.
\]
It turns out that it simplifies down to
\[
\Lcal( x, \lambda ) = - \lambda'b + \sum_{i=1}^n \min \bigl\{0, c_i + \lambda' a_i \bigr\} \,.
\]
The Lagrange dual is
\begin{align*}
	g(\lambda,\mu)
	&= \inf_x \Lcal(x,\lambda,\mu)\\
	&= - \lambda'b + \sum_{i=1}^n \min \bigl\{0, c_i + \lambda' a_i \bigr\}\,,
\end{align*}
and is actually independent of $\mu$.

Therefore the dual problem to the reformulated BLP becomes:
\[ - \lambda'b + \sum_{i=1}^n \min \bigl\{0, c_i + \lambda' a_i \bigr\} \to \max_{\lambda}\,, \]
subject to $\lambda_j \geq 0$ for all $j=1,\ldots,m$, where $a_i$ is the $i$-th
column of $A$. It is called the \emph{Lagrangian relaxation} of BLP.

By weak duality the optimal solution of this problem provides a lower bound to the
optimal solution of the original BLP. Let's derive the dual problem of the rBLP.
Its Lagrangian is given by:
\begin{align*}
	\Lcal( x, \lambda, \mu, \gamma )
	&= c'x - \lambda'(Ax-b) - \mu'x + \gamma'(x-\one) \\
	&= ( c - A'\lambda - \mu + \gamma )' x - \lambda'b + \gamma'\one \,.
\end{align*}

The KKT conditions are \begin{itemize}
	\item $Ax\preceq b$, $x\in [0,1]^n$;
	\item $\lambda_j\geq0$, $\mu_i,\gamma_i\geq0$ for $i=1,\ldots,n$ and $j=1,\ldots,m$;
	\item $\mu_i x_i = 0$, $\gamma_i (1 - x_i)= 0$ and $\lambda_j (A_jx - b_j) = 0$
	for $i=1,\ldots,n$ and $j=1,\ldots,m$;
	\item $c - A'\lambda - \mu + \gamma = \mathbf{0}$.
\end{itemize}
Using the first-order conditions, the Lagrangian simplifies to
\[ \Lcal( x, \lambda, \mu, \gamma ) = - \lambda'b + \gamma'\one \,. \]
Expressing $\gamma$ from the FOC yields this :
\[
\Lcal( x, \lambda, \mu, \gamma )
	= - \lambda'b + \sum_{i=1}^n \mu_i - (c_i + \lambda'a_i) \,.
\]

Consider some $i=1,\ldots, n$. If $c_i + \lambda'a_i > 0$, then the FOC implies
that $\mu_i > \gamma_i\geq 0$. The complementary slackness thus imply that $x_i = 0$.
In turn this means that $\gamma_i=0$, whence 
\[ \mu_i = c_i + \lambda'a_i \,. \]
Now if $c_i + \lambda'a_i < 0$ then, similarly, 
\[ \gamma_i = \mu_i - (c_i + \lambda'a_i) > \mu_i \geq 0\,, \]
whence the condition $\gamma_i (1-x_i)= 0$ gives $x_i = 1$, and $\mu_i = 0$.
Thus 
\[ \mu_i = \max\{0,c_i + \lambda'a_i\} \,, \]
and the sum in the Lagrangian becomes
\[
\sum_{i=1}^n \mu_i - (c_i + \lambda'a_i)
= \sum_{i=1}^n \min\{0,c_i + \lambda'a_i\}\,,
\]
since $y = \max\{y,0\} + \min\{y,0\}$ for any $y\in \Real$.

The Lagrangian thus becomes
\[
\Lcal( x, \lambda, \mu, \gamma )
	= - \lambda'b + \sum_{i=1}^n \min\{0,c_i + \lambda'a_i\} \,,
\]
and it is the Lagrangian dual function.

The dual problem of the rBLP is 
\[
- \lambda'b + \sum_{i=1}^n \min\{0,c_i + \lambda'a_i\} \to \max_{(\lambda_j)_{j=1}^m}\,,
\]
subject to $\lambda_j\geq 0$ for all $j=1,\ldots,m$.

It turns out that the dual problem of the rBLP coincides with the Lagrangian
relaxation problem, and therefore both have the exactly the same solution, provided
they are feasible. However they are simultaneously feasible or simultaneously
infeasible.

% section problem_2 (end)

\end{document}
