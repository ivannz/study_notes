\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\pr}[0]{{\mathbb{P}}}
\newcommand{\Var}[0]{{\text{Var}}}
\newcommand{\var}[0]{{\text{var}}}
\newcommand{\RSS}{\text{RSS}}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}
\newcommand{\tr}{\text{tr}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\rank}{\mathop{\text{rank}}\nolimits}



\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Document title}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section*{Preliminaries} % (fold)
\label{sec:preliminaries}

\url{http://cs.hse.ru/ai/mmdm}

The following topics would be covered in the course:
\begin{itemize}
	\item Decision theory with linear regression
	\item Shrinkage methods (Ridge regression, Lasso, elastic net)
	\item Polynomial regression, splines
	\item Model selection techniques, validation methods
	\item Classification problems (the Support Vector Machine, decision trees,
	bagging and boosting)
	\item Neural networks (flexible regression and classification)
\end{itemize}

Main books:
\begin{description}
	\item[Tibshirani2013] \hfill\\
		James, Witten, Hastie and Tibshirani (2013): ``An Introduction to Statistical Learning with Applications in R''
	\item[Bishop2006] \hfill\\
		Bishop (2006): ``Pattern Recognition and Machine Learning''
	\item[Tibshirani2013] \hfill\\
		Hastie, Tibshirani, Friedman (2013): ``The Elements of Statistical Learning''
\end{description}

% section* preliminaries (end)

\clearpage
\section{Lecture \# 1} % (fold)
\label{sec:lecture_1}

Linear regression lies in the class of supervised learning problems.
Such problems work with paired data of arbitrary nature: univariate, multivariate,
categorical, numerical, abstract (like graphs in chemical classification problems) et c.
In general there is a learning (training) sample $(x_i, T_i)_{i=1}^n$ of pairs
of an input variable $X$ and the associated output target $T$.

The goal is to construct a function $y(\cdot)$ of the input that predicts, forecasts,
estimates or otherwise returns the target, associated with the given input.

The target variable $T$ is always considered to be a random variable at least due
to measurement or other irreducible uncertainty, that is present in any system.
The input variable $X$, depending on the approach, may or may not be random.

In the case when it is an rv, the data is characterized by the join probability
density (distribution) function $p_{X,T}(x,t)$.
The joint pdf can be factorized into the density of the response conditional on
a particular input and the density (probability) of the input:
\[p_{X,T}(x,t) = p_{T\rvert X}(T\rvert X) p_X(x) \]
The analysis of the problem may now fork:
\begin{description}
	\item[Generative model] \hfill \\
		both the data and the response given data are modelled;
	\item[Discriminative model] \hfill \\
		model just the conditional response.
\end{description}

Statistical decision theory helps decide on a functional form of $y(\cdot)$:
choose $y(\cdot)$ so a to minimize the expected loss functional $\ex_{X,T} L(T,y(X))$
over the bivariate distribution of the input data.
This way the incorrect estimation of $t$ by $y(x)$ given $x$ is penalized.

Among the possible loss functions the most frequently used is the quadratic
loss function $L(t,y) \defn (t-y)^2$, which has its roots in the $L^p$ norm
\[\nrm{f}_p = \Bigl(\int \lvert f\rvert^p d\mu\Bigr)^\frac{1}{p}\]
for $p = 2$. Another possible loss function is the so called Minkowski cost,
defined as $L(t,y) \defn \abs{t-y}$, which is the $L^p$ norm with $p=1$.

Sidenote: conditional expectation as the projection of $f$ onto a subspace of $g$
measurable functions in the Hilbert space of square integrable functions with the
usual $\int f\bar{g} d\mu$ inner product.

% Remember the measure theory? with its sigma-finite measure \mu with respect to which another measure \nu is absolutely continuous, and there is f\in L^1(\mu) such that \nu=\int f d\mu and \int gd\nu = \int fg d\mu.

% Which approach to use?
The joint randomness of $(X,T)$ leads to the following problem in the case of
$L^2$ loss function:
\[\ex_{X,T} (T-y(X))^2 \to \min_y\]
Using the tower property of the conditional expectation the optimal $y$ is the
conditional expectation $\phi(X)\defn \ex(T\rvert X)$. Indeed, since
$\ex_{T\rvert X} \bigl(T-\phi(X) \bigr) = 0$, the mean squared error is
\begin{align*}
	\ex_{X,T} \bigl(T-y(X)\bigr)^2 &= \ex_{X} \Bigl( \ex_{T\rvert X} (T-y(X))^2 \Bigr) \\
		&= \ex_X \Bigl( \ex_{T\rvert X} \bigl( (T-\phi(X))^2+(\phi(X)-y(X))^2 \bigr) + \\
			&\quad + 2 \ex_{T\rvert X} (T-\phi(X))(\phi(X)-y(X)) \Bigr) \\
		&= \ex_X \bigl( (\phi(X)-y(X))^2 + \ex_{T\rvert X} (T-\phi(X))^2 \bigr) + \\
			&\quad + 2 \ex_X \bigl((\phi(X)-y(X)) \ex_{T\rvert X} (T-\phi(X)) \bigr) \\
		&= \ex_X \bigl( \ex_{T\rvert X} (T-\phi(X))^2 + \ex_{T\rvert X} (\phi(X)-y(X))^2 \bigr)
\end{align*}

The latter conditional expectation is vanishes whenever $y=\phi$, whilst the first
one is independent of $y$ and can be considered as the irreducible variance due
to structural variance. The mean prediction error is the irreducible variance and
the approximation error:
\[\text{error} \defn \ex_X \Var(T\rvert X) + \ex_x (\phi(X)-y(X))^2 \]

However it is never possible to achieve $y(x) = \ex(T\rvert X=x)$.
Therefore some insight is needed on $\ex_{T\rvert X} (\phi(X)-y(X))^2$.
\begin{description}
	\item[Frequentist] $\pr\bigl(\Dcal\rvert \Theta\bigr)$ -- uncertainty is due
	to dataset;
	\item[Bayesian] $\pr\bigl(\Dcal\rvert \Theta\bigr) \pi(\Theta)$ -- uncertainty
	is due to parameters, where $\pi(\Theta)$ is the prior distribution. The posterior
	distribution is proportional to
		\[\pr\bigl(\Theta\rvert \Dcal\bigr) \sim \pr\bigl(\Dcal\rvert \Theta\bigr) \pi(\Theta)\]
\end{description}

Error is for a particular training dataset, on which the $y$ is estimated. In effect
the functional form of $y$ depends on the data set $D$:
\[\ex_\Dcal(\text{Error}_\Dcal) = \text{Noise} + \ex_\Dcal \ex_X \bigl( y(x,D)-\phi(x) \bigr)^2\]
where noise is defined as $\ex_X \Var(T\rvert X)$.
Now put $f(x) \defn \ex_\Dcal y(x,D)$.
\begin{align*}
	\ex_\Dcal( y(x,D)-\phi(x) \pm f(x) )^2
		& = \ex_\Dcal( y(x,D) - f(x) )^2 + \ex_\Dcal( f(x) - \phi(x) )^2 \\
			&\quad + 2 \ex_\Dcal( y(x,D) - f(x) )( f(x) - \phi(x) ) \\
		& = \ex_\Dcal( y(x,D) - f(x) )^2 + ( f(x) - \phi(x) )^2 \\
			&\quad + 2( f(x) - \phi(x) ) \ex_\Dcal( y(x,D) - f(x) ) \\
		& = \ex_\Dcal( y(x,D) - f(x) )^2 + ( f(x) - \phi(x) )^2 \\
\end{align*}
Therefore the expected value of the loss is 
\[\ex_\Dcal(\text{Error}_\Dcal) = \text{Noise} + \text{Variance} + \text{Bias}^2\]
with the variance term rooted in the fact that the instance of $y$ is not static
with varying dataset $D$, and the bias being the measure of how close on average
the approximation $y(\cdot)$ is to the real conditional expectation $\ex(T\rvert X)$.
This is the decomposition of the loss into systemic noise, bias and variance.

% Non-parametric approach + low dimension: smoothing local data (kNN -- $k$ nearest neighbours).

Parametric approach would require a parametric expression of $y(\cdot)$ as an
approximation of $\ex(T\rvert X)$ at every $x$ (or at most almost all).

Suppose $\Lcal^2$ is the space in which the best approximation of $\phi(X)$ is
searched for. Projecting $\ex(T\rvert X)$ onto $\Lcal^2$ yields $\phi(X) = h^\perp(X) + h(X)$,
where $h^\perp \perp \Lcal^2$ (i.e. $\langle h^\perp,g\rangle = 0$ for all
$g\in \Lcal$) and $h\in \Lcal^2$.

% Here should be a picture of the projection: the conditional expectation is projected onto the subspace of square integrable functions.

% \ex_\Dcal y(X,D) -- is an element of the 

Since $f-h\in\Lcal^2$ it must be true that
\[\ex_X (f(X) - h(X)) h^\perp(X) = \langle f-h, h^\perp\rangle = 0 \]
whence the following decomposition must hold:
\begin{align*}
	\text{Bias}^2 &= \ex_X \bigl( \ex_\Dcal y(X,D) - \phi(X) \bigr)^2 
		= \ex_X (f(X) - h^\perp(X) - h(X))^2 \\
		& = \ex_X (f(X) - h)^2 + \ex_X (h^\perp(X))^2 + \ex_X (f(X) - h(X)) h^\perp(X) \\
		& = \ex_X (f(X) - h(X))^2 + \ex_X (h^\perp(X))^2
\end{align*}
This means that the squared bias is the sum of how close the model is to the projection
of $\ex(T\rvert X)$ onto $\Lcal^2$, and how well the projection approximates the
conditional expectation itself.

In linear regression problems the space $\Lcal^2$ is taken to be the set of all
linear functions of $X$.

\subsection*{Linear regression} % (fold)
\label{sub:linear_regression}

Suppose $\mathcal{X}$ is a $p$-dimensional vector of features and that we attempt to approximate
the $\ex(T\rvert \mathcal{X})$ with $y(x) \defn \beta_0 + \sum_{k=1}^p \beta_k x_k$.

In effect we suppose the following statistical model
\[T = \beta_0 + \sum_{k=1}^p \beta_k \mathcal{X}_p + \epsilon;\,\epsilon\sim\Ncal(0,\sigma^2)\]
Therefore conditional upon $\mathcal{X}$ the target $T$ is distributed like
\[\induc{T}\mathcal{X}\sim \Ncal(y(\mathcal{X}),\sigma^2)\]

In this setting the following questions arise: \begin{itemize}
	\item How $\beta\in \Real^{(p+1)\times 1}$ is estimated?
	\item Which coefficients $\beta_k$ are significant?
	\item What is their interpretation?
\end{itemize}
For $T,\epsilon\in \Real^{n\times 1}$, $\beta\in \Real^{(p+1)\times 1}$ and
$X\in \Real^{n\times (p+1)}$, given by $X = \begin{smallmatrix} \one & \mathcal{X} \end{smallmatrix}$
the linear regression problem is stated as follows:
\[T = X\beta + \epsilon\]
Each row of $X$ represents the observations of observed predictors augments by
a constant term (intercept). The predictors now look like this
\[
X = \begin{pmatrix}
\one & \underset{n\times 1}{x_1} & \ldots & \underset{n\times 1}{x_p}
\end{pmatrix}
\]
where $\one \defn \underset{n\times 1}{1}$ is the vector of ones. Effectively, the
set of predictors of the training set is ``enriched'' by a baseline feature, which
summarizes the effects on $T$ shielded from the influence of all other predictors --
the unexplained base value inherent in the target variable. (Such effect brings
about interpretability issues). 

Two approaches to estimating $\beta$ is possible:
the \textbf{L}east \textbf{S}quares and the \textbf{M}aximum \textbf{L}ikelihood.
Effectively the LS approach to parameter estimation is non-probabilistic. There
are different flavours of least squares: the ordinary, the generalised (with a
weighting matrix), two-stage least squares (2SLS), to name a few, and others. The
2SLS is an extension of the GLS that attempts to battle heteroskedasticity: first
run OLS to get the squared residuals, then GLS with weights that standardise the
residuals.

The goal of LS is to solve the following problem: minimize the \textbf{R}esidual
\textbf{S}um of \textbf{S}squares
\[\RSS \defn \brac{T-X\beta}'\brac{T-X\beta} \to \min_\beta\]
The first order conditions on the potential minimizer are in the matrix form:
\begin{align*}
	\frac{\partial}{\partial \beta}\RSS &= \frac{\partial}{\partial \beta}\brac{T'T-\beta'X'T - T'X\beta + \beta'X'X\beta}\\
	&= \frac{\partial}{\partial \beta} T'T - \frac{\partial}{\partial \beta} \beta'X'T - \frac{\partial}{\partial \beta} T'X\beta + \frac{\partial}{\partial \beta} \tr\brac{\beta'X'X\beta}\\
	&= - 2 \frac{\partial}{\partial \beta} T'X\beta + \beta'\brac{X'X + \brac{X'X}'}\\
	&= - 2 T'X + 2 \beta'X'X = 0
\end{align*}
If the matrix $X$ is full rank, then the matrix $X'X$ is invertible, and is positive-semidefinite.
Therefore $\frac{\partial^2}{\partial \beta\partial \beta}\RSS = 2 X'X \geq 0$,
whence the Least Squares solution $\hat{\beta} \defn \brac{X'X}^{-1} X'T$ is indeed
the minimizer of the RSS.

The best linear approximation to $\ex(T\rvert X)$ is the projection of $T$ onto
the space of linear functionals of $X$.

The matrix $\hat{H} \defn X\brac{X'X}^{-1}X'$ is called the \textbf{hat matrix} and
is in fact a projector onto the linear subspace spanned by the column vectors of $X$.

Every projector matrix has the following property: if $v$ is in the spanned linear
subspace, then $\hat{H}v = v$. Indeed, since $v\in \clo{X}$, there must exist
$\underset{(p+1)\times 1}{\alpha}\in \Real^{(p+1)}$ such that $v = X\alpha$.
Therefore
\[X\brac{X'X}^{-1}X'v = X\brac{X'X}^{-1}X'X\alpha = X\alpha = v\]

If the columns of $X$ are orthogonal, then $X'X$ is in fact a diagonal matrix
with the squared Euclidean norms of columns of $X$ on the diagonal. In this case
each of $\brac{\beta_k}_{k=0}^n$ is the coefficient in the projection of $T$ onto
the respective column of $X$.

% subsection* linear_regression (end)

\subsection*{Gram-Schmidt orthogonalisation} % (fold)
\label{sub:gram_schmidt_orthogonalisation}

Suppose there is a set of linearly independent vectors $\brac{f_i}_{i=1}^m$ in
some Euclidean space $V$.
\begin{description}
	\item[Initialisation] \hfill \\
		Set $e_1 \defn \frac{1}{\nrm{f_1}} f_1$;
	\item[General step $k=1,\ldots,m-1$] \hfill \\
		Let \[u_{k+1} \defn f_{k+1} - \sum_{i=1}^k \frac{\brkt{e_i, f_{k+1}}}{\brkt{e_i,e_i}} e_i\]
		and put $e_{k+1}\defn \frac{1}{\nrm{u_{k+1}}} u_{k+1}$.
\end{description}
An effective algorithm is given in \emph{Golub \& Van Loan 1996} section 5.2.8.

% subsection* gram_schmidt_orthogonalisation (end)

%% I don't recall what the section below is for...
In a bivariate case $X = (\one,x)$ and
\[\hat{\beta} = \brac{\begin{matrix} \one'\one & \one'x\\ x'\one & x'x \end{matrix}}^{-1} \brac{\begin{matrix} \one't \\ x't \end{matrix}}
= \frac{1}{n^2 \brac{\overline{x^2} - \bar{x}^2}} \brac{\begin{matrix} \one'\one \one't - \one'x x't \\ - x'\one \one't + x'x x't \end{matrix}}
\]
since $\one'\one = n$ and $x'\one = \one'x = n\bar{x}$.

%% p-predictors

Set $\hat{z}_0 \defn \one$. For each $j=1,\ldots,p$ regressing $x_k$ onto
$\brac{\hat{z}_j}_{j=0}^{k-1}$ to get the LS coefficients $\brac{\alpha_{jk}}_{j=0}^{k-1}$.
Construct $\hat{z}_k$ as the residuals of $x_k$ from projecting onto $\brac{\hat{z}_j}_{j=0}^{k-1}$ :
\[\hat{z}_k = x_k - \sum_{j=0}^{k-1} \alpha_{jk} \hat{z}_j\]
This is similar to the Gram-Schmidt procedure described previously.

Regressing $t$ on $z_k$ yields the coefficient $\hat{\beta}_k$ which is not the
OLS coefficient, but shows new information gained from the $k^\text{th}$ predictor.

Discriminative approach.
Since $\hat{\beta}$ is a linear transformation of $T$ and $\hat{\beta} = \beta + (X'X)^{-1}X'\epsilon$,
\textbf{provided the model is specified correctly}, it is therefore true that
\[\hat{\beta}\sim\Ncal\Bigl(\beta,(X'X)^{-1} \sigma^2\Bigr)\]

In fact if $\epsilon\sim \Ncal\brac{0,\Sigma}$ -- i.e the noise has some other
covariance structure, then $\Var(\hat{\beta}) = \brac{X'X}^{-1}X'\Sigma X\brac{X'X}^{-1}$
-- this is useful in GLS, mentioned above, especially if $\Sigma$ is decomposed
into $C'C$ with $C$ -- non-singular lower triangular matrix (Cholesky).

Indeed, if we transform (weigh) the observations according to matrix $W$ with
$W'W = \Sigma^{-1}$, then the LS model becomes $WT=WX\beta + W\epsilon$, whence
\[\hat{\beta} = \brac{X'W'WX}^{-1}X'W'WT = \brac{X'\Sigma^{-1}X}^{-1}X'\Sigma^{-1}T\]
whence $\hat{\beta}\sim\Ncal\brac{\beta, \brac{X'\Sigma^{-1}X}^{-1}}$.

\noindent\textbf{Cochran's Theorem} \hfill\\
Suppose $\brac{\xi_i}_{i=1}^n \sim\Ncal(0, 1)$ and that
\[\sum_{i=1}^m \xi_i^2 = \sum_{j=1}^k Q_j\]
where $Q_j = \xi' A_j \xi$ is a positive-semidefinite quadratic form for each $j=1,\ldots,k$.
If $\sum_{j=1}^k r_j = n$, with $r_j = \rank{Q_j}$, then \begin{enumerate}
	\item The random variables $Q_j$ are independent;
	\item each $Q_j\sim \chi^2_{r_j}$.
\end{enumerate}

Returning to the simple model, what is the estimator of $\sigma^2$?

It turns out that $\frac{\RSS}{\sigma^2}\sim \chi^2_{n-p-1}$.
Indeed, provided the model is specified correctly, \begin{align*}
	\RSS &= \brac{T-X\hat{\beta}}'\brac{T-X\hat{\beta}} \\
	& = \brac{T-X\beta - X\brac{X'X}^{-1}X'\epsilon}'\brac{T-X\beta - X\brac{X'X}^{-1}X'\epsilon} \\
	& = \epsilon' \brac{I - X\brac{X'X}^{-1}X'}'\brac{I - X\brac{X'X}^{-1}X'}\epsilon \\
	& = \epsilon' \brac{ I - X\brac{X'X}^{-1}X' } \epsilon = \epsilon' \brac{ I - \hat{H} } \epsilon
\end{align*}
Since the trace of an idempotent matrix equals its rank
\[\rank(I - \hat{H}) = \tr(I) - \tr(\hat{H}) = n- (p+1)\]
whence by Cochran's theorem $\frac{1}{\sigma}\RSS\sim\chi^2_{n-(p+1)}$.
Furthermore, $\hat{\sigma}^2 \perp \hat{\beta}$.

\subsection{Hypothesis testing} % (fold)
\label{sub:hypothesis_testing}

If $V_j \defn \brac{\brac{X'X}^{-1}}_{jj}$ -- the $j^\text{th}$ diagonal element
of $\brac{X'X}^{-1}$, whence $\Var\hat{\beta}_j\defn \sigma^2 V_j$.

To test $H_0:\beta_j=0$ versus $H_1:\beta_j\neq 0$, one should use the following
statistic, which under the null is distributed as follows:
\[\frac{\frac{\hat{\beta}_j}{\sqrt{\sigma^2 V_j}}}{\sqrt{ \frac{\frac{n-p-1}{\sigma^2} \hat{\sigma}^2 }{ n-p-1 } }} \sim \frac{\Ncal(0, 1)}{\sqrt{\chi^2_{n-p-1}}} = t_{n-p-1}\]

The $\alpha$-level confidence interval for $\beta_j$ is defined as
\[\clo{ \hat{\beta}_j - \hat{\sigma} \sqrt{ V_j },  \hat{\beta}_j + \hat{\sigma} \sqrt{ V_j } }\]

For the false rejection rate of the t-test $\alpha = 0.05$, on average $5\%$ of
statistically significant results would be false positives.

To test for simultaneous insignificance of $\brac{\beta_j}_{j=1}^p = 0$ one should
use the F-test (more generally an F-test for linear restriction). The following
takes place under the null hypothesis that the simpler model is correct :
\[F = \frac{\frac{\RSS_0-\RSS_1}{p_1-p_0}}{\frac{\RSS_1}{n-p_1-1}}\sim F(p_1-p_0, n-p_1-1)\]

To gauge the model accuracy one can use both the RSS as a measure of lack of fit
and the $R^2$ as a measure of goodness-of-fit.

If $\one\in \clo{X}$ then
\begin{align*}
	\text{TSS} &= T' \bigr(I - \one(\one'\one)^{-1}\one'\bigl) T \\
	&= T' \bigl(I - X(X'X)^{-1}X'\bigr) T + T' \bigl( X(X'X)^{-1}X' - \one(\one'\one)^{-1}\one'\bigr) T \\
	&= \RSS + \text{ESS}
\end{align*}
since in this case
\[\bigl(I - X(X'X)^{-1}X'\bigr)\bigl( X(X'X)^{-1}X' - \one(\one'\one)^{-1}\one'\bigr) = 0\]

The goodness-of-fit measure $R^2$ is the ratio of the explained variance to the
total variance
\[R^2 \defn \frac{\text{ESS}}{\text{TSS}} = 1 - \frac{\RSS}{\text{TSS}} \]
For a simple case of a bivariate regression
\[R^2 = \rho^2 = \frac{\abs{\text{corr}(x,t)}^2}{\Var(x)\Var(t)} = \beta^2 \frac{\Var(x)}{\Var(t)}\]


% subsection hypothesis_testing (end)

% section lecture_1 (end)

\clearpage
\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}
% 2015-01-20

The uncertainty on the model for a new input $x_0$ 

Prediction of the value of $T$ at a new input $x_0$ has two sources of uncertainty:
\begin{enumerate}
	\item Uncertainty of the first type -- due to the estimation of $y(\cdot)$
	($\hat{\beta}$) in $\Lcal^2_X$;
	\item Uncertainty of the second type -- due to the additive noise term. Even
	if the true $\beta$ were known, the true values of the response $T$ at $x_0$
	would still be unknown.
\end{enumerate}
Predictive intervals incorporate the noise as well as uncertainty due to imprecise
estimation of the coefficients.

Variability due to estimation of an approximate model.

\subsection{Shrinkage models} % (fold)
\label{sub:shrinkage_models}

In the ordinary linear squares method we minimized the following fitness function:
\[\text{min} \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}}^2\]

\subsubsection{Ridge regression} % (fold)
\label{ssub:ridge_regression}

Minimize the following penalized Residual Sum of Squares as:
\[\RSS_\text{ridge} \defn \sum_{i=1}^n \bigl(t_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigr)^2 + \lambda \sum_{j=1}^p \beta_j^2\]
the $\lambda>0$ is called the tuning parameter and it controls the amount of shrinkage.
If $\lambda = 0$ then there is absolutely no shrinkage, and the optimal $\hat{\beta}$
coincides with the OLS. But if $\lambda\to \infty$ then the optimal coefficient
vector tends to zero (that is why it is called \textbf{shrinkage}). This adjustment
effectively regularizes the parameter estimates.

In contrast to the ordinary linear regression, for this sort of shrinkage the scale
of each predictor $\brac{x_j}_{j=1}^p\in \Real^n$ matters! Thus the first step is
to standardize the variables before fitting the Ridge regression.

Next observation: the intercept $\beta_0$ is indeed excluded from the shrinkage
term so as not to penalize for the \emph{base} level of the target.

Suppose the $t$ and $X$ are standardized (mean and centre), which means that $\beta_0 \equiv 0$
Then in the matrix form the problem is equivalent to minimizing the following:
\[(t-X\beta)'(t-X\beta) + \lambda \beta'\beta \to \min_\beta\]
The first order conditions are given by just one equation:
\[\frac{\partial}{\partial \beta} \RSS_\text{ridge} = - 2 X'(t-X\beta) + 2 \lambda \beta\]
whence the optimal coefficients are given by
\[\hat{\beta}_\text{ridge} \defn (X'X + \lambda I_p)^{-1}X't\]

% subsubsection ridge_regression (end)

% subsection shrinkage_models (end)

% section lecture_2 (end)

\clearpage
\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}
The ridge regression problem can actually be rewrtiten as the following constrained
optimization problem (centred and scaled): minimize $(t-X\beta)'(t-X\beta)$ subject
to the ``budget'' $\beta'\beta\leq L$ where $L\geq0$ effectively limits the size
of the coefficients.

The modified ridge RSS with a tuning parameter $\lambda$ is defined as
\[
\RSS_\text{ridge}
\defn \sum_{i=1}^n \bigl( t_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \bigr)^2
	+ \lambda \sum_{j=1}^p \beta_j^2
\]
Important to note: \begin{itemize}
	\item Inputs must be either standardized by standard deviation or altogether
	dimensionless inputs. This would make elements of $\beta$ comparable and would
	protect against artificial enhancement of importance of any particular explanatory
	variable: the scale of $(x_{ij})_i$ inversely affects the magnitude of $\beta_j$.
	\item If the input vectors are centred, then the estimation is separable in
	$\beta_0$ and $\beta_j$ for $j\geq 1$.
\end{itemize}
The latter can be shown rigorously. The residual values for a given $\beta_0$
and $\beta$:
\begin{align*}
t-(\one\beta_0 + X\beta)
&= t-\bigl(\one\beta_0 + X\beta \pm \one(\one'\one)^{-1}\one'X\beta \bigr)\\
&= t-\bigl(\one\beta_0 + \pi X\beta + (I_n-\pi)X\beta \bigr)
\end{align*}
where $\pi = \one(\one'\one)^{-1}\one'$ is in fact a projection matrix onto a
linear subspace $[\one]$ spanned by one vector $\one$. Now define the following
centred variables:
\[X^c = X - \pi X = \pi_\perp X \text{ and } t^c = \pi_\perp t\]
where $\pi_\perp = I_n-\pi$ -- the projector onto space orthogonal to $[\one]$.
Variables are indeed centred, because $\bar{x} = (\one'\one)^{-1}\one'X$ is in
fact a row vector of sample means of each feature in $X$, and similarly
$\bar{t} = (\one'\one)^{-1}\one't$ is the sample mean of $t$. Therefore
\begin{align*}
t-(\one\beta_0 + X\beta)
&= t^c + \pi t - \one\beta_0 - \pi X\beta - X^c \beta\\
&= t^c - X^c \beta + (\pi t - \pi X\beta - \one\beta_0)
\end{align*}
Since it is true that $\one\beta_0 = \pi \one\beta_0$, we have
\[
\ldots
= \bigl(t^c - X^c\beta\bigr) + \pi\bigl( t - X\beta - \one\beta_0\bigr)
= \pi_\perp(t - X\beta) + \pi\bigl( t - X\beta - \one\beta_0\bigr)
\]
Taking the inner product of $t-(\one\beta_0 + X\beta)$ with itself:
\begin{multline*}
\bigl(t-(\one\beta_0 + X\beta)\bigr)'\bigl(t-(\one\beta_0 + X\beta)\bigr) \\
= (t - X\beta)'\pi_\perp'\pi_\perp(t - X\beta) + (t - X\beta - \one\beta_0)'\pi'\pi(t - X\beta - \one\beta_0)
\end{multline*}
where the intermediate products with $\pi_\perp \pi$ or $\pi \pi_\perp$ vanish
since $[\one]^\perp \perp [\one]$. Furthermore the term
\begin{align*}
(t - X\beta - \one\beta_0)'\pi'
	&= (t - X\beta - \one\beta_0)'\one(\one'\one)^{-1}\one'\\
	&= ((\one'\one)^{-1}\one't - (\one'\one)^{-1}\one'X\beta - (\one'\one)^{-1}\one'\one\beta_0)'\one'\\
	&= (\bar{t} - \bar{X}\beta - \beta_0)'\one'
\end{align*}
Since $(\bar{t} - \bar{X}\beta - \beta_0)$ is a scalar and $\one'\one = n$, the
main part of the ridge RSS turns out to be
\[
(t^c - X^c\beta)'(t^c - X^c\beta) + n (\bar{t} - \bar{X}\beta - \beta_0)^2
\]
This means that, provided the estimates of $\beta$ on the centred data, the estimate
of $\beta_0$ can easily be inferred: $\hat{\beta}_0 = \bar{t} - \bar{X}\hat{\beta}$.
Furthermore these manipulations are true for any penalized regression model with
the (regularization) penalty term explicitly ignoring the intercept.

The problem of penalized (ridge) regression in the matrix form, where $X$ is the
$n\times k$ matrix of centred standardized independent variables with no intercept:
\[\RSS_\text{ridge} = (t-X\beta)'(t-X\beta) + \lambda \beta'\beta\]
The solution is
\[\hat{\beta}_{\text{ridge}}\defn \brac{X'X + \lambda I}^{-1} X'T\]

An equivalent optimization problem:
\[
\hat{\beta}_{\text{ridge}}
= \argmin_{\beta} \sum_{i=1}^n \bigl(t_i - \sum_{j=1}^k \beta_j x_{ij}^c\bigr)^2
\text{ subject to } \sum_{j=1}^k \beta_j^2 \leq \eta
\]
$\eta$ -- the budget, $\lambda$ -- the complementary slackness coefficient.
There is a certain universality of the statement of the regression problem in
this form: it is possible to plug in different constraints ($L^1$ constraints -- LASSO).
LASSO is automatically performing feature selection due to $L^1$ norm.

How to tune $\lambda$? Wait until model selection and cross-validation. How does
the shrinkage work? Consider two cases of columns of $X$: orthogonal and correlated.

\subsection*{Orthogonal columns} % (fold)
\label{sub:orthogonal_columns}

Suppose $X$ are orthogonal, in which case $X'X = I_p$. The OLS solution is
\[\hat{\beta}_\text{OLS} = (X'X)^{-1}X't = X't\]
The ridge regression solution is given by 
\[
\hat{\beta}_\text{ridge}
= \brac{X'X + \lambda I_p}^{-1} X't
= \frac{1}{1+\lambda} \hat{\beta}_{\text{OLS}}
\]
Voil\`a -- shrinkage!

\subsubsection*{LASSO} % (fold)
\label{ssub:lasso}

The lasso solution:
\[
\hat{\beta}_\text{LASSO}
= \argmin_\beta (t-X\beta)'(t-X\beta) + \lambda \sum_{j=1}^k \lvert \beta_j\rvert
\]
Rewriting
\begin{align*}
	(t-X\beta)'(t-X\beta) + \lambda \sum_{j=1}^k \lvert \beta_j\rvert
	&= t't - 2 \beta'X't + \beta'X'X\beta  + \lambda \sum_{j=1}^k \lvert \beta_j\rvert \\
	&= t't - 2 \beta'\hat{\beta}_\text{OLS} + \beta'\beta  + \lambda \sum_{j=1}^k \lvert \beta_j\rvert
\end{align*}
Thus in this orthogonal case the LASSO minimization solution is equivalent to
the following:
\[
\hat{\beta}_\text{LASSO}
= \argmin_\beta \sum_{j=1}^k \beta_j^2 - 2 \beta_j \beta_{\text{OLS}\,j} + \lambda \lvert \beta_j\rvert
\]
The problem is additively separable in $\beta_j$'s, whence:
\[
\hat{\beta}_{\text{LASSO}\,j}
= \argmin_{\beta_j} \beta_j^2 - 2 \beta_j \beta_{\text{OLS}\,j} + \lambda \lvert \beta_j\rvert
\]
The FOC for a problem with $\beta_j\geq0$ is given by:
\[- \beta_{\text{OLS}\,j} + \beta_j + \frac{\lambda}{2} = 0\]
whence the optimal value is given by:
\[\hat{\beta}_{\text{LASSO}\,j} = \bigl(\beta_{\text{OLS}\,j} - \frac{\lambda}{2}\bigr)_+\]
For $\beta_j\leq 0$:
\[\hat{\beta}_{\text{LASSO}\,j} = \bigl(\beta_{\text{OLS}\,j} + \frac{\lambda}{2}\bigr)_-\]
Due to soft thresholding in the case of the LASSO there is feature selection.

% subsubsection* lasso (end)

\subsubsection*{The best subset selection} % (fold)
\label{ssub:the_best_subset_selection}
Suppose we have a set of $p$ predictors. Choose the subset with the least residual
sum of squares: for each $K = 1,\ldots, p$ pick the model with the lowest RSS:
\[
\min_\beta \sum_{i=1}^n \bigl(t_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigr)^2
\text{ subject to} \sum_{j=1}^p 1_{\beta_j \neq 0} \leq K
\]
This is equivalent to the following problem:
\[\min_{\beta} \sum_{j=1}^n \beta_j^2 - 2 \beta_j \beta_{\text{OLS}\,j} \]
subject to keeping at most $K$ coefficients non-zero.
\[
\hat{\beta}_{\text{BSS}\,i}
= \beta_{\text{OLS}\,i} 1_{[\hat{\beta}_{n-K}, \infty]}\bigl(\lvert \beta_{\text{OLS}\,i}\rvert\bigr)\]
where $\hat{\beta}$ is the $n-K$-th order statistic. This given hard thresholding.

% subsubsection* the_best_subset_selection (end)

% subsection* orthogonal_columns (end)
\subsection*{Correlated columns} % (fold)
\label{sub:correlated_columns}

Gram-Schmidt orthogonalization is similar to the QR decomposition. The best idea
is to use SVD decomposition of $X = U\Lambda V'$, with $U$ and $V$ orthogonal
matrices.

\subsection*{SVD} % (fold)
\label{sub:svd}

Suppose there is a matrix $X$ with dimensions $n\times p$ with rank $r$.
SVD decomposition claims that there exist square orthogonal matrices $U_{n\times n}$
and $V_{p\times p}$ and a diagonal matrix $\Sigma_{n\times p}$ of rank $r$, such
that $X = U \Sigma V'$:
\[X = U \Bigl(\begin{smallmatrix} \Lambda & 0\\ 0 & 0 \end{smallmatrix}\Bigr) V'\]

Orthogonality of $U$: $U'U = I_n$. Orthogonality greatly simplifies projectors
onto the column space of $X$: $\hat{t} = U U' t$ (?).

% subsection* svd (end)

\subsection*{Ridge regression} % (fold)
\label{sub:ridge_regression}

Suppose $X$ is full rank ($\rank(X) = p$) centred and scaled. Recall the expression
of the ridge regression solution:
\[\hat{\beta}_\text{ridge} = (X'X + \lambda I_p)^{-1} X't\]
The SVD of $X$ in this case is $U\Sigma V'$ where $\Sigma$ is a $n\times p$ 
matrix and $V \in \Real^{p\times p}$ and $U \in \Real^{n\times n}$ are orthogonal.
Furthermore $\Lambda'\Lambda = \Sigma^2$. Then $X'X = V\Sigma' U' U \Sigma V' = V\Lambda^2 V'$
and it must be true that 
\begin{align*}
	\hat{\beta}_{\text{ridge}}
	& = (V\Lambda^2 V' + \lambda VV')^{-1} X't\\
	& = V (\Lambda^2 + \lambda I_p)^{-1} V' V\Lambda U't \\
	& = V (\Lambda^2 + \lambda I_p)^{-1} \Lambda U't
\end{align*}
Therefore the ridge regression shrinks the coefficients over the tiniest principal
directions of $X$.

The sample correlation matrix
\[\frac{1}{n} X'X = \frac{1}{n} V'\Lambda^2 V\]
The principal components are in $V$ and the eigenvalue-eigenvector pair for the
sample correlation is given by the $\bigl(\frac{1}{n}\lambda_i^2, V_i\bigr)$
where $V_i$ is the $i$-th column of $V$. The $i$ th principal component is given
by $Z_i = X V_i = U_i \lambda_i$ and $\text{var}(z_i) = \lambda_i^2$. The ridge
regression shrinks the coefficients corresponding to the principal directions in
the column space of $X$ having the smallest variance.

% subsection* ridge_regression (end)

% subsection* correlated_columns (end)

\subsection*{The effective degrees of freedom} % (fold)
\label{sub:the_effective_degrees_of_freedom}

Should be a function of the tuning parameter $\lambda$ which governs the complexity
of the model. Complexity is the trace of the hat matrix (the projector) of the ridge regression:
\[\text{df}(\lambda) \defn \tr\bigl(X (X'X + \lambda I_p)^{-1} X'\bigr)\]
Using the SVD of $X$ gives us:
\begin{multline*}
\tr\bigl(X (X'X + \lambda I_p)^{-1} X'\bigr)
= \tr\bigl((X'X + \lambda I_p)^{-1} X'X \bigr) \\
= \tr\bigl( (\Lambda^2 + \lambda I_p)^{-1} V'V\Lambda' U'U\Lambda V'V\bigr)
= \sum_{j=1}^p \frac{\lambda_i^2}{\lambda_i^2 + \lambda^2}
\end{multline*}
If $\lambda = 0$ then $\text{df}(\lambda) = p$, and when $\lambda\to +\infty$,
then $\text{df}(\lambda) \to 0$.

% subsection* the_effective_degrees_of_freedom (end)

\subsubsection*{Not sure what this is about} % (fold)
\label{ssub:not_sure_what_this_is_about}

Correlation between this and this (?)
\begin{align*}
	\text{Var}(T) = \sigma^2 I
\end{align*}
Covariance of $\hat{T}$ and $T$.
% \[\ex(T, \hat{T}) = T' T\] 
Take a matrix of predictors $X$, its standardized version
\[X^\sigma \defn \frac{X}{\sigma}\]
and \[X^c \defn \frac{X-m}{\sigma}\]

% subsubsection* not_sure_what_this_is_about (end)

% section lecture_3 (end)

\clearpage
\section{Lecture \# 4} % (fold)
\label{sec:lecture_4}
Departing from the linearity of the Linear regression.

Transform $\brac{f_m}_{m=1}^M:\Real^p \to \Real$ since there are initially $p$
predictors. Sample space is augmented and now has $M$ predictors. The regression
function becomes:
\[f(X) = \sum_{m=1}^M \theta_m f_m(x)\]
This model generalises the original linear regressions since $f_m(x) = \pi_m(x)$ 

Polynomial regression problem: what happens in the tails? It is not very trustworthy
near the endpoints. In this respect polynomial regression is not a good idea. Why
not use local regression by partitioning the range into regions with different model
in each area: $f_m(X) = 1_{\clo{l_m, u_m}}(x_k)$

The behaviour of polynomial fit to data tends to be erratic near the boundaries.
And extrapolation can be dangerous!

\subsection{Piecewise polynomials and splines} % (fold)
\label{sub:piecewise_polynomials_and_splines}
%% The following really resembles the dummy variables in econometrics.

Suppose that $x$ is unidimensional -- only one feature.

Divide $x$ in several regions by different thresholds -- \textbf{(binding) knots},
since they link the regions ,and then fit a constant level within each region.
In order to cover the whole range of $x$ in this case one has to have three
``basis'' functions:
$1_{(-\infty, \xi_1]}$, $1_{(\xi_1, \xi_2]}$ and $1_{(\xi_2, \infty)}$.

Consider linear function on each region: we will be fitting something piecewise
linear. In this case we need $6$ basis functions, since we do not only model the
piecewise intercept, but also the piecewise slope.

Continuity requirement at each knot reduces the number of basis functions to four:
$h_1 = 1$, $h_2 = x$ (an indicator would introduce discontinuity),
$h_3 = (x-\xi_1)_+$ and $h_4 = (x-\xi_2)_+$ -- fix their intercepts.
This basis is known as the \textbf{truncated power basis}.

In general overlapping regions are refined into a partition, on which a proper
piecewise is later estimated. Then linear restriction tests could be run on the
estimated coefficients.

To add smoothness, one has to impose continuity of the derivatives at the knots.
and fit a smooth function within each region.

Consider three regions and two knots and require continuity up to the second
derivative at the first knot. The remaining degrees of ``freedom'' at this knot
are just the convexity of the fitted curve. The set of basis function is:
$h_1 = 1$, $h_2 = x$, $h_3 = x^2$, $h_4 = x^3$,
$h_5 = (x-\xi_1)_+^3$ and $h_6 = (x-\xi_2)_+^3$ (the ``piecewiseness'' is shifted
to the polynomial terms of higher order).

In general $M$-\textbf{splines} are piecewise $M$-degree polynomial regression
curves with $M-1$-smoothness restriction at binding knots (continuity with $M-1$
continuous derivatives). The basis functions for this kind of curve are
\begin{align*}
	h_j(x) &= x^{j-1}&\text{for }j=1,\ldots, M\\
	h_{M+j}(x) &= (x-\xi_j)_+^M &\text{for }j=1,\ldots,K
\end{align*}
For weirder piecewise $d$-degree polynomial curves, which are $C^{m-1}$ ($m\leq d$),
the basis might look like this:
\begin{align*}
	b_j(x) &= x^{j-1}&\text{for }j=1,\ldots, d\\
	h_{ij}(x) &= (x-\xi_j)_+^i &\text{for } j=1,\ldots,K\text{ and } i=m,\ldots,d
\end{align*}

Generally speaking, if there are $K$ knots then there are $K+1$ ``pieces'' in the
piecewise polynomial curve. If over each region a $d$-degree polynomial is to be
fit, then there are $(d+1)(K+1)$ parameters. If continuity of the curve is to be
desired, one needs to add one constraint on the parameters per each knot, since only
of the piece boundaries the curve is discontinuous. Requiring the curve to be $C^m$
with $m\leq d$ adds one constraint per knot per degree of smoothness which add up
to $(m+1) K$. Thus there are $(d-m)K + (d+1)$ free parameters left. For a cubic
spline with continuous $2$-nd derivative, $d=3$ and $m=2$, the number of ``degrees
of freedom'' is $K+4$.

One polynomial is global and smooth, but the higher the order the more oscillating
the behaviour a the ends (the Runge's phenomenon). More polynomials with a local
regression are not necessarily continuous. Splines are local and smooth!

What to do with the endpoints? Problems with the values out the common region of $x$.

\subsubsection{Natural Cubic Spline} % (fold)
\label{ssub:natural_cubic_spline}
An idea: within the range of the univariate data fit an $M$-spline, but to achieve
robustness beyond $[a,b]$ and get more trustworthy predictions require a linear
model. Thus for a \textbf{n}atural \textbf{c}ubic \textbf{s}pline within the main
region $[a,b]$ a $3$-spline is fit, whereas beyond the boundaries the function is
required to be linear (with continuity and all smoothness constraints).

With this kind of additional regularization the number of parameters for the NCS
is further reduced. The $3$-spline had $K+4$ basis functions, and the NCS has $K$
basis functions, sine beyond knots $\xi_1$ and $\xi_K$ the derivatives higher than
the first must be zero. Example: $3$-spline (cubic spline) has $K+4$ functions,
and the NCS has $K$ basis functions.
\[G(x) = \sum_{j=1}^K \theta_j N_j(x)\]
where $(N_j(\cdot))_{j=1}^K$ is the set of basis functions for the Natural Cubic
Spline.

Goal: show that it is enough to know the value of the NCS at $K$ knots to know it
everywhere. Consider a regression problem on some interval $[a,b]$. Let $K$ be the
number of knots $(\xi_k)_{k=1}^K\in [a,b]$ with
\[a < \xi_1<\ldots<\xi_k<\ldots<\xi_K<b\]

Suppose $g$ is the NCS on $[a,b]$ -- a piecewise polynomial curve, cubic within
the main region and linear beyond $[a,b]$ (?). For all $k=1,\ldots K$ let
$g_k = g(\xi_k)$.
Since NCS is cubic, the second derivative must be linear in $x$ between any two
consecutive knots: for $x\in (\xi_k,\xi_{k+1}]$, $k=1,\ldots, K-1$
\[g''(x) = \alpha_k x + \beta_k\]

Introduce a set of extra parameters $(\gamma_k)_{k=1}^K$ which are defined as the
value of the second derivative of $g$ at each knot: $\gamma_k=g''(\xi_k)$.
Obviously, due to linearity at the ends $\gamma_K = \gamma_1 = 0$.
% Given these new parameters, let's try to squeeze the parameters from within
% by the smoothness results
For $k=1,\ldots, K-1$ the continuity of $g''$ is equivalent to $g''(\xi_k+0) = g''(\xi_k-0)$
which in terms of $\gamma_k$ is identical to
\begin{align*}
	\gamma_k &= \alpha_k \xi_k + \beta_k\\
	\gamma_{k+1} &= \alpha_k \xi_{k+1} + \beta_k
\end{align*}
for all $k=1,\ldots,K-1$. Since $\xi_k<\xi_{k+1}$ this system of linear equation
resolves into
\begin{align*}
	\alpha_k &= \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k}\\
	\beta_k &= \frac{\xi_{k+1}\gamma_k - \xi_k\gamma_{k+1}}{\xi_{k+1}-\xi_k}
\end{align*}
Therefore $g''(x)$ on $x\in (\xi_k, \xi_{k+1}]$ for $k=1,\ldots, K-1$ is defined as
\[g''(x) = \frac{\gamma_{k+1}(x-\xi_k)+\gamma_k(\xi_{k+1}-x)}{\xi_{k+1}-\xi_k}\]
Integrating on $x\in (\xi_k,\xi_{k+1}]$ yields
\[g'(x) = A_k + \frac{1}{2}\frac{\gamma_{k+1}(x-\xi_k)^2-\gamma_k(x-\xi_{k+1})^2}{\xi_{k+1}-\xi_k}\]
whence second integration yields on the same region results in
\[
g(x)
= A_k(x - \xi_k) + B_k + \frac{1}{6}
	\frac{\gamma_{k+1}(x-\xi_k)^3+\gamma_k(\xi_{k+1}-x)^3}{\xi_{k+1}-\xi_k}
\]
for all $x\in (\xi_k,\xi_{k+1}]$ and $k=1, K-1$. The regions beyond the boundaries
by definition must be linear, therefore $g(x) = A_0 (\xi_1-x) + B_0$ for $x\leq \xi_1$
and $g(x) = A_K (x-\xi_1) + B_K$ for $x > \xi_K$.

Now let's for a moment forget about the smoothness constraints and try to find
the parameters $A_k$ and $B_k$ on each interval $(\xi_k,\xi_{k+1}]$ from the values
$g_k = g(\xi_k)$. Since $g$ is continuous, for each $k=1,\ldots,K-1$ it must be
true that $g_k = g(\xi_k-) = g(\xi_k+)$, which is equivalent to
\begin{align*}
g_k &= A_k(\xi_k - \xi_k) + B_k + \frac{1}{6}
	\frac{\gamma_{k+1}(\xi_k-\xi_k)^3+\gamma_k(\xi_{k+1}-\xi_k)^3}{\xi_{k+1}-\xi_k}\\
	&= B_k + \frac{1}{6} \gamma_k (\xi_{k+1}-\xi_k)^2\\
g_{k+1} &= A_k(\xi_{k+1}-\xi_k) + B_k + \frac{1}{6}
	\frac{\gamma_{k+1}(\xi_{k+1}-\xi_k)^3+\gamma_k(\xi_{k+1}-\xi_{k+1})^3}{\xi_{k+1}-\xi_k}\\
	&= A_k(\xi_{k+1}-\xi_k) + B_k + \frac{1}{6} \gamma_{k+1}(\xi_{k+1}-\xi_k)^2
\end{align*}
This linear inhomogeneous system is trivial, as one of the unknowns is already
solved for 
\[B_k = g_k - \frac{1}{6} \gamma_k \delta_k^2\]
where $\delta_k = \xi_{k+1}-\xi_k$ for brevity. The second unknown is thus
\[A_k = \frac{g_{k+1} - g_k}{\delta_k} - \frac{1}{6} (\gamma_{k+1}-\gamma_k) \delta_k\]
Plugging these expressions back into $g(x)$ yields
\begin{align*}
g(x) &= \Bigl(
		\frac{g_{k+1} - g_k}{\delta_k} - \frac{\delta_k}{6} (\gamma_{k+1}-\gamma_k)
	\Bigr) (x - \xi_k) + g_k - \frac{\delta_k^2}{6} \gamma_k\\
	&\quad + \frac{1}{6\delta_k} \bigl(\gamma_{k+1}(x-\xi_k)^3+\gamma_k(\xi_{k+1}-x)^3\bigr)\\
	&= \frac{g_{k+1} - g_k}{\delta_k}(x - \xi_k) + g_k
	- \frac{\delta_k}{6} (\gamma_{k+1}-\gamma_k)(x - \xi_k) - \frac{\delta_k^2}{6} \gamma_k\\
	&\quad + \frac{\delta_k^2}{6} \Bigl(\gamma_{k+1}\Bigl(\frac{x-\xi_k}{\delta_k}\Bigr)^3+\gamma_k\Bigl(\frac{\xi_{k+1}-x}{\delta_k}\Bigr)^3\Bigr)\\
	&= \frac{g_{k+1}(x - \xi_k) + g_k(\xi_{k+1} - x)}{\delta_k}
	- \frac{\delta_k}{6} \bigl( \gamma_{k+1} (x - \xi_k) + \gamma_k (\xi_{k+1}-x)\bigr)\\
	&\quad + \frac{\delta_k^2}{6} \Bigl(\gamma_{k+1}\Bigl(\frac{x-\xi_k}{\delta_k}\Bigr)^3+\gamma_k\Bigl(\frac{\xi_{k+1}-x}{\delta_k}\Bigr)^3\Bigr)\\
\end{align*}
because
\begin{align*}
\frac{g_{k+1} - g_k}{\delta_k}(x - \xi_k) + g_k
	&= \frac{g_{k+1} - g_k}{\delta_k}(x - \xi_k) + \frac{g_k\xi_{k+1} - g_k\xi_k}{\delta_k}\\
	&= \frac{g_{k+1}(x - \xi_k) + g_k(\xi_{k+1} - x)}{\delta_k}\\
\gamma_k \delta_k + (\gamma_{k+1}-\gamma_k) (x - \xi_k)
	& = \gamma_k \xi_{k+1} - \gamma_k \xi_k
		+ \gamma_{k+1} x - \gamma_k x
		- \gamma_{k+1}\xi_k + \gamma_k \xi_k \\
	&= \gamma_{k+1} (x - \xi_k) + \gamma_k (\xi_{k+1}-x) 
\end{align*}
Therefore for $x\in (\xi_k,\xi_{k+1}]$
\begin{align*}
g(x) &= \Bigl( g_{k+1} \frac{x - \xi_k}{\delta_k} + g_k \frac{\xi_{k+1}-x}{\delta_k} \Bigr)
	- \frac{\delta_k^2}{6} \Bigl( \gamma_{k+1} \frac{x - \xi_k}{\delta_k} + \gamma_k \frac{\xi_{k+1}-x}{\delta_k} \Bigr)\\
	&\quad+ \frac{\delta_k^2}{6} \Bigl( \gamma_{k+1}\Bigl(\frac{x-\xi_k}{\delta_k}\Bigr)^3+\gamma_k\Bigl(\frac{\xi_{k+1}-x}{\delta_k}\Bigr)^3\Bigr)
\end{align*}
Group the last two terms on the right hand side into
\[
- \frac{\delta_k^2}{6} \Bigl(
	\gamma_{k+1}\frac{x - \xi_k}{\delta_k} \Bigl\{1-\Bigl(\frac{x-\xi_k}{\delta_k}\Bigr)^2\Bigr\}
	+\gamma_k\frac{\xi_{k+1}-x}{\delta_k} \Bigl\{1-\Bigl(\frac{\xi_{k+1}-x}{\delta_k}\Bigr)^2\Bigr\}
\Bigr)
\]
Since 
\[
1-\frac{x-\xi_k}{\delta_k} = \frac{\xi_{k+1}-x}{\delta_k }
\text{ and }
1-\frac{\xi_{k+1}-x}{\delta_k} = \frac{x-\xi_k}{\delta_k }
\]
and the terms in curly brackets are differences of squares both terms reduce to
\begin{multline*}
\ldots = - \frac{\delta_k^2 }{6} \frac{\xi_{k+1}-x}{\delta_k} \frac{x-\xi_k}{\delta_k} \Bigl(
		\gamma_{k+1}\Bigl\{1+\frac{x-\xi_k}{\delta_k}\Bigr\}
		+ \gamma_k\Bigl\{1+\frac{\xi_{k+1}-x}{\delta_k}\Bigr\}
	\Bigr)\\
= - \frac{1}{6} (\xi_{k+1}-x)(x-\xi_k) \Bigl(
	\gamma_{k+1}\Bigl\{1+\frac{x-\xi_k}{\delta_k}\Bigr\}
	+\gamma_k\Bigl\{1+\frac{\xi_{k+1}-x}{\delta_k}\Bigr\}
\Bigr)
\end{multline*}
Thus over the partition within the main region the expression of the NCS becomes
\begin{align*}
g(x) &= - \frac{1}{6} (\xi_{k+1}-x)(x-\xi_k) \Bigl(
		\gamma_{k+1}\Bigl\{1+\frac{x-\xi_k}{\delta_k}\Bigr\}
		+\gamma_k\Bigl\{1+\frac{\xi_{k+1}-x}{\delta_k}\Bigr\}
	\Bigr)\\
	&\quad +\Bigl( g_{k+1} \frac{x - \xi_k}{\delta_k} + g_k \frac{\xi_{k+1}-x}{\delta_k} \Bigr)
\end{align*}
for any $x\in(\xi_k, \xi_{k+1}]$ and each $k=1,\ldots, K-1$. Since for all $k=1,\ldots, K-1$
\[\lim_{x\downarrow\xi_k} g(x) = g(\xi_k-) = g_k\]
and formally $g_k = g(\xi_k)$, the domain of each polynomial piece can be a closed
interval $[\xi_k, \xi_{k+1}]$.

As for the branches outside the main region, they are given by the following
expressions with two as of yet ``free'' parameters:
\[g(x) = \begin{cases}
	A_0(\xi_1-x)+B_0, &\text{ if }x\leq \xi_1\\
	A_K(x-\xi_K)+B_K, &\text{ if }x > \xi_K
\end{cases}\]
since integration of $g''(x)$, which is zero over these regions, yields lines. Using
the continuity conditions $g(\xi_1-)=g_1$ an $g(\xi_K+)=g_K$ at $\xi_1$ and $\xi_K$
respectively implies that
\[B_0 = g_1\text{ and }B_K = g_K\]

Recall that the derivative of $g(x)$ over $(\xi_k, \xi_{k+1})$ is (even though $A_k$
are mostly known)
\[g'(x) = A_k + \frac{1}{2\delta_k}\bigl(\gamma_{k+1}(x-\xi_k)^2-\gamma_k(x-\xi_{k+1})^2\bigr)\]
Though the domain of the derivative of each piece is $(\xi_k, \xi_{k+1})$,
the requirement that the derivative of $g$ be continuous makes it possible to use
closed intervals instead of half-open.

Now at the boundary at the $\xi_1$ knot the expressions for the left hand and the
right hand derivatives are
\begin{align*}
	g'(x) &= \bigl[ \text{ if } x \leq \xi_1 \bigr]
		= - A_0\\
	g'(x) &= \bigl[ \text{ if } x > \xi_1 \bigr]
		= A_1 + \frac{1}{2\delta_1}\bigl(\gamma_2(x-\xi_1)^2-\gamma_1(x-\xi_2)^2\bigr)
\end{align*}
The smoothness requirement
\[g'(\xi_1+) = \lim_{x\downarrow \xi_k} g'(x) = \lim_{x\uparrow \xi_k} g'(x) = g'(\xi_1-)\]
implies the following equation
\[- A_0 = A_1 - \frac{\delta_1}{2}\gamma_1\]
However $\gamma_1 = 0$ means that $- A_0 = A_1$. At the knot $\xi_K$ similar reasoning
yields
\[
A_K
= A_{K-1} + \frac{\delta_{K-1}}{2}\gamma_K
= A_{K-1}
\]
which means that the ``parameters'' of the line segments beyond the main region
were not free after all.

There must be some relation between $(g_k)_{k=1}^K$ and $(\gamma_k)_{k=2}^{K-1}$
since continuity and $2$-nd order smoothness across the knots are quite strict
requirements. For $k=1,\ldots,K-1$ and $x\in [\xi_k,\xi_{k+1}]$
\[g'(x) = A_k + \frac{1}{2\delta_k}\bigl(\gamma_{k+1}(x-\xi_k)^2-\gamma_k(x-\xi_{k+1})^2\bigr)\]
The very same smoothness condition $g'(\xi_k+)=g'(\xi_k-)$ for $k=2,\ldots,K-1$ results in
\begin{align*}
	g'(\xi_k+) &= A_k + \frac{1}{2\delta_k}\bigl(\gamma_{k+1}(\xi_k-\xi_k)^2-\gamma_k(\xi_k-\xi_{k+1})^2\bigr)\\
	g'(\xi_k-) &= A_{k-1} + \frac{1}{2\delta_{k-1}}\bigl(\gamma_k(\xi_k-\xi_{k-1})^2-\gamma_{k-1}(\xi_k-\xi_k)^2\bigr)
\end{align*}
whence for $k=2,\ldots,K-1$
\begin{align*}
	g'(\xi_k+) &= A_k - \frac{1}{2}\gamma_k\delta_k\\
	g'(\xi_k-) &= A_{k-1} + \frac{1}{2}\gamma_k\delta_{k-1}
\end{align*}
This gives an equation
\[A_k - A_{k-1} = \frac{1}{2}\gamma_k(\delta_k+\delta_{k-1})\]
where the left hand side is equal to
\[A_k - A_{k-1}
= \frac{g_{k+1} - g_k}{\delta_k} - \frac{1}{6} (\gamma_{k+1}-\gamma_k) \delta_k
- \frac{g_k - g_{k-1}}{\delta_{k-1}} + \frac{1}{6} (\gamma_k-\gamma_{k-1}) \delta_{k-1}
\]
After minor rearrangement
\[
\frac{1}{6}\bigl(3\gamma_k(\delta_k+\delta_{k-1})
+ (\gamma_{k+1}-\gamma_k) \delta_k
- (\gamma_k-\gamma_{k-1}) \delta_{k-1} \bigr)
= \frac{g_{k+1} - g_k}{\delta_k} - \frac{g_k - g_{k-1}}{\delta_{k-1}}
\]
The right hand side simplifies to
\begin{multline*}
\frac{g_{k+1} - g_k}{\delta_k} - \frac{g_k - g_{k-1}}{\delta_{k-1}}
= \frac{g_{k+1}\delta_{k-1} - g_k(\delta_k + \delta_{k-1}) + g_{k-1}\delta_k}{\delta_k\delta_{k-1}}\\
= \frac{1}{\delta_k}g_{k+1} + \Bigl(-\frac{1}{\delta_{k-1}} - \frac{1}{\delta_k}\Bigr)g_k + \frac{1}{\delta_{k-1}}g_{k-1}
\end{multline*}
while the right hand side (without the multiplier) reduces to
\begin{multline*}
	3\gamma_k(\delta_k+\delta_{k-1}) + (\gamma_{k+1}-\gamma_k) \delta_k - (\gamma_k-\gamma_{k-1}) \delta_{k-1}\\
	= 3\gamma_k(\delta_k+\delta_{k-1}) + \gamma_{k+1}\delta_k - \gamma_k\delta_k - \gamma_k\delta_{k-1} + \gamma_{k-1}\delta_{k-1}\\
	= 3\gamma_k(\delta_k+\delta_{k-1}) + \gamma_{k+1}\delta_k - \gamma_k(\delta_k +\delta_{k-1}) + \gamma_{k-1}\delta_{k-1}\\
	= \gamma_{k+1}\delta_k + 2\gamma_k(\delta_k+\delta_{k-1}) + \gamma_{k-1}\delta_{k-1}
\end{multline*}
Hence vectors $g = (g_k)_{k=1}^K\in \Real^{K\times 1}$ and
$\gamma = (\gamma_k)_{k=2}^{K-1}\in \Real^{(K-2)\times 1}$
are related through the following equations:
\[
\frac{\delta_k}{6} \gamma_{k+1} + \frac{\delta_k+\delta_{k-1}}{3} \gamma_k + \frac{\delta_{k-1}}{6}\gamma_{k-1}
= \frac{1}{\delta_k}g_{k+1} + \Bigl(-\frac{1}{\delta_{k-1}} - \frac{1}{\delta_k}\Bigr)g_k + \frac{1}{\delta_{k-1}}g_{k-1}
\]
Or in the matrix notation
\[Q'g = R\gamma\]
where $R\in \Real^{(K-2)\times(K-2)}$ and $Q\in \Real^{K\times(K-2)}$.
The matrix $R$ is a tri-diagonal square $(K-2)\times(K-2)$ matrix with elements
\[
r_{ij} = \begin{cases}
	\frac{\delta_{i+1}+\delta_i}{3}, &\text{ if } j=i\\
	\frac{1}{6}\delta_i, &\text{ if } j=i-1\\
	\frac{1}{6}\delta_{i+1}, &\text{ if } j=i+1\\
	0, &\text{ otherwise }
\end{cases}
\]
The matrix $Q$ is also a tri-diagonal matrix, but it is rectangular $K\times(K-2)$.
Its elements are given by
\[
q_{ij} = \begin{cases}
	\frac{1}{\delta_i}, &\text{ if } i=j\\
	-\Bigl(\frac{1}{\delta_i}+\frac{1}{\delta_{i+1}}\Bigr), &\text{ if } i=j+1\\
	\frac{1}{\delta_{i+1}}, &\text{ if } i=j+2\\
	0, &\text{ otherwise }
\end{cases}
\]
The matrices $Q$ and $R$ have the following structure:
\begin{align*}
Q &= \begin{pmatrix}
q_{11} 	& 0 		& 0 		& \cdots & 0 \\
q_{21} 	& q_{22} 	& 0 		& \cdots & 0 \\
q_{31} 	& q_{32} 	& q_{33} 	& \cdots & 0 \\
0 		& q_{42} 	& q_{43} 	& \cdots & 0 \\
0 		& 0 		& q_{53} 	& \cdots & 0 \\
\vdots 	& \vdots	& \vdots 	& \ddots & \vdots \\
0 		& 0			& 0 		& \cdots & q_{K(K-2)}
\end{pmatrix}_{K\times (K-2)}\\
R &= \begin{pmatrix}
r_{11} 	& r_{12} 	& 0 		& \cdots & 0 \\
r_{21} 	& r_{22} 	& r_{23} 	& \cdots & 0 \\
0 		& r_{32} 	& r_{33} 	& \cdots & 0 \\
\vdots 	& \vdots	& \vdots 	& \ddots & \vdots \\
0 		& 0			& 0 		& \cdots & r_{(K-2)(K-2)}
\end{pmatrix}_{(K-2)\times (K-2)}
\end{align*}
respectively. The matrix $R$ is symmetric, since for all $k=1,\ldots, K-2$
\[
r_{(k+1)k}
= r_{(k+1)((k+1)-1)}
= \frac{\delta_{k+1}}{6}
= r_{k(k+1)}
\]
and all other elements except for the diagonal are zero. Furthermore the matrix
$R$ is positive definite. Indeed the product $\gamma'R\gamma$ is given by
\begin{align*}
\gamma'R\gamma
	&= \sum_{k=2}^{K-1} \gamma_k\Bigl( \frac{\delta_k}{6} \gamma_{k+1} + \frac{\delta_k+\delta_{k-1}}{3} \gamma_k + \frac{\delta_{k-1}}{6}\gamma_{k-1} \Bigr)\\
	&= \sum_{k=2}^{K-1} \frac{\delta_k}{6} \bigl(\gamma_{k+1}\gamma_k + 2 \gamma_k^2\bigr)
		+ \frac{\delta_{k-1}}{6} \bigl( 2\gamma_k^2 + \gamma_{k-1}\gamma_k \bigr)\\
	&= \frac{\delta_{K-1}}{6} \bigl(\gamma_K\gamma_{K-1} + 2 \gamma_{K-1}^2\bigr)
		+ \sum_{k=2}^{K-2} \frac{\delta_k}{6} \bigl(\gamma_{k+1}\gamma_k + 2 \gamma_k^2\bigr) \\
	&\quad + \sum_{k=3}^{K-1} \frac{\delta_{k-1}}{6} \bigl( 2\gamma_k^2 + \gamma_{k-1}\gamma_k \bigr)
		+ \frac{\delta_1}{6} \bigl( 2\gamma_2^2 + \gamma_1\gamma_2 \bigr)\\
	&= \frac{\delta_{K-1}}{3} \gamma_{K-1}^2
		+ \sum_{k=2}^{K-2} \Bigl( \frac{\delta_k}{6} \bigl(\gamma_{k+1}\gamma_k + 2 \gamma_k^2\bigr)
			+ \frac{\delta_k}{6} \bigl( 2\gamma_{k+1}^2 + \gamma_k\gamma_{k+1} \bigr)\Bigr)
		+ \frac{\delta_1}{3}\gamma_2^2\\
	&= \frac{\delta_{K-1}}{3} \gamma_{K-1}^2 + \frac{\delta_1}{3}\gamma_2^2
		+ \sum_{k=2}^{K-2} \frac{\delta_k}{6} \bigl(2\gamma_{k+1}\gamma_k + 2 \gamma_k^2 + 2\gamma_{k+1}^2\bigr)\\
	&= \frac{\delta_{K-1}}{3} \gamma_{K-1}^2 + \frac{\delta_1}{3}\gamma_2^2
		+ \sum_{k=2}^{K-2} \frac{\delta_k}{6} \bigl((\gamma_{k+1}+\gamma_k)^2 -\gamma_{k+1}^2 - \gamma_k^2 + 2 \gamma_k^2 + 2\gamma_{k+1}^2\bigr)\\
	&= \frac{\delta_{K-1}}{3} \gamma_{K-1}^2 + \frac{\delta_1}{3}\gamma_2^2
		+ \sum_{k=2}^{K-2} \frac{\delta_k}{6} \bigl((\gamma_{k+1}+\gamma_k)^2 + \gamma_k^2 + \gamma_{k+1}^2\bigr)
\end{align*}
Therefore the matrix $R$ must be non-singular.

\noindent To reiterate, the final form of the NCS is given by
\[
g(x) = \begin{cases}
	A_0(\xi_1-x)+g_1, &\text{ if }x \leq \xi_1\\
	A_K(x-\xi_K)+g_K, &\text{ if }x \geq \xi_K\\
	\Bigl(
		g_{k+1} \frac{x - \xi_k}{\delta_k} + g_k \frac{\xi_{k+1}-x}{\delta_k}
	\Bigr) - \frac{1}{6} (\xi_{k+1}-x)(x-\xi_k) \cdot\\
	\quad\quad\cdot \Bigl\{
		\gamma_{k+1}\Bigl(1+\frac{x-\xi_k}{\delta_k}\Bigr)
		+\gamma_k\Bigl(1+\frac{\xi_{k+1}-x}{\delta_k}\Bigr)
	\Bigr\}, &\text{ if } x\in [\xi_k,\xi_{k+1}]
\end{cases}
\]
with
\begin{align*}
	A_0 &= -g'(\xi_1) = - A_1 =  \frac{1}{6} \gamma_2 \delta_1 - \frac{g_2 - g_1}{\delta_1}\\
	A_K &= g'(\xi_K) = A_{K-1} = \frac{g_K - g_{K-1}}{\delta_{K-1}} + \frac{1}{6} \gamma_{K-1} \delta_{K-1}
\end{align*}

\noindent\textbf{Theorem}\hfill \\
The vectors of values $g$ and $2$-nd derivatives $\gamma$ fully specify the NCS
if and only if $Q'g = R'\gamma$. When this is satisfied, then $\gamma = R^{-1}Q'g$ and
\[
\int_a^b \abs{g''(s)}^2 ds
= \gamma' R \gamma
= g' Q R^{-1} R R^{-1} Q' g
= g' K g
\]
where the ``penalty matrix'' $K = Q R^{-1} Q'$ is a positive semi-definite matrix. Indeed,
\begin{align*}
	\int_a^b \bigl|g''(s)\bigr|^2 ds
% Simple integration by parts
	&= \Bigl. g''(s) g'(s) \Bigr\rvert_a^b - \int_a^b g'(s) g'''(s) ds \\
% Since on $(a, \xi_1]$ and $[\xi_K, b)$ the function $g$ is linear. And g''(a) = g''(b) = 0
	&= - \int_{\xi_1}^{\xi_K} g'(s) g'''(s) ds
		= - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} g'(s) g'''(s) ds\\
% Due to piecewise linearity and continuity of g''
	&= - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} g'(s) \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} ds\\
% But g''' is constant over each interval (and the Lebesgue measure has  dx({s}) = 0)
	&= - \sum_{k=1}^{K-1} \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} \int_{\xi_k}^{\xi_{k+1}} g'(s) ds\\
% Since \int_{\xi_k}^{\xi_{k+1}} g'(s) ds = g(\xi_{k+1}) - g(\xi_k)
	&= - \sum_{k=1}^{K-1} \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} \bigl(g_{k+1}-g_k\bigr)\\
	&= \sum_{k=1}^{K-1} \gamma_k\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k}
		- \sum_{k=1}^{K-1} \gamma_{k+1}\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k}\\
% Since \gamma_1 = \gamma_K = 0
	&= \sum_{k=2}^{K-1} \gamma_k \Bigl(\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k} - \frac{g_k-g_{k-1}}{\xi_k-\xi_{k-1}} \Bigr)\\
% By definition of Q'g
	&= \sum_{k=2}^{K-1} \gamma_k \bigl[Q'g\bigr]_k\\
	&= \gamma' Q' g = \gamma' R \gamma
\end{align*}
A nice thing is that this integral can be used as a penalty term in a regularized
regression problem.

ANother important property of the NCS is summarized in the following theorem:
\noindent \textbf{Theorem}\hfill\\
the NCS has the minimum value of the smoothness integral $\int_a^b |g''(s)| ds$
among all $C^2[a,b]$-smooth curves interpolating the data $(\xi_k, g_k)_{k=1}^K$.

This means that formally we have the following: if $K\geq 2$ and $g$ is the NCS,
interpolating $g_1,\ldots,g_K$ on knots $a\leq \xi_1 <\ldots<\xi_K\leq b$ within
a bounded interval $[a,b]$, then for any other interpolating function $\tilde{g}$
on the same mesh, the total variation of $\tilde{g}''$ is necessarily grater than
that of $g$.

Indeed, consider an at least $C^2[a,b]$ smooth function $\tilde{g}$, which interpolates
the data $(\xi_k, g_k)_{k=1}^K$. Then the function $h \defn g - \tilde{g}$ has roots at
knots $(\xi_k)_k$. First notice that (this is true for any $g$ with piecewise linear
second derivative)
\begin{align*}
	\int_a^b h'' g'' ds
%% Integrate by parts using the nice properties of g 
	& = \Bigl. h'(s) g''(s)\Bigr\rvert_a^b - \int_a^b h'(s) g'''(s) ds \\
%% since g''' is zero outside [\xi_1, \xi_K] and g'' is zero at the
%%  endpoints of the interval [a,b]
	& = - \int_{\xi_1}^{\xi_K} h'(s) g'''(s) ds \\
	& = - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} h'(s) g'''(s) ds \\
%% g''' is piecewise constant
	& = \Bigl[g'''(x) = \alpha_k\text{ on } x\in[\xi_k, \xi_{k+1}]\Bigr]
	& = - \sum_{k=1}^{K-1}  \alpha_k \int_{\xi_k}^{\xi_{k+1}} h'(s) ds \\
	& = - \sum_{k=1}^{K-1}  \alpha_k (h(\xi_{k+1})-h(\xi_k)) = 0
\end{align*}
since $h(\xi_k)=0$ for all $k$. The total variation of the $2$-nd derivative
of $\tilde{g}$ is
\begin{align*}
	\int_a^b |\tilde{g}''|^2 ds
	&= \int_a^b \bigl|g'' + h''\bigr|^2 ds
	= \int_a^b |g''|^2 + |h''|^2 ds + 2 \int_a^b h'' g'' ds\\
	&= \int_a^b |g''|^2 + |h''|^2 ds \geq \int_a^b |g''|^2 ds
\end{align*}
Now, if $\tilde{g} = g+h \in C^2[a,b]$ interpolates the knots and is such that
\[\int_a^b |\tilde{g}''|^2 ds = \int_a^b |g''|^2 ds\]
then $\int_a^b |h''|^2 ds = 0$, which implies that $|h''|=0$ on $[a,b]$. Therefore
$h(x)=Ax + B$ and $\tilde{g}(x) = g(x) + A x + B$. But since $\tilde{g}$ and $g$
interpolate the data, it must be true that $A \xi_k + B = 0$ for all $k$. Hence
$A=B=0$ and $h=0$.

Obviously, if $\tilde{g}=g$ then $\int_a^b |\tilde{g}''|^2 ds=\int_a^b |g''|^2 ds$.

So far the following wonderful properties have been demonstrated for the NCS
\begin{itemize}
	\item on $[a,b]$ it is completely determined by its values $(g_k)_k$ at
	the interpolation knots and the values of the second derivatives at the
	internal nodes are computed via $\gamma = R^{-1}Q'g$;
	\item it brings the value of the $\int_a^b |g''|^2 ds$ to a minimum in the class
	of $C^2[a,b]$ smooth curves interpolating the data at the same knots;
	\item to interpolate at $K$ knots it requires only $K$ basis functions.
\end{itemize}

A side note: consider an arbitrary map $f\in C^2[a,b]$ and let $h = f - g$. Then
\begin{align*}
	\int_a^b h'' g'' ds &= h' g''\rvert_a^b - \int_a^b h' g''' ds \\
%% g''(a)=g''(b) = 0 and g'''(s) = 0 for s<\xi_1 or s>\xi_K
	&= - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} h' g''' ds \\
%% The third derivative of g
	&= \sum_{k=1}^{K-1} \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} (h_k - h_{k+1})\\
	&= \sum_{k=2}^{K-1} \gamma_k \bigl( \frac{h_{k+1}-h_k}{\xi_{k+1}-\xi_k}
		- \frac{h_k - h_{k-1}}{\xi_k-\xi_{k-1}}\bigr)\\
	&= \gamma'Q'h = g' Q R^{-1} Q' h = g' K h
\end{align*}
where $h = (h_k)_{k=1}^K$ with $h_k = h(\xi_k)$ for all $k=1,\ldots K$. Therefore
\[
\int_a^b |f''|^2 ds
= \int_a^b |h''|^2 ds + \int_a^b |g''|^2 ds + 2\int_a^b h''g'' ds
= \int_a^b |h''|^2 ds - h' K h + g' K h + h' K g + g' K g + h' K h
\]
whence for any $f\in C^2[a,b]$
\[ \int_a^b |f''|^2 ds - f' K f = \int_a^b |(f-g)''|^2 ds - (f-g)' K (f-g) \]
where $g$ is the NCS through $(\xi_k, g_k)_{k=1}^K$.

% subsubsection natural_cubic_spline (end)

\subsubsection{Tutorial \# 3 problem \# 1} % (fold)
\label{ssub:tutorial_3_problem_1}

Consider a cubic spline with $K$ knots
\[f(x) = \sum_{k=0}^3 \beta_k x^k + \sum_{k=1}^K \theta_k \bigl(x-\xi_k\bigr)_+^3\]
let's show that the boundary conditions for the NCS imply the following constraints
on the coefficients...

Indeed, first, let's find the derivatives of $f$ up to order $3$:
\begin{align*}
	f'(x) &= \sum_{k=1}^3 k\beta_k x^{k-1} + \sum_{k=1}^K 3 \theta_k \bigl(x-\xi_k\bigr)_+^2\\
	f''(x) &= \sum_{k=2}^3 k(k-1)\beta_k x^{k-2} + \sum_{k=1}^K 6 \theta_k \bigl(x-\xi_k\bigr)_+\\
	f'''(x) &= 6 \beta_3 + \sum_{k=1}^K 6 \theta_k 1_{x\geq \xi_k}
\end{align*}
For any $x\geq \xi_K$, we have $f'''(x) = 0$ and $1_{x\geq \xi_k}=1$, which
implies that
\[6 \beta_3 + 6 \sum_{k=1}^K \theta_k = 0\]
on the other hand, for $x < \xi_1$ we get 
\[6 \beta_3 = 0\]
Therefore $\beta_3 = 0$ and $\sum_{k=1}^K \theta_k = 0$.

With these results, the second derivative becomes
\[
f''(x)
= 2\beta_2
	+ 6 \sum_{k=1}^K \theta_k x 1_{x\geq \xi_k} - \theta_k \xi_k 1_{x\geq \xi_k}
\]
The second derivative of the NCS is restricted to be continuous, i.e.
$f''(\xi_j-)=f''(\xi_j+)$ for $j=1\ldots K$. In particular for $j=1,K$ this implies
that
\begin{align*}
	0 &= f''(\xi_1-) = f''(\xi_1+)
	= 2\beta_2 + 6 \theta_1 \xi_j - \theta_1 \xi_1\\
	0 &= f''(\xi_K+) = f''(\xi_K+)
	= 2\beta_2 + 6 \sum_{k=1}^{K-1} \theta_k \xi_K - \theta_k \xi_k\\
	&= 2\beta_2 + 6 \sum_{k=1}^K \theta_k \xi_K - \theta_k \xi_k
\end{align*}
whence $\beta_2 = 0$ and 
\[\sum_{k=1}^K \theta_k \xi_K = \sum_{k=1}^K \theta_k \xi_k\]
using previously obtained constraints
\[
\sum_{k=1}^K \theta_k \xi_K
= \xi_K \sum_{k=1}^K \theta_k
= 0
\]
Thus $\sum_{k=1}^K \theta_k \xi_k = 0$.

These constraints imply that a cubic spline, satisfying the NCS restrictions can be
expressed as
\[f(x) = \beta_0 + \beta_1 x + \sum_{k=1}^K \theta_k \bigl(x-\xi_k\bigr)_+^3\]
Now, the constraint $\sum_{k=1}^K \theta_k = 0$ yields
\[
\sum_{k=1}^K \theta_k \bigl(x-\xi_k\bigr)_+^3
= \sum_{k=1}^{K-1} \theta_k \Bigl((x-\xi_k)_+^3 - (x-\xi_K)_+^3\Bigr)
\]
Similarly $\theta_K = - \sum_{k=1}^{K-1} \theta_k$ implies
\[ \sum_{k=1}^K \theta_k \xi_k = \sum_{k=1}^{K-1} \theta_k ( \xi_k - \xi_K ) \]
The second restriction $\sum_{k=1}^K \theta_k \xi_k = 0$ implies
\[
\theta_{K-1} = - \frac{1}{\xi_K - \xi_{K-1}}\sum_{k=1}^{K-2} \theta_k ( \xi_K - \xi_k )
\]
Now define $\phi_k(x) = (x-\xi_k)_+^3 - (x-\xi_K)_+^3$ and observe that
\[
\sum_{k=1}^K \theta_k \bigl(x-\xi_k\bigr)_+^3
= \sum_{k=1}^{K-1} \theta_k \phi_k(x)
= \sum_{k=1}^{K-2} \theta_k \Bigl( \phi_k(x) - \frac{\xi_K - \xi_k}{\xi_K - \xi_{K-1}} \phi_{K-1}(x) \Bigr)
\]
whence
\[
\sum_{k=1}^K \theta_k \bigl(x-\xi_k\bigr)_+^3
= \sum_{k=1}^{K-2} \theta_k (\xi_K - \xi_k) \Bigl( \frac{\phi_k(x)}{\xi_K - \xi_k} - \frac{\phi_{K-1}(x)}{\xi_K - \xi_{K-1}} \Bigr)
\]
Thus for
\[ d_k(x) = \frac{\phi_k(x)}{\xi_K - \xi_k} \]
the following set of maps is a candidate for a ``basis'' of the NCS:
\begin{align*}
	N_1(x) &= 1\\
	N_2(x) &= x\\
	N_{k+2}(x) &= d_k(x) - d_{K-1}(x)
\end{align*}
and
\[f(x) = \sum_{k=1}^K \tilde{\theta}_k N_k(x)\]

% subsubsection tutorial_3_problem_1 (end)

\subsubsection{Tutorial \# 3 problem 3} % (fold)
\label{ssub:tutorial_3_problem_3}

Suppose the natural cubic spline is given by the linear combination of some basis
functions $\bigl(N_k(x)\bigr)_{k=1}^K$:
\[
g(x)
= \sum_{k = 1}^K \theta_k N_k(x)
= \mathbf{N}(x) \theta
\]
Since $\theta$ are constant with respect to $x$, the second derivative of $g$ is
\[
\frac{d^2}{dx^2} g(x)
= \frac{d^2}{dx^2} \Bigl(\mathbf{N}(x)\Bigr) \theta
= \mathbf{\omega}(x) \theta
\]
where $\mathbf{\omega}_k(x) = \frac{d^2}{dx^2}\mathbf{N}_k(x)$ and
$\mathbf{\omega}(x) \in \Real^{1\times n}$. Thus the goodness-of-fit functional
could be expressed in matrix notation:
\[
\RSS
= (t-N\theta)' (t-N\theta)
+ \lambda \int_a^b \theta' \mathbf{\omega}'\mathbf{\omega} \theta ds
\]
where $N = \bigl(\mathbf{N}_j(x_i)\bigr)_{ij=1}^n$ looks like
\[N = \begin{pmatrix}
	\mathbf{N}_1(x_1) & \mathbf{N}_2(x_1) & \cdots & \mathbf{N}_n(x_1)\\
	\mathbf{N}_1(x_2) & \mathbf{N}_2(x_2) & \cdots & \mathbf{N}_n(x_2)\\
	\vdots & \vdots & \ddots & \vdots\\
	\mathbf{N}_1(x_n) & \mathbf{N}_2(x_n) & \cdots & \mathbf{N}_n(x_n)
\end{pmatrix}\]
Since integration of a matrix in done elementwise, define $\Omega\in \Real^{n\times n}$ as
\[
\Omega_{ij}
= \int_a^b \mathbf{\omega}_i(s) \mathbf{\omega}_j(s) ds
= \int_a^b \mathbf{N}_i''(s) \mathbf{N}_j''(s) ds
\]
The matrix $\Omega$ is symmetric and positive semidefinite by construction. For
the NCS basis necessarily contains a border of zeroes corresponding to linear
basis functions in $(N_k)$.

Thus confined to the NCS family of functions the criterion reduces to
\[ \RSS(\theta, \lambda) = (t-N\theta)' (t-N\theta) + \lambda \theta'\Omega\theta \]
The solution (optimal NCS curve) is determined by the first order conditions given
by
\[ \frac{\partial}{\partial \theta} \RSS = 2\lambda \Omega\theta - 2 N't + 2 N'N \theta \]
whence
\[\hat{\theta} = (N'N + \lambda \Omega)^{-1} N' t\]
and 
\[\hat{t} = N (N'N + \lambda \Omega)^{-1} N' t\]
Since $\mathbf{N}$ is a set of linearly independent $C^2[a,b]$ functions, the matrix
$N$ must be of full rank $n$, which means that it it invertible. Therefore
\[
\hat{t}
= N (N'N + \lambda \Omega)^{-1} N' t
= \Bigl( {N'}^{-1} ( N'N + \lambda \Omega) N^{-1} \Bigr)^{-1} t 
= \Bigl( I_n + \lambda G'\Omega G \Bigr)^{-1} t
\]
where $G = N^{-1}$. Therefore $\hat{t} = S_\lambda t$ and $S_\lambda$ is in Reinsch
form $(I_n + \lambda H)$ for $H = G'\Omega G$.

% subsubsection tutorial_3_problem_3 (end)

% subsection piecewise_polynomials_and_splines (end)

\subsection{Smoothing spline} % (fold)
\label{sub:smoothing_spline}

The goal of fitting a smoothing spline is to find a function $g(x)$ among all functions
with two continuous derivatives, that minimizes the following penalized sum of squares
\[\RSS(g,\lambda) = \sum_{i=1}^n (t_i - g(x_i))^2 + \lambda \int |g''(s)|^2 ds\]
where $(x_i,t_i)_{i=1}^n$ is the data to be interpolated, $x_1<\ldots<x_i<\ldots<x_n$.
This functional basically rewards fitting the data as close as possible with $g$
while penalizing $g$ for irregular behaviour: the penalty term sums up non-linearities
in the smoother $g$ and is higher the more oscillations the $g$ has. On the whole, this
seems to be a functional problem of calculus of variations.

Intuitively, the regularizing parameter $\lambda$ should affect the optimal solution
in the following way: \begin{itemize}
	\item $\lambda\to \infty$ then the optimal solution is linear: $\int_a^b |g''|^2 ds > 0$
	gets penalized very highly, whence it should be optimal to chose $g$ with the
	irregularity penalty as close to $0$ as possible. In the limit of $\lambda$
	the term $\int_a^b |g''|^2 ds$ must be zero, which implies that $\hat{g}(x) = Ax + B$;
	\item $\lambda = 0$ then the optimal solution is some ``wild'' function, which
	matches all the points $(x_i, t_i)_{i=1}^n$, with no structure known beforehand.
\end{itemize}


%%% HERE -- sort through these crappy paragraphs below
Consider any function $f\in C^2$ which interpolates the data $(x_i, t_i)_{i=1}^n$.
Prior observations imply that necessarily
\[\int_a^b |f''|^2 ds \geq \int_a^b |g''|^2 ds\]
where $g$ is the NCS built at knots $(x_i, t_i)_{i=1}^n$. Therefore among all
interpolating functions with $f(x_i)=t_i$, the NCS has the smallest ``wiggliness''
penalty term.

\[ \int_a^b |f''|^2 ds - f' K f = \int_a^b |(f-g)''|^2 ds - (f-g)' K (f-g) \]

\[
\int_a^b |f(s)''|^2 ds
= \int_a^b |(f(s)-g(s))''|^2 ds + 2 (f-g)'K g + g'K g
\geq g'K g
\]
since $f(x)-g(x)$ has roots at $\xi_i$.

Indeed,
\[\RSS(g,\lambda) = \sum_{i=1}^n (t_i - g(x_i))^2 + \lambda \int_a^b |g''(s)|^2 ds\]

For an arbitrary $h\in C^2[a,b]$, it is true that
\begin{align*}
	\int_a^b h'' g'' ds
	&= h' g''\rvert_a^b - \int_a^b h' g''' ds\\
	&= - \sum_{i=1}^{n-1} \int_{x_i}^{x_{i+1}} h'(s) g'''(s) ds\\
	&= - \sum_{i=1}^{n-1}  \alpha_i \bigl(h(x_{i+1})-h(x_i)\bigr)
\end{align*}
where $\alpha_i = g(x_i+)$ for all $i$. Now, for $f = g+h$, it is true that
$\sum_{i=1}^n (t_i - f(x_i))^2 = \sum_{i=1}^n h(x_i)^2$ and
\[
\int_a^b |f''(s)|^2 ds
= \int_a^b |h''(s)|^2 ds + \int_a^b |g''(s)|^2 ds
+ 2\int_a^b h''(s)g''(s) ds
\]
whence
\[
\RSS(f,\lambda)
= \sum_{i=1}^n h(x_i)^2 + \lambda \int_a^b |h''(s)|^2 ds
	- 2 \lambda \sum_{i=1}^{n-1} c_i \bigl(h(x_{i+1})-h(x_i)\bigr)
	+ \RSS(g,\lambda)
\]



% Surprise! Infinite to finite reduction
Thus the optimal solution to this infinite dimensional problem is a quite finite
dimensional NCS. Therefore the problem reduces to the following quadratic optimization
problem:
\[\RSS(g,\lambda) = (t - g)'(t - g) + \lambda g'K g \to \min_g\]
or equivalently
\[\RSS(g,\lambda) = t't - g't - t'g + g'( I_n + \lambda K )g \to \min_g\]
Using matrix derivatives, the solution turns out to be:
\[\frac{\partial}{\partial g} \quad:\quad 2( I_n + \lambda K ) g - 2 t = 0\]
The matrix $K$ is not necessarily invertible, despite being symmetric and positive
semidefinite. However the matrix $I_n + \lambda K$ is invertible for any sufficiently
small $\lambda$, since the term $I_n$ ``regularizes'' the eigenvalues of $K$.
If $\eta, v$ is an eigenvalue-eigenvector pair of $K$, then
\[(I_n + \lambda K)x = (1 + \lambda\eta)x\]
which implies that the ``regularized'' matrix $I_n + \lambda K$ has eigenvalues
$1 + \lambda\eta > 0$. Therefore the roots of the characteristic polynomial of
$I_n + \lambda K$ are non-zero, implying that its determinant is nonzero as well,
whence follows invertibility.

The solution to the finite dimensional problem is
\[\hat{g} = (I_n + \lambda K)^{-1} t\]
This vector in fact uniquely identifies the NCS, thereby giving a closed form solution
to the original functional problem.

The form of the solution $\hat{g}$ resembles the linear regression case, where
the estimate of the target variable is a projection onto the space spanned by the
columns of $X$:
\[\hat{t} = X(X'X)^{-1}X't\]
The matrix $S_\lambda = (I_n + \lambda K)^{-1}$ is the so called \textbf{smoother matrix}.
Unfortunately, the smoother matrix is not a projector.

Since in the linear regression case the effective degrees freedom was defined as
the trace of the projector. Mimicking that idea, define
\[\text{df}(\lambda) = \tr(S_\lambda)\]

The matrices $K$ and the unit matrix are symmetric, whence $S_\lambda$ and its inverse
must be symmetric. Furthermore $S_\lambda$ is positive semi-definite, and all its
eigenvalues are non-negative.

To find out where the smoothing takes place consider the \textbf{SVD} of $S_\lambda$.
Since the matrix is symmetric and positive semi-definite, the SVD coincides with
the eigenvalue decomposition, where $U=V$: $S_\lambda = U M U'$, with
$M = \text{diag}(\mu_i)_{i=1}^n$. Thus $S_\lambda$ can be spectrally decomposed
into the sum
\[
S_\lambda
= U M U'
= \sum_{i=1}^n \mu_i u_i u_i'
\]
% It is an Hermitian matrix, hence the eigenvalues must be real.

In general, the eigenvectors of $S_\lambda$ should depend on $\lambda$. Luckily,
further analysis uncovers the following properties:
\begin{itemize}
	\item both $S_\lambda$ and $K$ have the same eigenvectors;
	\item the eigenvectors of $S_\lambda$ do not depend on $\lambda$.
\end{itemize}

For the first statement, consider an eigenvalue-eigenvector pair $(\mu, u)$ of
$S_\lambda$. Then, obviously, $S_\lambda u = \mu u$, whence due to invertibility
of $S_\lambda$ one has:
\[
u
= \mu S_\lambda^{-1} u
= \mu (I+\lambda K) u
= \mu u + \mu \lambda K u
\]
Therefore $u\neq 0$ is an eigenvector of $K$ corresponding to $\eta = \frac{1-\mu}{\lambda \mu}$.
The converse implication is similar: $K u = \eta u$ implies
\[ ( \lambda K + I ) u = ( \lambda \eta + 1 ) u \]
In conclusion, if $(\mu_k)_{k=1}^n$ are eigenvalues of $S_\lambda$ in non-increasing
order, then the corresponding eigenvectors $(u_k)_{k=1}^n$ are the eigenvectors
of $K$ with eigenvalues $(\eta_j)_{j=1}^n$, given by 
\[\eta_{n-j+1} = \frac{1}{1 + \lambda \mu_j}\]

As for the second conjecture, the matrix $K = Q R^{-1} Q'$ depends on the spacing
between the interpolation knots only, whence its eigenvalues and eigenvectors are
determined exclusively by the data to be interpolated.

The spectral decomposition of $S_\lambda$ yields:
\[
\hat{g}
= S_\lambda t
= \sum_{j=1}^n \mu_j u_j u_j' t
= \sum_{j=1}^n \mu_j \langle u_j, t \rangle u_j
\]
Since each eigenvector $u_j$ of $S_\lambda$ is independent of the actual value
of $\lambda$, the projection is made onto a fixed subspace, determined by the data
only. This observation makes it possible to do inter-model are comparisons with
respect to different values if the complexity parameter $\lambda$.

Now, the actual smoothing peculiarities can be inferred from the expression of
the eigenvalues of $S_\lambda$ through eigenvalues of $K$:
\[
\hat{g}
= \sum_{j=1}^n \frac{ \langle u_j, t \rangle }{1 + \lambda \eta_{n-j+1}} u_j
\]
The higher the index of the eigenvector $j$, the smaller the respective eigenvalue
of $S_\lambda$ is, and the more smoothing takes place. Thus the less variance in
the principal direction, the more of that direction is preserved as is.

To what maps in $C^2[a,b]$ do the first two eigenvectors of $S_\lambda$ correspond?
This question is equivalent to studying the functional properties of the eigenvectors
of $K$ with the smallest eigenvalues.

Higher $u_j$ correspond to more and more oscillating functions of $x$ and the smoothing
spline does not shrink linear components. The intuition for not shrinking linear
function of $x$ is that the penalty term involves the second derivative of $g$. Indeed,
\[
\int_a^b \Bigl|\frac{d^2}{ds} g(s)+l(s) \Bigr|^2 ds
= \int_a^b \Bigl|\frac{d^2}{ds} g(s) \Bigr|^2 ds
\]
for any $l$ -- linear in $x$. This means that linear components of $g$ ``pass through''
the regularization term and are not penalized at all. In fact, for a general penalty
term
\[ \int_a^b R\Bigl( \frac{d^{k_1}}{dx^{k_1}} g, \ldots, \frac{d^{k_p}}{dx^{k_p}} g\Bigr) ds\]
any additive component $h$ of $g$ with $\frac{d^k}{dx^k} h = 0$ does not get penalized,
where $k$ is the smallest among $(k_m)_{i=1}^m$.

Since smoothing spline does not shrink linear components, $\int_a^b |l''|^2ds = 0$
and the two least eigenvalues of $K$ ($\eta_n$ and $\eta_{n-1}$) are zero. Therefore
the nature of the matrix $K$ makes the first $\mu_1$ and $\mu_2$ equal to $1$:
\[
\hat{g} =
\langle u_1, t \rangle u_1 + \langle u_2, t \rangle u_2 +
+ \sum_{j=3}^n \mu_j \langle u_j, t \rangle u_j
\]


% Now, a matrix and its inverse have the same set of eigenvectors. Indeed, since
% $S_\lambda$ is non-singular, its eigenvalues must be non-zero for otherwise it
% would have been that $\text{det} S_\lambda = 0$. If $S_\lambda x = \mu x$ then
% \[
% S_\lambda^{-1} x
% = \frac{1}{\mu} S_\lambda^{-1} \mu x
% = \frac{1}{\mu} S_\lambda^{-1} S_\lambda x
% = \frac{1}{\mu} x
% \]

% A value $\eta$ is an eigenvalue of $K$ if and only if it is a root of the characteristic
% polynomial of $K$ 
% \[\text{det}(K - \eta I_n) = 0\]
% However, the multilinearity of determinants implies that for $\lambda>0$
% \begin{align*}
% \text{det}\Bigl(K - \eta I_n\Bigr)
% 	&= \lambda^{-n} \text{det}\Bigl(\lambda K + I_n - I_n - \lambda \eta I_n\Bigr) \\
% 	&= \lambda^{-n} \text{det}\Bigl(S_\lambda^{-1} - ( 1 + \lambda \eta ) I_n\Bigr)
% \end{align*}
% whence $1 + \lambda \eta$ must necessarily be an eigenvalue of $S_\lambda^{-1}$.
% Since the eigenvalues of the inverse are reciprocals of the original matrix, it
% therefore is true that $\frac{1}{1 + \lambda \eta}$ is an eigenvalue of $S_\lambda$.
% Note that the eigenvalues of $S_\lambda$ are non-negative decreasing functions
% of $\lambda$.

% Furthermore, if $A$ is Hermitian (symmetric), then it admits a diagonalized form
% with respect to an orthogonal basis of eigenvectors $V\in \Real^{n\times n}$:
% \[K = V \Lambda V'\]

% Consider the SVD of $K$: if the rank of $K$ is $r$ then there exist orthogonal
% matrices $U,V\in \Real^{n\times n}$ and a diagonal matrix $\Lambda$ of rank $r$
% such that
% \[K = U\Lambda V'\]

% Suppose there is a matrix $X$ with dimensions $n\times p$ with rank $r$.
% SVD decomposition claims that there exist square orthogonal matrices $U_{n\times n}$
% and $V_{p\times p}$ and a diagonal matrix $\Sigma_{n\times p}$ of rank $r$, such
% that $X = U \Sigma V'$:
% \[X = U \Bigl(\begin{smallmatrix} \Lambda & 0\\ 0 & 0 \end{smallmatrix}\Bigr) V'\]

% \item the eigenvalues of $K$ must be real and nonnegaitve, since $K$ is symmetric and ;

% Reproducing Hilbert spaces

% subsection smoothing_spline (end)

% section lecture_4 (end)

\clearpage
\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

\subsection{Smoothing splines (recap)} % (fold)
\label{sub:smoothing_splines_recap}

Recall that the following problem's optimal solution is in the family of piecewise
cubic polynomials from $C^2[a,b]$ with knots determined by the nature of the
interpolated data.
\[
\sum_{i=1}^n \bigl(t_i - g(x_i)\bigr)^2
+ \lambda \int |g''(s)|^2 ds
\to \min_{g\in C^2[a,b]}
\]
In the last lecture it was proven that the solution is given by the Natural Cubic
Spline, which effectively reduces the infinite dimensional problem to a finite
dimensional one:
\[(t-g)'(t-g) + \lambda g' K g\to \min_{g\in \Real^{n\times 1}}\]
where $(x_i, t_i)_{i=1}^n$ is the data to be interpolated, and $g$ is the vector
which enables unique identification of the NCS, $K$ is a symmetric and positive
semi-definite matrix, which depends on the knots only. The fitted $\hat{g}$ is
determined by the projection-like expression:
\[\hat{g} = S_\lambda t\]
where the smoother matrix $S_\lambda = ( I + \lambda K )^{-1}$ is positive
semi-definite and has eigenvalues $\mu_k = \frac{1}{1 + \lambda \eta_{n-j+1}}$,
where $(\eta_i)_{i=1}^n$ are the eigenvalues of $K$.

% Why are $\mu_1$ and $\mu_2$ equal to $1$?

%% See handwritten notes

What happens when the smoother is applied again to the smoothed output? In the
case of a linear regression we get the same output. However, re-applying the smoother
to $\hat{g}$ yields a more linear fit, since the first pair of eigenvalues are $1$
and correspond to linear maps in $C^2[a,b]$. Thus $S_\lambda$ preserves the linear
components, while diminishing the impact of other higher degree polynomial components.
Indeed,
\begin{align*}
	\hat{g} & = \sum_{k=1}^n \mu_k \langle\hat{g}, u_k\rangle u_k \\
	& = \sum_{k=1}^n \mu_k \Bigl\langle \sum_{j=1}^n \mu_j \langle g, u_j\rangle u_j, u_k\Bigr\rangle u_k \\
	& = \sum_{k=1}^n \sum_{j=1}^n \mu_j \mu_k \langle g, u_j\rangle \langle u_j, u_k\rangle u_k \\
	& = \sum_{k=1}^n \sum_{j=1}^n \mu_j \mu_k \langle g, u_j\rangle \delta_{kj} u_k \\
	& = \langle g, u_1\rangle u_1 + \langle g, u_2\rangle u_2
		+ \sum_{k=3}^n \mu_k^2 \langle g, u_k\rangle u_k
\end{align*}
and $\mu_1 = \mu_2 = 1$.

The projection matrix is idempotent, hence for any eigenvector-eigenvalue pair
$(\lambda, u)$ it is true that 
\[
\lambda u
= H u
= H^2 u
= H \lambda u
= \lambda H u
= \lambda^2 u
\]
whence $\lambda$ is either $\pm 1$ or $0$.

\subsubsection{Tutorial \# Problem 4} % (fold)
\label{ssub:tutorial_problem_4}

Consider the dataset $\bigl(x_i, t_i\bigr)_{i=1}^n$, with ties in $x_i$, to be 
fit by a curve $g$. The input data does not affect the penalty term, so one has
to deal with the goodness-of-fit term $\sum_{k=1}^n (t_k g_\theta(x_k))^2$
\[
\sum_{i=1}^n (t_i - g(x_i) )^2
= \sum_{k=1}^K \sum_{i=1}^{n_k} (t_{ki} - g(x_k) )^2
\]
where $K$ is the number of distinct $x_i$ and $t_{ki}$ is the $i$-th target value
in the $k$-th group with the same $x_k$. Next, focus on the inner sum:
\[\sum_{i=1}^{n_k} (t_{ki} - g(x_k) )^2\]
and note that for $\bar{t}_k = \frac{1}{n_k}\sum_{i=1}^{n_k} t_{ki}$
\[
(t_{ki} - \bar{t}_k + \bar{t}_k - g(x_k) )^2
= (t_{ki} - \bar{t}_k)^2
+ 2 (t_{ki} - \bar{t}_k)(\bar{t}_k - g(x_k) )
+ (\bar{t}_k - g(x_k) )^2
\]
Thus
\begin{align*}
	\sum_{i=1}^{n_k} (t_{ki} - g(x_k) )^2
	&= n_k (\bar{t}_k - g(x_k) )^2
	+ \sum_{i=1}^{n_k} (t_{ki} - \bar{t}_k)^2
	+ 2 (\bar{t}_k - g(x_k) ) \sum_{i=1}^{n_k} (t_{ki} - \bar{t}_k)\\
	&= n_k (\bar{t}_k - g(x_k) )^2 + \sum_{i=1}^{n_k} (t_{ki} - \bar{t}_k)^2
\end{align*}
since $\sum_{i=1}^{n_k} (t_{ki} - \bar{t}_k) = \sum_{i=1}^{n_k} t_{ki} - n_k \bar{t}_k = 0$.
This implies
\[
\sum_{i=1}^n (t_i - g(x_i) )^2
= \sum_{k=1}^K n_k (\bar{t}_k - g(x_k) )^2
+ \sum_{k=1}^K \sum_{i=1}^{n_k} (t_{ki} - \bar{t}_k)^2
\]
note that the last term is independent of $g$. Therefore the smoothing spline problem
for data with ties turns out to be equivalent to
\[
\min_{g\in C^2[a,b]} \Bigl\{
	\sum_{k=1}^K n_k (\bar{t}_k - g(x_k) )^2
	+ \lambda \int_a^b |g''(x)|^2 dx
\Bigr\}
\]
This is an instance of a generalization of the smoothing spline problem, which
essentially is the introduction of individual weighting of the data-points in the
dataset: for $(x_i, t_i)_{i=1}^n$ with no ties in $x_i$ and weights $(w_i)_{i=1}^n\geq 0$
\[
\sum_{i=1}^n w_i \bigl(t_i - g(x_i)\bigr)^2
+ \lambda \int_a^b |g''(s)|^2 ds
\to \min_{g\in C^2[a,b]}
\]
The solution to the weighted problem is again in the Natural Cubic spline family,
since we can consider an NCS with the same goodness-of-fit term, but with apriori
smaller penalty term (recall the proof that the natural cubic spline has the least
penalty in $C^[a,b]$). The solution in the weighted case is
\[\hat{g} = ( W + \lambda K )^{-1} W t\]

% subsubsection tutorial_problem_4 (end)

% subsection smoothing_splines_recap (end)

\subsection{Model selection} % (fold)
\label{sub:model_selection}

In this lecture we are going to consider four adjustments to the likelihood of
the model and data, which also incorporate the complexity of the model.

\subsubsection{Elements of information theory} % (fold)
\label{ssub:elements_of_information_theory}

Suppose $x$ is a discrete random variable. How much information does one get when
a realisation of $X$ is observed? This could be quantified by the amount of ``surprise'':
very likely event is not surprising, an unlikely occurrence -- very surprising.
Thus the measure of the amount of information $H$ should have the following properties:
\begin{description}
	\item[Monotonicity] $H$ is a monotonic functional of the probability measure
	function;
	\item[Independence] If events are independent or unrelated, then the information
	gained from both must be pooled together: in the case of densities this means that
	\[H(X,Y) = H(X)+H(Y)\]
	when $p_{X,Y}(x,y) = p_X(x) p_Y(y)$;
	\item[MISSING]
\end{description}
These ideas lead us to the notion of entropy -- the average amount of information.
For a discrete random variable
\[H(X) = \ex\brac{H(X)} = - \sum_x p(x) \log_2 p(x)\]
for a continuous:
\[H(X) = \int \log_2\frac{1}{p(x)} p(x) dx\]
in fact the base of the logarithm in not important, since it just determined the
units of measurement of entropy. In the context of entropy the logarithms are with
respect to to base $2$.

Suppose the unknown density $f$ is approximated (modelled) by $\hat{f}$.
The \textbf{Kullback-Leibler divergence} between $f$ and $\hat{f}$ is the additional
information needed to describe the random variable with density $f$ using $\hat{f}$:
\[
\text{KL}(f\|\hat{f})
= \int \frac{f}{\hat{f}} \log \frac{f}{\hat{f}} \hat{f} dx
= \int f \log \frac{1}{\hat{f}} dx - \int f \log \frac{1}{f} dx
\]
The divergence is a non-negative function of the densities. To show that, recall
the Jensen's inequality, which states that $\ex g(X) \geq g(\ex X)$ for any convex
function $g$. Since $x\to -\log x$ is convex, one has the following:
\begin{align*}
	\text{KL}(f\|\hat{f})
	&= \int f \log \frac{f}{\hat{f}} dx = \ex_f -\log \frac{\hat{f}}{f} \\
	&\geq -\log \ex_f \frac{\hat{f}}{f} = -\log \int f \frac{\hat{f}}{f} dx \\
	&= -\log 1 = 0
\end{align*}

% subsubsection elements_of_information_theory (end)

\subsubsection{Akaike (1973)} % (fold)
\label{ssub:akaike_1973}

\noindent First criterion of model selection -- the \textbf{A}kaike \textbf{I}nformation
\textbf{C}riterion.

Suppose $(Y_i)_{i=1}^n\sim Y$ is some sample of independent and identically
distributed random variables generated according a law with an unknown density
$g(y|\theta_0)$. Let there be a collection of parametric families of models
\[ M_k = \bigl\{\bigr. f(y|\theta) \bigl.\bigr\rvert \theta\in \Theta_k \bigl.\bigr\} \]
where $\Theta_k\subseteq \Real^k$ -- the $k$-dimensional parametric family.


% subsubsection akaike_1973 (end)

Let $\hat{\theta}_k$ be the MLE estimator of the parameter in $\theta\in \Theta_k$
and $\hat{f}_k = f(Y|\theta_k)$. Assume that the models are nested, and distinguished
by their dimension $k$.
Is $\Theta_k\subseteq \Theta_{k+1}$?

How far is $\hat{f}_k$ from $g$? The idea is to look at the Kullback-Leibler divergence
between $g$ and $f_k$, and choose the $f_k$ with the smallest distance:
\[\text{KL}(g\|f_k) = \int g \log g dY - \int g \log f_k dY\]
is the sum of the entropy inherent in $Y$ and the \textbf{cross entropy}.

Minimizing this function with respect to $f_k$ is equivalent to minimizing the
Kullback-Leibler \textbf{discrepancy}:
\[
d( \theta_k )
= - 2 \int g \log f_k dY
= - 2 \ex_g \log f_k
\]
But instead let's look at
\[
d( \hat{\theta}_k )
= - 2 \Bigl.\bigl(\ex_g \log f(Y|\theta) \bigr)\Bigr\rvert_{\theta = \hat{\theta}_k(Y)}
\]

The main insight suggested by Akaike, was to overcome the fact that $g$ is unknown
by studying the log likelihood as a biased estimator (proxy) for $d(\hat{\theta}_k)$.
The law of large numbers gives:
\[
% \frac{1}{n} \sum_{i=1}^n \log f(Y_i|\hat{\theta}_k)
\frac{1}{n} \sum_{i=1}^n \log f(Y_i|\theta)\Big\rvert_{\theta = \hat{\theta}_k(Y)}
\overset{a.s}{\to}
% \ex_g \log f(Y|\theta)
\ex_g \log f(Y|\theta)\Big\rvert_{\theta = \hat{\theta}_k(Y)}
\]
Therefore
\[ - 2 L\bigl(Y|\hat{\theta}_k\bigr) \approx d(\hat{\theta}_k) \]
-- a clear indication that Akaike has something great in mind.

In order to compute the bias, it is necessary to study the average Kullback-Leibler
distance with respect to the distribution of the MLE estimate $\hat{\theta}_k$
\[
\ex_{\hat{\theta}_k} d(\hat{\theta}_k)
- \ex_g\bigl(- \log f(Y|\theta = \hat{\theta}_k(Y)) \bigr)
\]

The AIC is up to the terms of the first order given by:
\[\text{AIC} = - 2 \log \hat{f}(Y|\hat{\theta}_k(Y)) + 2k \]

% When selecting the optimal $\hat{\theta}_k$ 
\textbf{ See the handwritten notes! }

\begin{align*}
	\ex d(\hat{\theta}_k)
	&= \ex\bigl( -2\log f(Y|\hat{\theta}_k(Y)) \bigr)\\
	&+ \Bigl\{ \ex\bigl( -2\log f(Y|\theta_0) \bigr)
		- \ex\bigl( -2\log f(Y|\hat{\theta}_k(Y)) \bigr) \Bigr\}\\
	&+ \Bigl\{ \ex d(\hat{\theta}_k)
		- \ex\bigl( -2\log f(Y|\theta_0) \bigr) \Bigr\}
	&\approx \ex\bigl( -2\log f(Y|\hat{\theta}_k(Y)) \bigr) + k + k
\end{align*}

\noindent \textbf{The first term}\hfill\\
Let $\phi(\theta) = \log f(Y|\theta)$. Since the models are nested and the true
model is included within the MLE estimator $\theta_k \to \theta_0$ as the sample
size grows ($n\to \infty$). The multivariate Taylor expansion of $\phi$ around
the MLE estimate $\hat{\theta}_k$ yields
\[
  \phi(\theta_0)
= \phi(\theta_k)
+ \nabla \phi' \rvert_{\hat{\theta}_k} (\theta_0 - \hat{\theta}_k)
+ \frac{1}{2} (\theta_0 - \hat{\theta}_k)' \nabla^2\phi \rvert_{\hat{\theta}_k} (\theta_0 - \hat{\theta}_k)
+ o(1)
\]
where $\nabla \phi(\theta) = \Bigl( \frac{\partial}{\partial \theta_j} \phi \Bigr)_{j=1}^k\rvert_\theta$
is a $k\times 1$ gradient vector of $\phi$, and $\nabla^2\phi$ is the Hessian matrix
of second partial derivatives of $\phi$. Since $\phi$ is log-likelihood at a given
parameter $\theta$, the Hessian at $\hat{\theta}_k$ coincides with the \textbf{observed
Fisher information matrix}:
\[
I(\hat{\theta}_k;Y)
= - \Bigl( \frac{\partial^2 f(Y|\hat{\theta}_k) }{\partial \theta_{ki}\partial \theta_{kj}} \Bigr)_{ij}
= - \nabla^2\phi \rvert_{\hat{\theta}_k}
\]
Furthermore, since $\hat{\theta}_k$ is the MLE the gradient of $\phi$ at this point
must be zero: $\nabla \phi' \rvert_{\hat{\theta}_k} = 0$. Therefore
\[
\phi(\theta_0) - \phi(\theta_k)
=
- \frac{1}{2} (\theta_0 - \hat{\theta}_k)' I( \hat{\theta}_k; Y ) (\theta_0 - \hat{\theta}_k)
+ o(1)
\]
and
\[
-2 \ex \bigl( \phi(\theta_0) - \phi(\theta_k) \bigr)
\approx
\ex (\theta_0 - \hat{\theta}_k)' I( \hat{\theta}_k; Y ) (\theta_0 - \hat{\theta}_k)
\]

\noindent \textbf{The second term}\hfill\\
For this analysis put $\phi(\theta) = \bigl( \ex f(Y|\theta) \bigr)_{\theta}$.
Then the Fisher information matrix is given by 
\[I(\theta_0) = \ex - \nabla^2 f(Y|\theta_0)\]
Another application of the Taylor expansion yields
\[
  \phi(\theta_0)
= \phi(\theta_k)
+ \nabla \phi' \rvert_{\hat{\theta}_k} (\theta_0 - \hat{\theta}_k)
+ \frac{1}{2} (\theta_0 - \hat{\theta}_k)' \nabla^2\phi \rvert_{\hat{\theta}_k} (\theta_0 - \hat{\theta}_k)
+ o(1)
\]

From scratch. Consider $Y^n = (Y_i)_{i=1}^n \sim g$ a sample of independent and
identically distributed random variables, where $g = f(\cdot|\theta_0)$. Consider
a parametric family of densities
\[\Mcal_k = \Bigl\{ f(\cdot|\theta)\Bigr.\Bigl\rvert \theta\in \Theta\subseteq \Real^k \Bigr\}\]
with $g\in \Mcal_k$ (i.e. $\theta_0\in\Theta$). The overall goal is to consider
a collection of nested families of models $\Mcal_k$ for various $k$ and select
the most 

Define the log-likelihood of the observed sample under parameter $\theta$
\[
l(Y^n;\theta)
= \log \Lcal(Y^n;\theta)
= \log \prod_{i=1}^n f(Y_i;\theta)
= \sum_{i=1}^n \log f(Y_i; \theta)
\]
The MLE of $\theta$ is given by
\[\hat{\theta}_n = \argmax_{\theta\in \Theta} \Lcal(Y^n;\theta)\]

Note that the likelihood is in fact a multivariate density of the sample $Y^n$.
This observation suggests one to study the Kullback-Leibler divergence of the MLE
of the product-density from the true theoretical likelihood (at $\theta_0$):
\[
\text{KL}( \Lcal(Y^n;\theta_0) \| \Lcal(Y^n;\hat{\theta}_n) )
= \ex_0 l(Y^n;\theta_0) - \ex_0 l(Y^n;\hat{\theta}_n)
\]
Since the first term does not depend on $\hat{\theta}_n$

% subsection model_selection (end)


Let $Y^n = (Y_i)_{i=1}^n\sim g = f(y|\theta_0)$ is an iid sample, and 
\[
M_k = \bigl\{f(\cdot|\theta)\rvert \theta\in \Theta_k\bigr\}
\]
with $\Theta_k \subseteq \Theta_{k+1}$.

Consider the likelihoood 
\[\Lcal(Y^n;\theta) = \prod_{k=1}^n f(Y_k\rvert \theta) \]


Consider another independent sample $Z^n$ independent from $Y^n$ but otherwise
identical to $Y^n$. Then
\[
d_n(\theta)
= - 2 \ex_g \log \Lcal(Z^n\rvert \theta)
= - 2 \sum_{k=1}^n \ex_g \log f(Z_k\rvert \theta)
\]
with the expectation taken over $Z^n\sim g$. Similarly
\[ d_n(\hat{\theta}_k(Y^n)) = \ex_{Z^n} \bigl( - 2 \log f(Z^n\rvert \hat{\theta}_k(Y^n))\bigr) \]
whence
\begin{align*}
	d_n(\hat{\theta}_k(Y^n))
	&= \ex_{Y^n}\bigl(-2\log \Lcal(Y^n\rvert \hat{\theta}_k(Y^n))\bigr) \\
	&+ \ex_{Y^n}\bigl(-2\log \Lcal(Y^n\rvert \theta_0)\bigr) - \ex_{Y^n}\bigl(-2\log\Lcal(Y^n\rvert \hat{\theta}_k(Y^n))\bigr) \\
	&+ \ex_{Z^n}\bigl(-2\log \Lcal(Z^n\rvert \hat{\theta}_k(Y^n)) \bigr) - \ex_{Y^n}\bigl(-2\log \Lcal(Y^n\rvert \theta_0)\bigr)
\end{align*}
Taking expectation with respect to $Y^n\sim g$
\begin{align*}
	\ex_{Y^n} d_n(\hat{\theta}_k(Y^n))
	&= \ex_{Y^n}\bigl(-2\log \Lcal(Y^n\rvert \hat{\theta}_k(Y^n))\bigr) \\
	&+ \ex_{Y^n}\bigl(-2\log \Lcal(Y^n\rvert \theta_0)\bigr) - \ex_{Y^n}\bigl(-2\log\Lcal(Y^n\rvert \hat{\theta}_k(Y^n))\bigr) \\
	&+ \ex_{Y^n} \ex_{Z^n}\bigl(-2\log \Lcal(Z^n\rvert \hat{\theta}_k(Y^n)) \bigr) - \ex_{Y^n}\bigl(-2\log \Lcal(Y^n\rvert \theta_0)\bigr)
\end{align*}

Consider the log-likelihood evaluated at the MLE $\hat{\theta}_k(Y^n)$.
\[
\frac{1}{n} \log \Lcal(Y^n\rvert \hat{\theta}_k(Y^n))\bigr)
= \frac{1}{n} \sum_{k=1}^n \log f(Y_k\rvert \hat{\theta}_k(Y^n) )
\]

Since the models are nested and the true model is contained within each $\Theta_k$,
the MLE $\hat{\theta}_k(Y^n)$ converges in probability to $\theta_0$ as the sample
size grows. Therefore if $\phi$ is twice differentiable around $\theta_0$, then 
\[
\phi(\hat{\theta}_k)
= \phi(\theta_0)
+ \nabla \phi (\hat{\theta}_k - \theta_0)
+ \frac{1}{2} (\hat{\theta}_k - \theta_0)' \nabla^2 \phi (\hat{\theta}_k - \theta_0)
+ o_P(1)
\]


% section lecture_5 (end)

\clearpage
\section{Lecture \# 6} % (fold)
\label{sec:lecture_6}

Akaike criterion combines parameter estimation and model selection

By looking at the Kullback-Leibler divergence between the estimated and the true

\[d(\theta_k) = - 2 \ex_g \ln f\brac{\induc{Y}\theta_k}\]

$g(y\vert \theta_0)$

\[d(\hat{\theta}_k) = - 2 \induc{\ex \ln f\brac{\induc{Y}\theta_k}}_{\theta_k = \hat{\theta}_k(y)}\]

\[\ex_{\hat{\theta}_k} d(\hat{\theta}_k) = - 2 \induc{\ex_z \ex_Y \ln f\brac{\induc{Y}\theta_k}}_{\theta_k = \hat{\theta}_k(z)}\]

The idea is to use the sample twice (law of Large Numbers)

What is the bias here?

Law of Large numbers
\[-2 \ln f\brac{\induc{y}\hat{\theta}_k(y)} \to \ex \]

Modified (Corrected) AIC

Akaike's information criterion in which the contribution of the error term is computed.
This way it is possible to relax the assumption of large sample size $n$.

Look at what happens in the normal linear regression setting.

AIC is is more general, but the bias of the estimate can be really large
$\text{AIC}_c$ requires the model class assumption -- restrictive, but offers greater precision.

\subsection{Derivation} % (fold)
\label{sub:derivation}
Assumptions \begin{description}
	\item[the true model]\hfill\\
	\[y = \underset{n\times p_0}{X}\underset{p_0\times 1}{\beta_0} + \epsilon\]
	where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$
	\item[the candidate model] \hfill\\
	\[y = \underset{n\times p}{X}\underset{p\times 1}{\beta_0} + \epsilon\]
	where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$.
	There are $k=p+1$ parameters to estimate: $\theta_0 = \brac{\beta_0,\sigma_0^2}$ and $\theta_k = \brac{\beta_k,\sigma_k^2}$.
	\item[Nested models] assume that $0\leq p_0 \leq p$ and $\beta_0$ has $(p-p_0)$ zeros at the end. The models are nested $f\brac{\induc{y}\theta_0}\in \Mcal_k$
\end{description}

The LS estimate of $\Mcal_k$:\begin{align*}
	\hat{\beta}_k &= \brac{X'X}^{-1} X'Y\\
	\hat{\sigma}^2_k &= \frac{1}{n}\brac{Y-X\hat{\beta}_k}'\brac{Y-X\hat{\beta}_k}
\end{align*}


the log likelihood is 
\[\ln f\brac{\induc{y}\theta} = -\frac{n}{2} \ln 2\pi -\frac{n}{2}\ln \sigma^2_k - \frac{1}{2\sigma^2_k} \brac{Y-X\beta_k}'\brac{Y-X\beta_k}\]

For $\theta_k$ this can be simplified to 
\[\ln f\brac{\induc{y}\hat{\theta}_k(y)} = -\frac{n}{2} \ln 2\pi -\frac{n}{2}\ln \hat{\sigma}^2_k - \frac{1}{2\hat{\sigma}^2_k} n \hat{\sigma}^2_k \]
\[\ln f\brac{\induc{y}\hat{\theta}_k(y)} = -\frac{n}{2} \brac{ 1 + \ln 2\pi} -\frac{n}{2}\ln \hat{\sigma}^2_k\]

However
\[-2\ln f\brac{\induc{y}\theta_k} = n \ln 2\pi +n\ln \sigma^2_k + \frac{1}{\sigma^2_k} \brac{Y-X\beta_k}'\brac{Y-X\beta_k}\]

Since 
\begin{align*}
	\brac{Y-X\beta_k}'\brac{Y-X\beta_k} &= \brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k} + \\
		& + \brac{\beta_0 - \beta_k}'X'\epsilon + \epsilon'X\brac{\beta_0 - \beta_k}
		& + \epsilon'\epsilon
\end{align*}

Therefore 
\[\ex-2\ln f\brac{\induc{y}\theta_k} = n \ln 2\pi + n\ln \sigma^2_k + \frac{1}{\sigma^2_k} \brac{ \brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k} + n\sigma_0^2 }\]

Now 
\[d\brac{\hat{\theta}_k} = n \ln 2\pi + n\ln \hat{\sigma}^2_k + \frac{1}{\hat{\sigma}^2_k}\brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k} + \frac{n\sigma_0^2}{\hat{\sigma}^2_k}\]

taking expectations
\[\ex d\brac{\hat{\theta}_k} = n \ln 2\pi + n\ex \ln \hat{\sigma}^2_k + n^2\ex \frac{\sigma_0^2}{n\hat{\sigma}^2_k} + n \ex \frac{1}{n\hat{\sigma}^2_k}\brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k}\]

Since $\frac{n\hat{\sigma}^2_k}{\sigma_0^2} \sim \chi^2_{n-p}$ and 
\[\brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k}{\sigma_0^2} \sim \chi^2_p\]
and are independent

Hence
\[\ex d\brac{\hat{\theta}_k} = n \ln 2\pi + n\ex \ln \hat{\sigma}^2_k + \frac{n^2}{n-p-2} + n \ex \frac{1}{n\hat{\sigma}^2_k} \ex \brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k}\]
and thus 
\[\ex d\brac{\hat{\theta}_k} = n \ln 2\pi + n\ex \ln \hat{\sigma}^2_k + \frac{n^2}{n-p-2} + \frac{n p}{n-p-2} \]

Whence 
\[\ex d\brac{\hat{\theta}_k} = n \brac{1+\ln 2\pi} + n\ex \ln \hat{\sigma}^2_k + \frac{2n(p+1)}{n-p-2}\]
\[\ex d\brac{\hat{\theta}_k} = \ex -2 \ln f\brac{\induc{y}\hat{\theta}_k} + \frac{2n(p+1)}{n-p-2}\]

Since 
\[\frac{2n(p+1)}{n-p-2} = \frac{2nk}{n-k-1} = 2k + \frac{2k(k+1)}{n-k-1}\]

the original AIC is
\[\text{AIC} = -2 \ln f\brac{\induc{y}\hat{\theta}_k(y)} + 2k\]

The corrected AIC is 
\[\text{AIC}_c = \text{AIC} + \frac{2k(k+1)}{n-k-1}\]

% subsection derivation (end)

\subsection{Mallow's $C_p$} % (fold)
\label{sub:mallows_cp}

The $C_p$ stands for the conceptual predictive statistic in the context of the multivariate regression.

Assumptions \begin{description}
	\item[the true model]\hfill\\
	The true model is $y = \underset{n\times p_0}{X}\underset{p_0\times 1}{\beta_0} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma_0^2 \underset{n\times 1}{E}}$;
	\item[the candidate model] \hfill\\
	The candidate is $y = \underset{n\times p}{X}\underset{p\times 1}{\beta} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$.
	There are $k=p+1$ parameters to estimate: $\theta_0 = \brac{\beta_0,\sigma_0^2}$ and $\theta_k = \brac{\beta_k,\sigma_k^2}$. THe MLE of $\theta_k$ is $\hat{\theta}_k$.
	\item[the density] $f\brac{\induc{y}\hat{\theta}_k} = f_k$.
\end{description}

The idea is to look athow the RSS behaves

\[\RSS = \sum_{i=1}^n \brac{y_i - x_i \hat{\beta}}^2 = \brac{Y-X\hat{\beta}}'\brac{Y-X\hat{\beta}} = Y'\brac{E-H}Y\]
where $H$ is the projection matrix $X\brac{X'X}^{-1}X'$.

The expected value of the RSS is given by
\begin{align*}
	\ex Y'\brac{E-H}Y &= \ex\tr{Y'\brac{E-H}Y} \\
		&= \beta_0'X_0' \brac{E-H} X_0\beta_0 + \tr\brac{\brac{E-H} \sigma^2_0 I}\\
		&= \beta_0'X_0' \brac{E-H} X_0\beta_0 + (n-p)\sigma^2_0
\end{align*}

Show that if $z\sim \Dcal_m\brac{\mu,\Sigma}$ and $A$ is a non-random $m\times m$ matrix, then
\[\ex\brac{z'Az} = \mu'A\mu + \tr\brac{A\Sigma}\] 

Define the $C_p^0$ as
\[C_p^0 = \frac{\RSS}{\sigma^2_0} - n + 2p\]

The expectation is 
\[\ex C_p^0 = p + \frac{1}{\sigma^2_0} \beta_0'X_0' \brac{E-H} X_0\beta_0\]

If the true model has bee selected then $\ex C_p^0 = p$, otherwise $\ex C_p^0 > p$.

Put $C_p$ to $\frac{\RSS}{\hat{\sigma}^2} - n + 2p$, with the hope, that it leads to the same expectation of $C_p^0$.

But which estimator of the reference $\sigma^2$ to use? Use the smallest -- for the full model
\[\hat{\sigma}^2_*\frac{1}{n-p} \sum_{i=1}^n\brac{y_i - X_i\hat{\beta}}^2\]
where $p$ is the number of parameters in the full model.

% subsection mallows_cp (end)

\subsection{Comparison between the $C_p$ and $\text{AIC}_c$} % (fold)
\label{sub:comparison_between_cp_and_aicc}

These criteria are asymptotically equivalent.

The AIC for (the biased estimator)
\[\text{AIC} = - \ln f\brac{\induc{y}\hat{\theta}_k} + 2k 
	= n \ln \hat{\sigma}^2 + n\brac{1 + \ln 2\pi} + 2(p+1)\]

Return the model with the minimum values of the AIC. For a fixed $n$ it is the same a looking at 
\[n\ln \hat{\sigma}^2 - n\ln \hat{\sigma}^2_* + 2 p \]

But this minimization is equivalent to minimization of the following 
\[ n\ln \frac{\hat{\sigma}^2}{\hat{\sigma}^2_*} + 2p\]

For large $n$ the ratio of $\hat{\sigma}^2$ to $\hat{\sigma}^2_*$ is approximately $1$ due to consistency.
Hence it is possible to expand the logarithm around $1$: $\ln(1+x)\approx x + o(x)$.

Hence
\[\approx b\brac{\frac{\hat{\sigma}^2}{\hat{\sigma}^2_*} - 1} + 2p = C_p\]

\textbf{Exercise}\\
Prove the expected Gauss discrepancy
\[\ex C_p^0 = \ex \frac{1}{\sigma^2_0}\brac{X_0\beta_0 -X \beta}'\brac{X_0\beta_0 -X \beta}\]
Indeed, take a look at the Gauss discrepancy:
\begin{multline*}
	\brac{X_0\beta_0 -X \beta}'\brac{X_0\beta_0 -X \beta} = \\
	= \brac{X_0\beta_0 - X \brac{X'X}^{-1}X'X_0\beta_0 - X \brac{X'X}^{-1}X'\epsilon }'\brac{ \ldots }\\
	= \brac{ (I - H )X_0\beta_0 - H \epsilon }'\brac{ (I - H )X_0\beta_0 - H \epsilon }\\
	= \beta_0'X_0'(I-H)X_0\beta_0 - \epsilon'H(I-H)X_0\beta_0 - \beta_0'X_0'(I-H)H \epsilon + \epsilon'H\epsilon\\
	= \beta_0'X_0'(I-H)X_0\beta_0 + \epsilon'H\epsilon
\end{multline*}
Since $I-H$ and $H$ are projectors onto orthogonal subspaces of $\Real^n$.

Taking expectations yields
\[\ex\brac{\beta_0'X_0'(I-H)X_0\beta_0 + \epsilon'H\epsilon} = \beta_0'X_0'(I-H)X_0\beta_0 + \ex\brac{\epsilon'H\epsilon}\]
while at the same time due to linearity of $\ex$ and $\tr$:

\begin{align*}
\ex(\epsilon'H\epsilon) &= \tr\ex(\epsilon'H\epsilon) = \ex\tr(\epsilon'H\epsilon) \\
&= \ex\tr(H\epsilon\epsilon) = \tr(H\ex\epsilon\epsilon)
\end{align*}

% subsection comparison_between_cp_and_aicc (end)

\subsection{The Bayesian information criterion} % (fold)
\label{sub:the_bayesian_information_criterion}

Also known as the \textbf{B}ayesian \textbf{I}nformation \textbf{C}riterion introduced by Schwartz as the competitor of the AIC.

This looks at the problem from the Bayesian point of view.

Assumptions \begin{description}
	\item[Data] $\brac{y_k}_{k=1}^n$;
	\item[Parametric] The distribution function is fully specified by the vector of parameters $\theta_k\in \Theta_k$.
	\item[the true model]\hfill\\
	The true model is $y = \underset{n\times p_0}{X}\underset{p_0\times 1}{\beta_0} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma_0^2 \underset{n\times 1}{E}}$;
	\item[the candidate model] \hfill\\
	The candidate is $y = \underset{n\times p}{X}\underset{p\times 1}{\beta} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$.
	There are $k=p+1$ parameters to estimate: $\theta_0 = \brac{\beta_0,\sigma_0^2}$ and $\theta_k = \brac{\beta_k,\sigma_k^2}$. THe MLE of $\theta_k$ is $\hat{\theta}_k$.
	\item[the density] $f\brac{\induc{y}\hat{\theta}_k} = f_k$.
\end{description}

\begin{itemize}
	\item There is prior distribution $\pi_k$ on the classes of models $\Mcal_k$;
	\item Each model class has a prior distribution $G_k\brac{\theta_k}$ on the true parameter $\theta_k$;
	\item 
\end{itemize}
The bayes formula yields the following posterior distribution of $\Mcal_k$ given $\mathbf{y}$:
\[p\brac{\induc{\Mcal_k}\mathbf{y}} \propto p\brac{\induc{\mathbf{y}}\Mcal_k} \pi_k\]
up to the factor $\frac{1}{f(\mathbf{y})}$.

Also
\begin{multline*}
p\brac{\induc{\mathbf{y}}\Mcal_k}
= \int p\brac{\induc{\mathbf{y}, \theta_k}\Mcal_k} d\theta_k
= \int p\brac{\induc{\mathbf{y}}\theta_k, \Mcal_k} p\brac{\induc{\theta_k}\Mcal_k}d\theta_k\\
= \int \induc{L}_{\Mcal_k}(\theta_k;\mathbf{y}) g_k(\theta_k) d\theta_k
\end{multline*}

\subsection{Laplace approximation} % (fold)
\label{sub:laplace_approximation}

Idea is to find a Gaussian approximation to a probability density defined over a set of continuous variables.

Univariate case
\[p(z) = \frac{1}{N_z}f(z)\]
where $N_z$ is the normalizing constant, and $f(z)$ is unimodal.
The idea is to approximate $f(z)$ with gaussians $q(z)$.

Consider the Taylor expansion of the logarithm of $f(z)$ around its mode $z_0$, given by $f'(z_0) = 0$.
\[\ln f(z) \approx \ln f(z_0) + \induc{\frac{f'(z)}{f(z)}}_{z=z_0} (z-z_0) - \frac{1}{2} \induc{\frac{d^2}{dz^2} \ln f(z)}_{z=z_0} \brac{z-z_0}^2 \]
the reminder term decays to zero faster than $\brac{z-z_0}^2$. Thus $f(z)\approx e^{ -\frac{1}{2} A {(z-z_0)}^2 }$.

Whence around $z_0$:
\[q(z) = \brac{\frac{A}{2\pi}}^\frac{1}{2}e^{ -\frac{1}{2} A {(z-z_0)}^2 }\]

In the multidimensional case for $A = \induc{-\frac{d^2}{dz'dz}\ln f(z)}_{z=z_0}$
\[q(\mathbf{z}) = \frac{\abs{A}^\frac{1}{2}}{{(2\pi)}^\frac{1}{2}} e^{-\frac{1}{2} (z-z_0)'A(z-z_0)}\]

Therefore
\[N_z = \int f(z) dz = f(z_0) \frac{{(2\pi)}^\frac{1}{2}}{\abs{A}^\frac{1}{2}}\]

% subsection laplace_approximation (end)

The integrand
\[f(\theta_k) = p\brac{\induc{y}\theta_k}g_k(\theta_k)\]

Using the Laplace approximation we can treat the integrand as a normal density.

Let's compute the Laplace approximation.

Let $\theta_k^* = \argmax_{\theta_k\in \Theta_k}\,f(\theta_k)$ and put $A_{\theta_k^*}$ as
\[\induc{- \frac{d^2}{d\theta_kd\theta_k'} \ln f(\theta_k)}_{\theta_k=\theta_k^*}\]

Could be ignore the contribution coming from $g_k$?

The log of
\[\ln p\brac{\induc{\mathbf{y}}\theta_k} = \sum_{i=1}^n p\brac{\induc{y_i}\theta_k}\]
which hints at the fact that $P\brac{\induc{\mathbf{y}}\theta_k}$ dominates the $\ln g_k(\theta_k)$.

Thus we can take $\theta_k^* = \hat{\theta}_k$ -- the MLE and the observed Fisher information matrix
\[I_{\hat{\theta}_k} = \induc{- \frac{\partial^2}{\partial\theta_k \partial\theta_k'}\ln p\brac{\induc{\mathbf{y}} \theta_k}} \]

In fact it is possible to have better Laplace approximations of $\int e^{n\phi(z)}dz = \text{approx} + o(n^{-\frac{1}{2}})$.

The posterior distribution is given by 
\[p\brac{\induc{\mathbf{y}}\Mcal_k} = \int f(\theta_k) d \theta_k = \int p\brac{\induc{\mathbf{y}}\theta_k} g_k(\theta_k) d \theta_k \approx p\brac{\induc{\mathbf{y}}\hat{\theta}_k} g_k(\hat{\theta}_k) \frac{\brac{2\pi}^\frac{k}{2}}{\abs{I_{\hat{\theta}_k}}^\frac{1}{2}}\]
the exponent $\frac{k}{2}$ comes from the dimension of $\theta_k$.

Therefore
\[\ln p\brac{\induc{\mathbf{y}}\Mcal_k} = \ln f\brac{\induc{\mathbf{y}}\hat{\theta}_k} + \ln g_k(\theta_k) + \frac{k}{2} \ln 2 \pi - \frac{1}{2}\ln \abs{I_{\hat{\theta}_k}} \]

For independent observations the full information matrix is just the one-observation information matrix times the number of observations.
Thus 
\[\abs{I_{\hat{\theta}_k}} = n^k \abs{I_1}\]

Hence 
\[\ln p\brac{\induc{\mathbf{y}}\Mcal_k} = \ln f\brac{\induc{\mathbf{y}}\hat{\theta}_k} + \ln g_k(\theta_k) + \frac{k}{2} \ln 2 \pi - \frac{k}{2}\ln n - \ln \abs{I_1} \]

But the terms 
\[ + \ln g_k(\theta_k) + \frac{k}{2} \ln 2 \pi - \ln \abs{I_1}\]
are bound with regardless of the values of $n$ and moreover are independent of it!

Therefore for asymptotic model comparison we can use
\[\text{BIC} = -2\ln f\brac{\induc{\mathbf{y}}\hat{\theta}_k} + k\ln n \]

The BIC tends to favour simpler models, because the penalty term is larger as $n\to \infty$.
In Bayesian modelling one needs to accumulate larger evidence in order to make estimation.
Uncertainty comes from the estimation and from the uncertainty of the underlying parameter.


The BIC is consistent
If the true model is in $\obj{\Mcal_{k_1},\ldots,\Mcal_{k_L}}$ then the the minimal BIC pick the true model with probability 1 as $n$ gets larger.

Heuristics on the values of BIC $\text{BIC}-\text{BIC}_{\min}$
\begin{itemize}
	\item if $\in\clo{0,2}$ -- virtually no evidence against;
	\item if $\in\clo{2,6}$ -- the evidence id positive;
	\item if $\in\clo{6,10}$ -- the evidence is strong;
	\item if $\in\clop{10, +\infty}$ -- the evidence is very strong.
\end{itemize}

% Kass & Raffey (1995)

% subsection the_bayesian_information_criterion (end)

% section lecture_6 (end)

\clearpage
\section{Lecture \# 7} % (fold)
\label{sec:lecture_7}

The goal is to estimate the test error given the small sample we have.

\subsection{Validation techniques} % (fold)
\label{sub:validation_techniques}

Enough data : split the data into two samples and fit on one subsample and test (validate) in the other.

Hold out a set of points in the training set $\brac{x_{2k}}_{k=1}^{n_2}$ on which to test the error of the model.
On the remaining subsample $\brac{x_{1k}}_{k=1}^{n_1}$ fit the model. The splitting is done randomly.
However there are some shortcomings:
\begin{description}
	\item[Variability] \hfill\\
		Depends on which observations are put in both subsamples ;
	\item[Test error] \hfill\\
		The true error might be overestimated due to having less data to estimate the model.
\end{description}

\subsubsection{Leave one out cross validation} % (fold)
\label{ssub:leave_one_out_cross_validation}

The idea is instead of splitting the dataset in two subsets, we test the fit on one single observation.

Is the full training sample is $X=\brac{x_k}_{k=1}^n$, then for each $j=1,\ldots n$ fit the model on $X_{-j} = \brac{X_k}_{k=1,\neq j}^n$.
Let $\hat{T}_j$ be the prediction of the model fit on $X_{-j}$, then the \textbf{M}ean \textbf{S}quare \textbf{E}rror is 
\[\text{MSE} \defn \frac{1}{n}\ \sum_{j=1}^n \big(\hat{T}_j - T_j\big)^2 \]

\begin{description}
	\item[Pros] \hfill\\
		\begin{itemize}
			\item Very general and applicable in many settings;
			\item It is not variable at all -- yields the same results, since the splitting is deterministic;
			\item Compared to validation, the bias of the model error is reduced;
		\end{itemize}
	\item[Cons] \hfill\\
		\begin{itemize}
			\item The variance of the MSE is increased, since there is a substantial overlap in the subsamples;
			\item It inflates the computational complexity (not true for linear estimators -- a linear combination of the response variables);
			\[\text{MSE} = \frac{1}{n}\sum_{j=1}^n \Big(\frac{y_j - \hat{y}_j}{1-h_{jj}}\Big)^2\]
			where $h_{jj}$ is the $j$-th diagonal element of the hat matrix. It is also known as the \emph{leverage}.
			\item Unsuitable for non-independent data.
		\end{itemize}
\end{description}

Covariance function of the residuals $\epsilon = Y - H Y$ is given by 
\[\text{cov}\big(\hat{\epsilon}\big) = \ex\big( (I-H)YY'(I-H) \big) = \sigma^2 (I-H)\]

Thus $\Var(\hat{\epsilon}_j) = \sigma^2 (1-h_{jj})$.

And we arrive at the concept of normalised residuals
\[r_j = \frac{\hat{\epsilon}_j}{\sigma \sqrt{1-h_{jj}}}\]

Since $\sigma$ is usually unknown, the \emph{Studentized} residuals arise:
\[ \hat{t}_j = \frac{\hat{\epsilon}_j}{\hat{\sigma} \sqrt{1-h_{jj}}}\]

% subsubsection leave_one_out_cross_validation (end)

\subsubsection{$k$-fold cross validation} % (fold)
\label{ssub:_k_fold_cross_validation}

Divide the training sample into ``folds'' all of approximately equal size. Estimate the model and the MSE on each fold. The $k$-fold MSE is given by 
\[\text{MSE}_{(k)} = \frac{1}{k}\sum_{j=1}^k \text{MSE}_j \]
where $\text{MSE}_j$ is the MSE of the $j$-th fold with the model estimated on all the remaining data. The LOOCV is a special case of $k$-fold with $k=n$.

Which method and what values of $k$? Using $k=5,10$ is universally recognized.

The mean square error:
\begin{description}
	\item[$k=10$] \hfill\\
		Bias is reduced, with lower variance;
	\item[$k=n$] \hfill\\
		Bias is further reduced at the cost of higher variance.
\end{description}

% subsubsection _k_fold_cross_validation (end)

\subsection{Wrong ways to cross validate} % (fold)
\label{sub:wrong_ways_to_cross_validate}

There are $p=10^4$ predictors, and the task is to keep at most $100$. If the model includes a preprocessing step which finds the $100$ best predictors, then it must be incorporated in x-validation.

In multi step modelling cross validation must be applied to the entire sequence of the modelling steps. This way the we will get the right estimate of the test error (of picking the ``best'' $100$ predictors).

% subsection wrong_ways_to_cross_validate (end)

% subsection validation_techniques (end)

\subsection{Bootstrap} % (fold)
\label{sub:bootstrap}

It is also a resampling technique, where the data is drawn from the already observed data points. The idea is to artificially recreate new samples.

If the training set is $Z=\brac{z_k}_{k=1}^n$, then the bootstrap sample $Z^*_b$ is created by sampling with replacement from $Z$ (non-parametric bootstrap).

This allows to compute the accuracy of any statistic, which is derived from the sample, using the empirical distributions function.

If $S(Z)$ is computed directly form the original sample $Z$, then 
\[\var\big(S(Z^*)\big) = \frac{1}{B}\sum_{b=1}^B S(Z^*_b)\]

Bootstrap seems a bit magical.
% (Peter Hall. Edgeworth's view of the bootstrap). 

Cross validation is simple: fit the model on the bootstrap sample and then compute the MSE on those observations, which did not make it into the bootstrap sample.

The probability that an observation ends up in a bootstrap sample is 
\[\Pr\big(\text{obs}_i\in Z^*_b\big) = 1 - \big(1-\frac{1}{n}\big)^n\approx = 1-e^{-1}\]

The number of distinct observations in a bootstrap sample is $Y$. Then $Y = n (1-\frac{1}{n})$.

Thus the bootstrap cross-validation is approximately equal to the $3$-fold cross validation.
Since bootstrapping is computationally expensive, $k$-fold cross validation is preferred.




% Model selection with penalty of complexity
Estimate the test error directly from the data.
Bootstrap is used to estimate the accuracy of the estimates.



% subsection bootstrap (end)

Classification procedures

% section lecture_7 (end)

\clearpage
\section{Lecture \# 8} % (fold)
\label{sec:lecture_8}

SUpervised learning

an input $X$ and the output $T$ assuming the existence of a link between them. Up to now the case of continuous response $T$ variable has been considered, which led to regression methods.

In classification the value space of the response is finite or countable (a class, category or any discrete value).

Thus there is no special order between the distinct values of $T$. 
Thus the problem of classification arises :
construct a function $y(\cdot)$ of $X$ so that it predicts the value of the target variable $T$.

Training data $\big(x_k, t_k\big)_{k=1}^n$ 

Consider a simple case of a binary $T$ -- a binary classification problem and let $X$ be univariate.

For small $x$ the response is zero, for large -- $T=1$, and in the intermediate region the intervals with $T=0$ and $T=1$ overlap.

Minimizing the expectd sum of squares, the minimizer is the conditional expectation $\ex\big(\big. T\big.\big\rvert X\big.\big)$, which is equal to 
\[\Pr\big(\big. T = 1\big.\big\rvert X = \mathbf{x}\big.\big) = \mathbf{x}\beta\]
and the classification is done through comparison of the probability of $T=1$ versus $T=0$.

%% an estimated slop
In classification the problem is to assign a new class $c\in \Ccal$ to an observation, whereas in the regression setting the goal was to predict the response gicen the new observation.

\subsection{As little misclassification as possible} % (fold)
\label{sub:as_little_misclassification_as_possible}

Minimize the misclassification, by looking at the probability of a mistake:
\[\Pr\big(y(X)\neq T\big)\]
observe that this is a generative approach.

The loss function is
\begin{multline*}
	\Pr\big(\big. y(X) = C_1\big.\big\rvert X\in C_2\big.\big) + \Pr\big(\big. y(X) = C_2\big.\big\rvert X\in C_1\big.\big)\\
	= \int_{R_1} p(x, C_2) dx + \int_{R_2} p(x, C_1) dx \to \min_{R_i}
\end{multline*}

If $p(x, C_2) > p(x, C_1)$ then classify $x$ in $C_2$, otherwise in $C_1$.
This is known as the Bayes classifier, and it is the best possible classification in this setting.

Consider the conditional probabilities and the Bayesian inference rules:
\[\Pr\big(\big. X\in C_1\big.\big\rvert X\big.\big) \Pr( X ) > \Pr\big(\big. X\in C_2\big.\big\rvert X\big.\big) \Pr( X ) > \]

There are two possibilities \begin{itemize}
	\item a generative approach, which models the joint distribution : linear discriminative analysis;
	\item a discriminative approach, with a parametric form, to come up with tractable models;
\end{itemize}

% subsection as_little_misclassification_as_possible (end)

\subsection{Expected loss} % (fold)
\label{sub:expected_loss}

A different perspective: minimize the expected loss, which is suitable under various circumstances where the risk of misclassification of significant.

Introduce a cost of misclassification of a new $x\in C_k$ to a region $R_j$ : $L_{jk}$.
and minimize the expected loss:
\[\text{EL} = \sum_{jk} \int_{R_j} L_{jk} p(x, C_k) dx\]

Thus for each $x$ minimize 
\[\sum_{jk} L_{jk} p(x, C_k) = \sum_{jk} L_{jk} p\big(\big. C_k\big.\big\rvert x\big.\big) p(x)\]

Indeed a similar problem.
% subsection expected_loss (end)

\subsection{Discriminative approach} % (fold)
\label{sub:discriminative_approach}

There is a function $y(\cdot):\mathcal{X}\to \Ccal$ which maps $x$ directly to q decision region.

In the case of two classes, assign $x$ to $C_1$ if $y(x) \geq 0$.

This approach does not have probabilistic motivation, whereas the previous approaches are not.

There is a problem with higher number of classes when using discriminant functions.

\begin{description}
	\item[One-versus-all]\hfill\\
		Classify according to the rule $C_j$ and $\neg C_j$ to get different classification boundaries.
		Basically fits $k$ classifiers. It works in some cases, but may yield undefined regions;
	\item[One-on-one] \hfill\\
		Consider the classes in pairs $C_i$ and $C_j$, neglecting the others. This results in $i-j$ boundaries,
		but requires $\frac{k(k-1)}{2}$ classifiers to fit. Still this can produce undefined regions;
	\item[]
	Consider a single $K$-class discriminant given by $K$ linear functions $\brac{y_k(\cdot)}_{i=1}^K$.
	In the $\Real^d$ case these function look like 
	\[y_k(x) = \beta_{0k} + \beta_k'x\]
	Classify $x$ to class $C_k$ if $y_k(x) > y_j(x)$ for all $j\neq k$.
	The boundaries are given by $y_k(x) = y_j(x)$ which define hyperplanes in $\Real^d$:
	\[\big(\beta_j-\beta_k\big)'x + (\beta_{0j}-\beta_{0k}) = 0\]

	This partitions the space into convex decision regions. Indeed:
	If $x_1,x_2\in R_k$, then for any $\lambda\in \clo{0,1}$ consider $x = \lambda x_1 + (1-\lambda x_2)$.
	Then $y_k(x)$ is given by:
	\begin{align*}
		y)k(x) &= \beta_k'x +\beta_{0k} = \lambda\beta_k'x_1 + (1-\lambda)\beta_k'x_2 + \beta_{0k} \\
		& = \lambda\big(\beta_k'x_1 +\beta_{0k}\big) + (1-\lambda)\big(\beta_k'x_2 +\beta_{0k}\big) \\
		& = \lambda y_k(x_1) + (1-\lambda)y_k(x_2)
	\end{align*}
	Since $x_1, x_2\in R_k$ we have $y_k(x_m)>y_j(x_m)$ for all $j\neq k$ and $m=1,2$, whence
	$y_k(x) > y_j(x)$ for all $j\neq k$, which means that $R_k$ is convex. And thus single connected (?)...
\end{description}

% subsection discriminative_approach (end)

\subsection{Logistic regression} % (fold)
\label{sub:logistic_regression}

Suppose there are $K$ distinct categories.

This is a discriminative approach: model $\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big)$ with
\[\sum_{j=1}^K \Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = 1\]

The form of a logistic regression:
\[\ln\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = \beta_{0j} + \sum_{s=1}^p\beta_{js} x_s \]

Take the last category as the reference category: for $j=1,\ldots,K-1$
\[\ln\frac{\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big)}{\Pr\big(\big. T=K\big.\big\rvert X=x\big.\big)}
	= \beta_{0j} + \sum_{s=1}^p\beta_{js} x_s = \beta_j'x \]
where $x_0 = \mathbf{1}$.

The probability that $T=j$ given $x$ is
\[\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = \Pr\big(\big. T=K\big.\big\rvert X=x\big.\big) e^{\beta_j'x}\]

Using the constraint yields
\[1 = \Pr\big(\big. T=K\big.\big\rvert X=x\big.\big) \Big(1 + \sum_{i=1}^{K-1} e^{\beta_i'x}\Big)\]

whence 
\[\Pr\big(\big. T=K\big.\big\rvert X=x\big.\big) = \frac{1}{1 + \sum_{i=1}^{K-1} e^{\beta_i'x}}\]

and
\[\pi_j = \Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = \frac{e^{\beta_j'x}}{1 + \sum_{i=1}^{K-1} e^{\beta_i'x}}\]

\subsubsection{Interpretation} % (fold)
\label{ssub:interpretation}

Valid for the two class classification only.

What might the interpretation of the coefficients in $\beta_j$.

First note that the odds of classifying to $j$
\[\frac{\pi_j}{1-\pi_j} = e^{\beta_j'x}\]
which means that
\[\ln \frac{\pi_j}{1-\pi_j} = \beta_j'x\]
The log of the odds of the posterior behaves linearly.

The \textbf{logit} transform of $x\in \brac{0,1}$ is 
\[\ln\frac{x}{1-x}\]

An increase of one unit in $x_s$ multiplies the odds by $e^\beta_{js}$

% subsubsection interpretation (end)

\subsection{Estimation} % (fold)
\label{sub:estimation}
Consider the case of binary classification of the training sample $\big(x_k,t_k\big)_{k=1}^n$. What is the MLE of $\beta$?
Let $\theta = \brac{\beta_{sj}}_{s=0, j=1}^{p, K}$.

\begin{align*}
	l(\theta) &= - \ln \prod_{i=1}^n p\big(\big. t_i\big.\big\rvert x_i, \theta\big.\big) \\
	&= - \sum_{i=1}^n \ln p\big(\big. t_i\big.\big\rvert x_i, \theta\big.\big)
\end{align*}

Notation: $t_i$ has zero-one values, and $\sigma_i = \sigma\big(\beta'x_i\big)$ where $\sigma(a) = \frac{e^a}{1-e^a}$.

Note that 

Therefore the log-likelihood is 
\begin{align*}
	l(\theta) &= - \sum_{i=1}^n \ln \big( \sigma_i^{t_i} (1-\sigma_i)^{1-t_i} \big) \\
	&= - \sum_{i=1}^n t_i\ln \sigma_i + (1-t_i)\ln (1-\sigma_i)
\end{align*}

The first order conditions are given by
\begin{align*}
	\frac{\partial}{\partial \beta_j}\ln \sigma_i & = \frac{\partial}{\partial \beta_j}-\ln ( 1 + e^{-\beta'x_i} ) \\  
	& = \frac{\partial}{\partial \beta_j} -\ln ( 1 + e^{-\beta'x_i} ) = \frac{x_{ij}e^{-\beta'x_i}}{1 + e^{-\beta'x_i}} \\
	& = x_{ij} (1-\sigma_i)
	\frac{\partial}{\partial \beta_j}\ln (1-\sigma_i) & = - \sigma_i x_{ij}
\end{align*}
notice that $\ln(1-\sigma_i) = -\beta'x_i - \ln(1 + e^{-\beta'x_i})$

therefore
\[\frac{\partial}{\partial \beta_j} l(\theta) = -\sum_{i=1}^n \big( t_i x_{ij} (1-\sigma_i) - (1-t_i)\sigma_i x_{ij} \big) = \sum_{i=1}^n (\sigma_i-t_i)x_{ij} \]

In the matrix notation we have the following:
\begin{align*}
	\nabla_\beta l(\beta) = \underset{n\times p+1}{X}' (\underset{n\times 1}{\sigma} - \underset{n\times 1}{t})
\end{align*}

in appears to be non-linear in $\beta$

In order to employ numerical procedures, the second order conditions must be checked.
\begin{align*}
	\frac{\partial}{\partial \beta_i}\frac{\partial}{\partial \beta_j} l(\theta)
	&= \frac{\partial}{\partial \beta_i} \sum_{i=1}^n (\sigma_i-t_i)x_{ij} \\
	&= \sum_{i=1}^n x_{ij}\frac{\partial}{\partial \beta_i} \sigma_i \\
	&= \sum_{i=1}^n x_{ij} x_{ik} \sigma_i (1 - \sigma_i)
\end{align*}

Introduce a diagonal matrix $B = \text{diag}\big(\sigma_i(1-\sigma_i)\big)_{i=1}^n$.

Therefore
\[\nabla_\beta^2 l(\beta) = X'BX\]
which means that the Hessian is a positive semidefinite matrix.
implying that the optimal $\beta$ is the minimizer. The whole problem is that of convex optimization.

\subsubsection{Newton-Raphson} % (fold)
\label{ssub:newton_raphson}

The idea is to find a second order approximation of $f:\Real^d\to \Real$ in order to find its roots. As usual use the multivariate Taylor expansion
\[f(x)-f(a)  = \nabla f(a)' \big(x-a\big) + \frac{1}{2}\big(x-a\big)'\nabla^2 f(a) \big(x-a\big) + \ldots \]
Thus we approximate $f$ by 
\[q(x) = \frac{1}{2}x'\nabla^2 f(a) x + \alpha' x + \beta\]
where $\alpha = \nabla f(a) - \nabla^2 f(a)a$ and $\beta = \ldots$.

The main idea is to approximate a function near its minimum by a quadratic.

The minimum of $q$ is
\[\nabla q(x) = \nabla^2 f(a) x + \alpha = 0\]
whence 
\[ x = -\big(\nabla^2 f(a)\big)^{-1} \alpha \]
and 
\[ x^* = a - \big(\nabla^2 f(a)\big)^{-1} \nabla f(a) \]

Let's apply this idea to the log-likelihood.
\begin{enumerate}
	\item begin with some initial $\beta_0$;
	\item update $\beta\to \beta_{\text{new}}$ using
	\[\beta_{\text{old}} - \big(X'BX\big)^{-1} X'(\sigma - t) = \big(X'BX\big)^{-1} X'B \big( X \beta_{\text{old}} - B^{-1} (\sigma- t ) \big)\]
	the term in brackets is the adjusted response $Z = X \beta_{\text{old}} - B^{-1} (\sigma - t )$ and the $\sigma$ is evaluated at $\beta_{\text{old}}$;
	use the weighted least squares.
\end{enumerate}


% subsubsection newton_raphson (end)

\subsubsection{Probit regression} % (fold)
\label{ssub:probit_regression}

Motivation: there is a latent variable $t^* = \beta'x + \epsilon$ with $\epsilon\sim \Ncal(0,1)$ and the observed binary variable $T = 1_{T^*(x)>0}(x)$.
Now $\Pr\big(\beta'x+\epsilon > 0\big) = \Pr\big(\epsilon > -\beta'x\big) = 1-\Phi(-\beta'x) = \Phi(\beta'x)$


% subsubsection probit_regression (end)

% subsection estimation (end)

% subsection logistic_regression (end)

% section lecture_8 (end)

\clearpage
\section{Lecture \# 9} % (fold)
\label{sec:lecture_9}

Two approaches discriminative and generative.

Discriminative: modelling the posterior (conditional) distribution of the class given the variable; do not necessarily have probabilistic interpretation.
Generative: modelling the joint distribution of the class and the variable;
Direct modelling of the probabilities.
Linear discriminant (quadratic) analysis.
Discriminant functions.
The sample data $(X_k)_{k=1}^n\in \Real^d$ in latent classes $\Ccal$. Want to model the distribution of $X$, conditional on the class to which it belongs.
Once there is the likelihood, ``flip'' it to get the Bayesian posterior. The Bayesian classifier has the lowest error rate $\Pr(C_k\vert x)$.

The probability that 
\[\Pr(T = j\vert X = \mathcal{x}) \propto \Pr(X = \mathcal{x}\vert T = j) \Pr( T = j ) = f_j(x) \pi_j\]
To recover the normalising factor compute $N(x) = \sum_{k=1}^K f_j(x) \pi_j$.

We can model the likelihoods as well as the priors.

Choices for $f_j(x)$: \begin{itemize}
	\item Gaussian yields the LDA or the QDA (access to analytical expression);
	\item each data point comes from a mixture if gaussians gives rise to the MDA (only numerical EM algorithms);
	\item for a large number of predictors assume independence between the components of $X$ within each class (conditionally on the class the features are indepenednent) this results in the na\"ive bayes classifier.
\end{itemize}


\subsection{Linear discriminant analysis} % (fold)
\label{sub:linear_discriminant_analysis}

Each
\[f_j(x) = \frac{1}{\sqrt{2\pi}^p \lvert \Sigma_j\rvert^\frac{n}{2}} \text{exp}\bigg(-\frac{1}{2}(x-\mu_j)'\Sigma_j^{-1}(x-\mu_j)\bigg)\]

all classes have the same covariance matrix $\Sigma_j = \Sigma$.

compare the log of the ration of the probability
\[\log \frac{\Pr\big(T=k\vert X=x\big)}{\Pr\big(T=j\vert X=x\big)} \]
for the posteriors:
\[ = \log \frac{\pi_k}{\pi_j} + \log \frac{\Pr\big(X=x\vert T=k\big)}{\Pr\big(X=x\vert T=j\big)} = \log \frac{\pi_k}{\pi_j} + \log \frac{f_k(x)}{f_j(x)} \]
thus
\[ = \log \frac{\pi_k}{\pi_j} + \Big( -\frac{1}{2}(x-\mu_k)'\Sigma_k^{-1}(x-\mu_k\Big) + \frac{1}{2}(x-\mu_j)'\Sigma_j^{-1}(x-\mu_j) \Big) + \log\frac{\lvert \Sigma_j\rvert}{\lvert \Sigma_k\rvert} \]

using the covariance matrix constance
\[ = \log \frac{\pi_k}{\pi_j} + -\frac{1}{2}\big(\mu_k'\Sigma^{-1}\mu_k - \mu_j'\Sigma^{-1}\mu_j \big) + x'\Sigma^{-1}\big( \mu_k - \mu_j\big) \]
which is linear in $x$ -- the linear discriminant analysis. Define the discriminant function
\[\delta_k(x) = x'\Sigma^{-1}\mu_k - \frac{1}{2} \mu_k'\Sigma^{-1}\mu_k + \log \pi_k\]
Then  classify $x$ as $k$ if $\delta_k(x)>\delta_j$ for $k\neq k$.

Since we have a probabilistic model is is sensible to estimate the parameters using the MLE.

\subsection{The MLE} % (fold)
\label{sub:the_mle}

Summary:
we have got $K$ classes with probabilities $\big(\pi_j\big)_{j=1}^K$ subject to $\sum_{j=1}^K \pi_j = 1$, and a sample $\big(X,T\big)$.


The likelihood of a particular observation $x_i$
\[\prod_{j=1}^K \pi_j^{t_{ij}} f_j(x_i)^{t_{ij}}\]
where $t_{ij}$ is the indicator whether the $i$-th observation belongs to the class $j$ in the training sample.

Thus the complete likelihood of the observed data is
\[L = \prod_{i=1}^n \prod_{j=1}^K \pi_j^{t_{ij}} f_j(x_i)^{t_{ij}}\]
whence
\[\mathcal{L} = \sum_{i=1}^n \sum_{j=1}^K t_{ij} \log \pi_j + t_{ij} \log f_j(x_i)\]
Using teh fact that everything is gaussian and the $\pi_j$ are related:
\[\mathcal{L} = \sum_{i=1}^n t_{i1} \log \big( 1- \sum_{j=2}^k\pi_j \big) +  \sum_{i=1}^n \sum_{j=2}^K t_{ij} \log \pi_j +  \sum_{i=1}^n \sum_{j=1}^K t_{ij} \log f_j(x_i)\]
thus the likelihood is separable in terms of the prior probabilities and the parameters of the gaussians.
Computing the derivatives:
\begin{align*}
	\sum_{i=1}^n \frac{-t_{i1}}{\pi_1} +  \sum_{i=1}^n t_{ij} \frac{t_{i1}}{\pi_j} = 0\\
\end{align*}
whence for all $j\geq 2$:
\[\frac{\pi_j}{\pi_1} = \frac{\sum_{i=1}^n t_{ij}}{\sum_{i=1}^n t_{i1}}\]
thus
\[\hat{\pi_j} = \frac{\sum_{i=1}^n t_{ij}}{\sum_{i=1}^n \sum_{l=1}^K t_{il}}\]

The parameters of the gaussians: maximize with respect to $\mu_j$ and $\Sigma$. The only contribution is
\[\ldots + \sum_{i=1}^n \sum_{j=1}^K t_{ij} \log f_j(x_i) \]
this is equivalent to maximizing
\[ -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^K t_{ij} \big(x_i-\mu_j\big)'\Sigma^{-1}\big(x_i-\mu_j\big)\] 
Thus
\[\sum_{i=1}^n t_{ij} \big(x_i-\hat{\mu}_j\big)'\Sigma^{-1} = 0\]
whence
\[\sum_{i=1}^n t_{ij} x_i = \sum_{i=1}^n t_{ij} \hat{\mu}_j\]
which leads to
\[\hat{\mu}_j = \frac{\sum_{i=1}^n t_{ij} x_i }{\sum_{i=1}^n t_{ij}}\]
the average of the $x_i$ belonging to the class $j$.

And finally the $\Sigma$, which is more tricky.
Collect the terms that depend on $\Sigma$ which depend on it. We get this to minimize:
\[ \sum_{i=1}^n \sum_{j=1}^K t_{ij} (x_i-\mu_j)'\Sigma^{-1}(x_i-\mu_j) + t_{ij} \log \lvert\Sigma\rvert \]
since the trace of a scalar is equal to its trace and $\tr(ABC) = \tr(BCA)$:
\[\sum_{i=1}^n \sum_{j=1}^K t_{ij} \tr\big( \Sigma^{-1}(x_i-\mu_j)(x_i-\mu_j)'\big) + t_{ij} \log \lvert\Sigma\rvert\] 
define $S_j = \frac{1}{n_j}\sum_{i=1}^n(x_i-\mu_j)(x_i-\mu_j)'$ with $n_j=\sum_{i=1}^n t_{ij}$. Therefore
\[\sum_{j=1}^K n_j \tr\big( \Sigma^{-1} S_j \big) - n \log \lvert\Sigma^{-1}\rvert\] 

Now the following could be proved directly:
\[\frac{\partial}{\partial A}\tr(AB) = \frac{\partial}{\partial A}\tr(BA) = B'\]
and
\[\frac{\partial}{\partial A}\log \lvert A\rvert = (A^{-1})'\]
therefore $\sum_{j=1}^K n_j S_j - n \hat{\Sigma} = 0$.

Remarks: \begin{itemize}
	\item if $\Sigma\neq\Sigma_j$ then we have quadratic decision boundaries, and 
	\[\delta_k(x) = - \frac{1}{2} (x-\mu_k)'\Sigma^{-1}(x-\mu_k) + \log \pi_k - \frac{1}{2}\log\lvert\Sigma_k^{-1}\rvert\]
	\item to get quadratice decision boundaries with LDA enrich the feature space by including functions of the predictors;
	\item QDA (and LDA) do not scale well with the number of predictors: use the Na\"ive Bayes;
	The na\"ive Bayes conditional independence. The na\"ive part
	\[\Pr\big(X=x\vert T=j\big) = \prod_{m=1}^p \Pr\big(X_m=x_m\vert T=j\big)\]

	the classification rule is using the Bayes part (maximizing the posterior):
	\[\hat{j}_x = \argmax_{j=1,\ldots, K} \Pr\big(X=x\vert T=j\big) \Pr\big(T=j\big)\]
	This has nothing to do with Bayesian inference. Thus there are Bayesian Na\"ive Bayes classifiers :).
	\item The difference between the logistic regression and the LDA. In logistic we looked at
	the log-ratio of class likelihoods given the data. It also linear in $x$, but the regression
	does not take the structure of the predictors.
	LDA is less flexible:
	\[\delta_k(x) = x'\Sigma^{-1}\mu_k - \frac{1}{2} \mu_k'\Sigma^{-1}\mu_k + \log \pi_k\]
	whereas the logistic had $\beta_0 + x\beta$ -- more general.
	If the true $x$ are Gaussian then it is better to use the LDA.
\end{itemize}

Some bias, but low variance due to simple linear hyper-boundaries.


% subsection the_mle (end)

% subsection linear_discriminant_analysis (end)

\subsection{Mixture of gaussians discriminant analysis} % (fold)
\label{sub:mixture_of_gaussians_discriminant_analysis}

\subsubsection{the Expectation-Maximisation} % (fold)
\label{ssub:the_expectation_maximisation}

the Expectation Maximization algorithm (Dempster et al. (1977)) is used in the case of latent or missing values.

Suppose the there is a probabilistic model with observed variables $X$ or $X,T$ and a set of latent variables $Z$.
In the mixture of distributions the latent variable is the mixture component, from which the observation was drawn.

Gaussian mixture is a superposition of Gaussian distributions:
\[g(x) \sim \sum_{k=1}^K \pi_k \phi_k(x\vert \mu_k, \Sigma_k)\]
such that $\sum_{k=1}^K \pi_k = 1$. It allows multimodality and greater flexibility.
The $\pi_k$ are the mixing coefficients and are the prior probabilities that $x$ comes from $k$-th component.

For the MLE we would have to estimate $\theta=(\pi_k,\mu_k,\Sigma_k)_{k=1}^K$ with the complete log-likelihood
\[l(\theta) = \sum_{i=1}^n \log \big(\sum_{k=1}^K\pi_k \phi_k(x\vert \mu_k, \Sigma_k)\big)\]

A way to get rid of the sum-under-the-log is to add the hidden information (here the original component).
Other examples include \textbf{H}idden \textbf{M}arkov \textbf{M}odels and the \textbf{M}ixture \textbf{D}iscriminant \textbf{A}nalysis.

We need a joint distribution of $(X,Z)$ governed by some $\theta$
\[p(X, Z\vert \theta)\]
the goal is to maximize the likelihood function
\[p(X\vert \theta) = \int p(X, Z\vert \theta) dZ = \sum_{z} p(X, Z=z\vert \theta) \]
This is in general intractable.
Suppose $q(Z\vert X,\theta)$ is an arbitrary density of $Z$ and proceed backwards from the goal.
\[l(\theta) = \log p(x\vert \theta) = \log \sum_z q(z\vert x,\theta) \frac{p(x\vert z, \theta)}{q(z\vert x,\theta)} \]
due to concavity of log, the Jensen's inequality yields:
\[\ldots \geq \sum_z q(z\vert x,\theta) \log \frac{p(x\vert z, \theta)}{q(z\vert x,\theta)} = F(q,\theta) \]

To maximize the $l(\theta)$, first maximize the lower bound. The process is
\begin{description}
	\item[E-step] \[q^{(t+1)} = \argmax_q F(q,\theta^{(t)})\]
	\item[M-step] \[\theta^{(t+1)} = \argmax_\theta F(q^{(t)},\theta)\]
\end{description}
But first we need to initialize the $\theta^{(0)}$ and the EM converges only to a local maximum.
However even in this setting the problem is intractable. We may find a sufficient condition such that the lower bound becomes the equality.

The sufficient condition on $q$ is when for all $z$
\[\frac{p(x\vert z, \theta)}{q(z\vert x,\theta)} = \text{const}\]
whence $q(z\vert x,\theta) \propto p(x\vert z, \theta)$ and 
\[q(z\vert x,\theta) = \frac{p(x, z\vert \theta)}{\sum_z p(x\vert z, \theta)} = p(z\vert x, \theta)\]
which leads to 
\[q^{(t+1)}(z\vert x,\theta) = p(z\vert x, \theta^{(t)})\]

The $M$-step:
\[l(\theta) \geq F(q,\theta) = \sum_z q(z\vert x,\theta) \log p(x\vert z, \theta) - \sum_z q(z\vert x,\theta) \log q(z\vert x,\theta)\]
-- the entropy and the cross entropy of $q$.
Since $q(z\vert x,\theta) = q^{(t+1)}(z\vert x,\theta) = p(z\vert x, \theta^{(t)})$, we have
\[F(q,\theta) = Q(\theta, \theta^{(t)}) + H(q)\]
and
\[Q(\theta, \theta^{(t)}) = \sum_z p(z\vert x, \theta^{(t)} \log p(x\vert z, \theta)
= \ex_{p(z\vert x, \theta^{(t)})} \log p(z\vert x, \theta) \]

In summary: \begin{itemize}
	\item The $E$-step is the computation of $Q(\theta, \theta^{(t)})$;
	\item The $M$-step return (update) the $\theta^{(t+1)} = \argmax_\theta Q(\theta, \theta^{(t)})$;
\end{itemize}
If the density family of the classes is exponential, then it is easy to maximize and compute the $Q$ as well.

\subsubsection{Proof of convergence of EM} % (fold)
\label{ssub:proof_of_convergence_of_em}

Show that $\theta^{(t)}$ converges and $l(\theta^{(t)})$ does not decrease.
Look at the values of $l(\theta)$:
\[l(\theta^{(t)}) = \sum_{i=1}^n \sum_z p(z_i\vert x_i, \theta^{(t)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(z_i\vert x_i, \theta^{(t)})}\]
where we used the chosen good $q$
\[\geq \sum_{i=1}^n \sum_z q(z_i\vert x_i, \theta^{(t+1)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(x_i\vert z_i, \theta)}\]
thus
\[\geq \sum_{i=1}^n \sum_z p(z_i\vert x_i, \theta^{(t+1)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(x_i\vert z_i, \theta^{(t+1)})}\]
since $Q(\theta^{(t+1)}, \theta^{(t)}) \geq Q(\theta^{(t)}, \theta^{(t)})$ :
\[\geq \sum_{i=1}^n \sum_z p(z_i\vert x_i, \theta^{(t)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(x_i\vert z_i, \theta^{(t)})}\]
In fact $Q(\theta^{(t)}, \theta^{(t)}) = l(\theta^{(t)})$ and $Q(\theta, \theta^{(t)})$ is convex (quadratic in $\theta$).

% subsubsection proof_of_convergence_of_em (end)

% subsubsection the_expectation_maximisation (end)

% subsection mixture_of_gaussians_discriminant_analysis (end)

% section lecture_9 (end)

\clearpage
\section{Lecture \# 10} % (fold)
\label{sec:lecture_10}

The EM algorithm is in two steps: \begin{description}
	\item[E-step] $Q\brac{\theta, \theta^k} = \ex_{p(z\lvert x, \theta^k) p(x,Z\lvert \theta)}$.
	\item[M-step] $\theta^{k+1}=\argmax_\theta Q(\theta, \theta^k)$.
\end{description}

We need to show that $l(\theta^{k+1})\geq l(\theta^k)$. Two things. The LHS:
\[l(\theta^k) = \sum \sum p(z_i\lvert x_i,\theta^k) \log\frac{p(x_i,z_i\lvert\theta^k)}{p(z_i\lvert x_i,\theta^k)}\]
The RHS:
\[l(\theta^{k+1}) \geq \sum \sum p(z_i\lvert x_i,\theta^k) \log\frac{p(x_i,z_i\lvert\theta^{k+1})}{p(z_i\lvert x_i,\theta^k)}\]
whence 
\[\ldots = Q(\theta^{k+1}, \theta^k) + \text{entropy}\geq Q(\theta^k, \theta^k) + \text{entropy} = l(\theta^k)\]
since $\theta^{k+1} = \argmax_\theta Q(\theta, \theta^k)$ (?) CHECK THIS.

\subsection{A typical application of the EM} % (fold)
\label{sub:a_typical_application_of_the_em}

\subsubsection{The independent mixture problem} % (fold)
\label{ssub:the_independent_mixture_problem}
Consider a binomial response (typically exponential families permit easy maximization). 
Thus the $i$-th observation is $\text{Bi}(n_i, p_j)$ if it is coming from the $j$-th category.

To use the EM we need to consider the hidden variables as well. Suppose $u_j(i)$ is the unobserved
indicator of $i$-th observation coming from $j$-th category.

The $\pi_j$ is the prior probability of $j$-th category.

The complete likelihood is
\[l(\pi, p\lvert x, z) = \log \prod_i \pi_{z_i} p_{z_i}^{x_i}(1-p_{z_i})^{n_i-x_i}\]
in a more convenient form
\[l(\pi, p\lvert x, z) = \sum_i \log \prod_j \big(\pi_j p_j^{x_i}(1-p_j)^{n_i-x_i}\big)^{u_j(i)}\]
whence
\[l(\pi, p\lvert x, z) = \sum_i\sum_j u_j(i)\log \big(\pi_j p_j^{x_i}(1-p_j)^{n_i-x_i}\big)\]
and 
\[l(\pi, p\lvert x, z) = \sum_i \sum_j u_j(i)\log \pi_j + u_j(i)\log p_j^{x_i}(1-p_j)^{n_i-x_i}\]
leading to 
\[l(\pi, p\lvert x, z) = \sum_i \sum_j u_j(i)\log \pi_j + \sum_{i,j} u_j(i)\log p_j^{x_i}(1-p_j)^{n_i-x_i}\]

\textbf{The E-step}. Averaging out of the hidden variables:
\[Q(\theta, \theta^k) =  \ex_{p(z\lvert x, \theta^k) p(x,Z\lvert \theta)}\]
define $\hat{u}_j(i) = \ex u_j(i)$. It is given by 
\[\ex u_j(i) = \Pr\big(Z_i=j\,\lvert\,X=x_i\big)\]
Thus
\[\hat{u}_j(i) = \frac{\Pr\big(X=x_i\,\lvert\,Z_i=j\big)\Pr\big(Z_i=j\big)}{\Pr\big(X=x_i\big)}
= \frac{ \pi_j f(x_i\lvert n_i, p_j)}{\sum_c \pi_c f(x_i\lvert n_i, p_c)}\]

\textbf{The M-step}. Consider the likelihood at $\hat{u}_j(i)$:
\[l(\pi, p\lvert x, z) = \sum_i \sum_j \hat{u}_j(i)\log \pi_j + \sum_{i,j} \hat{u}_j(i)\log p_j^{x_i}(1-p_j)^{n_i-x_i}\]

Maximizing with respect to $\big(\pi_j\big)$ subject to constraint $\sum_j \pi_j = 1$. The First order conditions:
\[\frac{\partial}{\partial \pi_j} l(\cdot) = \sum_i \hat{u}_j(i)\frac{1}{\pi_j} - \lambda = 0\]
whence
\[\sum_i \hat{u}_j(i) = \lambda \pi_j\]
and the ratio for $j\neq c$
\[\frac{\sum_i \hat{u}_j(i)}{\sum_i \hat{u}_c(i)} = \frac{\pi_j}{\pi_c}\]
Therefore
\[\frac{\sum_i \sum_j \hat{u}_j(i)}{\sum_i \hat{u}_c(i)} = \frac{1}{\pi_c}\]
and 
\[\hat{\pi}_c = \frac{\sum_i \hat{u}_c(i)}{\sum_i \sum_j \hat{u}_j(i)} = \frac{1}{n}\sum_i \hat{u}_c(i)\]
since by construction $\sum_j \hat{u}_j(i)=1$.

Now with respect to $p_j$:
\[\frac{\partial}{\partial p_j} l(\cdot) = \sum_i \hat{u}_j(i) \Big(\frac{x_i}{p_j} - \frac{n_i-x_i}{1-p_j}\Big) = 0\]
whence after simplification:
\[\hat{p}_j = \frac{\sum_i \hat{u}_j(i) x_i }{\sum_i \hat{u}_j(i) n_i}\]

\textbf{Initialisation}. Taking the uniform approach to the probability $p_j$ leads to a disaster.
Indeed, the E-step yields:
\[\hat{u}_j(i) = \frac{ \pi_j C_{n_i}^{x_i} p_j^{x_i}(1-p_j)^{n_i - x_i}}{\sum_c \pi_c C_{n_i}^{x_i} p_c^{x_i}(1-p_c)^{n_i - x_i}}\]
whence $\hat{u}_j(i) = \frac{\pi_j}{\sum_c \pi_c}$.

The M-step gives the number of successes:
\[\hat{p}_j = \frac{\sum_i x_i }{\sum_i n_i}\]
and no update whatsoever
\[\hat{\pi}_j = \frac{1}{n}\sum_i \frac{\pi_j}{\sum_c \pi_c} = \frac{\pi_j}{\sum_c \pi_c}\]

% subsubsection the_independent_mixture_problem (end)

% subsection a_typical_application_of_the_em (end)

\subsection{The problem of Mixture Discriminant Analysis} % (fold)
\label{sub:the_problem_of_mixture_discriminant_analysis}

It is a generalization of LDA and the QDA (1996 by Hastie and Tibshirani) . 
Assumptions: \begin{itemize}
	\item $K$ classes with probabilities $\big(\pi_k\big)$ and categories;
	\item given a $k$-th class the observation $X$ is a mixture:
	\[f_k(x) = \sum_j^{n_k}\alpha_{kj} \phi(x\,\lvert\,\mu_{kj}, \Sigma)\]
	\item classes are known in the training data, but the mixture proportion as are latent;
	\item We estimate $\alpha_{kj}$, $\mu_{kj}$, $\pi_k$ and $\Sigma$ (the latter is estimated on the whole data);
	\item To classify use the mode of the posterior:
	\[P(X=x\lvert T=k) \Pr(T=k) = \pi_k \sum_j^{n_k} \hat{\alpha}_{kj} \phi(x\,\lvert\,\hat{\mu}_{kj}, \hat{\Sigma})\]
\end{itemize}

The log-likelihood is:
\[l(\pi, \alpha, \mu, \sigma\lvert x, t) = \sum_i^n \sum_{k=1}^K \log \pi_k^{t_i(k)} \big[f_k(x_i)\big]^{t_i(k)}\]
with $t_i(k)$ indicating if the $i$-th observation belongs to class $k$.

Introduce the latent variables for the mixture $u_{kj}(i)$ which indicates whether the $i$-th observation, which
came from $k$-th class was produced the $j$-th mixture component. Thus
\[f_k(x_i, u_{k\cdot}(i)) = \prod_j^{n_k} \big[ \alpha_{kj} \phi(x_i\,\lvert\,\mu_{kj}, \Sigma)\big]^{u_{kj}(i)}\]

therefore
\[l(\pi, \alpha, \mu, \sigma\lvert x, t)
= \sum_i^n \sum_{k=1}^K t_i(k) \log \pi_k + \sum_i^n \sum_{k=1}^K t_i(k) \log f_k(x_i)\]

Pluggin in the product form of the density, with latent variables yields:
\begin{align*}
l(\pi, \alpha, \mu, \sigma\lvert x, t)
&= \sum_i^n \sum_{k=1}^K t_i(k) \log \pi_k\\
&+ \sum_i^n \sum_{k=1}^K \sum_j^{n_k} t_i(k) u_{kj}(i) \log \alpha_{kj}\\
&+ \sum_i^n \sum_j^{n_k} u_{kj}(i) \log \phi(x_i\,\lvert\,\mu_{kj}, \Sigma)
\end{align*}

The E-step. The optimal prediction of the latent variables is
\[\hat{u}_{kj}(i) = \Pr(X=x_i\lvert Z_i=j, C_i = k) \Pr(Z_i=j\lvert C_i = k) \Pr(C_i = k)\]
whence the posterior probability of being in category $j$ of the class $k$:
\[\hat{u}_{kj}(i) = \frac{\alpha_{kj} \phi(x\,\lvert\,\mu_{kj}, \Sigma)}{\sum_{r=1}^{n_k} \alpha_{kr} \phi(x\,\lvert\,\mu_{kr}, \Sigma) }\]

the M-step. Computing the $\pi_k$ is mundane. Let's compute the $\alpha_{kj}$ and the parameters of the normal density.
First for every $k$ the sum $\sum_{j=1}^{n_k}\alpha_{kj} = 1$. The first derivative with respect to $\alpha_{kj}$, note the traces of Lagrangian multipliers:
\[ \sum_i^n t_i(k) u_{kj}(i) \frac{1}{\alpha_{kj}} = \lambda\]
whence 
\[\hat{\alpha}_{kj} = \sum_i^n t_i(k) u_{kj}(i) \frac{1}{\lambda} \]
and the constraint on $\alpha_{kj}$ yields
\[\sum_{j=1}^{n_k} \sum_i^n t_i(k) u_{kj}(i) = \lambda \]
therefore
\[\hat{\alpha}_{kj}
= \sum_i^n t_i(k) u_{kj}(i) \frac{1}{\sum_i^n t_i(k) \sum_{j=1}^{n_k} u_{kj}(i)}
= \sum_i^n t_i(k) u_{kj}(i) \frac{1}{ \sum_i^n t_i(k) } \]
because $\sum_{j=1}^{n_k} u_{kj}(i) = 1$ as any observation comes from just one mixture component
and $\hat{u}_{jk}(i)$ is the joint posterior probability of category $j$ for observation $i$ in class $k$.

The third term: with respect to $\mu_{jk}$
\[\sum_j^{n_k} u_{kj}(i) \log \phi(x_i\,\lvert\,\mu_{kj}, \Sigma)
= - \frac{1}{2} \sum_j^{n_k} u_{kj}(i) \big(x_i - \mu_{jk}\big)'\Sigma^{-1}\big(x_i - \mu_{jk}\big)\]
which, when differentiating with respect to $\mu_{kj}$, turns into
\[ \Sigma^{-1}\Big( - \frac{1}{2} \sum_j^{n_k} u_{kj}(i) \big(x_i - \mu_{jk}\big) \Big) = 0\]
since $\Sigma^{-1}$ is obviously non-singular:
\[ - \frac{1}{2} \sum_j^{n_k} u_{kj}(i) \big(x_i - \mu_{jk}\big) = 0\]
whence the expression for $\hat{\mu}_{kj}$ follow.

Estimating the $\Sigma$, using the trace:
\begin{multline*}
\frac{\partial }{\partial \Sigma} \sum_j^{n_k} u_{kj}(i) \log \phi(x_i\,\lvert\,\mu_{kj}, \Sigma) = \\
	- \frac{1}{2} \sum_i^n \sum_j^{n_k} u_{kj}(i) \big(x_i - \mu_{jk}\big)'\Sigma^{-1}\big(x_i - \mu_{jk}\big)
	- \frac{1}{2} \sum_i^n \sum_j^{n_k} u_{kj}(i) \log \lvert \Sigma\rvert
\end{multline*}
which yields the estimate of $\Sigma$.

% subsection the_problem_of_mixture_discriminant_analysis (end)

% section lecture_10 (end)

\clearpage
\section*{Tutorial prob \# 7} % (fold)
\label{sec:tutorial_prob_7}

The Gaussian mixture density is 
\[p(x) = \sum_{k=1}^K \pi_k \phi(x\lvert \mu_k, \Sigma_k)\]
where $\phi$ is the $\Ncal_d$ density, $\mu_k\in \Real^d$ is the mean and
$\Sigma_k\in\Real^{d\times d}$ -- a positive definite symmetric covariance matrix.

The log-likelihood of the model with unobserved variable $u_k(i)$ denoting
if the $i$-th observation came form the $k$-th mixture component.
\[
l(\pi, \mu, \Sigma\lvert x, u)
= \sum_{i=1}^N \sum_{k=1}^K u_k(i) \bigl(\log \pi_k + \log \phi(x\lvert \mu_k, \Sigma_k)\bigr)
\]

\textbf{E-step}: find the expectation of the posterior distribution of $u_k(i)$ given $\pi$, $\mu$ and $\Sigma$:
\[
\hat{u}_k(i)
= \pr(C_i=k\lvert X=x_i)
= \pr(X=x_i\lvert C_i=k)\frac{\pr(C_i=k)}{\pr(X=x_i)}
= \frac{\phi(x_i\lvert \mu_k, \Sigma_k) \pi_k}{\sum_{r=1}^K \phi(x_i\lvert \mu_r, \Sigma_r) \pi_r}
\]

\textbf{M-step}: maximize the likelihood with respect to $\pi$, $\mu$ and $\Sigma$ given the estimates
of the latent states $\hat{u}_k(i)$. Note that the likelihood is additively separable in $\pi$ and the $\mu$-$\Sigma$ group
\[
l(\pi, \mu, \Sigma\lvert x, \hat{u})
= \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) \log \pi_k + \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i)\log \phi(x_i\lvert \mu_k, \Sigma_k)
\]
whence maximisation with respect to $\pi$ can be done separately.

\subsection*{$\pi$} % (fold)
\label{sub:_pi_}

Construct the Lagrangian
\[\Lcal = \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) \log \pi_k - \lambda\bigl( \sum_{k=1}^K \pi_k - 1\bigr)\]
The first order condition is
\[\frac{\partial \Lcal}{\partial \pi_k} = \sum_{i=1}^N \hat{u}_k(i) \frac{1}{\pi_k} - \lambda = 0\]
from which it follows that
\[\sum_{i=1}^N \hat{u}_k(i) = \lambda \pi_k\]
since $\sum_{k=1}^K \pi_k - 1$, we must have
\[\sum_{k=1}^K \sum_{i=1}^N \hat{u}_k(i) = \lambda\]
Because $\sum_{k=1}^K \hat{u}_k(i) = 1$it must be true that $\lambda > 0$ which means that
\[
\hat{\pi}_k
= \frac{ \sum_{i=1}^N \hat{u}_k(i) }{ \sum_{k=1}^K \sum_{i=1}^N \hat{u}_k(i) }
= \frac{1}{n} \sum_{i=1}^N \hat{u}_k(i)
\]

% subsection* _pi_ (end)

\subsection*{$\mu$, $\Sigma$} % (fold)
\label{sub:_mu_sigma_}

Consider the right half of $l(\pi, \mu, \Sigma\lvert x, \hat{u})$:
\[\sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i)\log \phi(x_i\lvert \mu_k, \Sigma_k)\]
if fact since $\phi$ is multivariate Gaussian
\[
\phi(x\lvert \mu_k, \Sigma_k)
= \frac{1}{(2\pi)^\frac{d}{2} \lvert \Sigma_k\rvert^\frac{1}{2} } e^{ -\frac{1}{2} (x-\mu_k)' \Sigma_k^{-1} (x-\mu_k) }
\]
its log-density provides the following expression for the is partial log-likelihood
\[
\sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) \frac{1}{2} \log\lvert\Sigma_k^{-1}\rvert 
	- \frac{d}{2} \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) \log2\pi 
	- \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) (x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k)
\]

\subsubsection*{$\mu$} % (fold)
\label{ssub:_mu_}

The part of log-likelihood with $\mu$ is
\[
l(\pi, \mu, \Sigma\lvert x, \hat{u})
= \ldots - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) (x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k) + \ldots
\]
The FOC:
\[
\frac{\partial l}{\partial \mu_k} = - \frac{1}{2} \sum_{i=1}^N \hat{u}_k(i) \Sigma_k^{-1} (x_i-\mu_k) = 0
\]
By assumption $\Sigma_k$ is non-singular (otherwise how would have its inverse existed). Hence
\[\sum_{i=1}^N \hat{u}_k(i) (x_i-\mu_k) = 0\]
and
\[
\hat{\mu}_k = \frac{\sum_{i=1}^N \hat{u}_k(i) x_i }{\sum_{i=1}^N \hat{u}_k(i)}
\]

% subsubsection* _mu_ (end)

\subsubsection*{$\Sigma$} % (fold)
\label{ssub:_sigma_}

$\Sigma$: It is impractical to find the $\Sigma$ directly by brute force differentiation, at least
because the expression involving $\Sigma$ actually depends on its inverse. Thus let's peruse $\Sigma^{-1}$
\[
l(\pi, \mu, \Sigma\lvert x, \hat{u})
= \ldots + \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) \frac{1}{2} \log\lvert\Sigma_k^{-1}\rvert 
	- \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) (x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k)
\]

% subsubsection* _sigma_ (end)

\subsubsection*{Some matrix calculus} % (fold)
\label{ssub:some_matrix_calculus}

First, using Laplace determinant expansion theorem, it is very easy to show that
\[\frac{\partial \lvert A\rvert}{\partial A} = \lvert A\rvert \bigl(A^{-1}\bigr)'\]
Indeed, expanding by $i$-th row
\[
\frac{\partial \lvert A\rvert}{\partial A_{ij}}
= \frac{\partial }{\partial A_{ij}} \sum_{k=1}^n A_{ik} \tilde{A}_{ik}
= \tilde{A}_{ij}
\]
where $\tilde{A_{ik}}$ is the cofactor (\rus{алгебраическое дополнение}) of
the element $A_{ij}$ in the square matrix $A$, and no element of the cofactor
depends on $A_{ij}$, as it basically the $i$,$j$ minor of $A$.

Second, the derivative of a trace of a product $AB$ ($n\times m$,$m\times n$)
with respect to an element $A_{ij}$ is given by
\[
\frac{\partial \tr(AB)}{A_{ij}}
= \frac{\partial }{A_{ij}} \sum_{s=1}^m A_{is} B_{si}
= B_{si}
\]
whence $\frac{\partial \tr(AB)}{A} = B'$.

Finally, there is the cyclical property of traces: $\tr(ABC) = \tr(BCA) = \tr(CAB)$.

% subsubsection* some_matrix_calculus (end)

\subsubsection*{back to the problem} % (fold)
\label{ssub:back_to_the_problem}
Notice that the term $(x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k)$ in the log-likelihood
is in fact a scalar. This means that it is equal to its own trace. Using the cyclical
property one may get
\[
\tr \bigl((x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k)\bigr)
= \tr \bigl(\Sigma_k^{-1} (x_i-\mu_k) (x_i-\mu_k)'\bigr)
\]
Therefore
\[
\frac{\partial }{\partial \Sigma_k^{-1}} (x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k)
= \bigl((x_i-\mu_k) (x_i-\mu_k)'\bigr)'
\]

Using all these observations one gets
\[
\frac{\partial }{\partial \Sigma_k^{-1}} l(\pi, \mu, \Sigma\lvert x, \hat{u})
= \frac{1}{2} \sum_{i=1}^N \hat{u}_k(i) \Bigl(\bigl(\Sigma_k^{-1}\bigr)^{-1}\Bigr)'
- \frac{1}{2} \sum_{i=1}^N \hat{u}_k(i) (x_i-\mu_k) (x_i-\mu_k)' = 0
\]
Since $\Sigma_k$ is a symmetric non-singular matrix, this expression is equivalent to
\[
\Sigma_k \sum_{i=1}^N \hat{u}_k(i) = \sum_{i=1}^N \hat{u}_k(i) (x_i-\mu_k) (x_i-\mu_k)'
\]
whence
\[
\hat{\Sigma}_k = \frac{1}{\sum_{i=1}^N \hat{u}_k(i)} \sum_{i=1}^N \hat{u}_k(i) \Gamma_{ik}
\]
where $\Gamma_{ik} = (x_i-\mu_k) (x_i-\mu_k)'$ is an $d\times d$ matrix of
``intra-cluster covariance''.

% subsubsection* back_to_the_problem (end)

\subsubsection*{Altogether} % (fold)
\label{ssub:altogether}

The E-step gives:
\[
\hat{u}_k^{t+1}(i)
= \frac{\phi(x_i\lvert \hat{\mu}_k^t, \hat{\Sigma}_k^t) \hat{\pi}_k^t}
	{\sum_{r=1}^K \phi(x_i\lvert \hat{\mu}_r^t, \hat{\Sigma}_r^t) \hat{\pi}_r^t}
\]
the M-step:
\begin{align*}
	\hat{\pi}_k^{t+1}
		&= \frac{1}{n} \sum_{i=1}^N \hat{u}_k^{t+1}(i)\\
	\hat{\mu}_k^{t+1}
		&= \frac{1}{\sum_{i=1}^N \hat{u}_k^{t+1}(i)} \sum_{i=1}^N \hat{u}_k^{t+1}(i) x_i\\
	\hat{\Sigma}_k^{t+1}
		&= \frac{1}{\sum_{i=1}^N \hat{u}_k^{t+1}(i)} \sum_{i=1}^N \hat{u}_k^{t+1}(i) \Gamma_{ik}^{t+1}\\
\end{align*}
where $\Gamma_{ik}^{t+1} = (x_i-\mu_k^{t+1}) (x_i-\mu_k^{t+1})'$
% subsubsection* altogether (end)

% subsection* _mu_sigma_ (end)

\subsection*{$k$-means clustering} % (fold)
\label{sub:k_means_clustering}

Consider the minimization problem of the functional
\[ \Lcal = \sum_{i=1}^N \sum_{k=1}^K r_k(i) (x_i-\mu_k)' (x_i-\mu_k) \to \min_{r, \mu} \]
where $r_k(i) = 1_{C_i=k}$ the $0-1$ indicator if the $i$-th observation belongs
to the $k$-th class.

First, suppose the $\mu=\bigl(\mu_k\bigr)_{k=1}^k$ are fixed and it is necessary
to optimize with respect to $r_k(i)$ for some particular observation $i$. The
functional can be rewritten in the following way
\[\Lcal = \ldots + \sum_{k=1}^K r_k(i) (x_i-\mu_k)' (x_i-\mu_k) \to \min_{r, \mu} + \ldots \]
where the dotted parts are independent of $r_k(i)$ for \textbf{this} $i$
provided $\mu$ is fixed.
In order to minimize this partial expression one must set $r_k(i)=0$ for all $k$
but the one which corresponds to the smallest value of $(x_i-\mu_k)' (x_i-\mu_k)$.
Thus, in effect the $i$-th observation is assigned to a class to the centroid $\mu_k$
of which it is the nearest.

Second, given fixed values of $r_k(i)$ for all $k$ and $i$, the minimization of
$\Lcal$ with respect to $\mu_k$ is a trivial exercise in optimization:
\[\frac{\partial \Lcal}{\partial \mu_k} = 2 \sum_{i=1}^N r_k(i) (x_i-\mu_k)\]
whence
\[\mu_k = \frac{\sum_{i=1}^N r_k(i) x_i}{\sum_{i=1}^N r_k(i)}\]

In, conclusion, the two-step maximization procedure must be the following one \begin{description}
	\item[find] Set $r_k(i)=0$ for $k\neq k_i$ and $r_{k_i}(i)=1$ otherwise, where
	$k_i = \argmin_k (x_i-\mu_k)'(x_i-\mu_k)$;
	\item[update] Compute the new centroids with
	\[\mu_k = \frac{\sum_{i=1}^N r_k(i) x_i}{\sum_{i=1}^N r_k(i)}\]
\end{description}
Given $\mu^t = \bigl(\mu_k^t\bigr)_{k=1}^K$, $r_k^{t+1}(i)$ are computed,
and afterwards $\mu^t$ is updated to get $\mu_k^{t+1}$. The stopping criterion
is the stabilization of $r_k(i)$.

Actually one needs to show that this iterative procedure does not increase
the functional $\Lcal$ by the end of each iteration. Indeed, by construction
of $r^t$ and $\mu^t$
\[\Lcal(r^{t+1}, \mu^t) = \min_r \Lcal(r, \mu^t) \leq \Lcal(r^t, \mu^t)\]
and
\[\Lcal(r^{t+1}, \mu^{t+1}) = \min_\mu \Lcal(r^{t+1}, \mu) \leq \Lcal(r^{t+1}, \mu^t)\]
whence $\Lcal$ indeed decreases with the number of iterations $t$.

% subsection* k_means_clustering (end)

%% \subsection*{Equivalence} % (fold)
%% \label{ssub:equivalence}
%% 
%% Consider the Gaussian mixture model with restriction $\Sigma_k = \epsilon I_d$ for $\epsilon>0$.
%% Therefore $\lvert \Sigma_k \rvert = \epsilo ^d$ and the density becomes
%% \[
%% \phi(x\lvert \mu_k, \Sigma_k)
%% = (2\pi \epsilon)^{-\frac{d}{2}} e^{-\frac{1}{2\epsilon} (x-\mu_k)'(x-\mu_k)}
%% \]
%% 
%% Thus
%% \[
%% \hat{u}_k^{t+1}(i)
%% = \frac{ e^{-\frac{1}{2\epsilon} (x-\mu_k)'(x-\mu_k)} \hat{\pi}_k^t}
%% 	{\sum_{r=1}^K e^{-\frac{1}{2\epsilon} (x-\mu_r)'(x-\mu_r)} \hat{\pi}_r^t}
%% \]
%% 
%% \hat{\pi}_k^{t+1}
%% 	&= \frac{1}{n} \sum_{i=1}^N \hat{u}_k^{t+1}(i)
%% \hat{\mu}_k^{t+1}
%% 	&= \frac{1}{\sum_{i=1}^N \hat{u}_k^{t+1}(i)} \sum_{i=1}^N \hat{u}_k^{t+1}(i) x_i
%% \hat{\Sigma}_k^{t+1}
%% 	&= \frac{1}{\sum_{i=1}^N \hat{u}_k^{t+1}(i)} \sum_{i=1}^N \hat{u}_k^{t+1}(i) \Gamma_{ik}^{t+1}
%%% subsection* equivalence (end)

% section* tutorial_prob_7 (end)

\clearpage
\section*{Mid-term test review} % (fold)
\label{sec:mid_term_test_review}
Ridge regression protects from the poor estimation in the direction of the least variance.
% section* mid_term_test_review (end)

\section{Revision} % (fold)
\label{sec:revision}

\subsection{EM-algorithm} % (fold)
\label{sub:em_algorithm}

Consider a model with $X$ observable variables and $Z$ -- latent described by
the likelihood function $L(X,Z;\theta)$.

For all $\theta$ it is true that 
\[\log p(X\;\theta) = \log p(X,Z;\theta) - \log p(Z\lvert X;\theta)\]
Let $q$ be some distribution of the latent variables $Z$, then
\[ \ex_q\log p(X\;\theta) = \ex_q\log p(X,Z;\theta) - \ex_q \log q + \ex_q\log {q}{p(Z\lvert X;\theta)} \]
whence 
\begin{align*}
\ex_q\log p(X,Z;\theta)
	&= \log p(X\;\theta) + \ex_q \log q - \ex_q\log {q}{p(Z\lvert X;\theta)} \\
	&= - KL\bigl(q\lvert\rvert p(Z\lvert X;\theta)\bigr) + \ex_q \log q + \log p(X\;\theta) \,.
\end{align*}

% subsection em_algorithm (end)

% section revision (end)

\section{Lecture \# 11} % (fold)
\label{sec:lecture_11}

An overview of module 4:
\begin{itemize}
	\item Linear classifiers (Fisher approach, and perceptron);
	\item Decision trees, (bagginng, boosting: AdaBoost et c.);
	\item Support Vector Machine (convex optimization);
	\item Reproducing Kernel Hilbert Spaces (classification and regression);
	\item Neural Networks;
	\item (optional) Bayesian regression and classification.
\end{itemize}

\subsection{Classifiers} % (fold)
\label{sub:classifiers}

There are at least tow approaches to multi-class problems: one-versus-one or one-
versus-all. Both, however, may yield undefined regions.

\subsubsection{A reminder on hyperplanes} % (fold)
\label{ssub:a_reminder_on_hyperplanes}
Consider a linear space $V$ with $\dim(V)=p$ and an inner product $\langle\cdot,\cdot\rangle$.
A hyperplane is a flat affine subspace $H$ of $V$ of dimension $p-1$, defined as
\[
H = \bigl\{x\in V\,:\, \langle x - p_0, \beta\rangle = 0 \bigr\}\,,
\]
for some anchor point $p_0$ and normal vector $\beta\in V$. note that by setting
$\beta_0$ to $-\langle p_0,\beta \rangle$ one recovers a simpler equation:
\[ \beta_0 + \langle x, \beta\rangle\,, \]
and in the Euclidean space : $\beta_0 + \beta'x = 0$.

Let $\beta_* = \frac{\beta}{\|\beta\|}$. The signed distance of an element $x\in V$
to the hyperplane $H$ is given by
\[ d(x, H) = \frac{y(x)}{\|\beta\|}\,, \]
where $y(x) = \beta_0 + \langle x, \beta\rangle$.

Indeed, for any $x_1, x_2\in H$ bilinearity of the inner product implies that
\[ y(x_1) - y(x_2) = \langle x_2-x_1, \beta\rangle = 0\,. \]
Now, for any $x\in V$ by the subspace projection there necessarily exist unique
elements $x_\perp\in [\beta_*]^\perp$ and $x_* \in [\beta_*]$ such that $x = x_\perp + x_*$.
Note, that $x_* = r\beta_*$ for some $r$, and $\|x_\perp\| = \inf_{y\in[\beta_*]}\{\|x-y\|\}$.
Therefore
\[
y(x)
= \langle \beta, x \rangle
= \langle \beta, x_\perp \rangle + \langle \beta, r \beta_* \rangle
= r \frac{\|\beta\|^2}{ \|\beta\| }\,,
\]


Now, fix any $x_1\in H$ and note that for $x_2 = x_1 + x_\perp$, $x_2\in H$. Therefore
\[
y(x)
= \beta_0 + \langle \beta, x_2-x_1 \rangle + r \frac{\|\beta\|^2}{ \|\beta\| }
= \beta_0 + r \frac{\|\beta\|^2}{ \|\beta\| } \,,
\]
Now, since $H$ is an affine space, not a subspace per se as it has no $\mathbf{0}$,

Thus $\inf_{x\in H}\|y-x\| = r\|\beta\|$ (?).


Next, for any $y\in V$ there exist a decomposition
\[ y = x^\perp + x^*\]
where $x^*\in H$ and $x^\perp = r\beta^*$. Therefore
\[
\langle\beta,y\rangle
= \langle\beta,x^*\rangle + \langle\beta,r\beta^*\rangle
= 0 + r \frac{\|\beta\|^2}{\|\beta\|}
\]
Thus $\inf_{x\in H}\|y-x\| = r\|\beta\|$ (?).

% subsubsection a_reminder_on_hyperplanes (end)

\subsubsection{The leas sqaure apporach} % (fold)
\label{ssub:the_leas_sqaure_apporach}

The problem of classification is to choose a class among $(c_k)_{k=1}^K$ by using
$K$ linear discriminant functions: for $x\in \Real^d$ and $k=1,\ldots, K$
\[y_k(x) = \beta_{k0} + \beta_k'x\]
Classify by choosing the class with the largest $y_k(x)$.

Let $B$ be a matrix defiend as 
\begin{align*}
\tilde{B}' &= \begin{pmatrix}
	\beta_{10} 	& \beta_{11} 		& \beta_{12} 		& \cdots & \beta_{1d} \\
	\vdots 	& \vdots	& \vdots 	& \ddots & \vdots \\
	\beta_{K0} 	& \beta_{K1} 		& \beta_{K2} 		& \cdots & \beta_{Kd} \\
	\end{pmatrix}_{K\times (D+1)}
\end{align*}
Let $\tilde{\beta}_k' = (\beta_{k0}\, \beta_k)$ where $\beta_k = (\beta_{km})_{m=1}^d$.

Furthermore let $t$ be used in a 1-of-K notation $t = e_k$ vector $K\times 1$.

The training data (?)
\begin{align*}
T &= \begin{pmatrix}
	\beta_{10} 	& \beta_{11} 		& \beta_{12} 		& \cdots & \beta_{1d} \\
	\vdots 	& \vdots	& \vdots 	& \ddots & \vdots \\
	\beta_{K0} 	& \beta_{K1} 		& \beta_{K2} 		& \cdots & \beta_{Kd} \\
	\end{pmatrix}_{n\times K}
\end{align*}

\textbf{Check other's notes!}(?)
\begin{align*}
\tilde{X} &= \begin{pmatrix}
	\beta_{10} 	& \beta_{11} 		& \beta_{12} 		& \cdots & \beta_{1d} \\
	\vdots 	& \vdots	& \vdots 	& \ddots & \vdots \\
	\beta_{K0} 	& \beta_{K1} 		& \beta_{K2} 		& \cdots & \beta_{Kd} \\
	\end{pmatrix}_{n\times (D+1)}
\end{align*}

Consider a problem of discriminating between two classes. The goal is to match the $t$
to $y$ by choosing a proper set of coefficients $\tilde{B}$. We want to minimize the
error function
\[\text{error} = \frac{1}{2} \tr \bigl(\tilde{X}\tilde{B} - T\bigr)'\bigl(\tilde{X}\tilde{B} - T\bigr)\]
\textbf{see the picture}
the solution is given by
\[
\bigl(\tilde{X}\tilde{B} - T\bigr)
+ \bigl(\tilde{X}\tilde{B} - T\bigr)'
= 0
\]
whence 
\[\tilde{B} = (\tilde{X}'\tilde{X})^{-1}(\tilde{X}'T)\]

Least squares estimation in not robust. If there is a third class, on the same
side of the bi-class hyperplane, then the outlying class will be much heavily
penalized, whence the hyperplane in this case will fail to properly distinguish
the first two classes (the plane shifts towards outlier).

The LS solution has the following property: it preserves the linear constraints
on the target vector. If every observation in the training set satisfies $a't_i + b = 0$
for all $i=1,\ldots N$ then $a'y(x) + b = 0$ for all $x\in \Real^d$.

Proof:
The notation $\tilde{B}'\in \Real^{K\times (d+1)}$:
\[\tilde{B}' = \bigl(\beta_0 \rvert B'\bigr)\]
with $\beta_0\in \Real^{K\times 1}$ and $B\in \Real^{K\times K}$.
The contribution of the bias can hereafter be extracted from the product
\[\tilde{X}\tilde{B} = \one\beta_0' + \tilde{X}B\]

The LS estimate (the first step):
\begin{align*}
\text{error}
	&= \frac{1}{2} \tr\bigl\{
		\beta_0\one'\one\beta_0' + \beta_0\one'\tilde{X}B - \beta_0\one'T \\
	&+ B\tilde{X}'\one\beta_0' + B\tilde{X}'\tilde{X}B - B\tilde{X}'T
		- T'\one\beta_0' - T'\tilde{X}B + T'T
	\bigr\} \,,
\end{align*}
differentiating with respect to $\beta_0$ and equating to zero yields
\[
\frac{\partial}{\partial \beta_0} \text{error} 
= n\beta_0 + (XB-T)'\one
= 0
\]
which implies that 
\[
\beta_0 = (\one'\one)^{-1} (T-XB)'\one
\]
if $\bar{t} = (\one'\one)^{-1}T'\one$ and $\bar{x} = (\one'\one)^{-1}X'\one$ 
then
\[\beta_0 = \bar{t} - B'\bar{x}\]

Therefore
\[
\text{error}
= \frac{1}{2} \tr\bigl\{ (XB + \one( \bar{t} - B'\bar{x} )' )' \times ( \ldots ) \bigr\}
\]
Letting $\bar{T} = \one \bar{t}'$ and $\bar{X} = \one \bar{x}'$ gives
\[
\text{error}
= \frac{1}{2} \tr\Bigl\{ \bigl( (X-\bar{X}) B - [T-\bar{T}] \bigr)' \times (\ldots) \Bigr\}
\]
whence finally the target criterion becomes
\[
\text{error}
= \frac{1}{2} \tr\Bigl\{ ( \hat{X} B - \hat{T} )' \times ( \hat{X} B - \hat{T} ) \Bigr\}
\]
which implies that 
\[B = (\hat{X}'\hat{X})^{-1}\hat{X}'\hat{T}\]

The second step: apply the linear constraints to $\bar{t}$.
\[
a'\bar{t} + b
= a'\frac{1}{N}T'\one + b
= \frac{1}{N} a'T'\one + b
= -\frac{1}{N} b\one' \one + b
\]
since $a'T' = -b\one'$ by the constraints.

The last step: compute the $y(x)$.
IT sis given by
\[
y(x^*)
= \beta_0 + B'x^*
= \bar{t} - B'\bar{x} + B'x^*
= \bar{t} + B'(x^*-\bar{x})
= \bar{t} + \hat{T}'\hat{X}(\hat{X}'\hat{X})^{-1}(x^*-\bar{x})
\]
now \begin{align*}
a'y(x^*)
	&= a'\bar{t} + a'\hat{T}'\hat{X}(\hat{X}'\hat{X})^{-1}(x^*-\bar{x})\\
\end{align*}
Simplifying gives
\[
a'\hat{T}'
= a'(T-\bar{T})'
= a'T'-a'\bar{T}'
= - b\one' - a'\bar{t}\one'
= - b\one' - b\one'
=0
\]
QED

% subsubsection the_leas_sqaure_apporach (end)

\subsubsection{Fisher's linear discriminant} % (fold)
\label{ssub:fisher_s_linear_discriminant}

Suppose $x\in \Real^{p}$ but there are only two classes $K=2$. The main idea is
to try to reduce the dimensionality of the data, to project it in a ``smart'' way
to enable easy classification. Use the following projection onto $[\beta]$:
\[y = \beta'x\]
usually $x$ do not include the intercept.
The direction of $\beta$ is fine but, there has to be a decision made on the
threshold $\beta_0$ of the discriminant hyperplane.
The classification rule is 
\[\begin{cases}
	\text{class}_1, &\text{ if } y(x)\geq -\beta_0\\
	\text{class}_2, &\text{ otherwise}
\end{cases}\]
The rationale : classes are well separated if the mean values are well separated.
Thus we need to maximize the separation of the projected means onto the flat world
inside the hyperplane.

The means and their projections are given by
\[
\bar{m}_k = \frac{1}{|C_k|}\sum_{x\in C_k} x,
\text{ and }
m_k = \beta'\bar{m}_k
\]
respectively. Fix the magnitude of $\beta$ since the separation increases to infinity
with $\|\beta\|$. The lagrangian
\[\Lcal = \beta'(\bar{m}_2-\bar{m}_1) + \lambda (\beta'\beta - 1)\to\max\]
whence
\[\bar{m}_2-\bar{m}_1 + 2\lambda\beta = 0\]
It turns out to be that the optimal $\beta$ must be parallel to the line between
$\bar{m}_1$ and $\bar{m}_2$, which is geometrically intuitive:
\[\beta \propto \bar{m}_1-\bar{m}_2\]

However, it seems that using the class variance might give better discrimination.
The ideas is the same: project in smart way, but also try to balance the separation
with the within class variance:
\[S_k = \sum_{x\in C_k} (\beta'x-\beta'\bar{m}_k)^2\]

Consider the following criterion (Fisher's)
\[J(\beta) =\frac{(m_1-m_2)^2}{S_1+S_2} \]
Let's make the dependence on $\beta$ explicit by computing the $S_k$.
By definition 
\[
(m_2-m_1)^2
= (m_2-m_1)(m_2-m_1)'
= \beta'(\bar{m}_2-\bar{m}_1)(\bar{m}_2-\bar{m}_1)'\beta
= \beta'S_B\beta
\]
where $S_B$ -- the between class variance matrix (scatter). As for the denominator
\begin{align*}
	S_2+S_1
	&= \sum_{x\in C_1} (\beta'x-\beta'\bar{m}_1)^2 + \sum_{x\in C_2} (\beta'x-\beta'\bar{m}_2)^2 \\
	&= \beta'\Bigl(\sum_{x\in C_1} (x-\bar{m}_1)(x-\bar{m}_1)'\\
		+ \sum_{x\in C_2} (x-\bar{m}_2)(x-\bar{m}_2)'\Bigr) \beta
	&= \beta'S_W\beta
\end{align*}
Thus the quotient we are trying to minimize is the Rayleigh quotient
\[J(\beta) = \mathcal{R} = \frac{\beta'S_B\beta}{\beta'S_W\beta}\]
Reexpress this as an optimization problem with respect to $\beta^* = \beta c$
to see that $c$ cancel. Therefore it is possible to fix the denominator and
then maximize the numerator. Thus we will be trying to maximize the 
\[\beta'S_B\beta\to\max_\beta\]
subject to $\beta'S_W\beta = 1$. The Lagrangian is
\[\Lcal = \beta'S_B\beta - \lambda(\beta'S_W\beta-1)\to\max_{\beta,\lambda}\]
The first order conditions
\begin{align*}
	\frac{\partial}{\partial\beta} \Lcal &= S_B\beta - \lambda S_W\beta = 0\\
	\frac{\partial}{\partial\lambda} \Lcal &= 1 - \beta'\lambda S_W\beta
\end{align*}
Which reduces to a generalized eigenvalue-eigenvector problem
\[
S_B\beta = \lambda S_W\beta
\]
But since $\beta'\lambda S_W\beta=1$ it turns out that
\[\beta'S_B\beta = \lambda \beta'S_W\beta\]
whence $\lambda = \beta'S_B\beta$. Therefore in the generalized eigenvalue-eigenvector
problem the eigenvector $\beta$ with the largest eigenvalue must be chosen. In fact
it is possible to reduce this generalized problem to the usual eugen-problem.
If $S_W$ is invertible then the problem can be transformed into
\[S_W^{-1}S_B \beta = \lambda \beta\]
otherwise it is necessary to regularize the $S_W$ with $S_W + cI_p$ for $c>0$.

Now notice that
\[
S_B\beta
= (\bar{m}_2-\bar{m}_1)(\bar{m}_2-\bar{m}_1)'\beta
= (\bar{m}_2-\bar{m}_1)\bigl((\bar{m}_2-\bar{m}_1)'\beta\bigr)_{1\times 1}
\propto \bar{m}_2-\bar{m}_1
\]
whence from the FOC
\[\beta\propto S_W^{-1} (\bar{m}_2-\bar{m}_1)\]

In order to find the threshold, one could fit the ML mean and variance.

% subsubsection fisher_s_linear_discriminant (end)

\subsubsection{Relation to the LDA} % (fold)
\label{ssub:relation_to_the_lda}

Again the case of $2$ classes. The LDA was
\[
\frac{\Pr\bigl(T=k\lvert X=x\bigr)}{\Pr\bigl(T=j\lvert X=x\bigr)}
= x'\Sigma^{-1}(\mu_k-\mu_j) + \text{terms independent of } x
\]
which is a hyperplane with $\beta = \Sigma^{-1}(\mu_k-\mu_j)$.
The MLE of $\hat{\Sigma}$ is given by
\[\hat{\Sigma} = \frac{1}{N}\sum_{k=1}^K \sum_{x\in C_k} (x - \mu_k)(x - \mu_k)'\]
where $n\hat{\Sigma} = S_W$.

Exercise: show that LDA and Fisher's approach are identical in one special case.


% subsubsection relation_to_the_lda (end)

% subsection classifiers (end)

% section lecture_11 (end)

\section{Lecture \# 12} % (fold)
\label{sec:lecture_12}

\subsection{The perceptron classifier} % (fold)
\label{sub:perceptron}

Recall that the signed distance form a hyperplane $\beta_0 + \beta'x = 0$ to a
point $p$ is given by
\[\nrm{p-H} = \frac{\beta_0 + \beta'x}{\sqrt{\beta'\beta}}\]

A target variable $t_i$ is given by $t_i = +1$ if $x_i\in C_1$ and $t_i = -1$
if $x_i\in C_2$ (otherwise). The convenience of such encoding is that the sign
coincides with the misclassification by the hyperplane. If $x_i$ is misclassified
as $C_2$ then $t_i>0$, but $\beta_0+\beta'x_i < 0$ and vice versa. Thus every time
a point is misclassified, we have $t_i(\beta_0 +\beta'x_i) < 0$, and the last term in
brackets is proportional to the distance of $x_i$ from the hyperplane. The following
criterion minimizes the overall distance of the points:
\[
E = - \sum_{i\in \Mcal} t_i( \beta_0 + \beta'x_i ) \to \min_{\beta_0, \beta}
\]
where $\Mcal$ is a the set of misclassified points and $\|\beta\| = 1$ -- this restricts
the problem to well-defined hyperplanes only.

The gradient of $E$ with respect to $\beta_0$ is given by:
\[\frac{\partial}{\partial \beta_0} E = - \sum_{i\in \Mcal} t_i\]
and the 
\[\frac{\partial}{\partial \beta} E = - \sum_{i\in \Mcal} t_i x_i \]

The batch algorithm is using all the datapoints at the same time
\[
\bigl(\begin{smallmatrix} \beta_0\\ \beta \end{smallmatrix}\bigr)
\leftarrow \bigl(\begin{smallmatrix} \beta_0\\ \beta \end{smallmatrix}\bigr)
+\eta \nabla E
\]
where $\eta$ is the convergence parameter, usually set to $1$.

Stochastic gradient descent is going to update the estimate of $\beta_0$ and $\beta$ 
by going through the dataset point by point:
\[
\bigl(\begin{smallmatrix} \beta_0\\ \beta \end{smallmatrix}\bigr)
\leftarrow \bigl(\begin{smallmatrix} \beta_0\\ \beta \end{smallmatrix}\bigr)
+\eta \nabla \bigl(\begin{smallmatrix} t_i\\ t_i x_i \end{smallmatrix}\bigr)
\]
Update if the point is currently misclassified.

If the data is linearly separable, then the perceptron algorithm converges in a finite
number of iterations.

Depending on a starting values the convergence will take place to a possibly different local minimum.

If the data is not linearly separable then the algorithm will loop (go in cycles).

% subsection perceptron (end)

\subsubsection{Proof of the algorithm} % (fold)
\label{ssub:proof_of_the_algorithm}
Suppose the data is linearly separable. then there exits some $\beta$ and $\beta_0$
such that $t_i ( \beta_0 + \beta'x_i) > 0$ for all $i=1,\ldots, n$.

Denote by $b$ the vector built by stacking $\beta_0$ atop $\beta$ and let
$x_i^* = \bigl(\begin{smallmatrix} 1\\ x_i \end{smallmatrix}\bigr)$. Furthermore let
$z_i = \frac{x_i^*}{\|x_i^*\|}$. Thus $t_i b'z_i > 0 $ for all $i$, whence there must
exist $m>0$ with $t_ib'z_i \geq m$ (since there are finitely many points in the dataset).
Let $b_\text{sep} = \frac{b}{m}$. For such $b_\text{sep}$ it is true that for all $i$
\[t_i b_\text{sep}' z_i \geq 1\]

The perceptron algorithm updates the estimate by (force $eta_i = 1$)
\[b^\text{new} = b_\text{old} + \eta_i t_i z_i\]
Hence
\[
b^\text{new} - b_\text{sep} = b_\text{old} - b_\text{sep} + \eta_i t_i z_i
\]
and the point $i$ is misclassified. The squared norm of the updated parameters is
\begin{align*}
	\|b^\text{new} - b_\text{sep}\|^2
	&= \|b_\text{old} - b_\text{sep}\|^2 + \eta_i^2 \|t_i z_i\|^2
		+ 2 t_i (b_\text{old} - b_\text{sep})'z_i \\
	&= \|b_\text{old} - b_\text{sep}\|^2 + |t_i|^2 \|z_i\|^2
		+ 2 \eta_i t_i (b_\text{old} - b_\text{sep})'z_i \,.
\end{align*}
Since $i$ is misclassified we have $t_i b_\text{old}' z_i < 0$ whereas by construction $t_i b_\text{sep}' z_i \geq 1$. In other words:
\[2 t_i b_\text{old}'z_i - 2 t_i b_\text{sep}'z_i \leq 2 t_i b_\text{old}'z_i - 2\leq -2\]
whence
\[
\|b^\text{new} - b_\text{sep}\|^2 \leq \|b_\text{old} - b_\text{sep}\|^2 + \eta_i^2 - 2 \eta_i
\]

% subsubsection proof_of_the_algorithm (end)

\subsection{Perceptron and Neural networks} % (fold)
\label{sub:neural_networks}

A simple mathematical model of a neuron is that of an input integrator, which fires
when a certain threshold is surpassed.
\begin{description}
	\item[Dendrites] collect the input from other neurons;
	\item[Soma] is responsible for aggregating the input from the dendrites
	and releases an electric spike upon saturation;
	\item[Axon] is the conduit through which the signal from soma is propagated
	further through the network.
\end{description}

Mathematically the model is :
\[x\to f\bigl( w_j'x + w_{j0}\bigr)\]
where $w_{j0}$ is the bias (shift, baseline weight), and $w_j$ is the vector of
dendrite ``weights''. The argument of the activation function $f$ is known as the
soma potential Can be extended to stochastic neural networks by treating the result
as the rate of the Poisson process, governing the firing of a neuron.

Typical activation functions are \begin{description}
	\item[Threshold] $f(z) = 1_{z\geq \theta}$;
	\item[Sigmoid] $f(z) = \frac{1}{1+e^{-z}}$.
\end{description}
A simple perceptron cannot realise a bitwise XOR function, since it is not linearly
separable.

The idea is to group perceptrons in layers and stack layer upon each other and
then use that to construct non-linear decision boundaries.

% subsection neural_networks (end)

\subsection{Decision trees} % (fold)
\label{sub:decision_trees}

This is a completely another topic: nonlinear classification. The idea is to partition
the input space into hyper-rectangles. The question is how to construct the regions
and then to use them for classification purposes.

\subsubsection{Classification and Regression Trees} % (fold)
\label{ssub:classification_and_regression_trees}

The construction methods:\textbf{C}lassification \textbf{a}nd \textbf{R}egression
\textbf{T}rees (Brieman;1984), ID3, C4.5.

The set of rules for classification can be summarized in a tree: easy interpretation
and highly transparent decision procedure.
\begin{itemize}
	\item How to choose the splitting variable;
	\item how to predict and classify in each region.
\end{itemize}

Consider the usual training data $(x_i,t_i)$ with $x_i = (x_{ij})_{j=1}^p$. If the
non-overlapping regions $(R_m)_{m=1}^M$ are constructed, then the regression function
can be defined as the following step function
\[f(x) = \sum_{m=1}^M c_m 1_{R_m}(x)\]
The criterion for minimization is 
\[\sum_{i=1}^N \bigl(t_i - f(x_i)\bigr)^2\]
whence the optimal values of $c_m$ are given by local means within the region:
\[\hat{c}_m = \frac{\sum_{i=1}^N t_i 1_{R_m}(x_i)}{|R_m|}\]
this answers the second question.


The first question is harder, since there are infinitely many way to partition the
space into $M$ regions. The algorithm which is going to be used is the following greedy
algorithm.

Suppose $j$ is the splitting feature (variable) and $s$ id the splitting point.
Let 
\[R_1(j,s)
= \bigl\{x_i\rvert x_{ij}\leq s\bigr\}
= X \bigcap \Bigl( (-\infty, s] \times \prod_{m\neq j} \Real \Bigr)
\]
and $R_2(j,s)$ be the compliment of $R_1(j,s)$.

Let's look for $j$ and $s$ such that 
\[
  \min_{c_1}\sum_{x_i\in R_1(j,s)} (t_i - c_1)^2
+ \min_{c_2}\sum_{x_i\in R_2(j,s)} (t_i - c_2)^2
\to \min_{j,s}
\]
For a given choice of $j,s$ the parameters $c_1$ and $c_2$ are given by the local
averages. An important observation is that the space of effective $s$ for a given $j$
is finite and is given by the set of all possible values of the $j$-th feature.
Therefore exhaustive search is possible, since there are $p$ dimensions and finitely
many data points.

The tree size can be regarded as the model complexity. Possible approaches:
\begin{itemize}
	\item Myopic approach: continue until the reward from splitting is less than
	some threshold;
	\item Pruning approach: start with a large tree $T_0$ and consider a subtree
	$T\subseteq T_0$ obtained by collapsing the branching points, until some
	optimality criterion is met;
\end{itemize}

Trade off between the goodness of fit and the complexity odf hte tree.
Define the \textbf{cost-complexity pruning}
\[C_\lambda(T) = \sum_{m=1}^M\sum_{x_i\in R_m} (t_i - \hat{c}_m)^2 + \lambda |T| \]
where $|T|$ is the number of leaves in tree $T$. If $\lambda=0$ then the optimal
tree is $T_0$, otherwise for $\lambda\to \infty$ the optimal trees would have
fewer and fewer leaves.

Rewrite the criterion as
\[C_\lambda(T) = \sum_{m=1}^M |R_m| Q_m(T)+ \lambda |T| \]
for the \textbf{measure of impurity}
\[Q_m(T) = \frac{1}{|R_m|}\sum_{x_i\in R_m} (t_i - \hat{c}_m)^2\]

Choosing the values of $\lambda$? Use cross-validation! Construct $K$ folds, withhold
one fold and on the remaining $K-1$ folds grow the largest tree $T_0$.
Use the cost-complexity pruning to obtain a sequence of trees $(T_\lambda)$, and then
evaluate the means squared error of the withheld fold. Repeat $K$ times and then average
the $K$ estimates. Find the $\hat{\lambda}$ -- the minimum of the \textbf{MSE}.
For the optimal $\lambda$ grow the most complex tree $T_0$ and then do the cost-complexity
pruning to return an optimal $T_{\hat{\lambda}}\subseteq T_0$.

But how to find $T_\lambda$ from $T_0$? How is branch collapsing done?

THe idea is to iteratively collapse just one node, which creates the smallest increase
in the goodness-of-fit score. This is known as the weakest link pruning:
\begin{itemize}
	\item for a given $\lambda$ collapse the node which gives the smallest increase in
	\[\sum_{m=1}^M |R_m| Q_m(T)\]
	\item Stop at root.
\end{itemize}
It can be shown, that the optimal tree $T_\lambda$ is necessary among the sequence of
single-node collapses.

% subsubsection classification_and_regression_trees (end)

\subsection{Classification trees} % (fold)
\label{sub:classification_trees}

The idea is similar, but a better criterion for splitting and fitting is wanted.
Let $\hat{p}_{mk}$ the proportion of point in region $R_m$ that belong to class $k$:
\[\hat{p}_{km} = \frac{1}{|R_m|} \sum_{x_i\in R_m} 1_{\{k\}}(t_i) \]
classify an observation in $R_m$ according to the majority vote:
\[k(m) = \argmax_k \hat{p}_{km}\]

The choice of the impurity measure $Q_m(T)$
\subsubsection{The misclassification error} % (fold)
\label{ssub:the_misclassification_error}

The misclassification error:
\[
Q_m(T)
= \sum_{i \in R_m} 1_{\{\neq k\}}(t_i)
= 1 - \hat{p}_{km}
\]
But this is not the best measure there is.
% subsubsection the_misclassification_error (end)

\subsubsection{Gini's impurity index} % (fold)
\label{ssub:gini_s_impurity_index}

Not to confuse with the inequality measure. In ecological applications, where it
emerged, it measures the biodiversity. Lots of minority (non-dominant) species means
greater diversity$\sim_{s\in\mathcal{S}} s (1-p_s)$. Literally this translates to a measure
of how many other classes are present at a node $m$:
\[Q_m(T) = \sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\]
where $K$ is the number of classes. This measure is employed by CART.

% subsubsection gini_s_impurity_index (end)

\subsubsection{Impurity measure based on entropy} % (fold)
\label{ssub:impurity_measure_based_on_entropy}

Consider the cross-entropy (ID3, C4.5). The idea to weight the least represented
classes not by the residual proportion, but by the logarithm:
\[Q_m(T) = -\sum_{k=1}^K\hat{p}_{mk} \log \hat{p}_{mk}\]
(Maybe this is not cross-, but a usual entropy (?))

% subsubsection impurity_measure_based_on_entropy (end)

\subsection{Binary classification} % (fold)
\label{sub:binary_classification}

Suppose $p$ is the proportion of observations in class $C_1$
\begin{align*}
	\text{ME} &= 1 - \max\{p, 1-p\}\\
	\text{Gini} &= 2 p (1-p)\\
	\text{X-entropy} &= - p\log p - (1-p)\log (1-p)
\end{align*}

% subsection binary_classification (end)

\subsubsection{Interpreation} % (fold)
\label{ssub:interpreation}

Decision and classification trees are interpretable and explicable. However, in
general trees have poor prediction accuracy. There are various method for improving
the accuracy by aggregating many poor classifiers. Simple aggregation is not robust,
and highly variable.

% subsubsection interpreation (end)

% subsection classification_trees (end)

% subsection decision_trees (end)

\subsection{Bagging and boosting} % (fold)
\label{sub:bagging_and_boosting}

\subsubsection{\emph{B}ootstrap \emph{agg}regation} % (fold)
\label{ssub:bootstrap_aggregation}

This is a general procedure for reducing the variance of a statistical classification
method. The idea is to use bootstrap samples toward this reduction. There are two types
of bootstrap: parametric and non-parametric.

Generate $B$ bootstrap samples, and use each sample as input for the statistical
learning regression method $\hat{f}^*_b(x)$. The bagging prediction for a regression
problem is given by
\[\hat{f}_\text{bag}(x) = \frac{1}{B} \sum_b \hat{f}^*_b(x)\]
for the classification problem the bagging classification is just the ``majority'' vote in
favour of a certain class:
\[\hat{f}_\text{bag}(x) = \argmax_k \sum_b 1_{\{=k\}}(\hat{f}^*_b(x))\]
As $B$ grows larger, the sum under the argmax operator is converging to the
theoretical share of the class $k$.

% There are deep theoretical issues with simple majority voting

Assign each data point in the dataset a probability $\hat{q}_i$. In a non-parametric
bootstrap setting $\hat{q}_i = \frac{1}{n}$. Use $\hat{q}$ to simulate a sample
\[\hat{f}^*(x) \to (x^*_i, t^*_i)_{i=1}^n\]
where $(x^*_i, t^*_i)\sim \hat{q}$. The ``true'' bagging estimate is given by the 
\[\ex_{\hat{q}} \bigl(\hat{f}^*(x)\bigr)\]
-- the expected value of the empirical process. Since it is hard to study the
properties of a random estimate, we are going to focus on its limit.

% subsubsection bootstrap_aggregation (end)

% subsection bagging_and_boosting (end)

% section lecture_12 (end)

\section{Lecture \# 13} % (fold)
\label{sec:lecture_13}

\subsection{Bagging (cntd.)} % (fold)
\label{sub:bagging_cntd}

In the regression setting:
\[ \hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^*_b(x) \]
In the classification setting:
\[ \hat{f}_{\text{bag}}(x) = \argmax_{k}\frac{1}{B} \sum_{b=1}^B 1\{\hat{f}^*_b(x)=k\} \]

When should bagging is to be used:
nonlinear;
work with already better-than-random classifiers.

Consider the dataset $z=(x_i,t_i)_{i=1}^n$ and we need to choose the function $\hat{f}$.
Consider the $1$-dimensional case, and we are going to fit the cubic spline.
The cubic spline has $K+4$ parameters
\[\mu(x) =\sum_{j=1}^{K+4} \beta_j h_j(x) = h'(x) \beta \]
Let $H = (h_j(x_i)) \in \Real^{n\times (K+4)}$.

The LS estimate of $\beta$ is $\hat{\beta} = (H'H)^{-1}H't$, and the fit is
\[ \hat{\mu}(x) = \sum_{j=1}^{K+4} \beta_j h_j(x) = h'(x) \hat{\beta} \]
Let's study the variance of the fitted model. IN order to do that we need to 
impose q mode on $t$
\[t = \mu(x) + \epsilon\]
where $\epsilon\sim D(0,\sigma^2)$. Consider the biased estimate of $\sigma^2$:
\[\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (t_i-\hat{\mu}(x_i))^2\]

Now, for $X = (x_i)$
\[\ex t = \mu(X) = H\beta\]
and 
\[
\Sigma_{\hat{\beta}}
= \ex(\hat{\beta}-\beta)(\hat{\beta}-\beta)'
= \sigma^2 (H'H)^{-1}
\]
and finally
\[
\text{sd}\hat{\mu(x)}
= \bigl( h'(x) \Sigma_{\hat{\beta}} h(x) \bigr)^\frac{1}{2}
= \sigma \bigl( h'(x) (H'H)^{-1} h(x) \bigr)^\frac{1}{2}
\]

Now consider the parametric bootstrapping. Once we've got estimate of $\mu(x)$ and $\sigma^2$, we can generate a new target vector
\[t^*_i = \hat{\mu}(x_i) + \epsilon^*_i\]
where $\epsilon^*_i\sim \Ncal(0, \hat{\sigma}^2)$

THe bootstrap dataset is given by the following $Z^* = (x_i, t^*_i)_{i=1}^n$ (discriminative approach).
Let 
\[
\hat{\mu}^*(x)
= h'(x)\hat{\beta}^*
= h'(x)(H'H)^{-1}H't^*
\]
The implications of this for one bootstrap estimate are that $\hat{\mu}^*(x)$ is
normally distributed (linear combination of normally distributed rv) (for each $x$);
The mean of $\hat{\mu}^*(x)$ is 
	\[
	\ex\hat{\mu}^*(x)
	= h'(x) (H'H)^{-1}H'\ex t^*
	= h'(x) (H'H)^{-1}H'H\hat{\beta}
	= h'(x) \hat{\beta}
	= \hat{\mu}(x)
	\]
The variance of $\hat{\mu}^*(x)$ is
\begin{align*}
	\var\hat{\mu}^*(x)
	&= h'(x) (H'H)^{-1}H' \Sigma_{t^*} H(H'H)^{-1} h(x)\\
	&= h'(x) (H'H)^{-1}H' \hat{\sigma}^2 I_n H(H'H)^{-1} h(x)\\
	&= \hat{\sigma}^2 h'(x) (H'H)^{-1} h(x)
		= \hat{\sigma}^2 h'(x) (H'H)^{-1} h(x)
\end{align*}
Therefore
\[\hat{\mu}^*(x) \sim \Ncal\bigl(\hat{\mu}(x), \hat{\sigma}^2 h'(x) (H'H)^{-1} h(x)\bigr)\]

Now performing $B$ bootstrap replications of the estimator we get:
\[
\hat{\mu}^*_{\text{bag}}(x)
= \frac{1}{B} \sum_{b=1}^B \hat{\mu}^*_b(x)
\]
whence using the LLN on gets as $B\to \infty$
\[
\hat{\mu}^*_{\text{bag}}(x) \to \ex\hat{\mu}^*_{\text{bag}}(x) = \hat(x)
\]
there is no gain in using bagging for linear regression. To get benefits for bagging
one should use non-linear models (trees, neural networks, ridge regressions et c.)
The same conclusion applies to the case of non-parametric bootstrap. cf. ``On bagging
and non-linear estimation'' (Friedman, Hall; 1999)

The bagging estimate $\hat{f}_{\text{bag}}^*(x)$ is a Monte-Carlo approximation of $\ex_{\hat{\rho}}\hat{f}_{\text{bag}}$. Consider the ideal aggregate estimator $f_{\text{agg}}(x)$ for $\ex_\rho\hat{f}^*(x)$, where 
$\hat{f}^*(x)$ is computed from bootstrap sample $z^* = (x^*, t^*)\sim \rho$
using distribution $\rho$.

Consider a square loss function
\[
\ex_\rho \bigl( T-\hat{f}_{\text{bag}}(x)\bigr)^2
= 
\ex_\rho \bigl( T-f_{\text{agg}}(x)+f_{\text{agg}}(x)-\hat{f}_{\text{bag}}(x)\bigr)^2
\]
and
\[
\ex_\rho \bigl( T-f_{\text{agg}}(x)\bigr)\bigl( f_{\text{agg}}(x) - \hat{f}_{\text{bag}}(x)\bigr)
= 0
\]
whence
\[
\ex_\rho \bigl( T-\hat{f}_{\text{bag}}(x)\bigr)^2
= \ex_\rho \bigl( T-f_{\text{agg}}(x)\bigr)^2 + 
\ex_\rho \bigl( f_{\text{agg}}(x) - \hat{f}_{\text{bag}}(x)\bigr)^2
\geq \ex_\rho \bigl( T-f_{\text{agg}}(x)\bigr)^2
\]

Thus the ideal aggregate estimator will never increase the MSE (it suggests that).
Hence the bagging estimator should be doing well as well.

The \textbf{o}ut-\textbf{o}f-\textbf{b}ag estimate of the test error. Let $I(b)$
be the set of indices of observations, not included in the $b$-th bootstrap sample.
thus the $i$-th output
\[
t^*_{iB} = \frac{1}{\#\{b \rvert i\in I(b)\}} \sum_{b \rvert i\in I(b)} \hat{f}^*_b(x_i)
\]
for classification 
\[
t^*_{iB} = \argmax_k \sum_{b \rvert i\in I(b)} 1\{\hat{f}^*_b(x_i) = k\}
\]
Thus the oob-error for regression is given by
\[
\text{oob}\bigl(\hat{f}_{\text{bag}}\bigr)
= \frac{1}{n} \sum_{i=1}^n (t_i - \hat{t}^*_{iB})^2
\]
and for classification
\[
\text{oob}\bigl(\hat{f}_{\text{bag}}\bigr)
= \frac{1}{n} \sum_{i=1}^n 1\{t_i \neq \hat{t}^*_{iB}\}
\]

When bagging should be used? The motivation is by example in a simple setup
with $K=2$ classes. For a given input $x$ one gets $B$ independent classifiers
$\hat{f}^*_b(x)$. Assume the classifiers have a misclassification error $0.4$.
If the true class of $x$ is $1$ then $\pr(\hat{f}^*_b(x) = 2) = 0.4$. The bagging
is 
\[
\hat{f}_\text{bag}(x)
= \argmax_k \sum_{b=1}^B 1\{\hat{f}^*_b(x) = k\}
\]
If $B_k = \sum_{b=1}^B 1\{\hat{f}^*_b(x) = k\}$, then $B_k\sim\text{Bin}(B, 0.4)$. Thus
the misclassification error of the bagging estimate is 
\[\pr\bigl(\hat{f}_\text{bag}(x) = 2\bigr) = \pr\bigl(B_2\geq \frac{B}{2}\bigr)\]
using the LLN $\frac{1}{B} B_2 \to \ex B_2$, whence
\[\pr\bigl(\hat{f}_\text{bag}(x) = 2\bigr) \overset{b\to \infty}{\to} 0\]
Bagged classifier has perfect prediction accuracy. But if the misclassification error is
$0.6$ then the bagging classifier is universally bad (perfectly inaccurate):
\[\pr\bigl(\hat{f}_\text{bag}(x) = 2\bigr) \overset{b\to \infty}{\to} 1\]

Bagging a good estimator can improve the accuracy, whereas bagging a bad classifier
deteriorates the prediction accuracy.

Why bagging does not reduce the variance in non-parametric bootstrap case.

Consider a sample $X^n = (X_i)_{i=1}^n$-iid $(\mu, \sigma^2)$. and estimate the mean using
the sample mean $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$. Now bootstrap non-parametrically
to get
\[\bar{X}^*_b = \frac{1}{n} \sum_{i=1}^n X^*_{bi} \]
where $X^*_b\sim \hat{F}_n$ the empirical CDF of $X^n$. Bag the estimator
\[
\bar{X}_{\text{bag}} = \frac{1}{B}\sum_{b=1}^B \bar{X}^*_b
\]
we expect that this will produce no reduction in variance either.

% subsection bagging_cntd (end)

\subsection{Random forests} % (fold)
\label{sub:random_forests}

This is a modification of bagging is the sense that it tries to decorrelate the
bootstrapped trees.
We want to decorrelate the trees: we have bootstrap sample of size $B$ with variance
$\sigma^2$ with pairwise correlation given by $\rho$, then
\[
\var \bar{X} = \rho \sigma^2 + \frac{1-\rho}{B}\sigma^2
\]
the correlation makes variance persistent.

How can one reduce correlation between bootstrapped trees. The idea:
when one grows a tree, take a random selection of $m$ ($\sqrt{p}$) predictors as
potential candidates of CART (permitted to split only along this subset of features).

Then grow a total of $B$ trees
\[\hat{f}^B_{\text{rf}}(x) = \frac{1}{B} \sum_{b=1}^B T(x, \theta_b)\]
where $\theta_b$ contains the splitting variables, splitting thresholds and prediction
values. The growth continues until a minimum of $n_\text{min}$ points at the leaves.
One can use the OOB error estimate when (?)

For regression $m\approx \lfloor \frac{p}{3}\rfloor$ and $n_\text{min}=5$;
For classification $m\approx \sqrt{p}$ and $n_\text{min}=1$.
But in general these parameters are problem dependent.

How large $m$ should be? Consider the random forest predictor
\[
\hat{f}^B_{\text{rf}}(x)
= \frac{1}{B} \sum_{b=1}^B T(x, \theta_b)
\overset{B\to\infty} \ex_{\theta|Z} T(x, \theta(Z))
\]
what is this $\ex_{\theta|Z} T(x, \theta(Z))$? The parameter depends on $Z$, with
$Z$ -- the original dataset.

The variance of $\hat{f}^B_{\text{rf}}(x)$ converges towards
\[\var \hat{f}^B_{\text{rf}}(x)\to \rho(x) \sigma^2(x)\]

Now $\rho(x)$ is the correlation between a pair of trees
\[
\rho(x) = \text{cor}\bigl( T(x,\theta_1(Z)), T(x,\theta_2(Z)) \bigr)
\]
Generate some sample $Z$ and then consider two bootstrapped trees...

AS for the variance of a single tree, it is given by
\[\sigma^2(x) = \var T(x,\theta(Z))\]
which is a variance of a single grown tree.

Suppose the right-hand side of the limit depends on $m$. As $m$ grows, the bootstrapped
trees are going to look alike, which means that correlation increases.

Now
\begin{align*}
	\sigma^2(x)
	&= \var T(x,\theta(Z))
	= \var_Z \ex_{\theta|Z} T(x,\theta(Z)) + \ex_Z \var_{\theta|Z} T(x,\theta(Z)) \\
	&= \var_Z \hat{f}^B_{\text{rf}}(x) + \ex_Z \var_{\theta|Z} T(x,\theta(Z))
\end{align*}
so as $m$ decreases the choice of splitting variables is going to be more and more random,
which implies that the grown trees would be more variable.

% subsection random_forests (end)

\subsection{Boosting} % (fold)
\label{sub:boosting}

\subsubsection{\textbf{Ada}ptive \textbf{Boost}ing} % (fold)
\label{ssub:adaboost}

It seems similar to bagging and train many predictors and then average them out.
The procedure is to combine the effort of many ``weak'' classifiers to produce a
better predictor. A weak classifier is a little better than a random guess.

Construct a bunch of weak classifier. Let $X,T$ be the training sample and grow a
classifier $G_1(x)$ on it. Then grow another classifier $G_2(x)$ on a weighted sample
with weights, emphasizing observations classified poorly on the previous iteration.
Finish with $M$ iteratively grown classifiers and then construct the final classifier
as a weighted combination of $G_k(x)$. The output of the final classifier is the sign
of the combination. Adaboost is suitable for binary classification.

\[f_M(x) = \sum_{m=1}^M \beta_m b(x,\gamma_m)\]
for trees $\gamma_m$ is the splitting variable, splitting point and the estimated value.
\[f(x) = \sum{m=1}^M \beta_m 1_{R_m}(x)\]
 
Try to minimize the following sum
\[ \min_{\beta_m, \gamma_m} \sum_{i=1}^n L(t_i, f_M(x_i) ) \]
this is intractable.

Approximation: the \textbf{f}orward \textbf{s}tagewise \textbf{a}dditive \textbf{m}odelling
Try to estimate iteratively: make the best possible choice at each step (greedy).

Start by initializing $f_0(x)$ to anything (take the mean class, for instance).
Then compute the optimal $(\beta_m,\gamma_m)$ such that 
\[\argmin_{\beta, \gamma} \sum_{i=1}^n L(t_i,f_{m-1}(x_i) + \beta b(x_i, \gamma))\]
once one has this, update $f_{m-1}(x)$
\[
f_m(x) = f_{m-1}(x) + \beta_m b(x, \gamma_m)
\]
Differences to bagging: boosting uses the whole sample.

Consider the case of a square loss: minimization yields
\[ \ex_{X,T} L(T, y(X)) = \ex_{X,T} (T - y(X))^2 \to \min_y \]
gives $y^*(x) = \ex(T|X)$.

Thus
\[
L(t_i,f_{m-1}(x_i) + \beta b(x_i, \gamma))
= (t_i - f_{m-1}(x_i) + \beta b(x_i, \gamma))^2
= (r_{im} + \beta b(x_i, \gamma))^2
\]
this is tractable, but square loss is not suited well for classification.

Use exponential loss:
\[L(t,y(x)) = e^{-t y(x)} \]
where $t\in\{-1,+1\}$. 
Find the minimizer of expected exp-loss for each $x$
\[
\ex_{X,T} L(T, y(X))
= \ex_{X,T} e^{ - T y(X)}
= \ex_X \ex_{T|X} e^{ - T y(X)}
= \ex_X f(x)
\to \min_y
\]
This is a calculus of variations problem.
\begin{align*}
	f(x) &= \ex_{T|X} e^{ - T a }
	f(x) &= e^{ - a } \pr(T=1|X) + e^{ a } \pr(T=-1|X)
\end{align*}
for $a=y(X)$ is constant given $X$. The optimal $a$ is given by
\[
y^*(X)
= \frac{1}{2} \log\frac{\pr(T=1|X)}{\pr(T=-1|X)}
= \frac{1}{2} \text{odds ratio}
\]
Thus
\[
G(x)
= \text{sign} \bigl( y^*(X)\bigr)
= \text{sign} \Bigl( \sum_{m=1}^M \beta_m G_m(X) \Bigr)
\]
the weighted majority vote.

Sin the exponential loss was used:
\[
(\beta_m, \gamma_m) 
= \argmin_{\beta, G} \sum_{i=1}^n e^{-t_i\bigl(f_{m-1}(x_i) + \beta G(x_i) \bigr)}
\]
so a scaled tree is added.
\[
(\beta_m, \gamma_m)
= \argmin_{\beta, G} \sum_{i=1}^n w^m_i e^{- \beta t_i G(x_i)}
\]
where $w^m_i = e^{-t_i f_{m-1}(x_i)}$. Now
\[
\sum_{i=1}^n w^m_i e^{- \beta t_i G(x_i)}
= \sum_{i| t_i = G(x_i)} w^m_i e^{- \beta} 
+ \sum_{i| t_i \neq G(x_i)} w^m_i e^\beta
\]
whence
\[
\sum_{i=1}^n w^m_i e^{- \beta t_i G(x_i)}
= e^{- \beta} \sum_{i=1}^n w^m_i
+ \sum_{i| t_i \neq G(x_i)} w^m_i \bigl( e^\beta - e^{- \beta} \bigr)
\]
Since the first term is independent of $G$, at step $m$ the optimal $G$ is
going to minimize the weighted misclassification (error) rate
\[
G_m = \argmin_G \sum_{i| t_i \neq G(x_i)} w^m_i \to \min_G
\]
Consider a modification of the CART algorithm:
\[\hat{p}_{mk} = \frac{1}{|R_m|} \sum_{i|x_i\in R_m} 1_{t_i}(k)\]
a simple modification is
\[\hat{p}_{mk} = \frac{ \sum_{i|x_i\in R_m} w^m_i 1_{t_i}(k) }{ \sum_{i|x_i\in R_m} w^m_i }\]
weighted proportion of class $k$ observations in region $m$.

% subsubsection adaboost (end)

% subsection boosting (end)

% section lecture_13 (end)

\section{Lecture \# 14} % (fold)
\label{sec:lecture_14}

\subsection{Boosting (cntd.)} % (fold)
\label{sub:boosting_continued}

Adaboost minimizes the 
\[\ex_{(X,T)} L(T, y(X))\]
for $T\in \{-1,+1\}$ and the loss function is given by 
\[L(t, y(x)) = e^{-t y(x)}\]
where the values of te exponent is called the margin.

The optimal binary classifier is 
\[y^*(x) = \frac{1}{2} \log\frac{\Pr(T=+1\rvert x)}{\Pr(T=-1\rvert x)}\]
which is unknown, since the probabilities are unavailable. Thus use an approximation
\[f_M(x) = \sum_{m=1}^M \beta_m G_m(x)\]
where $G_m(x)$ is a weak binary classifier. The final classifier is defined through
the discriminant function:
\[G_M(x) = \text{sign}\bigl(f_M(x) \bigr)\]

The parameters are given by the best 
\[
(\beta_m, G_m)
= \argmin_{\beta, G} \sum_{i=1}^n e^{-t_i \bigl( f_{m-1}(x) + \beta G(x_i) \bigr) }\]

the optimal $G_m$ is given by the minimizer:
\[ G_m = \argmin_{G} \sum_{i=1}^n w^m_i 1_{t_i \neq G(x_i) } \]
where $w^m_i = e^{-t_i f_{m-1}(x_i)}$.

Let's find the minimizer $\beta$ of 
\[\sum_{i=1}^n w^m_i e^{-t_i \beta G(x_i) }\]
First split the sum
\[\ldots 
= \sum_{i: t_i \neq G(x_i)} w^m_i  e^\beta
+ \sum_{i: t_i = G(x_i)} w^m_i  e^{-\beta} \]
whence
\[\ldots 
= e^{-\beta} \bigl( \sum_{i=1}^n w^m_i 
+ (e^{2\beta}-1) \sum_{i=1}^n w^m_i 1_{t_i \neq G(x_i)}
\Bigr) \]
and
\[\ldots 
= e^{-\beta} \bigl( \sum_{i=1}^n w^m_i \Bigr) \Bigl\{ 
1 + (e^{2\beta}-1) \text{err}_m
\Bigr\} \]
where
\[\text{err}_m = \frac{\sum_{i=1}^n w^m_i 1_{t_i \neq G(x_i)}}{\sum_{i=1}^n w^m_i} \]
This implies that the minimizer is given by
\[
\beta_m = \frac{1}{1}\log \frac{1-\text{err}_m}{\text{err}_m}
\]

Let's update the current calssifier: add the function with the optimal parameters
that were found.
\[ f_m(x) \leftarrow f_{m-1}(x) + \beta_m G_m(x) \]
and then update teh weights
\[
w^{m+1}_i \leftarrow e^{-t_i f_m(x_i)} = w^m_i e^{-t_i \beta_m G_m(x_i)}
\]
and then renormalise. This expression could be simplified. First note a neat trick
that under the current binary encoding one gets
\[ -t_i G_m(x_i) = 2 1_{t_i \neq G_m(x_i)} - 1 \]
Therefore the updating rule for the weihgts becomes 
\[
w^{m+1}_i
\leftarrow w^m_i e^{\beta_m ( 2 1_{t_i \neq G_m(x_i)} - 1 )}
= w^m_i e^{\alpha_m 1_{t_i \neq G_m(x_i)}} e^{-\beta_m}
\]
with $\alpha_m = 2\beta_m$. For proper functioning $\alpha_m$ needs to be larger than
zero, since otherwise the weights would be updated in the opposite direction.

To summarize, the AdaBoost is given by
\begin{enumerate}
	\item Initialize the weights to $w^1_i = \frac{1}{n}$;
	\item For $m=1,\ldots, M$ do: \begin{enumerate}
		\item Fit a binary classifier $G_m(x)$ to the training data using the current
		weights $w^m_i$;
		\item Compute the current classification error:
		\[ \text{err}_m = \frac{\sum_{i=1}^n w^m_i 1_{t_i \neq G(x_i)}}{\sum_{i=1}^n w^m_i} \]
		\item Get $\alpha_m = \log\frac{1-\text{err}_m}{\text{err}_m}$ and
			$\beta_m = \frac{\alpha_m}{2}$;
		\item Finally update the wieghts
		\[ w^{m+1}_i \propto w^m_i e^{\alpha_m 1_{t_i \neq G_m(x_i)}} e^{-\beta_m} \]
	\end{enumerate}
	\item Output the final classifier 
		\[G(x) = \text{sign} \Bigl( \sum_{m=1}^M \alpha_m G_m(x) \Bigr)\]
\end{enumerate}

% subsection boosting_continued (end)

\subsection{Remarks} % (fold)
\label{sub:remarks}

There are many other loss functions, what are their merits and which are appropriate?
\begin{itemize}
	\item The binomial deviance. Find the minimizer of
	\[\ex_{T|X} -l( T, y(X))\]
	where $l$ is the log-likelihood. Let $p(x) = \pr(T=1|x)$ and it is equal to
	\[l(t, p(x)) = \log\Bigl( p^{t'}(x) (1-p(x))^{1-t'} \Bigr)\]
	where $t' = \frac{t+1}{2}\in \{ 0,1\}$.
	Since $y(x) = \frac{1}{2}\log\frac{p(x)}{1-p(x)}$, whence
	\[
	l(t, y(x))
	= \log\Bigl( \frac{1}{1+e^{-2y(x)}}\Bigr)^{t'} \Bigl( \frac{e^{-2y(x)}}{1+e^{-2y(x)}}\Bigr)^{1-t'}
	= \log\Bigl\{ \frac{e^{-2(1-t')y(x)}}{1+e^{-2y(x)}}\Bigr\}
	\]
	whence
	\[
	-l(t,y(x))
	= \log\bigl( 1+e^{-2y(x)}\bigr) e^{2(1-t')y(x)}
	= \log\bigl( 1+e^{-2y(x)}\bigr) e^{(1-t)y(x)}
	= \log 1+e^{-2 t y(x)}
	\]
	which is the binomial deviance criterion:
	\[
	\ex_{T|X} -l( T, y(X))
	= \ex_{T|X} \log 1+e^{-2 T y(X)}
	\]
	The $y(\cdot)$ that minimizes this deviance is given by the minimizer of 
	\[\ex_{T|X} e^{-2 T y(X)}\]
	\item The square error loss (bad function), given by
	\[(t-y(x))^2 = (1-\frac{y(x)}{t})^2  = (1 - t y(x))^2 \]
	since $|t|=1$.
\end{itemize}

The robustness can be achieved with a loss function, which does not assign large
weights to outliers.

Deviance generalizes quite well in the case of multiclass classification.

\[
p_k(x)
= \pr(T\in C_k|x)
= \frac{ e^{f_k(x)} }{\sum_{j=1}^K e^{f_j(x)} }
\]
Since $f_j$'s are unique up to a constant shift, the identifiability can be achieved
by restricting them to sum to one. Anyway the negative log-likelihood is given by
\[-l(t, p(x)) = -\sum_{k=1}^K 1_{t\in C_k} f_k(x) + \log \sum_{k=1}^K e^{f_k(x)}\]

\subsubsection{Loss function for regression problems} % (fold)
\label{ssub:loss_function_for_regression_problems}

The square loss: $l(t, y(x)) = \frac{1}{2}(t-y(x))^2$;
The absolute loss: $l(t, y(x)) = |t-y(x)|$;
Huber loss: $l(t, y(x)) = \epsilon^2 1_{|\epsilon|\leq \delta} + (2\delta |\epsilon| - \delta^2) 1_{|\epsilon| > \delta}$
for $\delta = \pm 1$;

% subsubsection loss_function_for_regression_problems (end)

% subsection remarks (end)

\subsection{Gradient boosting} % (fold)
\label{sub:gradient_boosting}

This is a generalization of boosting/ Boosted tree model
\[f_M(x) = \sum_{m=1}^M T(x, \theta_m)\]
the parameter is given by 
\[\hat{\theta}_m = \argmin_\theta \sum_{m=1}^M \Lcal( t_i, f_{m-1}(x_i) + T(x_i, \theta) )\]
where the $\theta$'s summarize the parameters of the constructed tree:
\[\theta_m = \bigl(R_{jm}, \gamma_{jm}\bigr)_{j=1}^{J_m}\]
Binary classification: exponential loss, -- Regression -- square loss.
How to adapt the AdaBoost to the Huber loss function.

Current model is $f_{m-1}$. Find such an adjustment $h$ that
\[f_{m-1}(x_i) + h(x_i) = t_i\]
which can be generalized to out-of-sample observations. Such``good'' regression
tree (or function) might not exist.

Approximately fit a regression (tree) function to the data :
\[\Bigl(x_i, t_i - f_{m-1}(x_i)\Bigr)_{i=1}^n\]
Consider a loss function given by
\[L(t, y(x)) = \frac{1}{2}(t-y(x))^2\]
the derivative of the empirical loss is
\[
\frac{\partial}{\partial f_{m-1}(x_i) } \sum_{i=1}^n L(t_i, f_{m-1}(x_i))
= -( t_i - f_{m-1}(x_i) )
= - r_{im}
\]
Fit an $h$ to this dataset
\[\Bigl((x_i, -r_{im})\Bigr)_{i=1}^n\]

\subsubsection{Algorithm \# 1} % (fold)
\label{ssub:algorithm_1}
\begin{enumerate}
	\item Initialize $f_0(x) = \bar{t}$;
	\item For $m=1,\ldots, M$ compute: \begin{enumerate}
		\item $r_{im} = - \frac{\partial}{\partial f_{m-1}(x_i) } L\Bigl(t_i, f_{m-1}(x_i)\Bigr)$;
		\item Fit $h_m$ to $\Bigl((x_i, r_{im})\Bigr)_{i=1}^n$;
		\item Choose \[
		\gamma_m  = \argmin_{\gamma} \sum_{i=1}^n \Lcal( t_i, f_{m-1}(x_i) + \gamma h_m(x_i) )\]
		\item Update $f_m(x) = f_{m-1}(x) + \gamma_m h_m(x)$.
	\end{enumerate}
	\item Output $\hat{f}(x) = f_M(x)$.
\end{enumerate}
This looks like a gradient search in some functional space. The multiplier was added to
mimic the gradient search.

Find $f$ such that $\sum_{i=1}^n \Lcal(t_i, f(x_i))$. If $f_i = f(x_i)$ then minimize
\[ J(f) = \sum_{i=1}^n \Lcal(t_i, f_i) \]
Using the gradient descent method gives:
\[f^+ = f^- - \rho \nabla J(f) \approx f^- + h(x)\]
but this does not generalise to $h$ outside of the given $x_i$'s.

% subsubsection algorithm_1 (end)

\subsubsection{Algorithm \# 2} % (fold)
\label{ssub:algorithm_2}

Friedman: Tree boost, for $h_m$ being in the class of regression trees. Then the
algorithm is modified as
\begin{itemize}
	\item Fit $h_m(x) = \sum_{j=1}^{J_m} b_{jm} 1_{x\in R_{jm}}$;
	\item multiply each $b_{jm}$ by a single $\gamma_m$ to get
	\[ \gamma_m h_m(x) = \sum_{j=1}^{J_m} \gamma_m b_{jm} 1_{x\in R_{jm}} \]
\end{itemize}

\begin{enumerate}
	\item Initialize $f_0(x) = \bar{t}$;
	\item For $m=1,\ldots, M$ compute: \begin{enumerate}
		\item $r_{im} = - \frac{\partial}{\partial f_{m-1}(x_i) } L\Bigl(t_i, f_{m-1}(x_i)\Bigr)$;
		\item Fit a regression tree $h_m$ to $\Bigl((x_i, r_{im})\Bigr)_{i=1}^n$ producing just
		the regions $R_{jm}$;
		\item Choose \[
			\gamma_{jm}  = \argmin_{\gamma} \sum_{i: x_i\in R_{jm}} \Lcal( t_i, f_{m-1}(x_i) + \gamma )\]
		\item Update $f_m(x) = f_{m-1}(x) + \sum_{j=1}^{J_m} \gamma_{jm} b_{jm} 1_{x\in R_{jm}} $.
	\end{enumerate}
	\item Output $\hat{f}(x) = f_M(x)$.
\end{enumerate}
This looks like a gradient search in some functional space. The multiplier was added to
mimic the gradient search.

% subsubsection algorithm_2 (end)

\subsection{Generalisations} % (fold)
\label{sub:generalisations}

Instead of using exponential or deviance loss function, one could plug any other
differentiable loss function instead of $\Lcal$. 

% subsection generalisations (end)
% subsection gradient_boosting (end)

% section lecture_14 (end)

\section{Lecture \# 15} % (fold)
\label{sec:lecture_15}

\subsection{Elements of convex optimization} % (fold)
\label{sub:elements_of_convex_optimization}

\subsubsection{Convexity} % (fold)
\label{ssub:convexity}

A convex set $M$ is sich that for all $x,y\in M$ and $\theta \in [0,1]$ then
\[ \theta x + (1 - \theta) y \in M \]
A polyhedron is a convex set and is a solution of finitely many linear inequalities
in $\Real^n$ : $A_i'x \leq b_i$ for $i=1,\ldots,m$, $A_i\in \Real^{n\times 1}$ and
$A\in \Real^{n\times m}$. Hyperplanes, lines, halfspaces are polyhedra.

A conic conbination of $x_1,x_2\in \Real^n$ is 
\[\theta_1 x_1 + \theta_2 x_2\]
for all $\theta_1,\theta_2 \geq 0$. The set of all conic combination of $x_1$
and $x_2$ is called a ``cone'' an is gicen by
\[ \text{cone}(x_1,x_2) = \Bigl\{ \theta_1 x_1 + \theta_2 x_2 \Bigr\rvert \Bigl. \theta_1,\theta_2\geq0 \Bigr\} \]
An important exmaple of a cone set is the set of symmetric (and positive semidefinite
matrices).

Start with a simple object that is convex and then search for transformations
that preserves convexty.
\begin{itemize}
	\item intersection of convex sets is convex;
	\item the image of a convex set under an affine transformation $f(x) = Ax + b$ is convex;
	\item the linear fractional function also preserves convexity
	\[x\to \frac{Ax + b}{Cx + d}\]
	with domain $\{Cx\neq d\}$.
\end{itemize}

A function $f:\Real^n \to\Real$ is convex if its domain is convex and 
\[f\bigl(\theta x+(1-\theta)y\bigr) \leq \theta f(x) + (1-\theta)f(y)\]
for all $\theta\in[0,1]$.

Examples: exponential, affine functions, power $x^\alpha$ on $(0,+\infty)$,
the $l^p$ norm for $p\geq 1$: for any $x\in \Real^n$
\[\|x\|_p = \Bigl(\sum_{k=1}^n |x_k|^p\Bigr)^\frac{1}{p}\]

If $f$ is differentiable, then the first order conditions for convexity are
\[f(y)\geq f(x) + \nabla f(x)'(y-x)\]
for all $x,y\in \text{dom} f$.

The second order condition for strict convexity if and only if
\[0 \preceq \nabla^2 f\]
All quadratic functions are convex:
\[ f(x) = x'Px + qx + c\]
with $\nabla f(x) = Px + q$ and $\nabla^2 f(x) = P\preceq 0$.

The log-sum-exp is convex:
\[f(x) = \log \sum_{i=1}^n e^{x_i}\]
indeed, the second derivatinve of $f$ is
\[ \frac{\partial f}{\partial x_k} = \frac{e^{x_k}}{\sum_{i=1}^n e^{x_i}} \]
and for $j\neq k$
\[ \frac{\partial^2 f}{\partial x_k\partial x_j} = -\frac{e^{x_k}e^{x_j}}{(\sum_{i=1}^n e^{x_i})^2} \]
and 
\[ \frac{\partial^2 f}{\partial x_j^2} = -\frac{e^{2x_k}}{(\sum_{i=1}^n e^{x_i})^2} + \frac{e^{x_k}}{\sum_{i=1}^n e^{x_i}} \]
It is easy to see (?) that $u'\nabla^2f(x) u \geq 0$ for all $u\in \Real^n$.

Further convexity preserving function are 
\begin{itemize}
	\item non-negative weighted sums;
	\item convex function of affine transformation; 
	\item point-wise maximum (supremum): $f(x) = \max_{j=1,\ldots,m}f_j(x)$;
	\item the sum of $r<n$ largest components of a vector $x$:
	\[f(x) = \sum_{k=1}^r x_{(n-k+1)}\]
	since it is reexpressible as a maximum of $C^r_n$ sums of the form $x_{i_1} + \ldots + x_{i_r}$;
	\item If $f(x,y)$ is convex in $y$ for all $x\in A$, then $g(y) = \sup_{x\in A} f(x,y)$ is convex;
	\item Concavity is just minus the convexity!
\end{itemize}

% subsubsection convexity (end)

\subsubsection{An optimization problem} % (fold)
\label{ssub:an_optimization_problem}

Consider a minimization of $f_0$ subject to a bunch of constraints:
\begin{itemize}
	\item Minimize $f_0(x)$ with respec to $x\in \Real^n$ subject to:
	\item inequality constraints $f_i(x) \leq 0$ for $i=1,\ldots,m$;
	\item equlaity constraints $h_i(x) = 0$ for $i=1,\ldots,p$.
\end{itemize}
this is the ``standard form'' of an optimization problem.
The function $f_0:\Real^n \to \Real$ is the objective function, the functions
$f_i:\Real^n\to\Real$ are inequality constraint functions and $h_i:\Real^n\to\Real$
are the equlaity constraint functions.

The optimal value, if one exists, is denoted by $p^*$:
\[p^* = \inf\{ f_0(x) \rvert f_i(x) \leq 0, h_j(x) = 0 \}\]
if the problem is infeasible, i.e. there are no $x$ Schwartz that all the constraints
are satisfied, then $p^*=+\infty$. In contrast, if $f_0$ is unbounded below, on the
feasible set then we assume $p^* = -\infty$.

A point $x$ is feasible if $x\in \text{dom}f_0$ and satisfies all constraints. The
feasible set of an optimization problem is the set of all feasible points. A point
$x$ is optimal if $f_0(x) = p^*$, i.e. the optimal value is atatined at $x$.

% subsubsection an_optimization_problem (end)

\subsubsection{Convex optimization problems} % (fold)
\label{ssub:convex_optimization_problems}

By adding certain restrictions on the objective and the constraint functions, one gains
many wonderful properties of the optimal solution.
\[ f_0(x) \to \min_x \]
subject to
\begin{align*}
	f_i(x) &\leq 0 \\
	a_j'x + b = 0
\end{align*}
for all $i=1,\ldots, m$ and $j=1,\ldots,p$.

Linear programming is when $f_0$ and $f_i$ are all affine functions. This subset of
optimization problems is widely studied. The feasible set is a polyhedron on which
an affine $f_0(x) =c'x + d$ is minimized. The minimization is performed by looking
at level sets of $f_0$ and then checking the vertices of the the feasible polyhedron.
A well known theorem from LP states that if one solution exists the it at a vertex,
and if two solutions exist, then there are infinitely many solution.

Quadratic programming is when $f_0$ is quadratic and the feasible set is still a
polyhedron: $f_0(x) = x'Px + q'x + r$ subject to $g_i'x\leq b_i$ and $a_j'x=c$.

% subsubsection convex_optimization_problems (end)

\subsubsection{Duality} % (fold)
\label{ssub:duality}

Consider a general optimization problem:
\[ f_0(x) \to \min_x\]
subject to
\[ f_i(x) \leq 0\,\text{and}\, h_j(x) = 0\]
For $x\in D$ where 
\[D = (\bigcap_{i=0}^m \text{dom} f_i) \cap (\bigcap_{j=1}^p \text{dom} h_j)\]
consider the Lagrangian $\Lcal: \Real^n\times \Real^m\times\Real^p\to\Real$ defined as:
\[\Lcal(x,\lambda, \nu) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) + \sum_{j=1}^p \nu_j h_j(x)\]
for $\lambda_i\geq 0$. This is a weighted sum of the objective and constraints and
it quite looks like regularization.

The Lagrange dual function $g:\Real^m\times \Real^p\to\Real$ is given by
\[g(\lambda,\nu) = \inf_x \Lcal(x,\lambda,\nu)\]
The Lagrangian $\Lcal$ is affine in $\lambda$ and $\nu$, and since infimum preserves
concavity. And so $g$ is concave whatever $f_i$ and $h_i$ are. Furthermore $g(\lambda,\nu)$
is a lower bound for the optimal value $p^*$.

Indeed, suppose $\tilde{x}$ is a \textbf{feasible} point then
\[ f_0(\tilde{x}) \geq \Lcal(\tilde{x},\lambda,\nu) \]
for all $\lambda\geq0$ and $\nu$. Therefore
\[ f_0(\tilde{x}) \geq \inf_x \Lcal(x,\lambda,\nu) = g(\lambda,\nu)\]
minimizing over all feasible $x$ (denoted by$F$) yields
\[ p^* = \inf_{x\in F} f_0(x) \geq g(\lambda,\nu)\]

The dual problem is finding the ``best'' lower bound: maximize $g(\lambda,\nu)$
subject to $\lambda \geq 0$ to ensure that $g$ provides a lower bound. But there
is absolutely no guarantee that this lower bound is good, even if its the best.

Let $d^*$ be the optimal value of the dual problem and the dual is feasible if
$\lambda\geq 0$ and $(\lambda,\nu)\in\text{dom}g$.

\textbf{Weak duality}: $d^*<p^*$, -- always holds and can be used to find non-trivial
lower bounds.
\textbf{Strong duality}: $d^*=p^*$, -- quite ofetn holds for convex problems.
The duality gap is $p^*-d^*\geq 0$.

What are the sufficient conditions for strong duality? Slater's conditions.
In fact any conditions for strong duality are called ``constraint qualifications''.

The problem 
\[ f_0(x) \to \min_x\,\text{st} f_i(x) \leq 0\,\text{and}\, h_j(x) = 0\]
is strongly dual, if there exists at least one feasible $x\in \text{int}D$
such that $f_i(x) < 0$ and $a_j'x = b_j$ for all $i=1,\ldots,m$. (?) For affine
constraints $f_i(x) = g_i'x - b_i$ and convex problem, the strict inequality is
not necessary.

If $p^*>-\infty$ then $d^*>-\infty$ (dual is feasible) and there exist $\lambda^*$
and $\nu^*$ such that $g(\lambda^*, \nu^*) = p^*$.

Optimality conditions. Suppose that stong duality holds. Denote by $x^*$ the optimal
point of the primal problem, and $(\lambda^*, \nu^*)$ the optimal of the dual problem.
Then the duality gap is zero, whence
\[
f_0(x^*)
= g(\lambda^*,\nu^*)
= \inf_x f_0(x) + \sum_{i=1}^m\lambda_i^* f_i(x) + \sum_{j=1}^p\nu_j^* h_i(x)
\]
Obviously
\[
f_0(x^*)
= g(\lambda^*,\nu^*)
\leq f_0(x^*) + \sum_{i=1}^m\lambda_i^* f_i(x^*) + \sum_{j=1}^p\nu_j^* h_i(x^*)
\]
Since $x^*$ is feasible one has $f_i(x^*)\leq 0$ and $h_i(x^*) = 0$ whence for
$\lambda^*$ and $\nu^*$
\[
f_0(x^*) + \sum_{i=1}^m\lambda_i^* f_i(x^*) + \sum_{j=1}^p\nu_j^* h_i(x^*)
\leq f_0(x^*)
\]
Thus equalities hold and $x^*$ minimizes $\Lcal(x,\lambda^*, \nu^*)$ and
\[
\sum_{i=1}^m\lambda_i^* f_i(x^*) + \sum_{j=1}^p\nu_j^* h_i(x^*) = 0
\]
however since $ \sum_{j=1}^p\nu_j^* h_i(x^*) = 0 $ one arrives at the complementary
slackness conditions:
\[ \sum_{i=1}^m \lambda_i^* f_i(x^*) = 0 \]
which implies that $\lambda_i^* f_i(x^*) = 0$ for all $i=1,\ldots,m$.

\textbf{K}arush-\textbf{K}uhn-\textbf{T}ucker conditions:
\begin{itemize}
	\item primal constraints: $f_i(x)\leq 0$ for $i=1,\ldots,m$ and $h_j(x) = 0$ for $j=1,\ldots,p$;
	\item dual constraints: $\lambda_i \geq 0$ to ensure that $g\leq p^*$;
	\item Complementary slackness: $\lambda_i f)i(x) = 0$ for $i=1,\ldots,m$;
	\item Gradient of $\Lcal$ vanishes: $\nabla_x \Lcal(x) = 0$.
	\[\nabla_x \Lcal = \nabla f_0(x) + \sum_{i=1}^m \lambda_i \nabla f_i(x) + \sum_{j=1}^p \nu_j \nabla h_j(x)\]
\end{itemize}
Summary, for any optimization problem if $x^*$ and $(\lambda^*,\nu^*)$ are such that
$f_0(x^*) = g(\lambda^*,\nu^*)$ then $(x^*,\lambda^*,\nu^*)$ satisfy the \textbf{KKT}
conditions (necessary conditions). For a convex optimization problem the KKT are also
sufficient.

Proof for the convex case. Sufficiency.
Suppose $(x^*,\lambda^*,\nu^*)$ satisfy the KKT. Then $x^*$ is feasible.
For any feasible
\[
\Lcal(x,\lambda^*,\nu^*)
= f_0(x) + \sum_{i=1}^m\lambda_i^* f_i(x) + \sum_{j=1}^p\nu_j^* h_i(x)
= f_0(x) + \sum_{i=1}^m\lambda_i^* f_i(x)
\]
which is a non-negative weighted $(\lambda_i^*)_{i=1}^m$ sum of convex functions
$f_i$. Therefore $\Lcal(x,\lambda^*,\nu^*)$ is convex. Whence the last KKT condition implies
that $x^*$ is the minimizer and $g(\lambda^*,\nu^*) = \Lcal(x^*,\lambda^*,\nu^*)$, and
\[
d^*
= g(\lambda^*,\nu^*)
= f_0(x^*) + \sum_{i=1}^m\lambda_i^* f_i(x^*) + \sum_{j=1}^p\nu_j^* h_i(x^*)
= f_0(x^*)
= p^*
\]
due to complementary slackness conditions.

In the case of linear programming if either primal of dual are feasible, then strong duality
holds.

% subsubsection duality (end)

% subsection elements_of_convex_optimization (end)

% section lecture_15 (end)

\section{Lecture \# 16} % (fold)
\label{sec:lecture_16}

Talking first about the SVM before introducing the concept of Kernels.

\subsection{SVM (contd.)} % (fold)
\label{sub:svm_contd}

Consider a standard two-class classification problem on the training data
$(x_i,t_i)_{i=1}^n$ with $\pm1$ notation of class labels: $t_i\in\{-1,+1\}$.
We are looking at hyperplane discriminant function given by:
\[y(x) = x'\beta + \beta_0\]
in the hope that the classes would be at least well separated.
The perceptron is an example of such a separation classifier, provided the 
training data is linearly separable.

The problem with a separating hyperplane is that there are infinitely many
solutions, which is where the \textbf{S}upport \textbf{V}ector \textbf{M}achine
come into picture.

\subsubsection{Linearly separable case} % (fold)
\label{ssub:linearly_separable_case}

For starter consider a linearly separating case. We want to find the best hyperplane
in the sense that the margin of the hyperplane is the largest. This would enable a
certain degree of robustness to the classifier with respect to the variance of
the data.

So the goal is to maximize the margin $M$ the distance to the closes data point
with respect to $\beta,\beta_0$ and subject to $\|\beta\|=1$ (Euclidean norm).
The margin, or the signed distance to some datapoint $x$ is given by
\[ \frac{y(x)}{\|\beta\|} \]
which under the given restrictions reduces to
\[ t_i(x_i'\beta + \beta_0) \geq M \]
for all $i=1,\ldots,n$.

Therefore the problem becomes $ M \to \max_{\beta, \beta_0}$ subject to
$\sum_{j=1}^p \beta_j^2 = 1$ and $t_i( x_i'\beta+\beta_0 )\geq M$.
The restrictions transform into
\[ t_i( x_i'\frac{\beta}{M}+\frac{\beta_0}{M} )\geq 1 \]
Denoting $\gamma = \frac{\beta}{M}$ and $\gamma_0 = \frac{\beta_0}{M}$,
note that $M = \|\gamma\|^{-1}$, whence the final SVM problem becomes
\[
\frac{1}{2}\|\beta\|^2 \to \min_{\beta_0,\beta}
\text{ s.t. }
t_i(\beta_0+x_i'\beta)\geq 1\,\forall i=1,\ldots,n
\]

Now $x'\beta = p\|\beta\|$, for a projection $p$ of $x$ onto $\beta$. If $\beta_0=0$
then the restrictions reduce to $t_i p_i\|\beta\|\geq 1$. This means that if projections
are large, then the SVM could have more room to decrease the norm of $\beta$.

Employ the convex optimization. The primal problem's Lagrangian id given by
\[
\Lcal(\beta_0, \beta, \lambda)
= \frac{1}{2} \|\beta\|^2 + \sum_{i=1}^n \lambda_i \bigl(1 - t_i(x_i'\beta+\beta_0) \bigr)
\]
The Lagrange dual is given by
\[
g(\lambda) = \inf_{\beta_0,\beta} L(\beta_0, \beta, \lambda)
\]
by weak duality $g(\lambda)\leq \frac{1}{2}\|\beta^*\|^2$, where $\beta^*$ is the
optimal parameter.

Linear separability implies that it is feasible, and the objective function is
convex and thee restrictions are linear, -- use Slater's constraint requirements.
Thus by strong duality $\sup_\lambda g(\lambda) = \frac{1}{2} \|\beta^*\|$.
Thus the triple $(\beta_0^*,\beta^*,\lambda^*)$ satisfies the KKT conditions:\begin{itemize}
	\item Primal constraints: $t_i(x_i'\beta^*+\beta_0)\geq1$;
	\item Dual constraints: $\lambda^*_i\geq 0$;
	\item Complementary slackness: $\lambda^*_i \bigl( 1 - t_i(x_i'\beta^*+\beta_0) \bigr) = 0$;
	\item The gradient vanishes:
	\[ \frac{\partial \Lcal}{\partial \beta_0} = - \sum_{i=1}^n \lambda_i t_i = 0 \]
	and
	\[ \frac{\partial \Lcal}{\partial \beta} = \beta - \sum_{i=1}^n \lambda_i t_i x_i = 0 \]
\end{itemize}
the optimal parameter $\beta$, given $\lambda$, is $\beta^*(\lambda) = \sum_{i=1}^n \lambda_i t_i x_i$.
The Lagrange dual is given by
\[
g(\lambda)
= \frac{1}{2}\|\beta(\lambda)\|^2
+ \sum_{i=1}^n \lambda_i
- \sum_{i=1}^n \lambda_i t_i x_i'\beta^*(\lambda)
- \sum_{i=1}^n \lambda_i t_i \beta_0
\]
whenever by the first part of the gradient vanishing requirement
\[
g(\lambda)
= \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_j \lambda_i t_j t_i x_i'x_j
- \sum_{i=1}^n \sum_{j=1}^n \lambda_j \lambda_i t_j t_i x_i' x_j
+ \sum_{i=1}^n \lambda_i
\]
Note that the expression $t_j t_i x_i' x_j$ is nothing but the inner product
in the Euclidean $\Real^p$ space: $\langle t_i x_i, t_j x_j\rangle$.

Let $H_{ij}$ be the inner product of the $i$-th and $j$-th data point. In the
matrix form the Lagrangian dual is represented by
\[
g(\lambda) = \one' \lambda - \frac{1}{2} \lambda'H\lambda
\]
Thus the final dual problem becomes
\[ \one' \lambda - \frac{1}{2} \lambda'H\lambda \to \max_{\lambda}\]
subject to 
\[ \lambda\geq 0 \text{ and } \lambda't = 0 \]
where the last constraint is implicit, and comes from the $4$-th KKT condition.
This problem is still convex and linear, and returns the optimal $\lambda^*$,
which permits the computation of the $\beta^* = \beta^*(\lambda^*)$.

The complementary slackness conditions allow one to get insight into the nature
of the optimal solution. The product
\[\lambda_i \bigl(1-t_i(x_i'\beta^*+\beta^*_0)\bigr) = 0\]
If $\lambda^*_i >0$ then $t_i ( x_i'\beta^*+\beta^*_0) = 1$. Such datapoints $i$
with $\lambda^*_i>0$ necessarily lie on the margin and are known as the \textbf{support}
\textbf{vectors}. The points that are far away from the margin, i.e. $t_i y(x_i)>0$
must have $\lambda^*_i=0$. The fact that only a few of datapoints have $\lambda^*_i>0$
gives the so called sparse solution.

To compute the $\beta^*_0$ use the support vectors. Indeed for all $i$ with
$\lambda^*_i>0$ one has $t_i x_i'\beta^* + t_i\beta^*_0 = 1$, whence
\[ 
x_i'\beta^* + t_i\beta^*_0 - t_i 
= \sum_{j=1}^n \lambda^*_j t_j x_i'x_j + \beta^*_0 - t_i
\]
since $t_i = \pm 1$. Thus 
\[
\beta^*_0 = t_i - \sum_{j:\lambda^*_i>0} \lambda^*_j t_j x_i'x_j
\]
In practice, for numerical stability one uses the average of this expression
over all support vectors:

\[
\beta^*_0
= \frac{1}{|\text{SV}|}\sum_{i\in \text{SV}}\Bigl(
	t_i - \sum_{j\in \text{SV}} \lambda^*_j t_j x_i'x_j
\Bigr)
\]
where $\text{SV} = \{i=1,\ldots,n| \lambda^*_i>0\}$ is the set of support vectors.

What is the optimal margin? recall the complementary slackness conditions
\[
t_i \beta^*_0 + \Bigl(\sum_{j=1}^n \lambda^*_j t_jx_j\Bigr)' t_i x_i =1
\]
which yields
\[
\sum_{j=1}^n \lambda^*_j t_i t_j x_j'x_i = 1 - t_i\beta^*_0
\]
The margin is given by
\begin{align*}
	\|\beta^*\|^2
	&= \sum_{i=1}^n \sum_{j=1}^n \lambda_j^* \lambda_i^* t_j t_i x_i'x_j \\
	&= \sum_{i=1}^n \lambda_i^* \sum_{j=1}^n \lambda_j^* t_j t_i x_i'x_j \\
	&= \sum_{i=1}^n \lambda_i^* ( 1 - t_i\beta^*_0 ) \\
	&= \sum_{i=1}^n \lambda_i^*
\end{align*}

% subsubsection linearly_separable_case (end)

\subsubsection{Linearly non-separable case} % (fold)
\label{ssub:linearly_non_separable_case}

In the non-separable case the problem collapses, since there might not even be
a solution. In order to tackle this case, allow a certain slackness in the classification,
for extra flexibility. Let $\xi_i\geq0$ be the slack variable for a data point $i$.
Each $\xi_i$ can be seen as a penalty for misclassification.

In the linearly separable case $t_i y(x_i) > 0$ for some separating hyperplane $y(\cdot)$.
Thus instead of the constraint $t_i y(x_i)\geq 1$ consider
\[ t_i y(x_i)\geq 1-\xi_i \]
The idea is that for correctly classified point there is no need for ``help'' from
the newly introduced slack variables. Therefore \begin{itemize}
	\item $\xi_i = 0$ for points on the correct side of the margin;
	\item $\xi_i\in(0,1]$ for correctly classified points on the wrong side of the margin;
	\item $\xi_i = 1$ for the points on the separating hyperplane;
	\item $\xi_k > 1$ for datapoints that are misclassified.
\end{itemize}

Thus the goal is to maximize the margin as before while ``soft-penalizing'' the points
on the wrong side of the plane. The objective is to make the margin as large as possible
by minimizing
\[ \frac{1}{2} \|\beta\|^2 + C \sum_{i=1}^n \xi_i \]
with respect to the original classification constraints.

If $C\to\infty$ then the margin is smaller and the problem converges to a linearly
separable case. For a large $C$ the margin is narrow, and for a small $C$ the margin
is wide. 

The primal problem thus becomes
\[ \frac{1}{2} \|\beta\|^2 + C \sum_{i=1}^n \xi_i \to \min_{\beta, \beta_0,(\xi_i)}\]
subject to 
\[ t_i y(x_i) \geq 1-\xi_i \text{ and } \xi_i\geq 0 \]
for all $i=1,\ldots,n$.

The Lagrangian is given by
\begin{align*}
	\Lcal(\beta_0,\beta,\xi,\lambda,\mu)
	&= \frac{1}{2} \|\beta\|^2 + C \sum_{i=1}^n \xi_i \\
	&+ \sum_{i=1}^n \lambda_i \Bigl( 1 - \xi_i - t_i ( x_i'\beta + \beta_0 ) \Bigr)
	+ \sum_{i=1}^n \mu_i ( - \xi_i )\,.
\end{align*}
The KKT condition are \begin{itemize}
	\item $t_i y(x_i) \geq 1-\xi_i$ and $\xi_i\geq 0$;
	\item $\lambda_i,\mu_i \geq 0$;
	\item $\lambda_i \Bigl( 1 - \xi_i - t_i ( x_i'\beta + \beta_0 ) \Bigr) = 0$ and $\mu_i ( - \xi_i ) = 0$;
	\item the first-order conditions are \begin{description}
		\item[$\beta_0$] $ -\sum_{i=1}^n \lambda_i t_i = 0$;
		\item[$\beta$] $ \beta - \sum_{i=1}^n \lambda_i t_i x_i = 0$;
		\item[$\xi$] $ C - \lambda_i - \mu_i = 0$.
	\end{description}
\end{itemize}
The Lagrange dual is thus
\begin{align*}
	g(\lambda,\mu)
	&= \frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n \lambda_i\lambda_j t_i t_j x_i'x_j
		+ C \sum_{i=1}^n \xi_i
		- \sum_{i=1}^n\sum_{j=1}^n \lambda_i\lambda_j t_i t_j x_i'x_j \\
	&- \sum_{i=1}^n \lambda_i t_i \beta_0
		+ \sum_{i=1}^n \lambda_i
		- \sum_{i=1}^n \lambda_i \xi_i
		- \sum_{i=1}^n \mu_i \xi_i
\end{align*}
whence the FOC imply
\[
g(\lambda,\mu) =
- \frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n \lambda_i\lambda_j t_i t_j x_i'x_j
+ \sum_{i=1}^n \lambda_i
\]
Furthermore since $\mu_i \geq 0$, one has $\lambda_i\leq C$. Thus the dual problem
becomes: maximize
\[ \one'\lambda - \frac{1}{2}\lambda'H\lambda \to \max_\lambda \]
subject to $0\leq \lambda\leq C$ -- the box constraint and $\lambda't = 0$.

The solution is characterized as follows
\begin{description}
	\item[$\lambda^*_i = 0$] Then $\mu^*_i = C$, $\xi^*_i = 0$ and $t_i y(x_i) > 1-0$;
	\item[$\lambda^*_i > 0$] Then $t_i y(x_i) = 1 - \xi^*_i$ and $\xi^*_i \geq 0$.
	If $\lambda^*_i < C$ then $\mu^*_i > 0$, whence $\xi^*_i = 0$ and $t_i y(x_i) = 1$ -- a
	support vector.
	\item[$\lambda^*_i = C$] Then $t_i y(x_i) = 1 - \xi^*_i$, $\mu^*_i=0$ and the
	point is either inside the margin and correctly classified $\xi^*_i \in (0,1]$, or
	the point is misclassified and $\xi^*_i>1$.
\end{description}

To get $\beta_0$ one once again, may pick a point $i$ lying on the margin
and compute 
\[ \beta^*_0 = t_i - \sum_{j\in\text{SVE}} \lambda^*_j t_j x_i'x_j \]
where $\text{SVE} = \{i:\lambda^*_i > 0 \text{ and } \xi^*_i = 0\}$.
Is there a guarantee that there necessarily exists $i$ with $\xi^* = 0$ and
$\lambda^*_i>0$.

% subsubsection linearly_non_separable_case (end)

\subsubsection{Relation between the SVM and the Logistic regression} % (fold)
\label{ssub:relation_between_the_svm_and_the_logistic_regression}

Let's rewrite the SVM problem in a penalized regression form.

Points that are on the correct side of the margin have $t_i (x_i) \geq 1$ and $\xi_i = 0$.
Remaining points have $\xi_i = 1-t_i y(x_i)$. Thus the objective function looks
like
\[ \sum_{j=1}^n \bigl(1-t_j y(x_j)\bigr)_+ + \lambda \|\beta\|^2 \]
which is a penalized risk minimization problem. The $(\cdot)_+$ is the ``Hinge error''.

Hard margin problem
\[ \sum_{i=1}^n E_\infty(t_i y(x_i) - 1 ) + \lambda \|\beta\|^2\]
where $E_\infty(x) = \infty 1_{x\geq 0}$.

% subsubsection relation_between_the_svm_and_the_logistic_regression (end)

% subsection svm_contd (end)

% section lecture_16 (end)

\section{Lecture \# 17} % (fold)
\label{sec:lecture_17}

\subsection{Reproducing Kernel Hilbert Spaces} % (fold)
\label{sub:reproducing_kernel_hilbert_spaces}

Suppose we have a binary classification problem. THe data is $(x_i, t_i)_{i=1}^n$
with $t_i\in \{0,1\}$. Now, for some new $x'$ one is interested in inferring its label $t'$.
Basically this is determined by how similar a new $x'$ is to the $x$ from the training
sample.

A similarity kernel is defined as map $K:X\times X\to \Real$. The similarity kernel
must be symmetric and have other conditions.

In the Euclidean space similarity kernel might be defined as the inner product
(the dot product). Introducing an inner product to a linear space automatically
creates notions of angles, norms and distances.

The idea is to enlarge the feature space to include more features.

We are going to start with some abstract space $X$ and then consider some transform of
the input data $\Phi:X\to \Hcal$, where $\Hcal$ is the new feature space with more
dimensions than $X$. For example $\Phi$ maps some finite-dimensional space $X$ into
an infinite dimensional space $\Hcal$. Also recall the cubic splines and the NCS.

We are going to look for functions that have really nice features, but still the
solutions would be finite dimensional.

The goal is to see how $\Phi$ is related to $K$ and $\Hcal$ to $X$.

% subsection reproducing_kernel_hilbert_spaces (end)

\subsection{Kernels} % (fold)
\label{sub:kernels}

A Hilbert space is a complete vector space of the field $\Real$. Completeness is
with respect to the metric induced by the inner product's norm (norm topology).

A function $K:X\times X\to\Real$ is called a kernel if there exists a Hilbert space
$\Hcal$ and a mapping $\Phi:X\to\Hcal$ such that
\[ K(x,y) = \bigl\langle \Phi(x), \Phi(y) \bigr\rangle_\Hcal \]
i.e. to the inner product in $\Hcal$.

Thus this allows efficient evaluation of inner products in $\Hcal$ through the kernel.

% subsection kernels (end)

\subsection{Properties of kernels} % (fold)
\label{sub:properties_of_kernels}

Given kernels $K_1(x,y)$ and $K_2(x,y)$, the following bivariate function are kernels\begin{itemize}
	\item $K(x,y) = c K_1(x,y)$;
	\item $K(x,y) = f(x) K_1(x,y) f(y)$ where $f$ is any function;
	\item $K(x,y) = q( K_1(x,y) )$, provided $q$ is polynomial with positive coefficients;
	\item $K(x,y) = \text{exp}\{ K_1(x,y) \}$;
	\item $K(x,y) = K_1(x,y) + K_2(x,y)$;
	\item $K(x,y) = K_1(x,y) \cdot K_2(x,y)$;
\end{itemize}
The first one is simple, since the exists a Hilbert space $\Hcal$ and a mapping
$\Phi:X\to \Hcal$ such that
\[ K(x,y) = \bigl\langle \Phi(x), \Phi(y) \bigr\rangle_\Hcal \]
whence $\Psi(x) = \sqrt{c} \Phi(x)$ does the trick for the same Hilbert space.

% subsection properties_of_kernels (end)

\subsubsection{Examples} % (fold)
\label{ssub:examples}

Polynomial kernels. Let $X=\Real^n$ equipped with Euclidean inner product. Put
\[ K(x,y) = \bigl(1+\langle x, y \rangle \bigr)^m \]
it is not clear as to why this is a kernel.

Step 1. Consider a simpler problem: $K(x,y) = \langle x, y \rangle^m$. The dot product
can be expanded into
\[
\langle x, y \rangle^m
= \Bigl(\sum_{i=1}^n x_i y_i\Bigr)^m
= \sum_{j_1+\ldots+j_m = m} \frac{m!}{j_1!\cdots j_m!} (x_1 y_1)^{j_1}\cdots (x_n y_n)^{j_n}
\]
This expands to
\[
\ldots 
= \sum_{j_1+\ldots+j_m = m} \sqrt{\frac{m!}{j_1!\cdots j_m!}} x_1^{j_1}\cdots x_n^{j_n} \sqrt{\frac{m!}{j_1!\cdots j_m!}} y_1^{j_1}\cdots y_n^{j_n}
= \sum_{j_1+\ldots+j_m = m} \phi_j(x) \phi_j(x)
\]
Stacking all $\phi_j(\cdot)$ atop one another for $j=\in J = \{(j_l)_{l=1}^m \in \mathbb{N}:\sum_{l=1}^m j_l = m\}$
one gets a vector $\Phi(x) = (\phi_j(x))_{j\in J}$ in space $\Real^J$ which is
complete and endowed with an inner product.

Now, for $n=m=2$ one has:
\begin{align*}
	K(x,y)
	&= (1 + \langle x, y \rangle)^2 \\
	&= (1 + x_1 y_1 + x_2 y_2 )^2 \\
	&= 1 + x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 y_1 + 2 x_2 y_2 + 2 x_1 y_1 x_2 y_2\\
	&= ( 1\, x_1\, x_2\, \sqrt{x_1}\, \sqrt{x_1}\, 2x_1x_2\, )' (\ldots y \ldots)
\end{align*}
Thus in this case the Hilbert space is of all monomials of order $\leq 2$. This gives
us intuition that in the general case of $K(x,y)$ one gets all monomials of order $\leq m$.

Gaussian kernel is defined as
\[ K(x,y) = \text{exp}\bigl\{ \frac{1}{\eta} \|x-y\|^2\bigr\} \]
for $x,y\in \Real^n$. Using the bilinearity of the inner product one obtains
\[  K(x,y) = e^{-\frac{1}{2}\|x\|^2} e^{-\frac{1}{2}\|y\|^2} e^{ \frac{2}{\eta}\langle x, y \rangle } \]
Expanding the exponential as infinite series:
\[  K(x,y) = e^{-\frac{1}{2}\|x\|^2} e^{-\frac{1}{2}\|y\|^2} \sum_{k\geq0} \frac{2^k}{k! \eta^k} \langle x, y \rangle^k \]
whence
\[
K(x,y) = e^{-\frac{1}{2}\|x\|^2} e^{-\frac{1}{2}\|y\|^2}
\sum_{k\geq0} \sum_{j_1+\ldots+j_n = k} \frac{2^k}{k! \eta^k} x_1^{j_1}\cdots x_n^{j_n} y_1^{j_1}\cdots y_n^{j_n}
\]
and
\[
K(x,y)
= e^{-\frac{1}{2}\|x\|^2} e^{-\frac{1}{2}\|y\|^2}
\sum_{j_1+\ldots+j_n \geq 0} \frac{2^{j_1+\ldots+j_n}}{(j_1+\ldots+j_n)! \eta^{j_1+\ldots+j_n}}
x_1^{j_1}\cdots x_n^{j_n} y_1^{j_1}\cdots y_n^{j_n}
\]
Putting 
\[
\phi_j(x) = \sqrt{ \frac{2^{j_1+\ldots+j_n}}{(j_1+\ldots+j_n)! \eta^{j_1+\ldots+j_n}} } x_1^{j_1}\cdots x_n^{j_n}
\]
where $j = (j_l)_{l=1}^n$ -- a multiindex.

% subsubsection examples (end)

\subsection{Theorem} % (fold)
\label{sub:theorem}

A function $K:X\times X \to \Real$ is a kernel if and only if it is symmetric and
positive-definite. By positive-definiteness one means: for all $n\geq1$
\[ \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j K(x_i, x_j) \geq 0 \]
for all $x \in X$ and $\lambda$ in the ... (?).

$\Rightarrow$ If $K$ is a kernel, then there exists a Hilbert space $\Hcal$ and
a mapping $\Phi:X\to \Hcal$ such that $K(\cdot, \cdot) = \langle \Phi(\cdot), \Phi(\cdot)\rangle_\Hcal$.

Consider some $n\geq1$ and some $(x_i)_{i=1}^n \in X$:
\begin{align*}
	\sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j K(x_i, x_j) 
	&= \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j \langle \Phi(x_i), \Phi(x_j)\rangle_\Hcal 
	= \sum_{i=1}^n \sum_{j=1}^n \langle \lambda_i \Phi(x_i), \lambda_j \Phi(x_j)\rangle_\Hcal \\
	&= \Bigl\langle \sum_{i=1}^n \lambda_i \Phi(x_i), \sum_{j=1}^n \lambda_j \Phi(x_j)\Bigr\rangle_\Hcal
	= \| \sum_{i=1}^n \lambda_i \Phi(x_i)\|_\Hcal
	\geq 0
\end{align*}

$\Leftarrow$ Consider a symmetric and positive-definite function $K:X\times X\to \Real$.
We want to find a Hilbert space and a mapping that meet the required criteria.

Consider the space of all functions that are finite linear combinations
\[
\Hcal_0 = \bigl\{ f : X \to \Real :
f = \sum_{i=1}^n \alpha_i K(\cdot, x_i), n\geq1,
(x_i)_{i=1}^n\in X, (\alpha_i)_{i=1}^n\in \Real \bigr\}
\]
This is not yet a Hilbert space, since it should be equipped with an inner product,
and proven to be complete.

Let $f,g\in \Hcal_0$
\[
\langle f, g \rangle_{\Hcal_0} = \sum_{i,j} \alpha_i \gamma_j K(x_i, y_j)
\]
for $f = \sum_{i=1}^n \alpha_i K(\cdot, x_i)$ and $g = \sum_{j=1}^m \alpha_j K(\cdot, y_i)$

Is this well defined? Yes! Indeed,
\[
\sum_{i,j} \alpha_i \gamma_j K(x_i, y_j)
= \sum_j \gamma_j \sum_i \alpha_i K(x_i, y_j)
= \sum_j \gamma_j f(y_j)
= \sum_i \alpha_i g(x_i)
\]
This ``inner product'' is symmetric by definition and is bilinear, using the
same argument as above. Furthermore, $\langle f, g \rangle_{\Hcal_0}$ is a
positive definite kernel on $\Hcal_0$.

Necessarily for any $h\in \Hcal_0$ one has by assumption on $K$
\[
\langle h, h \rangle_{\Hcal_0}
= \sum_{i,j} \lambda_i \lambda_j K(x_i,x_j)
\geq 0
\]
For any $(h_j)_{j=1}^m\in \Hcal_0$ and $(\gamma_j)_{j=1}^m\in \Real$ by bilinearity one has
\[
\sum_{i,j} \gamma_i \gamma_j \langle h_i, h_j \rangle_{\Hcal_0}
= \langle \sum_i \gamma_i h_i, \sum_j \gamma_j h_j \rangle \geq 0
\]

Now any kernel satisfies Cauchy-Schwartz-Bunyakovsky inequality:
\[ |K(x,y)|^2 \leq K(x,x) K(y,y) \]
Consider a Gram matrix
\[
\begin{pmatrix}
	K(x_1,x_1) & K(x_1,x_2) \\
	K(x_2,x_1) & K(x_2,x_2) 
\end{pmatrix}
\]
which implies that $K_{22}K_{11} - K^2_{12}\geq 0$, proving the inequality (?).

Now if $\langle h, h \rangle_{\Hcal_0} = 0$ for some $h\in \Hcal_0$, then $h=0$.
Indeed, if $h(x) = \sum_{i=1}^n \lambda_i K(x,x_i)$, then
\[ f(x) = \langle f, K(\cdot,x) \rangle_{\Hcal_0} \]
by definition (?).

In the $L^2$ one has
\[ f(x) = \int \delta_x(y) f(y) dy = \langle \delta_x(\cdot), f(\cdot)\rangle\]
which reproduces $f$, even though $\delta_x \notin L^2$.

Suppose $f(\cdot) = K(\cdot,y)$ for some $y\in X$, then
\[
K(x,y) = f(x)
= \langle f, K(\cdot, x) \rangle_{\Hcal_0}
= \langle K(\cdot, y), K(\cdot, x) \rangle_{\Hcal_0}
= \langle K(\cdot, x), K(\cdot, y) \rangle_{\Hcal_0}
\]
this is the so-called reproducibility property of $K$. In fact this gives us a mapping
$X\to \Hcal_0$.

Now for all $x\in X$:
\[
|f(x)| = |\langle f, K(\cdot, x) \rangle_{\Hcal_0}|^2
\leq |\langle K(\cdot, x), K(\cdot, y) \rangle_{\Hcal_0}| |\langle f, f \rangle_{\Hcal_0}|
\leq K(x,x) |\langle f, f \rangle_{\Hcal_0}|
\]
whence $f(x) = 0$.

Thus currently $\Hcal_0$ is pre-Hilbert space (just an inner product space) with inner
product $\langle\cdot, \cdot\rangle_{\Hcal_0}$ with
\[ f(x) = \langle f, K(\cdot, x)\rangle_{\Hcal_0}\]
and 
\[ K(x,y) = \langle K(\cdot, x),K(\cdot, y)\rangle_{\Hcal_0}\]

Completing the $\Hcal_0$ is done by adding all the limiting points of Cauchy sequences
(pointwise) and extending the inner product.

For any $f,g\in \Hcal$ define
\[ \langle f,g \rangle = \lim_{n\to \infty} \langle f_n, g_n\rangle_{\Hcal_0} \]
where $(f_n)_{n\geq1}$ and $(f_n)_{n\geq1}$ are Cauchy sequences in $\Hcal_0$ converging
to $f$ and $g$. The following question arise: \begin{itemize}
	\item Does the limit exist? 
	\item Does $\langle f,g \rangle$ depend on the sequences chosen?
	\item Is $\langle \cdot, \cdot \rangle$ an inner product?
	\item Is $\Hcal_0$ dense in $\Hcal$?
	\item Is $\Hcal$ is a Hilbert space?
	\item Does $K$ defined in $\Hcal_0$ have the reproducible property in$\Hcal$?
\end{itemize}

The first lemma: for any $f,g\in \Hcal$ and $x\in X$
\[ |f(x) - g(x)| \leq C \|f-g\|_{\Hcal_0} \]
Now $\delta_x f = f(x)$ the evaluation functional which is linear. It is continuous therefore bounded over the space $\Hcal_0$. 
\[ |f(x) - g(x)| = |\delta_x(f-g)| \leq C\|f-g\|_{\Hcal_0} \]
How one has to show that $\delta_x$ is indeed linear and continuous (bounded). (?)

The second lemma:
let $(f_n)_{n\ge1}$ be a Cauchy sequence in $\Hcal_0$ that converges to $0$. Then
\[\|f_n\|_{\Hcal_0} \to 0\]

Because $f_n$ is Cauchy, it must be bounded. Let $A>0$ then $\|f_n\|_{\Hcal_0} < A$ for all $n\geq1$.

Let $\epsilon>0$. Then there exists $N_1\geq 1$ such that
\[ \|f_n - f_m\|_{\Hcal_0} < \frac{\epsilon}{2A}\] 
for all $n,m\geq N_1$. Put $f_{N_1} = \sum_{i=1}^k \alpha_i K(\cdot, x_i)$.
Since $(f_n)$ converges pointwise to $0$ then there is $N_2\geq 1$ such that
\[ |f_n(x_i)| |< \frac{\epsilon}{2k|\alpha_i|} \]

Thus for $n\geq N_1\wedge N_2$ on gets
\[
\|f_n\|^2_{\Hcal_0} \leq | \langle f_n -f_{N_1}, f_n\rangle_{\Hcal_0}|
+ | \langle f_{N_1}, f_n\rangle_{\Hcal_0}|
\leq \|f_n\|_{\Hcal_0} \|f_n-f_{N_1}\|_{\Hcal_0} + \sum_{i=1}^k |\alpha_i| |f_n(x)|
\]
whence the convergence in norms follows.

If $f_n\to f$ and $g_n\to g$ pointwise, then $\alpha_n = \langle f_n, g_n\rangle_{\Hcal_0}$.
Consider
\[
|\alpha_n - \alpha_m | 
= |\langle f_n, g_n\rangle_{\Hcal_0}
- \langle f_m, g_m\rangle_{\Hcal_0} |
\]
whence
\begin{align*}
	| \alpha_n - \alpha_m | 
	&= |\langle f_n-f_m, g_n\rangle_{\Hcal_0}
	- \langle f_m, g_n-g_m\rangle_{\Hcal_0} | \\
	&\leq |\langle f_n-f_m, g_n\rangle_{\Hcal_0}|
	+ |\langle f_m, g_n-g_m\rangle_{\Hcal_0} | \\
	&\leq \|f_n-f_m\|_{\Hcal_0} \|g_n\|_{\Hcal_0}
	+ \|f_m\|_{\Hcal_0} \|g_n-g_m\|_{\Hcal_0}\,,
\end{align*}
since $\|f_m\|_{\Hcal_0}$ and $\|g_m\|_{\Hcal_0}$ are bounded and $f_n$ and $g_n$ are Cauchy,
one definitely has that $\alpha_n$ is Cauchy.

Lemma: The limit $\alpha = \lim_{n\to \infty} \alpha_n$ is independent of the Cauchy sequences chosen.

Let $f'_n\to f$ and $g'_n\to g$ with $\alpha'_n = \langle f'_n, g'_n\rangle_{\Hcal_0}$, whence
\[
|\alpha_n - \alpha'_n| \leq \|f_n-f'_n\|_{\Hcal_0} \|g_n\|_{\Hcal_0}
+ \|f'_n\|_{\Hcal_0} \|g_n-g'_n\|_{\Hcal_0}
\]
which converges to $0$. Thus the object
\[
\langle \cdot, \cdot \rangle = \lim_{n\to \infty}\langle \cdot,\cdot \rangle_{\Hcal_0}
\]
is well defined.

Reproducibility property of $K$ on $\Hcal$:
\[
\langle f, K(\cdot,x) \rangle
= \lim_{n\to \infty} \langle f_n,K(\cdot, x) \rangle_{\Hcal_0}
= \lim_{n\to \infty} f_n(x)
= f(x)
\]
by pointwise convergence of $f_n$ (?).

% subsection theorem (end)

\subsection{RKHS} % (fold)
\label{sub:rkhs}

Let $X$ be some non-empty set and $\Hcal$ be a HIlbert space of functions $f:X\to\Real$.

The space $\Hcal$ is \textbf{R}eproducible \textbf{K}ernel \textbf{H}ilbert \textbf{S}pace
with the inner product $\langle\cdot, \cdot\rangle_\Hcal$ if there exists $K:X\times X\to \Real$
such that \begin{itemize}
	\item $K$ has the reproducibility property: $f(x) = \langle f, K(\cdot, x)\rangle_\Hcal$;
	\item $K$ spans the whole $\Hcal$ : $\Hcal = [ \{ K(x,\cdot) : x\in X\}]$;
\end{itemize}
An the $K$ is unique (by reproducibility).

An equivalent and more abstract definition.
An RKHS is a Hilbert space of functions $X\to \Real$, such that all evaluation functionals
$\delta_x:\Hcal\to\Real$ with $f\to \delta_x(f) = f(x)$ are continuous.

If $L$ is a linear and continuous functional on $\Hcal$, then by Riesz Representation theorem
there exists $g\in \Hcal$ such that $L(f) = \langle f,g\rangle_\Hcal$ for any $f\in \Hcal$.

Thus for any $x\in X$ there is $g_x\in \Hcal$ such that the evaluation functional is equal
\[ \delta_x(f) = \langle f, g_x\rangle_\Hcal \]
Just put $K(\cdot,x) = g_x(\cdot)$.

% subsection rkhs (end)

\subsection{Riesz representation theorem} % (fold)
\label{sub:riesz_representation_theorem}

Let $\Hcal$ be a Hilbert space over filed $K = \Cplx$ or $\Real$:
\begin{itemize}
	\item $\Hcal$ is a linear space over $K$;
	\item $\Hcal$ is equipped with an inner product $\langle\rangle$;
	\item $\Hcal$ is a complete space with respect to the natural metric:
	a norm-metric induced by the inner product.
\end{itemize}



\subsubsection{Projection: closed convex sets} % (fold)
\label{ssub:closed_convex_projection}

%% Statement:
for any convex set $C\subseteq \Hcal$ and any $x_0\in \Hcal$, there exists a unique
$x^*\in C$ such that 
\[ \|x^*-x_0\| = \inf\{\|x-x_0\|\,:\, x\in C\}\,. \]

%% Proof:
Let $C$ be a closed convex subset of $\Hcal$, i.e. convexity -- for any $x,y\in C$
and all $t\in[0,1]$ one has $t x + (1-t) y \in C$, closedness -- the complement of 
$C$ is open. Let $x_0\in \Hcal$ be arbitrary.

Consider $\delta = \inf\{\|x-x_0\|\,:\,x\in C\}$. Then there exists a sequence
$(x_n)_{n\geq1}\in C$ such that $\|x_n-x_0\|\downarrow \delta$. Note that since
the map $x\mapsto x^2$ is continuous, $\|x_n-x_0\|^2\downarrow \delta^2$.

Pick any $n,m\geq1$. Then by the parallelogram equality
\[ \|x-y\|^2 + \|x+y\|^2 = 2\|x\|^2 + 2\|y\|^2\,, \]
one has
\begin{align*}
	\|x_n-x_m\|^2
	&= 2\|x_n-x_0\|^2 + 2\|x_m-x_0\|^2
	- \|x_n+x_m - 2 x_0\|^2\\
	&= 2\|x_n-x_0\|^2 + 2\|x_m-x_0\|^2
	- 4 \bigl\|(\tfrac{1}{2}x_n+\tfrac{1}{2}x_m) - x_0\bigr\|^2\,.
\end{align*}
Since $C$ is convex it is true that $\tfrac{1}{2}(x_n+x_m)\in C$, whence
\[ \|x_n-x_m\|^2 \leq 2\|x_n-x_0\|^2 + 2\|x_m-x_0\|^2 - 4\delta\,, \]
by definition of $\delta$.

Now for any $\epsilon>0$ there is $N\geq1$ such that for all $n\geq N$
\[ \|x_n-x_0\|^2-\delta^2 < \frac{1}{4} \epsilon\,, \]
which implies that for all $n,m\geq N$
\[
\|x_n-x_m\|^2 < 2 \frac{1}{4} \epsilon + 2 \frac{1}{4} \epsilon = \epsilon \,.
\]

Since the sequence $(x_n)_{n\geq1}$ is Cauchy in $\Hcal$ with respect to natural
metric of this space, there exists $x^*\in \Hcal$ such that $x_n\overset{\Hcal}{\to} x^*$.
As the set $C$ is closed in $\Hcal$ (with respect to the norm topology of $\Hcal$)
the element $x^*\in C$.

Let's check if $\|x^*-x_0\| = \delta$. Since $x^*\in C$ one has $\delta \leq \|x^*-x_0\|$.
Note that the triangle inequality implies for all $x,y\in\Hcal$
\[ \bigl\lvert \|x\| - \|y\| \bigr\rvert \leq \| x-y \|\,, \]
whence
\[
\bigl\lvert \|x^*-x_0\| - \|x_n-x_0\| \bigr\rvert \leq \|x_n-x^*\| \,,
\]
(the first line also proves continuity of the norm $x\mapsto \|\cdot\|$ in $\Hcal$).
Since the limit of a convergent sequence in $K$ is unique (it is Hausdorff) one has
\[  \|x^*-x_0\| = \lim_{n\to \infty} \|x_n-x_0\| = \delta \,. \]

Finally if there existed another $y^*\in C$ such that $\|y^*-x_0\| = \delta$
the parallelogram equality would imply uniqueness:
\[
\|x^*-y^*\|^2
= 2\|x^*-x_0\|^2 +2\|y^*-x_0\|^2
- 4\|\tfrac{1}{2}x^*+\tfrac{1}{2}y^* - x_0\|^2
\leq 2\delta^2 + 2\delta^2 - 4 \delta^2 = 0\,.
\]

Recall that a subspace orthogonal to a given set $C$ in an inner product space $\Hcal$ is
defined as
\[ C^\perp = \Bigl\{ x\in \Hcal\,:\, \forall y\in C\, \langle x,y\rangle = 0 \Bigr\} \,. \]
It is indeed a subspace, even if $C$ is not, because, if $x,y\in C^\perp$ and $\alpha\in K$,
then for all $z\in C$
\begin{align*}
	\langle x+y,z\rangle &= \langle x,z\rangle + \langle y,z\rangle = 0\\
	\langle \alpha x,z\rangle &= \alpha \langle x,z\rangle + \langle y,z\rangle = 0\,.
\end{align*}
Furthermore, if $x\in C^\perp \cap C$ implies that
\[ \langle x,z\rangle = 0 \,, \]
for all $z\in C$, and in particular for $z=x$, which implies that $\|x\|^2 = 0$
and $x = \mathbf{0}$.

% subsubsection closed_convex_projection (end)

\subsubsection{Projection: linear subspace} % (fold)
\label{ssub:projection_linear_subspace}

%% Statement:
If $C$ is a closed linear subspace of $\Hcal$ then for any $x_0\in \Hcal$ there
exist unique $x^*\in C$ and $x^\perp\in C^\perp$ such that $x_0 = x^* + x^\perp$.

%% Proof :
Indeed, any linear subspace is convex and thus using the previous lemma one has
that there is $x^*\in C$ with
\[ \| x_0-x^* \| = \inf\{\|x-x_0\|\,:\, x\in C\} \,. \]
Let $x^\perp = x_0 - x^*$ and note that since $x^*\in C$ and $C$ is a linear space
one has $x^*+\alpha y$ for any $y\in C$ and $\alpha\in \Real$. Therefore 
\[
\| x^\perp \|^2 \leq \| x^* + \alpha y - x_0 \|^2 = \| x^\perp - \alpha y \|^2\,.
\]
Expanding the squared norm on the right hand side as inner products gives \begin{align*}
	\| x^\perp - \alpha y \|^2
	&= \| x^\perp \|^2 + |\alpha|^2 \| y \|^2
	- \alpha \langle y, x^\perp \rangle
	- \overline{\alpha \langle y, x^\perp \rangle} \,.
\end{align*}
Hence
\[
0 \leq |\alpha|^2 \| y \|^2
	- \alpha \langle y, x^\perp \rangle
	- \overline{\alpha \langle y, x^\perp \rangle} \,.
\]
Now since $\alpha$ is arbitrary, one may set it to
\[ \alpha = \frac{\overline{\langle y, x^\perp \rangle}}{\|y\|^2}\,, \]
so that the products with complex $\langle y, x^\perp \rangle$ would reduce to squares
of complex moduli. Indeed, for such carefully chosen $\alpha$ :
\[
0 \leq \frac{\bigl\lvert \langle y, x^\perp \rangle \bigr\rvert^2 }{\|y\|^2 \|y\|^2} \| y \|^2
	- 2 \frac{\bigl\lvert \langle y, x^\perp \rangle \bigr\rvert^2 }{\|y\|^2}\,,
\]
whence the following result follows:
\[
\bigl\lvert \langle y, x^\perp \rangle \bigr\rvert^2 \leq 0 \,.
\]
Therefore for any $y\in C$ it is true that $\langle y, x^\perp \rangle = 0$, which
implies that $x^\perp \in C^\perp$.

If there is another pair $y^*\in C$ and $y^\perp\in C^\perp$ with $y^*+y^\perp = x_0$
then $y^\perp-x^\perp = x^* - y^*$, whence by linearity of both $C$ and $C^\perp$
\[ y^\perp-x^\perp \in C^\perp\,\text{ and }\, y^*-x^*\in C^*\,, \]
implying that $y^\perp-x^\perp = \mathbf{0}$ and $y^*=x^*$.

% subsubsection projection_linear_subspace (end)

\subsubsection{Linear functionals} % (fold)
\label{ssub:linear_functionals}

%% Statement:
A linear functional is continuous if and only if it is bounded.

%% Proof:
If $\lambda$ is a continuous linear functional at $x_0\in\Hcal$, then for any $x\in\Hcal$
and $\epsilon>0$ there is $\delta>0$ such that for all $\|x-y\|<\delta$, setting
$h = x_0 + (x-y)$ gives such a vector that $\|x_0-h\|<\delta$. By linearity of $\lambda$
\[
\lambda(x) - \lambda(y) = \lambda(x_0 + x - y - x_0) = \lambda(x_0) - \lambda(h)\,,
\]
whence
\[
\bigl\lvert \lambda(x) - \lambda(y) \bigr\rvert < \epsilon \,.
\]
Therefore $\lambda$ is continuous everywhere on $\Hcal$.

Now, by continuity at $x_0$ there exists $\eta > 0$ such that for all $x\in \Hcal$
with $\|x_0-x\| \leq \eta$:
\[ \bigl\lvert \lambda(x) - \lambda(x_0) \bigr\rvert \leq 1 \,, \]
whence by linearity of $\lambda$ for such $x$
\[ \bigl\lvert \lambda(x-x_0) \bigr\rvert \leq 1 \,. \]
Since the space $\Hcal$ is linear, for any $x\in \Hcal$ with $\|x\|\leq \eta$ the
above implies
\[ \bigl\lvert \lambda(x) \bigr\rvert \leq 1 \,. \]
Now consider any $x\in \Hcal$ such that $x\neq \mathbf{0}$. Then the vector
\[ z = \frac{\eta}{\|x\|} x \in \Hcal\,, \]
is such that $\|z\|\leq \eta$ and thus
\[ \bigl\lvert \lambda(z) \bigr\rvert \leq 1 \,. \]
Therefore by linearity of $\lambda$ for all $x\in\Hcal$, $x\neq 0$:
\[ | \lambda(x) | \leq \frac{1}{\eta} \|x\| \,, \]
and $\lambda(\mathbf{0}) = 0$ by linearity. Thus a linear functional continuous at
some $x_0\in\Hcal$ is continuous everywhere and is bounded.

Suppose a linear functional is bounded: there exists $M\geq 0$ such that for all
$x\in\Hcal$ :
\[ \bigl\lvert \lambda(x) \bigr\rvert \leq M \|x\| \,. \]
Then for any $x,y\in\Hcal$ the linearity of $\lambda$ implies that
\[
\bigl\lvert \lambda(x) - \lambda(y) \bigr\rvert
= \bigl\lvert \lambda(x - y) \bigr\rvert 
\leq M \|x-y\|\,,
\]
whence continuity of $\lambda$ in $\Hcal$ follows.

% subsubsection linear_functionals (end)

\subsubsection{Riesz theorem} % (fold)
\label{ssub:riesz_theorem}
%% Statement:
For any non-trivial bounded (continuous) linear functional $\lambda:\Hcal\to K$,
there exists a unique $q\in \Hcal$ such that for all $x\in \Hcal$
\[ \lambda(x) = \langle x, q \rangle \,. \]

%% Proof:
Now, consider a continuous linear functional $\lambda:\Hcal \to K$ such that for
some $x\in \Hcal$, $\lambda(x)\neq 0$. The kernel of $\lambda$ is defined as a subset
of $\Hcal$ that is mapped to $0\in K$ :
\[ \text{ker}\lambda = \lambda^{-1}(\{0\}) \,. \]
Since $\lambda$ is continuous $\Hcal\to K$ and $\{0\}$ is closed in $K$, its kernel
must be a closed set in $\Hcal$. Next, linearity of $\lambda$ implies that $\text{ker} \lambda$
is a linear space. Put $C = \text{ker}\lambda $.

Now, since $\lambda(x)\neq 0$ for some $x_0\in \Hcal$, $x_0\notin C$. Thus there
exist unique $x^*\in C$ and $x^\perp \in C^\perp$ with $x_0 = x^*+x^\perp$, and
for $z = \frac{1}{\|x^\perp\|} x^\perp$, $z\in C^\perp$.

For any $\alpha \neq 0$ the multi-linearity of the inner product implies that for
all $x\in \Hcal$ :
\[ \lambda(x) = \frac{\lambda(x)}{\bar{\alpha}} \langle z, \alpha z \rangle \,. \]
But note that for any vector $x\in \Hcal$, the vector
\[ v = x - \frac{\lambda(x)}{\bar{\alpha}} z\,, \]
for a universal constant $\alpha = \overline{\lambda(z)}$ is well defined, since
$z\notin C$, and is such that $\lambda(v) = 0$. Furthermore, since $z\in C^\perp$
and $v\in C$, one has $\langle v, \alpha z \rangle = 0$, whence
\begin{align*}
	\langle x, \alpha z \rangle
	&= \biggl \langle \frac{\lambda(x)}{\bar{\alpha}} z, \alpha z \biggr\rangle
	+ \langle v, \alpha z \rangle \\
	&= \frac{\lambda(x)}{\bar{\alpha}} \langle z, \alpha z \rangle \,.
\end{align*}
Therefore there exists a universal $q\in \Hcal$ given by $ \overline{\lambda(z)} z$, such
that for any $x\in\Hcal$ :
\[ \lambda(x) = \langle x, q \rangle \,. \]
If were is another $h\in \Hcal$ with such property, then for $x=q-h$
\[ \langle q-h, q \rangle =  \langle q-h, h \rangle \,, \]
would imply that $q-h = \mathbf{0}$.

% subsubsection riesz_theorem (end)

% subsection riesz_representation_theorem (end)

% section lecture_17 (end)

\section{Lecture \# 18} % (fold)
\label{sec:lecture_18}

\subsection{RKHS (contd.)} % (fold)
\label{sub:rkhs_contd}
Last week we started with a kernel and constructed a Hilbert space. Another approach
was that RKHS is Hilbert space with all linear functions being continuous.

% subsection rkhs_contd (end)

\subsection{Merser's theorem} % (fold)
\label{sub:merser_s_theorem}
Under some regularity conditions a kernel can be expressed as
\[ K(x,y) = \sum_{j\geq1} \lambda_j \psi_j(x) \psi_j(y)\,, \]
for all $x,y\in X$, and $X$ is compact.

The feature map is given by
\[ \Phi(x) = (\sqrt{\lambda_j} \psi_j(x))_{j\geq 1}\,. \]

Though the kernel is unique, the feature map might be not. What is the Hilbert space
here? $\Hcal$ is the space of all square integrable sequences:
\[
\sum_{j\geq1} \bigr(\sqrt{\lambda_j} \psi_j(x)\bigr)^2 = K(x,x) < \infty\,.
\]

Symmetric matrices there is the spectral decomposition, which ensures that
for any symmetric $S$ the eigenvectors constitute an orthogonal basis and
\[ S = \sum_{j=1}^n \lambda_j e_je_j' \,. \]
The Merser's theorem exposes a similar representation.

Let's introduce an operator $T_k : L_2(X) \to L_2(X) $ defined as 
\[ T_K f = \int K(x,y) f(y) dy \,. \]
then for this operator there is an eigenvalue-eigenfunction decomposition:
\[  T_K \phi_j = \lambda_j \psi_j\,, \]
and 
\[ \int \psi_j(x)\psi_i(x) dx = \delta_{ij}\,. \]
This can be generalized to an arbitrary measure space.

Consider the space $\Hcal$ such that
\[ \Hcal = \{ f \big| f(x) = \sum_{j\geq1} c_j \psi_j(x) \}\,. \]
This is the same RKHS as before, provided that the norm $\|f\|_K<\infty$.
The inner product is defined as
\[
\langle f,g\rangle
= \bigl\langle \sum_{j\geq1}c_j \psi_j, \sum_{j\geq1} d_j \psi_j \bigr\rangle
= \sum_{j\geq1} \frac{c_j d_j}{\lambda_j}\,.
\]
We must ensure that the norm is finite
\[ \sum_{j\geq1} \frac{c_j^2}{\lambda_j} < +\infty\,, \]
It is possible to show that the eigenvalues in the Merser's theorem decrease to $0$,
which means that at least $c_j = o(\sqrt{\lambda_j})$ as $j\to \infty$.

For this space and this inner product $K$ has the reproducibility property:
\begin{align*}
	\langle f, K(\cdot,x)\rangl
	&= \bigl\langle \sum_{j\geq1}c_j \psi_j(\cdot), \sum_{j\geq1} \lambda_j \psi_j(x) \psi_j(\cdot) \bigr\rangle\\
	&= \sum_{j\geq1} \frac{c_j \lambda_j \psi_j(x) }{\lambda_j}\,.
\end{align*}
Hence the scaling by $\lambda_j$.

Show that $K(\cdot, x)\in \Hcal$ :
\[
K(\cdot, x)
= \sum_{j\geq1} \lambda_j \psi_j(x) \psi_j(\cdot)
= \sum_{j\geq1} c_j \psi_j(\cdot)\,,
\]
for $c_j = \lambda_j \psi_j(x)$ for all $j\geq1$. Thus we need to make sure that the norm is finite.
Indeed
\[ \|K(\cdot,x)\| = \sum_{j\geq1} \frac{\lambda_j^2}{\lambda_j} \psi_j(x) \psi_j(x) = K(x,x) \,, \]
and $K(x,x)$ is finite.

Let's show that this recently defined inner product coincides with the one introduced earlier:
\[
\langle f,g\rangle_0 = \sum_{i,j} \alpha_i \gamma_j K(x_i, y_j) \,,
\]
for $f = \sum_{i=1}^n \alpha_i K(\cdot, x_i)$ and $g = \sum_{j=1}^m \alpha_j K(\cdot, y_i)$

First, since $K(\cdot, x)\in \Hcal$, $f,g\in \Hcal$ and thus the newer inner product is defined.
Hence
\begin{align*}
	f(\cdot) &= \sum_{i=1}^n \alpha_i K(\cdot, x_i)\\
	&= \sum_{i=1}^n \alpha_i \sum_{k\geq1} \lambda_k \psi_k(\cdot) \psi_k(x_i)\\
	&= \sum_{k\geq1} \lambda_k \bigl(\sum_{i=1}^n \alpha_i \psi_k(x_i)\bigr) \psi_k(\cdot)\,,
\end{align*}
and similarly with $g$ :
\[
g = \sum_{k\geq1} \lambda_k \bigl(\sum_{j=1}^m \gamma_j \psi_k(x_j)\bigr) \psi_k(\cdot)\,.
\]
Now, for the recent inner product:
\[
\langle f,g\rangle
= \sum_{k\geq1} \lambda_k \bigl(\sum_{j=1}^m \gamma_j \psi_k(x_j)\bigr) \bigl(\sum_{i=1}^n \alpha_i \psi_k(x_i)\bigr)\,,
\]
whence rearranging the internal sums:
\begin{align*}
	\langle f,g\rangle
	&= \sum_{k\geq1} \sum_{j=1}^m \sum_{i=1}^n \alpha_i \gamma_j \lambda_k \psi_k(x_j) \psi_k(x_i) \\
	&= \sum_{j=1}^m \sum_{i=1}^n \alpha_i \gamma_j \sum_{k\geq1} \lambda_k \psi_k(x_j) \psi_k(x_i) \\
	&= \sum_{j=1}^m \sum_{i=1}^n \alpha_i \gamma_j K(x_i,x_j) \\
	&= \langle f,g\rangle_0 \,,
\end{align*}
since the limit of a sum is the sum of limits.

% subsection merser_s_theorem (end)

\subsection{Applications} % (fold)
\label{sub:applications}

Consider some training data: $(x_i, t_i)_{i=1}^n \in X\times \Real$ and let
\[L : \Real \times \Real \to [0,+\infty)\,,\]
be some cost function.
The goal is to find a function $y(\cdot)$ that provides an estimate of $t$ for
each $x$.

Suppose $\Omega:\Real_+\to\Real$ is a strictly increasing function. Let $\Hcal$
be an RKHS with reproducible kernel $K$ on $X$. The goal thus becomes:
\[ y^* = \argmin_y \frac{1}{n} \sum_{i=1}^n L(t_i,y(x_i)) + \Omega \bigl( \|y\|_{\Hcal} \bigr)\,. \]
The representer theorem states that necessarily
\[ y^* = \sum_{i=1}^n \alpha_i K(x,x_i)\,. \]
Though the space $\Hcal$ is infinite dimensional, the solution to the cost optimization
is finite dimensional, even though the RKHS is very rich.

What does the norm of $y$ have to do with smoothing? For an $f\in \Hcal$, it can
be expressed as a linear combination 
\[f = \sum_{j\geq1} \alpha_j \sqrt{\lambda_j}\psi_j\,,\]
whence the norm
\[ \|f\|^2 = \sum_{j\geq1} \alpha_j^2 < +\infty\,,\]
and thus $\alpha_j = o(j^{\tfrac{1}{2}})$ as $j\to \infty$ (fast enough).

For a Gaussian kernel: 
\[K(x,y) = \text{exp}\bigl( -\frac{1}{2} \frac{(x-y)^2}{\sigma^2}\bigr) \,. \]
Then \begin{align*}
	\lambda_j &= \sqrt{\frac{2a}{A}} B^j\\
	\psi_j &= e^{-(c-a) x^2} H_i\bigl(x\sqrt{2c}\bigr)\,,
\end{align*}
for $a^{-1} = 4\sigma^2$, $b^{-1} = 2\sigma^2$, $c = \sqrt{ a^2 + 2ab }$,
$A = a+ b+c$ and $B = \tfrac{b}{A} < 1$. 

At least for the Gaussian kernel, as $j$ increases, the eigenfunction $\psi_j$ oscillates
more. Since $f$ is expressed as
\[ f = \sum_{j\geq1} \alpha_j \sqrt{\lambda_j} \psi_j\,, \]
and $\alpha_j \sqrt{\lambda_j} \to 0$. Thus the functions in the RKHS are quite
smooth and do not oscillate too much. Hence this penalty will favour smoother functions.

\subsubsection{Proof of the representer theorem} % (fold)
\label{ssub:proof_of_the_representer_theorem}

Consider some $y\in \Hcal$ and let's project it onto the subspace 
\[ \Hcal_* = \{\sum_{i=1}^n \gamma_i K(\cdot, x_i ) \,\:\,(\gamma_j)_{j=1}^n\}\,, \]
spanned by $(K(\cdot, x_i ))_{i=1}^n$.
By the projection theorem on Hilbert space there exists a unique pair 
$y_\perp(x)\in\Hcal_*^\perp$ and $y_* \in \Hcal_*$ such that
\[ y = y_\perp + y_* \,. \]

Let's use the reproducibility property of the kernel:
\[
y(x_i) = \langle y, K(\cdot,x_i) \rangle = \langle y_*, K(\cdot,x_i) \rangle\,,
\]
since $K(\cdot,x_i)\in \Hcal_0$. Thus
\[
\langle y, K(\cdot,x_i) \rangle
= \sum_{i=1}^n \gamma_j \langle K(\cdot, x_j ), K(\cdot,x_i) \rangle
= \sum_{i=1}^n \gamma_j K(x_i, x_j ) \,.
\]
thus each $y(x_i)$ and, hence, the whole cost function is independent of the orthogonal
component. The norm of $y$ is thus
\[
\Omega(\|y\|_\Hcal)
= \Omega\Bigl( \sqrt{\|y_*\|_\Hcal + \|y_\perp\|_\Hcal} \Bigr) 
\geq \Omega(\|y_*\|_\Hcal)\,,
\]
with equality if and only if $\|y_\perp\|_\Hcal = 0$. Considering independence of the
cost on $y_\perp$, the solution to the problem must necessarily by of the form
\[ y = \sum_{i=1}^n \alpha_i K(x, x_i) \,. \]
We may disregard the orthogonal component of any $y$!

% subsubsection proof_of_the_representer_theorem (end)

\subsubsection{Kernel ridge regression} % (fold)
\label{ssub:kernel_ridge_regression}

Consider penalized problem
\[ C(y) = \sum_{i=1} (t_i - y(x_i))^2 + \lambda \|y\|^_\Hcal\,,\]
where $y\in \Hcal$ for some RKHS $\Hcal$ with kernel $K$.

Using representer theorem necessarily the solution to this problem:
$C(y) \to \min_{y\in\Hcal}$, -- is
\[y(x) = \sum_{i=1}^n \alpha_i K(x,x_i)\,, \]
or $y(x) = K_x' \alpha$ in the matrix notation with $K_x = (K(x,x_i))_{i=1}^n$ and 
$\alpha = (\alpha_i)_{i=1}^n \in \Real^{n\times 1}$.

Thus $y = K \alpha$, where $K$ is the Gram matrix given by
\[ K = \begin{pmatrix} K_{x_1}'\\ \vdots\\ K_{x_n}'\\ \end{pmatrix}\,. \]
Hence
\[
\sum_{i=1}^n (t_i - y(x_i))^2 = (t-K\alpha)'(t-K\alpha)\,,
\]
and
\begin{align*}
\|y\|_{\Hcal}^2
	&= \sum_{i,j} \alpha_i \alpha_j \langle K(\cdot,x_i), K(\cdot,x_j)\rangle\\
	= \sum_{i,j} \alpha_i \alpha_j K(x_i,x_j)
	= \alpha' K \alpha\,.
\end{align*}
The problem of $y^* \argmin_y C(y)$ is equivalent to 
\[
\alpha^* = \argmin_\alpha (t-K\alpha)'(t-K\alpha) + \lambda \alpha' K \alpha\,,
\]
which gives the optimal $\alpha$ as
\[ \alpha^* = (K+\lambda I)^{-1} t\,. \]
The prediction of $t$ at a new $x$ is given by
\[ \hat{t}(x) = K_x'\alpha^* = K_x' (K+\lambda I)^{-1} t \,. \]

% subsubsection kernel_ridge_regression (end)

\subsubsection{Kernel SVM} % (fold)
\label{ssub:kernel_svm}

The training data is $(x_i,t_i)_{i=1}^n \in X\times \{-1,+1\}$. The problem of the
SVM was to fit a hyperplane $y(x) = \beta_0 + \langle\beta, x\rangle$.
The primal problem was
\[ \frac{1}{2}\|\beta\|^2 \tom \min_\beta\,, \]
subject to $t_i (\beta_0 + \beta'x_i )\geq 1$.
The dual problem was
\[ \one'\lambda - \frac{1}{2} \lambda'H \lambda \to \max_\lambda\,, \]
subject to $\lambda_i \geq 0$ and $\lambda't = 0$, where $H$ is given by
\[ H_{ij} = t_i t_j \langle x_i,x_j\rangle\,. \]
The whole idea of RKHS in SVM is to find some larger dimensional feature space,
where there is linear separability (at least approximately). In that enlarged
feature space we find a fat boundary, even though the original space $X$ was not
linearly separable. The main property of RKHS is that
\[ \langle \Phi(x_i),\Phi(x_j) \rangle = K(x_i,x_j)\,, \]
The only thing we care about is the kernel: we do not even need to know the feature
maps $\Phi(\cdot)$ -- only the inner product and thus the kernel matters.

Classifying a new point:
\[ G(x) = \text{sign} \bigl( \beta_0^* + \sum_{i\in \text{SV}} \lambda^*_i t_i K(x,x_i) \bigr)\,. \]

% subsubsection kernel_svm (end)

% subsection applications (end)

% section lecture_18 (end)

\section{Exam topics} % (fold)
\label{sec:exam_topics}

Next Tuesday $16$-th of June.
\begin{itemize}
	\item 5 questions for 3 hours;
	\item Multiple choice questions $\times 15$. Total mark $30$;
	\item $2,3$ total to 45 points: \begin{itemize}
		\item derivation of the lasso;
		\item general fitting with OLS;
		\item the smoothing spline;
		\item Laplace approximation;
		\item reweighted least squares for binary classification;
		\item explain an understand the CART;
		\item Understand the strong duality;
		\item derive AdaBoost;
		\item derive the maximum margin classifier;
		\item Proof of the representer theorem;
		\item RKHS;
	\end{itemize}
	\begin{itemize}
		\item any topic during the lectures.
	\end{itemize}
\end{itemize}

% section exam_topics (end)

\end{document}
