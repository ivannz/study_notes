\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\pr}[0]{{\mathbb{P}}}
\newcommand{\Var}[0]{{\text{Var}}}
\newcommand{\var}[0]{{\text{var}}}
\newcommand{\RSS}{\text{RSS}}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}
\newcommand{\tr}{\text{tr}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\rank}{\mathop{\text{rank}}\nolimits}



\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Document title}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section*{Preliminaries} % (fold)
\label{sec:preliminaries}

\url{http://cs.hse.ru/ai/mmdm}

The folowing topics would be covered in the course:
\begin{itemize}
	\item Decision theory wit linear regression
	\item Shrinkage methods (Ridge regression, Lasso, elastic net)
	\item Polynomial regression, splines
	\item Model selection techniques, validation methods
	\item Classification problems (the Support Vector Machine, decision trees,
	bagging and boosting)
	\item Neural networks (flexible regression and classification)
\end{itemize}

Main books:
\begin{description}
	\item[Tibshirani2013] \hfill\\
		James, Witten, Hastie and Tibshirani (2013): ``An Introduction to Statistical Learning with Applications in R''
	\item[Bishop2006] \hfill\\
		Bishop (2006): ``Pattern Recognition and Machine Learning''
	\item[Tibshirani2013] \hfill\\
		Hastie, Tibshirani, Friedman (2013): ``The Elements of Statistical Learning''
\end{description}

% section* preliminaries (end)

\clearpage
\section{Lecture \# 1} % (fold)
\label{sec:lecture_1}

Linear regression lies in the class of supervised learning problems.
Such problems work with paired data of arbitrary nature: univariate, multivariate,
categorical, numerical, abstract (like graphs in chemical classification problems) et c.
In general there is a learning (training) sample $(x_i, T_i)_{i=1}^n$ of pairs
of an input variable $X$ and the associated output target $T$.

The goal is to construct a function $y(\cdot)$ of the input that predicts, forecasts,
estimates or otherwise returns the target, associated with the given input.

The target variable $T$ is always considered to be a random variable at least due
to measurement or other irreducible uncertainty, that is present in any system.
The input variable $X$, depending on the approach, may or may not be random.

In the case when it is an rv, the data is characterized by the join probability
density (distribution) function $p_{X,T}(x,t)$.
The joint pdf can be factorized into the density of the response conditional on
a particular input and the density (probability) of the input:
\[p_{X,T}(x,t) = p_{T\rvert X}(T\rvert X) p_X(x) \]
The analysis of the problem may now fork:
\begin{description}
	\item[Generative model] \hfill \\
		both the data and the response given data are modelled;
	\item[Discriminative model] \hfill \\
		model just the conditional response.
\end{description}

Statistical decision theory helps decide on a functional form of $y(\cdot)$:
choose $y(\cdot)$ so a to minimize the expected loss functional $\ex_{X,T} L(T,y(X))$
over the bivariate distribution of the input data.
This way the incorrect estimation of $t$ by $y(x)$ given $x$ is penalized.

Among the possible loss functions the most frequently used is the quadratic
loss function $L(t,y) \defn (t-y)^2$, which has its roots in the $L^p$ norm
\[\nrm{f}_p = \Bigl(\int \lvert f\rvert^p d\mu\Bigr)^\frac{1}{p}\]
for $p = 2$. Another possible loss function is the so called Minkowski cost,
defined as $L(t,y) \defn \abs{t-y}$, which is the $L^p$ norm with $p=1$.

Sidenote: conditional expectation as the projection of $f$ onto a subspace of $g$
measurable functions in the Hilbert space of square integrable functions with the
usual $\int f\bar{g} d\mu$ inner product.

% Remember the measure theory? with its sigma-finite measure \mu with respect to which another measure \nu is absolutely continuous, and there is f\in L^1(\mu) such that \nu=\int f d\mu and \int gd\nu = \int fg d\mu.

% Which approach to use?
The joint randomness of $(X,T)$ leads to the following problem in the case of
$L^2$ loss function:
\[\ex_{X,T} (T-y(X))^2 \to \min_y\]
Using the tower property of the conditional expectation the optimal $y$ is the
conditional expectation $\phi(X)\defn \ex(T\rvert X)$. Indeed, since
$\ex_{T\rvert X} \bigl(T-\phi(X) \bigr) = 0$, the mean squared error is
\begin{align*}
	\ex_{X,T} \bigl(T-y(X)\bigr)^2 &= \ex_{X} \Bigl( \ex_{T\rvert X} (T-y(X))^2 \Bigr) \\
		&= \ex_X \Bigl( \ex_{T\rvert X} \bigl( (T-\phi(X))^2+(\phi(X)-y(X))^2 \bigr) + \\
			&\quad + 2 \ex_{T\rvert X} (T-\phi(X))(\phi(X)-y(X)) \Bigr) \\
		&= \ex_X \bigl( (\phi(X)-y(X))^2 + \ex_{T\rvert X} (T-\phi(X))^2 \bigr) + \\
			&\quad + 2 \ex_X \bigl((\phi(X)-y(X)) \ex_{T\rvert X} (T-\phi(X)) \bigr) \\
		&= \ex_X \bigl( \ex_{T\rvert X} (T-\phi(X))^2 + \ex_{T\rvert X} (\phi(X)-y(X))^2 \bigr)
\end{align*}

The latter conditional expectation is vanishes whenever $y=\phi$, whilst the first
one is independent of $y$ and can be considered as the irreducible variance due
to structural variance. The mean prediction error is the irreducible variance and
the approximation error:
\[\text{error} \defn \ex_X \Var(T\rvert X) + \ex_x (\phi(X)-y(X))^2 \]

However it is never possible to achieve $y(x) = \ex(T\rvert X=x)$.
Therefore some insight is needed on $\ex_{T\rvert X} (\phi(X)-y(X))^2$.
\begin{description}
	\item[Frequentist] $\pr\bigl(\Dcal\rvert \Theta\bigr)$ -- uncertainty is due
	to dataset;
	\item[Bayesian] $\pr\bigl(\Dcal\rvert \Theta\bigr) \pi(\Theta)$ -- uncertainty
	is due to parameters, where $\pi(\Theta)$ is the prior distribution. The posterior
	distribution is proportional to
		\[\pr\bigl(\Theta\rvert \Dcal\bigr) \sim \pr\bigl(\Dcal\rvert \Theta\bigr) \pi(\Theta)\]
\end{description}

Error is for a particular training dataset, on which the $y$ is estimated. In effect
the functional form of $y$ depends on the data set $D$:
\[\ex_\Dcal(\text{Error}_\Dcal) = \text{Noise} + \ex_\Dcal \ex_X \bigl( y(x,D)-\phi(x) \bigr)^2\]
where noise is defined as $\ex_X \Var(T\rvert X)$.
Now put $f(x) \defn \ex_\Dcal y(x,D)$.
\begin{align*}
	\ex_\Dcal( y(x,D)-\phi(x) \pm f(x) )^2
		& = \ex_\Dcal( y(x,D) - f(x) )^2 + \ex_\Dcal( f(x) - \phi(x) )^2 \\
			&\quad + 2 \ex_\Dcal( y(x,D) - f(x) )( f(x) - \phi(x) ) \\
		& = \ex_\Dcal( y(x,D) - f(x) )^2 + ( f(x) - \phi(x) )^2 \\
			&\quad + 2( f(x) - \phi(x) ) \ex_\Dcal( y(x,D) - f(x) ) \\
		& = \ex_\Dcal( y(x,D) - f(x) )^2 + ( f(x) - \phi(x) )^2 \\
\end{align*}

Therefore the expected value of the loss is 
\[\ex_\Dcal(\text{Error}_\Dcal) = \text{Noise} + \text{Variance} + \text{Bias}^2\]
with the variance is due to the fact that the instance of $y$ is not static with
varying dataset $D$, and the bias is the measure of average closeness of the
approximation $y(\cdot)$ to the real conditional expectation $\ex(T\rvert X)$.
This is the decomposition of the loss into systemic noise, bias and variance.

% Non-parametric approach + low dimension: smoothing local data (kNN -- $k$ nearest neighbours).

Parametric approach would require a parametric expression of $y(\cdot)$ as an
approximation of $\ex(T\rvert X)$ at every $x$ (or at most almost all).

Suppose $\Lcal^2$ is the space in which the best approximation of $\phi(X)$ is
searched for. Projecting $\ex(T\rvert X)$ onto $\Lcal^2$ yields $\phi(X) = h^\perp(X) + h(X)$,
where $h^\perp \perp \Lcal^2$ (i.e. $\langle h^\perp,g\rangle = 0$ for all
$g\in \Lcal$) and $h\in \Lcal^2$.

% Here should be a picture of the projection: the conditional expectation is projected onto the subspace of square integrable functions.

% \ex_\Dcal y(X,D) -- is an element of the 

Since $f-h\in\Lcal^2$ it must be true that
\[\ex_X (f(X) - h(X)) h^\perp(X) = \langle f-h, h^\perp\rangle = 0 \]
whence the following decomposition must hold:
\begin{align*}
	\text{Bias}^2 &= \ex_X \bigl( \ex_\Dcal y(X,D) - \phi(X) \bigr)^2 
		= \ex_X (f(X) - h^\perp(X) - h(X))^2 \\
		& = \ex_X (f(X) - h)^2 + \ex_X (h^\perp(X))^2 + \ex_X (f(X) - h(X)) h^\perp(X) \\
		& = \ex_X (f(X) - h(X))^2 + \ex_X (h^\perp(X))^2
\end{align*}
This means that the squared bias is the sum of how close the model is to the projection
of $\ex(T\rvert X)$ onto $\Lcal^2$, and how well the projection approximates the
conditional expectation itself.

In linear regression problems the space $\Lcal^2$ is taken to be the set of all
linear functions of $X$.

\subsection*{Linear regression} % (fold)
\label{sub:linear_regression}

Suppose $X$ is a $p$-dimensional vector of features and that we attempt to approximate
the $\ex(T\rvert X)$ with $y(x) \defn \beta_0 + \sum_{k=1}^p \beta_k x_k$.

In effect we suppose the following statistical model
\[T = \beta_0 + \sum_{k=1}^p \beta_k X_p + \epsilon;\,\epsilon\sim\Ncal(0,\sigma^2)\]
Therefore conditional upon $X$ the target $T$ is distributed like \[\induc{T}X\sim \Ncal(y(X),\sigma^2)\]

In this setting the following questions arise: \begin{itemize}
	\item How $\beta_{1\times(p+1)}$ is estimated?
	\item Which coefficients $\beta_k$ are significant?
	\item What is their interpretation?
\end{itemize}

In the matrix form, the linear regression problem is stated as follows:
\[
\underset{n\times 1}{T}
= \underset{n\times (1+p)}{X}\underset{(1+p)\times 1}{\beta}
+ \underset{n\times 1}{\epsilon}
\]
where the vector $X$ of the predictors of the training set has been altered by
adding a constant term -- the intercept. The predictors now look like this
\[
X = \Bigl( \one \defn \underset{n\times 1}{1}, \underset{n\times 1}{x_1}, \ldots, \underset{n\times 1}{x_p} \Bigr)
\]

Two approaches to estimating $\beta$ is possible:
the \textbf{L}east \textbf{S}quares and the \textbf{M}aximum \textbf{L}ikelihood.
Effectively the LS approach to parameter estimation is non-probabilistic.
There are different flavours of least squares: the ordinary, the generalised
(with a weighting matrix), two-stage least squares (2SLS), to name a few, and others.
The 2SLS is an extension of the GLS that attempts to battle heteroskedasticity:
first run OLS to get the squared residuals, then GLS with weights that standardise
the residuals.

The goal of LS is to solve the following problem:
minimize the \textbf{R}esidual \textbf{S}um of \textbf{S}squares
\[\RSS \defn \brac{T-X\beta}'\brac{T-X\beta} \to \min_\beta\]
The first order conditions on the potential minimizer are in the matrix form:
\begin{align*}
	\frac{\partial}{\partial \beta}\RSS &= \frac{\partial}{\partial \beta}\brac{T'T-\beta'X'T - T'X\beta + \beta'X'X\beta}\\
	&= \frac{\partial}{\partial \beta} T'T - \frac{\partial}{\partial \beta} \beta'X'T - \frac{\partial}{\partial \beta} T'X\beta + \frac{\partial}{\partial \beta} \tr\brac{\beta'X'X\beta}\\
	&= - 2 \frac{\partial}{\partial \beta} T'X\beta + \beta'\brac{X'X + \brac{X'X}'}\\
	&= - 2 T'X + 2 \beta'X'X = 0
\end{align*}
If the matrix $X$ is full rank, then the matrix $X'X$ is invertible, and is positive-semidefinite.
Therefore $\frac{\partial^2}{\partial \beta\partial \beta}\RSS = 2 X'X \geq 0$,
whence the Least Squares solution $\hat{\beta} \defn \brac{X'X}^{-1} X'T$ is indeed
the minimizer of the RSS.

The best linear approximation to $\ex(T\rvert X)$ is the projection of $T$ onto
the space of linear functionals of $X$.

The matrix $\hat{H} \defn X\brac{X'X}^{-1}X'$ is called the \textbf{hat matrix} and
is in fact a projector onto the linear subspace spanned by the column vectors of $X$.

Every projector matrix has the following property: if $v$ is in the spanned linear
subspace, then $\hat{H}v = v$. Indeed, since $v\in \clo{X}$, there must exist
$\underset{(p+1)\times 1}{\alpha}\in \Real^{(p+1)}$ such that $v = X\alpha$.
Therefore
\[X\brac{X'X}^{-1}X'v = X\brac{X'X}^{-1}X'X\alpha = X\alpha = v\]

If the columns of $X$ are orthogonal, then $X'X$ is in fact a diagonal matrix
with the squared Euclidean norms of columns of $X$ on the diagonal. In this case
each of $\brac{\beta_k}_{k=0}^n$ is the coefficient in the projection of $T$ onto
the respective column of $X$.

% subsection* linear_regression (end)

\subsection*{Gram-Schmidt orthogonalisation} % (fold)
\label{sub:gram_schmidt_orthogonalisation}

Suppose there is a set of linearly independent vectors $\brac{f_i}_{i=1}^m$ in
some Euclidean space $V$.
\begin{description}
	\item[Initialisation] \hfill \\
		Set $e_1 \defn \frac{1}{\nrm{f_1}} f_1$;
	\item[General step $k=1,\ldots,m-1$] \hfill \\
		Let \[u_{k+1} \defn f_{k+1} - \sum_{i=1}^k \frac{\brkt{e_i, f_{k+1}}}{\brkt{e_i,e_i}} e_i\]
		and put $e_{k+1}\defn \frac{1}{\nrm{u_{k+1}}} u_{k+1}$.
\end{description}
An effective algorithm is given in \emph{Golub \& Van Loan 1996} section 5.2.8.

% subsection* gram_schmidt_orthogonalisation (end)

%% I don't recall what the section below is for...
In a bivariate case $X = (\one,x)$ and
\[\hat{\beta} = \brac{\begin{matrix} \one'\one & \one'x\\ x'\one & x'x \end{matrix}}^{-1} \brac{\begin{matrix} \one't \\ x't \end{matrix}}
= \frac{1}{n^2 \brac{\overline{x^2} - \bar{x}^2}} \brac{\begin{matrix} \one'\one \one't - \one'x x't \\ - x'\one \one't + x'x x't \end{matrix}}
\]
since $\one'\one = n$ and $x'\one = \one'x = n\bar{x}$.

%% p-predictors

Set $\hat{z}_0 \defn \one$. For each $j=1,\ldots,p$ regressing $x_k$ onto
$\brac{\hat{z}_j}_{j=0}^{k-1}$ to get the LS coefficients $\brac{\alpha_{jk}}_{j=0}^{k-1}$.
Construct $\hat{z}_k$ as the residuals of $x_k$ from projecting onto $\brac{\hat{z}_j}_{j=0}^{k-1}$ :
\[\hat{z}_k = x_k - \sum_{j=0}^{k-1} \alpha_{jk} \hat{z}_j\]
This is similar to the Gram-Schmidt procedure described previously.

Regressing $t$ on $z_k$ yields the coefficient $\hat{\beta}_k$ which is not the
OLS coefficient, but shows new information gained from the $k^\text{th}$ predictor.

Discriminative approach.
Since $\hat{\beta}$ is a linear transformation of $T$ and $\hat{\beta} = \beta + \brac{X'X}^{-1}X'\epsilon$,
\textbf{provided the model is specified correctly}, it is therefore true that
\[\hat{\beta}\sim\Ncal\brac{\beta,\brac{X'X}^{-1} \sigma^2}\]

In fact if $\epsilon\sim \Ncal\brac{0,\Sigma}$ -- i.e the noise has some other
covariance structure, then $\Var(\hat{\beta}) = \brac{X'X}^{-1}X'\Sigma X\brac{X'X}^{-1}$
-- this is useful in GLS, mentioned above, especially if $\Sigma$ is decomposed
into $C'C$ with $C$ -- non-singular lower triangular matrix (Cholesky).

Indeed, if we transform (weigh) the observations according to matrix $W$ with $W'W = \Sigma^{-1}$, then the LS model becomes $WT=WX\beta + W\epsilon$, whence
\[\hat{\beta} = \brac{X'W'WX}^{-1}X'W'WT = \brac{X'\Sigma^{-1}X}^{-1}X'\Sigma^{-1}T\]
whence $\hat{\beta}\sim\Ncal\brac{\beta, \brac{X'\Sigma^{-1}X}^{-1}}$.

\noindent\textbf{Cochran's Theorem} \hfill\\
Suppose $\brac{\xi_i}_{i=1}^n \sim\Ncal(0, 1)$ and that
\[\sum_{i=1}^m \xi_i^2 = \sum_{j=1}^k Q_j\]
where $Q_j = \xi' A_j \xi$ is a positive-semidefinite quadratic form for each $j=1,\ldots,k$.
If $\sum_{j=1}^k r_j = n$, with $r_j = \rank{Q_j}$, then \begin{enumerate}
	\item The random variables $Q_j$ are independent;
	\item each $Q_j\sim \chi^2_{r_j}$.
\end{enumerate}

Returning to the simple model, what is the estimator of $\sigma^2$?

It turns out that $\frac{\RSS}{\sigma^2}\sim \chi^2_{n-p-1}$.
Indeed, provided the model is specified correctly, \begin{align*}
	\RSS &= \brac{T-X\hat{\beta}}'\brac{T-X\hat{\beta}} \\
	& = \brac{T-X\beta - X\brac{X'X}^{-1}X'\epsilon}'\brac{T-X\beta - X\brac{X'X}^{-1}X'\epsilon} \\
	& = \epsilon' \brac{I - X\brac{X'X}^{-1}X'}'\brac{I - X\brac{X'X}^{-1}X'}\epsilon \\
	& = \epsilon' \brac{ I - X\brac{X'X}^{-1}X' } \epsilon = \epsilon' \brac{ I - \hat{H} } \epsilon
\end{align*}
Since the trace of an idempotent matrix equals its rank
\[\rank(I - \hat{H}) = \tr(I) - \tr(\hat{H}) = n- (p+1)\]
whence by Cochran's theorem $\frac{1}{\sigma}\RSS\sim\chi^2_{n-(p+1)}$.
Furthermore, $\hat{\sigma}^2 \perp \hat{\beta}$.

\subsection{Hypothesis testing} % (fold)
\label{sub:hypothesis_testing}

If $V_j \defn \brac{\brac{X'X}^{-1}}_{jj}$ -- the $j^\text{th}$ diagonal element
of $\brac{X'X}^{-1}$, whence $\Var\hat{\beta}_j\defn \sigma^2 V_j$.

To test $H_0:\beta_j=0$ versus $H_1:\beta_j\neq 0$, one should use the following
statistic, which under the null is distributed as follows:
\[\frac{\sfrac{\hat{\beta}_j}{\sqrt{\sigma^2 V_j}}}{\sqrt{ \frac{\sfrac{n-p-1}{\sigma^2} \hat{\sigma}^2 }{ n-p-1 } }} \sim \frac{\Ncal(0, 1)}{\sqrt{\chi^2_{n-p-1}}} = t_{n-p-1}\]

The $\alpha$-level confidence interval for $\beta_j$ is defined as
\[\clo{ \hat{\beta}_j - \hat{\sigma} \sqrt{ V_j },  \hat{\beta}_j + \hat{\sigma} \sqrt{ V_j } }\]

For the false rejection rate of the t-test $\alpha = 0.05$, on average $5\%$ of
statistically significant results would be false positives.

To test for simultaneous insignificance of $\brac{\beta_j}_{j=1}^p = 0$ one should
use the F-test (more generally an F-test for linear restriction). The following
takes place under the null hypothesis that the simpler model is correct :
\[F = \frac{\sfrac{\RSS_0-\RSS_1}{p_1-p_0}}{\sfrac{\RSS_1}{n-p_1-1}}\sim F(p_1-p_0, n-p_1-1)\]

To gauge the model accuracy one can use both the RSS as a measure of lack of fit
and the $R^2$ as a measure of goodness-of-fit.

If $\one\in \clo{X}$ then
\begin{align*}
	\text{TSS} &= T' \bigr(I - \one(\one'\one)^{-1}\one'\bigl) T \\
	&= T' \bigl(I - X(X'X)^{-1}X'\bigr) T + T' \bigl( X(X'X)^{-1}X' - \one(\one'\one)^{-1}\one'\bigr) T \\
	&= \RSS + \text{ESS}
\end{align*}
since in this case
\[\bigl(I - X(X'X)^{-1}X'\bigr)\bigl( X(X'X)^{-1}X' - \one(\one'\one)^{-1}\one'\bigr) = 0\]

The goodness-of-fit measure $R^2$ is the ratio of the explained variance to the
total variance
\[R^2 \defn \frac{\text{ESS}}{\text{TSS}} = 1 - \frac{\RSS}{\text{TSS}} \]
For a simple case of a bivariate regression
\[R^2 = \rho^2 = \frac{\abs{\text{corr}(x,t)}^2}{\Var(x)\Var(t)} = \beta^2 \frac{\Var(x)}{\Var(t)}\]


% subsection hypothesis_testing (end)

% section lecture_1 (end)

\clearpage
\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}
% 2015-01-20

The uncertainty on the model for a new input $x_0$ 

Prediction of the value of $T$ at a new input $x_0$ has two sources of uncertainty:
\begin{enumerate}
	\item Uncertainty of the first type -- due to the estimation of $y(\cdot)$
	($\hat{\beta}$) in $\Lcal^2_X$;
	\item Uncertainty of the second type -- due to the additive noise term. Even
	if the true $\beta$ were known, the true values of the response $T$ at $x_0$
	would still be unknown.
\end{enumerate}
Predictive intervals incorporate the noise as well as uncertainty due to imprecise
estimation of the coefficients.

Variability due to estimation of an approximate model.

\subsection{Shrinkage models} % (fold)
\label{sub:shrinkage_models}

In the ordinary linear squares method we minimized the following fitness function:
\[\text{min} \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}}^2\]

\subsubsection{Ridge regression} % (fold)
\label{ssub:ridge_regression}

Minimize the following penalized Residual Sum of Squares as:
\[\RSS_\text{ridge} \defn \sum_{i=1}^n \bigl(t_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigr)^2 + \lambda \sum_{j=1}^p \beta_j^2\]
the $\lambda>0$ is called the tuning parameter and it controls the amount of shrinkage.
If $\lambda = 0$ then there is absolutely no shrinkage, and the optimal $\hat{\beta}$
coincides with the OLS. But if $\lambda\to \infty$ then the optimal coefficient
vector tends to zero (that is why it is called \textbf{shrinkage}). This adjustment
effectively regularizes the parameter estimates.

In contrast to the ordinary linear regression, for this sort of shrinkage the scale
of each predictor $\brac{x_j}_{j=1}^p\in \Real^n$ matters! Thus the first step is
to standardize the variables before fitting the Ridge regression.

Next observation: the intercept $\beta_0$ is indeed excluded from the shrinkage
term so as no to penalize for the \emph{base} level of the target.

Suppose the $t$ and $X$ are standardized (mean and centre), which means that $\beta_0 \equiv 0$
Then in the matrix form the problem is equivalent to minimizing the following:
\[(t-X\beta)'(t-X\beta) + \lambda \beta'\beta \to \min_\beta\]
The first order conditions are given by just one equation:
\[\frac{\partial}{\partial \beta} \RSS_\text{ridge} = - 2 X'(t-X\beta) + 2 \lambda \beta\]
whence the optimal coefficients are given by
\[\hat{\beta}_\text{ridge} \defn (X'X + \lambda I_p)^{-1}X't\]

% subsubsection ridge_regression (end)

% subsection shrinkage_models (end)

% section lecture_2 (end)

\clearpage
\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}
The ridge regression problem can actually be rewrtiten as the following constrained
optimization problem (centred and scaled): minimize $(t-X\beta)'(t-X\beta)$ subject
to the ``budget'' $\beta'\beta\leq L$ where $L\geq0$ effectively limits the size
of the coefficients.

The modified ridge RSS with a tuning parameter $\lambda$ is defined as
\[
\RSS_\text{ridge}
\defn \sum_{i=1}^n \bigl( t_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \bigr)^2
	+ \lambda \sum_{j=1}^p \beta_j^2
\]
Important to note: \begin{itemize}
	\item Inputs must be either standardized by standard deviation or altogether
	dimensionless inputs. This would make elements of $\beta$ comparable and would
	protect against artificial enhancement of importance of any particular explanatory
	variable: the scale of $(x_{ij})_i$ inversely affects the magnitude of $\beta_j$.
	\item If the input vectors are centred, then the estimation is separable in
	$\beta_0$ and $\beta_j$ for $j\geq 1$.
\end{itemize}
The latter can be shown rigorously. The residual values for a given $\beta_0$
and $\beta$:
\begin{align*}
t-(\one\beta_0 + X\beta)
&= t-\bigl(\one\beta_0 + X\beta \pm \one(\one'\one)^{-1}\one'X\beta \bigr)\\
&= t-\bigl(\one\beta_0 + \pi X\beta + (I_n-\pi)X\beta \bigr)
\end{align*}
where $\pi = \one(\one'\one)^{-1}\one'$ is in fact a projection matrix onto a
linear subspace $[\one]$ spanned by one vector $\one$. Now define the following
centred variables:
\[X^c = X - \pi X = \pi_\perp X \text{ and } t^c = \pi_\perp t\]
where $\pi_\perp = I_n-\pi$ -- the projector onto space orthogonal to $[\one]$.
Variables are indeed centred, because $\bar{x} = (\one'\one)^{-1}\one'X$ is in
fact a row vector of sample means of each feature in $X$, and similarly
$\bar{t} = (\one'\one)^{-1}\one't$ is the sample mean of $t$. Therefore
\begin{align*}
t-(\one\beta_0 + X\beta)
&= t^c + \pi t - \one\beta_0 - \pi X\beta - X^c \beta\\
&= t^c - X^c \beta + (\pi t - \pi X\beta - \one\beta_0)
\end{align*}
Since it is true that $\one\beta_0 = \pi \one\beta_0$, we have
\[
\ldots
= \bigl(t^c - X^c\beta\bigr) + \pi\bigl( t - X\beta - \one\beta_0\bigr)
= \pi_\perp(t - X\beta) + \pi\bigl( t - X\beta - \one\beta_0\bigr)
\]
Taking the inner product of $t-(\one\beta_0 + X\beta)$ with itself:
\begin{multline*}
\bigl(t-(\one\beta_0 + X\beta)\bigr)'\bigl(t-(\one\beta_0 + X\beta)\bigr) \\
= (t - X\beta)'\pi_\perp'\pi_\perp(t - X\beta) + (t - X\beta - \one\beta_0)'\pi'\pi(t - X\beta - \one\beta_0)
\end{multline*}
where the intermediate products with $\pi_\perp \pi$ or $\pi \pi_\perp$ vanish
since $[\one]^\perp \perp [\one]$. Furthermore the term
\begin{align*}
(t - X\beta - \one\beta_0)'\pi'
	&= (t - X\beta - \one\beta_0)'\one(\one'\one)^{-1}\one'\\
	&= ((\one'\one)^{-1}\one't - (\one'\one)^{-1}\one'X\beta - (\one'\one)^{-1}\one'\one\beta_0)'\one'\\
	&= (\bar{t} - \bar{X}\beta - \beta_0)'\one'
\end{align*}
Since $(\bar{t} - \bar{X}\beta - \beta_0)$ is a scalar and $\one'\one = n$, the
main part of the ridge RSS turns out to be
\[
(t^c - X^c\beta)'(t^c - X^c\beta) + n (\bar{t} - \bar{X}\beta - \beta_0)^2
\]
This means that, provided the estimates of $\beta$ on the centred data, the estimate
of $\beta_0$ can easily be inferred: $\hat{\beta}_0 = \bar{t} - \bar{X}\hat{\beta}$.
Furthermore these manipulations are true for any penalized regression model with
the (regularization) penalty term explicitly ignoring the intercept.

The problem of penalized (ridge) regression in the matrix form, where $X$ is the
$n\times k$ matrix of centred standardized independent variables with no intercept:
\[\RSS_\text{ridge} = (t-X\beta)'(t-X\beta) + \lambda \beta'\beta\]
The solution is
\[\hat{\beta}_{\text{ridge}}\defn \brac{X'X + \lambda I}^{-1} X'T\]

An equivalent optimization problem:
\[
\hat{\beta}_{\text{ridge}}
= \argmin_{\beta} \sum_{i=1}^n \bigl(t_i - \sum_{j=1}^k \beta_j x_{ij}^c\bigr)^2
\text{ subject to } \sum_{j=1}^k \beta_j^2 \leq \eta
\]
$\eta$ -- the budget, $\lambda$ -- the complementary slackness coefficient.
There is a certain universality of the statement of the regression problem in
this form: it is possible to plug in different constraints ($L^1$ constraints -- LASSO).
LASSO is automatically performing feature selection due to $L^1$ norm.

How to tune $\lambda$? Wait until model selection and cross-validation. How does
the shrinkage work? Consider two cases of columns of $X$: orthogonal and correlated.

\subsection*{Orthogonal columns} % (fold)
\label{sub:orthogonal_columns}

Suppose $X$ are orthogonal, in which case $X'X = I_p$. The OLS solution is
\[\hat{\beta}_\text{OLS} = (X'X)^{-1}X't = X't\]
The ridge regression solution is given by 
\[
\hat{\beta}_\text{ridge}
= \brac{X'X + \lambda I_p}^{-1} X't
= \frac{1}{1+\lambda} \hat{\beta}_{\text{OLS}}
\]
Voil\`a -- shrinkage!

\subsubsection*{LASSO} % (fold)
\label{ssub:lasso}

The lasso solution:
\[
\hat{\beta}_\text{LASSO}
= \argmin_\beta (t-X\beta)'(t-X\beta) + \lambda \sum_{j=1}^k \lvert \beta_j\rvert
\]
Rewriting
\begin{align*}
	(t-X\beta)'(t-X\beta) + \lambda \sum_{j=1}^k \lvert \beta_j\rvert
	&= t't - 2 \beta'X't + \beta'X'X\beta  + \lambda \sum_{j=1}^k \lvert \beta_j\rvert \\
	&= t't - 2 \beta'\hat{\beta}_\text{OLS} + \beta'\beta  + \lambda \sum_{j=1}^k \lvert \beta_j\rvert
\end{align*}
Thus in this orthogonal case the LASSO minimization solution is equivalent to
the following:
\[
\hat{\beta}_\text{LASSO}
= \argmin_\beta \sum_{j=1}^k \beta_j^2 - 2 \beta_j \beta_{\text{OLS}\,j} + \lambda \lvert \beta_j\rvert
\]
The problem is additively separable in $\beta_j$'s, whence:
\[
\hat{\beta}_{\text{LASSO}\,j}
= \argmin_{\beta_j} \beta_j^2 - 2 \beta_j \beta_{\text{OLS}\,j} + \lambda \lvert \beta_j\rvert
\]
The FOC for a problem with $\beta_j\geq0$ is given by:
\[- \beta_{\text{OLS}\,j} + \beta_j + \frac{\lambda}{2} = 0\]
whence the optimal value is given by:
\[\hat{\beta}_{\text{LASSO}\,j} = \bigl(\beta_{\text{OLS}\,j} - \frac{\lambda}{2}\bigr)_+\]
For $\beta_j\leq 0$:
\[\hat{\beta}_{\text{LASSO}\,j} = \bigl(\beta_{\text{OLS}\,j} + \frac{\lambda}{2}\bigr)_-\]
Due to soft thresholding in the case of the LASSO there is feature selection.

% subsubsection* lasso (end)

\subsubsection*{The best subset selection} % (fold)
\label{ssub:the_best_subset_selection}
Suppose we have a set of $p$ predictors. Choose the subset with the least residual
sum of squares: for each $K = 1,\ldots, p$ pick the model with the lowest RSS:
\[
\min_\beta \sum_{i=1}^n \bigl(t_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigr)^2
\text{ subject to} \sum_{j=1}^p 1_{\beta_j \neq 0} \leq K
\]
This is equivalent to the following problem:
\[\min_{\beta} \sum_{j=1}^n \beta_j^2 - 2 \beta_j \beta_{\text{OLS}\,j} \]
subject to keeping at most $K$ coefficients non-zero.
\[
\hat{\beta}_{\text{BSS}\,i}
= \beta_{\text{OLS}\,i} 1_{[\hat{\beta}_{n-K}, \infty]}\bigl(\lvert \beta_{\text{OLS}\,i}\rvert\bigr)\]
where $\hat{\beta}$ is the $n-K$-th order statistic. This given hard thresholding.

% subsubsection* the_best_subset_selection (end)

% subsection* orthogonal_columns (end)
\subsection*{Correlated columns} % (fold)
\label{sub:correlated_columns}

Gram-Schmidt orthogonalization is similar to the QR decomposition. The best idea
is to use SVD decomposition of $X = U\Lambda V'$, with $U$ and $V$ orthogonal
matrices.

\subsection*{SVD} % (fold)
\label{sub:svd}

Suppose there is a matrix $X$ with dimensions $n\times p$ with rank $r$.
SVD decomposition claims that there exist square orthogonal matrices $U_{n\times n}$
and $V_{p\times p}$ and a diagonal matrix $\Sigma_{n\times p}$ of rank $r$, such
that $X = U \Sigma V'$:
\[X = U \Bigl(\begin{smallmatrix} \Lambda & 0\\ 0 & 0 \end{smallmatrix}\Bigr) V'\]

Orthogonality of $U$: $U'U = I_n$. Orthogonality greatly simplifies projectors
onto the column space of $X$: $\hat{t} = U U' t$ (?).

% subsection* svd (end)

\subsection*{Ridge regression} % (fold)
\label{sub:ridge_regression}

Suppose $X$ is full rank ($\rank(X) = p$) centred and scaled. Recall the expression
of the ridge regression solution:
\[\hat{\beta}_\text{ridge} = (X'X + \lambda I_p)^{-1} X't\]
The SVD of $X$ in this case is $U\Sigma V'$ where $\Sigma$ is a $n\times p$ 
matrix and $V \in \Real^{p\times p}$ and $U \in \Real^{n\times n}$ are orthogonal.
Furthermore $\Lambda'\Lambda = \Sigma^2$. Then $X'X = V\Sigma' U' U \Sigma V' = V\Lambda^2 V'$
and it must be true that 
\begin{align*}
	\hat{\beta}_{\text{ridge}}
	& = (V\Lambda^2 V' + \lambda VV')^{-1} X't\\
	& = V (\Lambda^2 + \lambda I_p)^{-1} V' V\Lambda U't \\
	& = V (\Lambda^2 + \lambda I_p)^{-1} \Lambda U't
\end{align*}
Therefore the ridge regression shrinks the coefficients over the tiniest principal
directions of $X$.

The sample correlation matrix
\[\frac{1}{n} X'X = \frac{1}{n} V'\Lambda^2 V\]
The principal components are in $V$ and the eigenvalue-eigenvector pair for the
sample correlation is given by the $\bigl(\frac{1}{n}\lambda_i^2, V_i\bigr)$
where $V_i$ is the $i$-th column of $V$. The $i$ th principal component is given
by $Z_i = X V_i = U_i \lambda_i$ and $\text{var}(z_i) = \lambda_i^2$. The ridge
regression shrinks the coefficients corresponding to the principal directions in
the column space of $X$ having the smallest variance.

% subsection* ridge_regression (end)

% subsection* correlated_columns (end)

\subsection*{The effective degrees of freedom} % (fold)
\label{sub:the_effective_degrees_of_freedom}

Should be a function of the tuning parameter $\lambda$ which governs the complexity
of the model. Complexity is the trace of the hat matrix (the projector) of the ridge regression:
\[\text{df}(\lambda) \defn \tr\bigl(X (X'X + \lambda I_p)^{-1} X'\bigr)\]
Using the SVD of $X$ gives us:
\begin{multline*}
\tr\bigl(X (X'X + \lambda I_p)^{-1} X'\bigr)
= \tr\bigl((X'X + \lambda I_p)^{-1} X'X \bigr) \\
= \tr\bigl( (\Lambda^2 + \lambda I_p)^{-1} V'V\Lambda' U'U\Lambda V'V\bigr)
= \sum_{j=1}^p \frac{\lambda_i^2}{\lambda_i^2 + \lambda^2}
\end{multline*}
If $\lambda = 0$ then $\text{df}(\lambda) = p$, and when $\lambda\to +\infty$,
then $\text{df}(\lambda) \to 0$.

% subsection* the_effective_degrees_of_freedom (end)

\subsubsection*{Not sure what this is about} % (fold)
\label{ssub:not_sure_what_this_is_about}

Correlation between this and this (?)
\begin{align*}
	\text{Var}(T) = \sigma^2 I
\end{align*}
Covariance of $\hat{T}$ and $T$.
% \[\ex(T, \hat{T}) = T' T\] 
Take a matrix of predictors $X$, its standardized version
\[X^\sigma \defn \frac{X}{\sigma}\]
and \[X^c \defn \frac{X-m}{\sigma}\]

% subsubsection* not_sure_what_this_is_about (end)

% section lecture_3 (end)

\clearpage
\section{Lecture \# 4} % (fold)
\label{sec:lecture_4}
Departing from the linearity of the Linear regression.

Transform $\brac{f_m}_{m=1}^M:\Real^p \to \Real$ since there are initially $p$
predictors. Sample space is augmented and now has $M$ predictors. The regression
function becomes:
\[f(X) = \sum_{m=1}^M \theta_m f_m(x)\]
This model generalises the original linear regressions since $f_m(x) = \pi_m(x)$ 

Polynomial regression problem: what happens in the tails? It is not very trustworthy
near the endpoints. In this respect polynomial regression is not a good idea. Why
not use local regression by partitioning the range into regions with different model
in each area: $f_m(X) = 1_{\clo{l_m, u_m}}(x_k)$

The behaviour of polynomial fit to data tends to be erratic near the boundaries.
And extrapolation can be dangerous!

\subsection{Piecewise polynomials and splines} % (fold)
\label{sub:piecewise_polynomials_and_splines}
%% The following really resembles the dummy variables in econometrics.

Suppose that $x$ is unidimensional -- only one feature.

Divide $x$ in several regions by different thresholds -- \textbf{(binding) knots},
since they link the regions ,and then fit a constant level within each region.
In order to cover the whole range of $x$ in this case one has to have three
``basis'' functions:
$1_{(-\infty, \xi_1]}$, $1_{(\xi_1, \xi_2]}$ and $1_{(\xi_2, \infty)}$.

Consider linear function on each region: we will be fitting something piecewise
linear. In this case we need $6$ basis functions, since we do not only model the
piecewise intercept, but also the piecewise slope.

Continuity requirement at each knot reduces the number of basis functions to four:
$h_1 = 1$, $h_2 = x$ (an indicator would introduce discontinuity),
$h_3 = (x-\xi_1)_+$ and $h_4 = (x-\xi_2)_+$ -- fix their intercepts.
This basis is known as the \textbf{truncated power basis}.

In general overlapping regions are refined into a partition, on which a proper
piecewise is later estimated. Then linear restriction tests could be run on the
estimated coefficients.

To add smoothness, one has to impose continuity of the derivatives at the knots.
and fit a smooth function within each region.

Consider three regions and two knots and require continuity up to the second
derivative at the first knot. The remaining degrees of ``freedom'' at this knot
are just the convexity of the fitted curve. The set of basis function is:
$h_1 = 1$, $h_2 = x$, $h_3 = x^2$, $h_4 = x^3$,
$h_5 = (x-\xi_1)_+^3$ and $h_6 = (x-\xi_2)_+^3$ (the ``piecewiseness'' is shifted
to the polynomial terms of higher order).

In general $M$-\textbf{splines} are piecewise $M$-degree polynomial regression
curves with $M-1$-smoothness restriction at binding knots (continuity with $M-1$
continuous derivatives). The basis functions for this kind of curve are
\begin{align*}
	h_j(x) &= x^{j-1}&\text{for }j=1,\ldots, M\\
	h_{M+j}(x) &= (x-\xi_j)_+^M &\text{for }j=1,\ldots,K
\end{align*}
For weirder piecewise $d$-degree polynomial curves, which are $C^{m-1}$ ($m\leq d$),
the basis might look like this:
\begin{align*}
	b_j(x) &= x^{j-1}&\text{for }j=1,\ldots, d\\
	h_{ij}(x) &= (x-\xi_j)_+^i &\text{for } j=1,\ldots,K\text{ and } i=m,\ldots,d
\end{align*}

Generally speaking, if there are $K$ knots then there are $K+1$ ``pieces'' in the
piecewise polynomial curve. If over each region a $d$-degree polynomial is to be
fit, then there are $(d+1)(K+1)$ parameters. If continuity of the curve is to be
desired, one needs to add one constraint on the parameters per each knot, since only
of the piece boundaries the curve is discontinuous. Requiring the curve to be $C^m$
with $m\leq d$ adds one constraint per knot per degree of smoothness which add up
to $(m+1) K$. Thus there are $(d-m)K + (d+1)$ free parameters left. For a cubic
spline with continuous $2$-nd derivative, $d=3$ and $m=2$, the number of ``degrees
of freedom'' is $K+4$.

One polynomial is global and smooth, but the higher the order the more oscillating
the behaviour a the ends (the Runge's phenomenon). More polynomials with a local
regression are not necessarily continuous. Splines are local and smooth!

What to do with the endpoints? Problems with the values out the common region of $x$.

\subsubsection{Natural Cubic Spline} % (fold)
\label{ssub:natural_cubic_spline}
An idea: within the range of the univariate data fit an $M$-spline, but to achieve
robustness beyond $[a,b]$ and get more trustworthy predictions require a linear
model. Thus for a \textbf{n}atural \textbf{c}ubic \textbf{s}pline within the main
region $[a,b]$ a $3$-spline is fit, whereas beyond the boundaries the function is
required to be linear (with continuity and all smoothness constraints).

With this kind of additional regularization the number of parameters for the NCS
is further reduced. The $3$-spline had $K+4$ basis functions, and the NCS has $K$
basis functions, sine beyond knots $\xi_1$ and $\xi_K$ the derivatives higher than
the first must be zero. Example: $3$-spline (cubic spline) has $K+4$ functions,
and the NCS has $K$ basis functions.
\[G(x) = \sum_{j=1}^K \theta_j N_j(x)\]
where $(N_j(\cdot))_{j=1}^K$ is the set of basis functions for the Natural Cubic
Spline.

Goal: show that it is enough to know the value of the NCS at $K$ knots to know it
everywhere. Consider a regression problem on some interval $[a,b]$. Let $K$ be the
number of knots $(\xi_k)_{k=1}^K\in [a,b]$ with
\[a < \xi_1<\ldots<\xi_k<\ldots<\xi_K<b\]

Suppose $g$ is the NCS on $[a,b]$ -- a piecewise polynomial curve, cubic within
the main region and linear beyond $[a,b]$ (?). For all $k=1,\ldots K$ let
$g_k = g(\xi_k)$.
Since NCS is cubic, the second derivative must be linear in $x$ between any two
consecutive knots: for $x\in (\xi_k,\xi_{k+1}]$, $k=1,\ldots, K-1$
\[g''(x) = \alpha_k x + \beta_k\]

Introduce a set of extra parameters $(\gamma_k)_{k=1}^K$ which are defined as the
value of the second derivative of $g$ at each knot: $\gamma_k=g''(\xi_k)$.
Obviously, due to linearity at the ends $\gamma_K = \gamma_1 = 0$.
% Given these new parameters, let's try to squeeze the parameters from within
% by the smoothness results
For $k=1,\ldots, K-1$ the continuity of $g''$ is equivalent to $g''(\xi_k+0) = g''(\xi_k-0)$
which in terms of $\gamma_k$ is identical to
\begin{align*}
	\gamma_k &= \alpha_k \xi_k + \beta_k\\
	\gamma_{k+1} &= \alpha_k \xi_{k+1} + \beta_k
\end{align*}
for all $k=1,\ldots,K-1$. Since $\xi_k<\xi_{k+1}$ this system of linear equation
resolves into
\begin{align*}
	\alpha_k &= \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k}\\
	\beta_k &= \frac{\xi_{k+1}\gamma_k - \xi_k\gamma_{k+1}}{\xi_{k+1}-\xi_k}
\end{align*}
Therefore $g''(x)$ on $x\in (\xi_k, \xi_{k+1}]$ for $k=1,\ldots, K-1$ is defined as
\[g''(x) = \frac{\gamma_{k+1}(x-\xi_k)+\gamma_k(\xi_{k+1}-x)}{\xi_{k+1}-\xi_k}\]
Integrating on $x\in (\xi_k,\xi_{k+1}]$ yields
\[g'(x) = A_k + \frac{1}{2}\frac{\gamma_{k+1}(x-\xi_k)^2-\gamma_k(x-\xi_{k+1})^2}{\xi_{k+1}-\xi_k}\]
whence second integration yields on the same region results in
\[
g(x)
= A_k(x - \xi_k) + B_k + \frac{1}{6}
	\frac{\gamma_{k+1}(x-\xi_k)^3+\gamma_k(\xi_{k+1}-x)^3}{\xi_{k+1}-\xi_k}
\]
for all $x\in (\xi_k,\xi_{k+1}]$ and $k=1, K-1$. The regions beyond the boundaries
by definition must be linear, therefore $g(x) = A_0 (\xi_1-x) + B_0$ for $x\leq \xi_1$
and $g(x) = A_K (x-\xi_1) + B_K$ for $x > \xi_K$.

Now let's for a moment forget about the smoothness constraints and try to find
the parameters $A_k$ and $B_k$ on each interval $(\xi_k,\xi_{k+1}]$ from the values
$g_k = g(\xi_k)$. Since $g$ is continuous, for each $k=1,\ldots,K-1$ it must be
true that $g_k = g(\xi_k-) = g(\xi_k+)$, which is equivalent to
\begin{align*}
g_k &= A_k(\xi_k - \xi_k) + B_k + \frac{1}{6}
	\frac{\gamma_{k+1}(\xi_k-\xi_k)^3+\gamma_k(\xi_{k+1}-\xi_k)^3}{\xi_{k+1}-\xi_k}\\
	&= B_k + \frac{1}{6} \gamma_k (\xi_{k+1}-\xi_k)^2\\
g_{k+1} &= A_k(\xi_{k+1}-\xi_k) + B_k + \frac{1}{6}
	\frac{\gamma_{k+1}(\xi_{k+1}-\xi_k)^3+\gamma_k(\xi_{k+1}-\xi_{k+1})^3}{\xi_{k+1}-\xi_k}\\
	&= A_k(\xi_{k+1}-\xi_k) + B_k + \frac{1}{6} \gamma_{k+1}(\xi_{k+1}-\xi_k)^2
\end{align*}
This linear inhomogeneous system is trivial, as one of the unknowns is already
solved for 
\[B_k = g_k - \frac{1}{6} \gamma_k \delta_k^2\]
where $\delta_k = \xi_{k+1}-\xi_k$ for brevity. The second unknown is thus
\[A_k = \frac{g_{k+1} - g_k}{\delta_k} - \frac{1}{6} (\gamma_{k+1}-\gamma_k) \delta_k\]
Plugging these expressions back into $g(x)$ yields
\begin{align*}
g(x) &= \Bigl(
		\frac{g_{k+1} - g_k}{\delta_k} - \frac{\delta_k}{6} (\gamma_{k+1}-\gamma_k)
	\Bigr) (x - \xi_k) + g_k - \frac{\delta_k^2}{6} \gamma_k\\
	&\quad + \frac{1}{6\delta_k} \bigl(\gamma_{k+1}(x-\xi_k)^3+\gamma_k(\xi_{k+1}-x)^3\bigr)\\
	&= \frac{g_{k+1} - g_k}{\delta_k}(x - \xi_k) + g_k
	- \frac{\delta_k}{6} (\gamma_{k+1}-\gamma_k)(x - \xi_k) - \frac{\delta_k^2}{6} \gamma_k\\
	&\quad + \frac{\delta_k^2}{6} \Bigl(\gamma_{k+1}\Bigl(\frac{x-\xi_k}{\delta_k}\Bigr)^3+\gamma_k\Bigl(\frac{\xi_{k+1}-x}{\delta_k}\Bigr)^3\Bigr)\\
	&= \frac{g_{k+1}(x - \xi_k) + g_k(\xi_{k+1} - x)}{\delta_k}
	- \frac{\delta_k}{6} \bigl( \gamma_{k+1} (x - \xi_k) + \gamma_k (\xi_{k+1}-x)\bigr)\\
	&\quad + \frac{\delta_k^2}{6} \Bigl(\gamma_{k+1}\Bigl(\frac{x-\xi_k}{\delta_k}\Bigr)^3+\gamma_k\Bigl(\frac{\xi_{k+1}-x}{\delta_k}\Bigr)^3\Bigr)\\
\end{align*}
because
\begin{align*}
\frac{g_{k+1} - g_k}{\delta_k}(x - \xi_k) + g_k
	&= \frac{g_{k+1} - g_k}{\delta_k}(x - \xi_k) + \frac{g_k\xi_{k+1} - g_k\xi_k}{\delta_k}\\
	&= \frac{g_{k+1}(x - \xi_k) + g_k(\xi_{k+1} - x)}{\delta_k}\\
\gamma_k \delta_k + (\gamma_{k+1}-\gamma_k) (x - \xi_k)
	& = \gamma_k \xi_{k+1} - \gamma_k \xi_k
		+ \gamma_{k+1} x - \gamma_k x
		- \gamma_{k+1}\xi_k + \gamma_k \xi_k \\
	&= \gamma_{k+1} (x - \xi_k) + \gamma_k (\xi_{k+1}-x) 
\end{align*}
Therefore for $x\in (\xi_k,\xi_{k+1}]$
\begin{align*}
g(x) &= \Bigl( g_{k+1} \frac{x - \xi_k}{\delta_k} + g_k \frac{\xi_{k+1}-x}{\delta_k} \Bigr)
	- \frac{\delta_k^2}{6} \Bigl( \gamma_{k+1} \frac{x - \xi_k}{\delta_k} + \gamma_k \frac{\xi_{k+1}-x}{\delta_k} \Bigr)\\
	&\quad+ \frac{\delta_k^2}{6} \Bigl( \gamma_{k+1}\Bigl(\frac{x-\xi_k}{\delta_k}\Bigr)^3+\gamma_k\Bigl(\frac{\xi_{k+1}-x}{\delta_k}\Bigr)^3\Bigr)
\end{align*}
Group the last two terms on the right hand side into
\[
- \frac{\delta_k^2}{6} \Bigl(
	\gamma_{k+1}\frac{x - \xi_k}{\delta_k} \Bigl\{1-\Bigl(\frac{x-\xi_k}{\delta_k}\Bigr)^2\Bigr\}
	+\gamma_k\frac{\xi_{k+1}-x}{\delta_k} \Bigl\{1-\Bigl(\frac{\xi_{k+1}-x}{\delta_k}\Bigr)^2\Bigr\}
\Bigr)
\]
Since 
\[
1-\frac{x-\xi_k}{\delta_k} = \frac{\xi_{k+1}-x}{\delta_k }
\text{ and }
1-\frac{\xi_{k+1}-x}{\delta_k} = \frac{x-\xi_k}{\delta_k }
\]
and the terms in curly brackets are differences of squares both terms reduce to
\begin{multline*}
\ldots = - \frac{\delta_k^2 }{6} \frac{\xi_{k+1}-x}{\delta_k} \frac{x-\xi_k}{\delta_k} \Bigl(
		\gamma_{k+1}\Bigl\{1+\frac{x-\xi_k}{\delta_k}\Bigr\}
		+ \gamma_k\Bigl\{1+\frac{\xi_{k+1}-x}{\delta_k}\Bigr\}
	\Bigr)\\
= - \frac{1}{6} (\xi_{k+1}-x)(x-\xi_k) \Bigl(
	\gamma_{k+1}\Bigl\{1+\frac{x-\xi_k}{\delta_k}\Bigr\}
	+\gamma_k\Bigl\{1+\frac{\xi_{k+1}-x}{\delta_k}\Bigr\}
\Bigr)
\end{multline*}
Thus over the partition within the main region the expression of the NCS becomes
\begin{align*}
g(x) &= - \frac{1}{6} (\xi_{k+1}-x)(x-\xi_k) \Bigl(
		\gamma_{k+1}\Bigl\{1+\frac{x-\xi_k}{\delta_k}\Bigr\}
		+\gamma_k\Bigl\{1+\frac{\xi_{k+1}-x}{\delta_k}\Bigr\}
	\Bigr)\\
	&\quad +\Bigl( g_{k+1} \frac{x - \xi_k}{\delta_k} + g_k \frac{\xi_{k+1}-x}{\delta_k} \Bigr)
\end{align*}
for any $x\in(\xi_k, \xi_{k+1}]$ and each $k=1,\ldots, K-1$. Since for all $k=1,\ldots, K-1$
\[\lim_{x\downarrow\xi_k} g(x) = g(\xi_k-) = g_k\]
and formally $g_k = g(\xi_k)$, the domain of each polynomial piece can be a closed
interval $[\xi_k, \xi_{k+1}]$.

As for the branches outside the main region, they are given by the following
expressions with two as of yet ``free'' parameters:
\[g(x) = \begin{cases}
	A_0(\xi_1-x)+B_0, &\text{ if }x\leq \xi_1\\
	A_K(x-\xi_K)+B_K, &\text{ if }x > \xi_K
\end{cases}\]
since integration of $g''(x)$, which is zero over these regions, yields lines. Using
the continuity conditions $g(\xi_1-)=g_1$ an $g(\xi_K+)=g_K$ at $\xi_1$ and $\xi_K$
respectively implies that
\[B_0 = g_1\text{ and }B_K = g_K\]

Recall that the derivative of $g(x)$ over $(\xi_k, \xi_{k+1})$ is (even though $A_k$
are mostly known)
\[g'(x) = A_k + \frac{1}{2\delta_k}\bigl(\gamma_{k+1}(x-\xi_k)^2-\gamma_k(x-\xi_{k+1})^2\bigr)\]
Though the domain of the derivative of each piece is $(\xi_k, \xi_{k+1})$,
the requirement that the derivative of $g$ be continuous makes it possible to use
closed intervals instead of half-open.

Now at the boundary at the $\xi_1$ knot the expressions for the left hand and the
right hand derivatives are
\begin{align*}
	g'(x) &= \bigl[ \text{ if } x \leq \xi_1 \bigr]
		= - A_0\\
	g'(x) &= \bigl[ \text{ if } x > \xi_1 \bigr]
		= A_1 + \frac{1}{2\delta_1}\bigl(\gamma_2(x-\xi_1)^2-\gamma_1(x-\xi_2)^2\bigr)
\end{align*}
The smoothness requirement
\[g'(\xi_1+) = \lim_{x\downarrow \xi_k} g'(x) = \lim_{x\uparrow \xi_k} g'(x) = g'(\xi_1-)\]
implies the following equation
\[- A_0 = A_1 - \frac{\delta_1}{2}\gamma_1\]
However $\gamma_1 = 0$ means that $- A_0 = A_1$. At the knot $\xi_K$ similar reasoning
yields
\[
A_K
= A_{K-1} + \frac{\delta_{K-1}}{2}\gamma_K
= A_{K-1}
\]
which means that the ``parameters'' of the line segments beyond the main region
were not free after all.

There must be some relation between $(g_k)_{k=1}^K$ and $(\gamma_k)_{k=2}^{K-1}$
since continuity and $2$-nd order smoothness across the knots are quite strict
requirements. For $k=1,\ldots,K-1$ and $x\in [\xi_k,\xi_{k+1}]$
\[g'(x) = A_k + \frac{1}{2\delta_k}\bigl(\gamma_{k+1}(x-\xi_k)^2-\gamma_k(x-\xi_{k+1})^2\bigr)\]
The very same smoothness condition $g'(\xi_k+)=g'(\xi_k-)$ for $k=2,\ldots,K-1$ results in
\begin{align*}
	g'(\xi_k+) &= A_k + \frac{1}{2\delta_k}\bigl(\gamma_{k+1}(\xi_k-\xi_k)^2-\gamma_k(\xi_k-\xi_{k+1})^2\bigr)\\
	g'(\xi_k-) &= A_{k-1} + \frac{1}{2\delta_{k-1}}\bigl(\gamma_k(\xi_k-\xi_{k-1})^2-\gamma_{k-1}(\xi_k-\xi_k)^2\bigr)
\end{align*}
whence for $k=2,\ldots,K-1$
\begin{align*}
	g'(\xi_k+) &= A_k - \frac{1}{2}\gamma_k\delta_k\\
	g'(\xi_k-) &= A_{k-1} + \frac{1}{2}\gamma_k\delta_{k-1}
\end{align*}
This gives an equation
\[A_k - A_{k-1} = \frac{1}{2}\gamma_k(\delta_k+\delta_{k-1})\]
where the left hand side is equal to
\[A_k - A_{k-1}
= \frac{g_{k+1} - g_k}{\delta_k} - \frac{1}{6} (\gamma_{k+1}-\gamma_k) \delta_k
- \frac{g_k - g_{k-1}}{\delta_{k-1}} + \frac{1}{6} (\gamma_k-\gamma_{k-1}) \delta_{k-1}
\]
After minor rearrangement
\[
\frac{1}{6}\bigl(3\gamma_k(\delta_k+\delta_{k-1})
+ (\gamma_{k+1}-\gamma_k) \delta_k
- (\gamma_k-\gamma_{k-1}) \delta_{k-1} \bigr)
= \frac{g_{k+1} - g_k}{\delta_k} - \frac{g_k - g_{k-1}}{\delta_{k-1}}
\]
The right hand side simplifies to
\begin{multline*}
\frac{g_{k+1} - g_k}{\delta_k} - \frac{g_k - g_{k-1}}{\delta_{k-1}}
= \frac{g_{k+1}\delta_{k-1} - g_k(\delta_k + \delta_{k-1}) + g_{k-1}\delta_k}{\delta_k\delta_{k-1}}\\
= \frac{1}{\delta_k}g_{k+1} + \Bigl(-\frac{1}{\delta_{k-1}} - \frac{1}{\delta_k}\Bigr)g_k + \frac{1}{\delta_{k-1}}g_{k-1}
\end{multline*}
while the right hand side (without the multiplier) reduces to
\begin{multline*}
	3\gamma_k(\delta_k+\delta_{k-1}) + (\gamma_{k+1}-\gamma_k) \delta_k - (\gamma_k-\gamma_{k-1}) \delta_{k-1}\\
	= 3\gamma_k(\delta_k+\delta_{k-1}) + \gamma_{k+1}\delta_k - \gamma_k\delta_k - \gamma_k\delta_{k-1} + \gamma_{k-1}\delta_{k-1}\\
	= 3\gamma_k(\delta_k+\delta_{k-1}) + \gamma_{k+1}\delta_k - \gamma_k(\delta_k +\delta_{k-1}) + \gamma_{k-1}\delta_{k-1}\\
	= \gamma_{k+1}\delta_k + 2\gamma_k(\delta_k+\delta_{k-1}) + \gamma_{k-1}\delta_{k-1}
\end{multline*}
Hence vectors $g = (g_k)_{k=1}^K\in \Real^{K\times 1}$ and
$\gamma = (\gamma_k)_{k=2}^{K-1}\in \Real^{(K-2)\times 1}$
are related through the following equations:
\[
\frac{\delta_k}{6} \gamma_{k+1} + \frac{\delta_k+\delta_{k-1}}{3} \gamma_k + \frac{\delta_{k-1}}{6}\gamma_{k-1}
= \frac{1}{\delta_k}g_{k+1} + \Bigl(-\frac{1}{\delta_{k-1}} - \frac{1}{\delta_k}\Bigr)g_k + \frac{1}{\delta_{k-1}}g_{k-1}
\]
Or in the matrix notation
\[Q'g = R\gamma\]
where $R\in \Real^{(K-2)\times(K-2)}$ and $Q\in \Real^{K\times(K-2)}$.
The matrix $R$ is a tri-diagonal square $(K-2)\times(K-2)$ matrix with elements
\[
r_{ij} = \begin{cases}
	\frac{\delta_{i+1}+\delta_i}{3}, &\text{ if } j=i\\
	\frac{1}{6}\delta_i, &\text{ if } j=i-1\\
	\frac{1}{6}\delta_{i+1}, &\text{ if } j=i+1\\
	0, &\text{ otherwise }
\end{cases}
\]
The matrix $Q$ is also a tri-diagonal matrix, but it is rectangular $K\times(K-2)$.
Its elements are given by
\[
q_{ij} = \begin{cases}
	\frac{1}{\delta_i}, &\text{ if } i=j\\
	-\Bigl(\frac{1}{\delta_i}+\frac{1}{\delta_{i+1}}\Bigr), &\text{ if } i=j+1\\
	\frac{1}{\delta_{i+1}}, &\text{ if } i=j+2\\
	0, &\text{ otherwise }
\end{cases}
\]
The matrices $Q$ and $R$ have the following structure:
\begin{align*}
Q &= \begin{pmatrix}
q_{11} 	& 0 		& 0 		& \cdots & 0 \\
q_{21} 	& q_{22} 	& 0 		& \cdots & 0 \\
q_{31} 	& q_{32} 	& q_{33} 	& \cdots & 0 \\
0 		& q_{42} 	& q_{43} 	& \cdots & 0 \\
0 		& 0 		& q_{53} 	& \cdots & 0 \\
\vdots 	& \vdots	& \vdots 	& \ddots & \vdots \\
0 		& 0			& 0 		& \cdots & q_{K(K-2)}
\end{pmatrix}_{K\times (K-2)}\\
R &= \begin{pmatrix}
r_{11} 	& r_{12} 	& 0 		& \cdots & 0 \\
r_{21} 	& r_{22} 	& r_{23} 	& \cdots & 0 \\
0 		& r_{32} 	& r_{33} 	& \cdots & 0 \\
\vdots 	& \vdots	& \vdots 	& \ddots & \vdots \\
0 		& 0			& 0 		& \cdots & r_{(K-2)(K-2)}
\end{pmatrix}_{(K-2)\times (K-2)}
\end{align*}
respectively. The matrix $R$ is symmetric, since for all $k=1,\ldots, K-2$
\[
r_{(k+1)k}
= r_{(k+1)((k+1)-1)}
= \frac{\delta_{k+1}}{6}
= r_{k(k+1)}
\]
and all other elements except for the diagonal are zero. Furthermore the matrix
$R$ is positive definite. Indeed the product $\gamma'R\gamma$ is given by
\begin{align*}
\gamma'R\gamma
	&= \sum_{k=2}^{K-1} \gamma_k\Bigl( \frac{\delta_k}{6} \gamma_{k+1} + \frac{\delta_k+\delta_{k-1}}{3} \gamma_k + \frac{\delta_{k-1}}{6}\gamma_{k-1} \Bigr)\\
	&= \sum_{k=2}^{K-1} \frac{\delta_k}{6} \bigl(\gamma_{k+1}\gamma_k + 2 \gamma_k^2\bigr)
		+ \frac{\delta_{k-1}}{6} \bigl( 2\gamma_k^2 + \gamma_{k-1}\gamma_k \bigr)\\
	&= \frac{\delta_{K-1}}{6} \bigl(\gamma_K\gamma_{K-1} + 2 \gamma_{K-1}^2\bigr)
		+ \sum_{k=2}^{K-2} \frac{\delta_k}{6} \bigl(\gamma_{k+1}\gamma_k + 2 \gamma_k^2\bigr) \\
	&\quad + \sum_{k=3}^{K-1} \frac{\delta_{k-1}}{6} \bigl( 2\gamma_k^2 + \gamma_{k-1}\gamma_k \bigr)
		+ \frac{\delta_1}{6} \bigl( 2\gamma_2^2 + \gamma_1\gamma_2 \bigr)\\
	&= \frac{\delta_{K-1}}{3} \gamma_{K-1}^2
		+ \sum_{k=2}^{K-2} \Bigl( \frac{\delta_k}{6} \bigl(\gamma_{k+1}\gamma_k + 2 \gamma_k^2\bigr)
			+ \frac{\delta_k}{6} \bigl( 2\gamma_{k+1}^2 + \gamma_k\gamma_{k+1} \bigr)\Bigr)
		+ \frac{\delta_1}{3}\gamma_2^2\\
	&= \frac{\delta_{K-1}}{3} \gamma_{K-1}^2 + \frac{\delta_1}{3}\gamma_2^2
		+ \sum_{k=2}^{K-2} \frac{\delta_k}{6} \bigl(2\gamma_{k+1}\gamma_k + 2 \gamma_k^2 + 2\gamma_{k+1}^2\bigr)\\
	&= \frac{\delta_{K-1}}{3} \gamma_{K-1}^2 + \frac{\delta_1}{3}\gamma_2^2
		+ \sum_{k=2}^{K-2} \frac{\delta_k}{6} \bigl((\gamma_{k+1}+\gamma_k)^2 -\gamma_{k+1}^2 - \gamma_k^2 + 2 \gamma_k^2 + 2\gamma_{k+1}^2\bigr)\\
	&= \frac{\delta_{K-1}}{3} \gamma_{K-1}^2 + \frac{\delta_1}{3}\gamma_2^2
		+ \sum_{k=2}^{K-2} \frac{\delta_k}{6} \bigl((\gamma_{k+1}+\gamma_k)^2 + \gamma_k^2 + \gamma_{k+1}^2\bigr)
\end{align*}
Therefore the matrix $R$ must be non-singular.

\noindent To reiterate, the final form of the NCS is given by
\[
g(x) = \begin{cases}
	A_0(\xi_1-x)+g_1, &\text{ if }x \leq \xi_1\\
	A_K(x-\xi_K)+g_K, &\text{ if }x \geq \xi_K\\
	\Bigl(
		g_{k+1} \frac{x - \xi_k}{\delta_k} + g_k \frac{\xi_{k+1}-x}{\delta_k}
	\Bigr) - \frac{1}{6} (\xi_{k+1}-x)(x-\xi_k) \cdot\\
	\quad\quad\cdot \Bigl\{
		\gamma_{k+1}\Bigl(1+\frac{x-\xi_k}{\delta_k}\Bigr)
		+\gamma_k\Bigl(1+\frac{\xi_{k+1}-x}{\delta_k}\Bigr)
	\Bigr\}, &\text{ if } x\in [\xi_k,\xi_{k+1}]
\end{cases}
\]
with
\begin{align*}
	A_0 &= -g'(\xi_1) = - A_1 =  \frac{1}{6} \gamma_2 \delta_1 - \frac{g_2 - g_1}{\delta_1}\\
	A_K &= g'(\xi_K) = A_{K-1} = \frac{g_K - g_{K-1}}{\delta_{K-1}} + \frac{1}{6} \gamma_{K-1} \delta_{K-1}
\end{align*}

\noindent\textbf{Theorem}\hfill \\
The vectors of values $g$ and $2$-nd derivatives $\gamma$ fully specify the NCS
if and only if $Q'g = R'\gamma$. When this is satisfied, then $\gamma = R^{-1}Q'g$ and
\[
\int_a^b \abs{g''(s)}^2 ds
= \gamma' R \gamma
= g' Q R^{-1} R R^{-1} Q' g
= g' K g
\]
where the ``penalty matrix'' $K = Q R^{-1} Q'$ is a positive semi-definite matrix. Indeed,
\begin{align*}
	\int_a^b \bigl|g''(s)\bigr|^2 ds
% Simple integration by parts
	&= \Bigl. g''(s) g'(s) \Bigr\rvert_a^b - \int_a^b g'(s) g'''(s) ds \\
% Since on $(a, \xi_1]$ and $[\xi_K, b)$ the function $g$ is linear. And g''(a) = g''(b) = 0
	&= - \int_{\xi_1}^{\xi_K} g'(s) g'''(s) ds
		= - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} g'(s) g'''(s) ds\\
% Due to piecewise linearity and continuity of g''
	&= - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} g'(s) \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} ds\\
% But g''' is constant over each interval (and the Lebesgue measure has  dx({s}) = 0)
	&= - \sum_{k=1}^{K-1} \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} \int_{\xi_k}^{\xi_{k+1}} g'(s) ds\\
% Since \int_{\xi_k}^{\xi_{k+1}} g'(s) ds = g(\xi_{k+1}) - g(\xi_k)
	&= - \sum_{k=1}^{K-1} \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} \bigl(g_{k+1}-g_k\bigr)\\
	&= \sum_{k=1}^{K-1} \gamma_k\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k}
		- \sum_{k=1}^{K-1} \gamma_{k+1}\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k}\\
% Since \gamma_1 = \gamma_K = 0
	&= \sum_{k=2}^{K-1} \gamma_k \Bigl(\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k} - \frac{g_k-g_{k-1}}{\xi_k-\xi_{k-1}} \Bigr)\\
% By definition of Q'g
	&= \sum_{k=2}^{K-1} \gamma_k \bigl[Q'g\bigr]_k\\
	&= \gamma' Q' g = \gamma' R \gamma
\end{align*}
A nice thing is that this integral can be used as a penalty term in a regularized
regression problem.

ANother important property of the NCS is summarized in the following theorem:
\noindent \textbf{Theorem}\hfill\\
the NCS has the minimum value of the smoothness integral $\int_a^b |g''(s)| ds$
among all $C^2[a,b]$-smooth curves interpolating the data $(\xi_k, g_k)_{k=1}^K$.

This means that formally we have the following: if $K\geq 2$ and $g$ is the NCS,
interpolating $g_1,\ldots,g_K$ on knots $a\leq \xi_1 <\ldots<\xi_K\leq b$ within
a bounded interval $[a,b]$, then for any other interpolating function $\tilde{g}$
on the same mesh, the total variation of $\tilde{g}''$ is necessarily grater than
that of $g$.

Indeed, consider an at least $C^2[a,b]$ smooth function $\tilde{g}$, which interpolates
the data $(\xi_k, g_k)_{k=1}^K$. Then the function $h \defn g - \tilde{g}$ has roots at
knots $(\xi_k)_k$. First notice that (true for any $g$ with piecewise linear second
derivative)
\begin{align*}
	\int_a^b h'' g'' ds
%% Integrate by parts using the nice properties of g 
	& = \Bigl. h'(s) g''(s)\Bigr\rvert_a^b - \int_a^b h'(s) g'''(s) ds \\
%% since g''' is zero outside [\xi_1, \xi_K] and g'' is zero at the
%%  endpoints of the interval [a,b]
	& = - \int_{\xi_1}^{\xi_K} h'(s) g'''(s) ds \\
	& = - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} h'(s) g'''(s) ds \\
%% g''' is piecewise constant
	& = \Bigl[g'''(x) = C_k\text{ on } x\in[\xi_k, \xi_{k+1}]\Bigr]
	& = - \sum_{k=1}^{K-1}  C_k \int_{\xi_k}^{\xi_{k+1}} h'(s) ds \\
	& = - \sum_{k=1}^{K-1}  C_k (h(\xi_{k+1})-h(\xi_k)) = 0
\end{align*}
The total variation of the $2$-nd derivative of $\tilde{g}$ is
\begin{align*}
	\int_a^b |\tilde{g}''|^2 ds
	&= \int_a^b \bigl|g'' + h''\bigr|^2 ds
	= \int_a^b |g''|^2 + |h''|^2 ds + 2 \int_a^b h'' g'' ds\\
	&= \int_a^b |g''|^2 + |h''|^2 ds \geq \int_a^b |g''|^2 ds
\end{align*}
Now, if $\tilde{g} = g+h \in C^2[a,b]$ interpolates the knots and is such that
\[\int_a^b |\tilde{g}''|^2 ds = \int_a^b |g''|^2 ds\]
then $\int_a^b |h''|^2 ds = 0$, which implies that $|h''|=0$ on $[a,b]$. Therefore
$h(x)=Ax + B$ and $\tilde{g}(x) = g(x) + A x + B$. But since $\tilde{g}$ and $g$
interpolate the data, it must be true that $A \xi_k + B = 0$ for all $k$. Hence
$A=B=0$ and $h=0$.

Obviously, if $\tilde{g}=g$ then $\int_a^b |\tilde{g}''|^2 ds=\int_a^b |g''|^2 ds$.

So far the following wonderful properties have been demonstrated for the NCS
\begin{itemize}
	\item on $[a,b]$ it is completely determined by its values $(g_k)_k$ at
	the interpolation knots and the values of the second derivatives at the
	internal nodes are computed via $\gamma = R^{-1}Q'g$;
	\item it brings the value of the $\int_a^b |g''|^2 ds$ to a minimum in the class
	of $C^2[a,b]$ smooth curves interpolating the data at the same knots;
	\item to interpolate at $K$ knots it requires only $K$ basis functions.
\end{itemize}

% subsubsection natural_cubic_spline (end)

\subsubsection{Tutorial \# 3 problem \# 1} % (fold)
\label{ssub:tutorial_3_problem_1}

Consider a cubic spline with $K$ knots
\[f(x) = \sum_{k=0}^3 \beta_k x^k + \sum_{k=1}^K \theta_k \bigl(x-\xi_k\bigr)_+^3\]
let's show that the boundary conditions for the NCS imply the following constraints
on the coefficients...

Indeed, first, let's find the derivatives of $f$ up to order $3$:
\begin{align*}
	f'(x) &= \sum_{k=1}^3 k\beta_k x^{k-1} + \sum_{k=1}^K 3 \theta_k \bigl(x-\xi_k\bigr)_+^2\\
	f''(x) &= \sum_{k=2}^3 k(k-1)\beta_k x^{k-2} + \sum_{k=1}^K 6 \theta_k \bigl(x-\xi_k\bigr)_+\\
	f'''(x) &= 6 \beta_3 + \sum_{k=1}^K 6 \theta_k 1_{x\geq \xi_k}
\end{align*}
For any $x\geq \xi_K$, we have $f'''(x) = 0$ and $1_{x\geq \xi_k}=1$, which
implies that
\[6 \beta_3 + 6 \sum_{k=1}^K \theta_k = 0\]
on the other hand, for $x < \xi_1$ we get 
\[6 \beta_3 = 0\]
Therefore $\beta_3 = 0$ and $\sum_{k=1}^K \theta_k = 0$.

With these results, the second derivative becomes
\[
f''(x)
= 2\beta_2
	+ 6 \sum_{k=1}^K \theta_k x 1_{x\geq \xi_k} - \theta_k \xi_k 1_{x\geq \xi_k}
\]
The second derivative of the NCS is restricted to be continuous, i.e.
$f''(\xi_j-)=f''(\xi_j+)$ for $j=1\ldots K$. In particular for $j=1,K$ this implies
that
\begin{align*}
	0 &= f''(\xi_1-) = f''(\xi_1+)
	= 2\beta_2 + 6 \theta_1 \xi_j - \theta_1 \xi_1\\
	0 &= f''(\xi_K+) = f''(\xi_K+)
	= 2\beta_2 + 6 \sum_{k=1}^{K-1} \theta_k \xi_K - \theta_k \xi_k\\
	&= 2\beta_2 + 6 \sum_{k=1}^K \theta_k \xi_K - \theta_k \xi_k
\end{align*}
whence $\beta_2 = 0$ and 
\[\sum_{k=1}^K \theta_k \xi_K = \sum_{k=1}^K \theta_k \xi_k\]
using previously obtained constraints
\[
\sum_{k=1}^K \theta_k \xi_K
= \xi_K \sum_{k=1}^K \theta_k
= 0
\]
Thus $\sum_{k=1}^K \theta_k \xi_k = 0$.

These constraints imply that a cubic spline, satisfying the NCS restrictions can be
expressed as
\[f(x) = \beta_0 + \beta_1 x + \sum_{k=1}^K \theta_k \bigl(x-\xi_k\bigr)_+^3\]
Now, the constraint $\sum_{k=1}^K \theta_k = 0$ yields
\[
\sum_{k=1}^K \theta_k \bigl(x-\xi_k\bigr)_+^3
= \sum_{k=1}^{K-1} \theta_k \Bigl((x-\xi_k)_+^3 - (x-\xi_K)_+^3\Bigr)
\]
Similarly $\theta_K = - \sum_{k=1}^{K-1} \theta_k$ implies
\[ \sum_{k=1}^K \theta_k \xi_k = \sum_{k=1}^{K-1} \theta_k ( \xi_k - \xi_K ) \]
The second restriction $\sum_{k=1}^K \theta_k \xi_k = 0$ implies
\[
\theta_{K-1} = - \frac{1}{\xi_K - \xi_{K-1}}\sum_{k=1}^{K-2} \theta_k ( \xi_K - \xi_k )
\]
Now define $\phi_k(x) = (x-\xi_k)_+^3 - (x-\xi_K)_+^3$ and observe that
\[
\sum_{k=1}^K \theta_k \bigl(x-\xi_k\bigr)_+^3
= \sum_{k=1}^{K-1} \theta_k \phi_k(x)
= \sum_{k=1}^{K-2} \theta_k \Bigl( \phi_k(x) - \frac{\xi_K - \xi_k}{\xi_K - \xi_{K-1}} \phi_{K-1}(x) \Bigr)
\]
whence
\[
\sum_{k=1}^K \theta_k \bigl(x-\xi_k\bigr)_+^3
= \sum_{k=1}^{K-2} \theta_k (\xi_K - \xi_k) \Bigl( \frac{\phi_k(x)}{\xi_K - \xi_k} - \frac{\phi_{K-1}(x)}{\xi_K - \xi_{K-1}} \Bigr)
\]
Thus for
\[ d_k(x) = \frac{\phi_k(x)}{\xi_K - \xi_k} \]
the following set of maps is a candidate for a ``basis'' of the NCS:
\begin{align*}
	N_1(x) &= 1\\
	N_2(x) &= x\\
	N_{k+2}(x) &= d_k(x) - d_{K-1}(x)
\end{align*}
and
\[f(x) = \sum_{k=1}^K \tilde{\theta}_k N_k(x)\]

% subsubsection tutorial_3_problem_1 (end)

% subsection piecewise_polynomials_and_splines (end)

\subsection{Smoothing spline} % (fold)
\label{sub:smoothing_spline}

The goal of fitting a smoothing spline is to find a function $g(x)$ among all functions
with two continuous derivatives, that minimizes the following penalized sum of squares
\[\RSS(g,\lambda) = \sum_{i=1}^n \brac{t_i - g(x_i)}^2 + \lambda \int |g''(s)|^2 ds\]
The functional basically tries to fit the data as much as possible with $g$ and
while penalizing it for irregular behaviour. The penalty term sums up non-linearities
in the smoother $g$ and is higher the more oscillations the $g$ has. This is a problem
of calculus of variations and functional analysis.

Intuitively, the regularizing parameter $\lambda$ should affect the optimal solution
in the following way: \begin{itemize}
	\item $\lambda\to \infty$ then the optimal solution is linear: $\int_a^b |g''|^2 ds > 0$
	gets penalized very highly, whence it should be optimal to chose $g$ with the
	irregularity penalty as close to $0$ as possible. In the limit of $\lambda$
	the term $\int_a^b |g''|^2 ds$ must be zero, which implies that $\hat{g}(x) = Ax + B$;
	\item $\lambda = 0$ then the optimal solution is some ``wild'' function, which
	matches all the points $(x_i, t_i)_{i=1}^n$, with no structure known beforehand.
\end{itemize}

%%% HERE!!!

Consider a function $g(\xi_k) = \bar{g}(\xi_k)$ for all $k$.
Consider as $\bar{g}$ an NCS at knots given by $\brac{\xi_k}_{k=1}^K$.
Since an NCS has the lest variation integral, the solution of the problem cannot be anything but the NCS.
\[\int \abs{g''(s)}^2 ds\geq \int \abs{\bar{g}''(s)}^2 ds\]

Thus the optimal solution to this infinite dimensional problem is a quite finite dimensional NCS.
% Surprise!
Therefore the problem reduces to the following quadratic optimization problem:
\[\RSS(g,\lambda) = {(t - g)}'{(t - g)} + \lambda g'K g \to \min_g\]
\[\RSS(g,\lambda) = t't - g't - t'g + g'( I + \lambda K )g  \to \min_g\]
using the matrix derivatives, the solution turns out to be
\[\frac{\partial}{\partial g} \quad:\quad 2( I + \lambda K )g - 2 t = 0\]
which reduces to
\[\hat{g} = \brac{I + \lambda K}^{-1} t\]

The matrix $\brac{I + \lambda K}^{-1}$ is called the \textbf{smoother matrix} $S_\lambda$.

Where does the smoothing take place? 

Define the effective degree freedom as, again, the trace of $S_\lambda$.
\[\text{df}(\lambda) = \tr\brac{S_\lambda}\]

Use the SVD of $S_\lambda$ to extract the eigenvalues and eigenvectors.

Since $K \defn Q R^{-1} Q'$, whence $K$ is symmetric. The unit matrix is symmetric, hence $S_\lambda$ is symmetric and its inverse is symmetric.
% Thus it is an Hermitian matrix, hence the eigenvalues must be real.
\[S_\lambda = \sum_{i=1}^n \lambda_i u_i u_i' = U\Lambda U'\]

Since  $S_\lambda$ is positive-semidefinite, all eigenvalues are non-negative.

\begin{itemize}
	\item The eigenvalues of $K$ do not depend on the parameter $\lambda$;
	\item Both $S_\lambda$ and $K$ have the same eigenvectors.
\end{itemize}

If $\eta$ -- the eigenvalues of $K$, then $\text{det}\brac{K-\eta I} = 0$.
Thus \[\text{det}\brac{\frac{1}{\lambda}\brac{\brac{I+\lambda K} -(1 + \lambda \eta) I} } = 0\]

Thus $1 + \lambda \eta$ are the eigenvalues of $\brac{I+\lambda K}$, and $\frac{1}{1 + \lambda \eta}$ is the eigenvalue of $S_\lambda$.
Thus $\lambda_j = \frac{1}{1+\lambda \eta_{n-j+1}}$ is a decreasing function of $\lambda$.

Consider a $u_j$ the eigenvector of $S_\lambda$ corresponding to $\lambda_j$.
Then $S_\lambda u_j = \lambda_j u_j$ and
\[u_j = \lambda_j \brac{S_\lambda}^{-1} u_j
= \lambda_j \brac{I+\lambda K} u_j
= \lambda_j u_j + \lambda_j \lambda K u_j\]
Whence
\[K u_j = \frac{1-\lambda_j}{\lambda \lambda_j} u_j = \eta_{n-j+1} u_j\]
therefore the eigenvectors of $S_\lambda$ are the eigenvectors of $K$ (with other eigenvalues).


Thus due to the spectral decomposition:
\[g = S_\lambda t = \sum_{j=1}^n \lambda_j u_j u_j' t = \sum_{j=1}^n \lambda_j \brkt{u_j, t} u_j \]

Since each eigenvector $u_j$ is independent of the $\lambda$, the projections on are invariant under $\lambda$. Therefore models are comparable.

\[\sum_{j=1}^n \frac{1}{1+\lambda\eta_{n-j+1}} \brkt{u_j, t} u_j\]
some smoothing take place

The higher the $j$ the more smoothing take place:
	the less principal component is, the more smoothed it is.

\[g = \lambda_1\brkt{u_1, t} u_1 + \lambda_2\brkt{u_2, t} u_2 + \sum_{j=3}^n \lambda_j \brkt{u_j, t} u_j \]
The nature of the matrix $K$ makes the first $\lambda_1$ and $\lambda_2$ equal to $1$.

What is the relationship between $x$ and $u_1$?

The smoothing spline does not shrink linear components.

Higher $u_j$ correspond to more and more oscillating functions of $x$.

% Reproducing Hilbert spaces

% subsection smoothing_spline (end)

% section lecture_4 (end)

\clearpage
\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

\[\RSS = \sum_{i=1}^n \brac{t_i - g(x_i)}^2 + \lambda \int \abs{g''(s)}^2ds\]
The solution to this problem is the Natural cubic spline , which is finite dimensional reduces the problem to
\[\brac{t-g}'\brac{t-g} + \lambda g'K g\]
where $K$ is a symmetric and positive  semi-definite matrix
the fitted $\hat{g}$ are determined by the 
\[S_\lambda = \brac{I+\lambda H}^{-1}\]

The smoother matrix $S_\lambda$ is positive semi-definite and has eigenvalues $\lambda_k$. Why does $\lambda_1$ and $\lambda_2$ are equal to $1$?

%% See handwritten notes

What happens when the soother is applied again to the smoothed output?
In the case of a linear regression we get the same output.

Re-applying the smoother to $\hat{g}$ yields a more linear fit, since the first pair of eigenvalues are unit, thus preserving the linear components, while other higher degree polynomial components diminish.

Indeed, \begin{align*}
	\hat{g}_2 & = \sum_{k=1}^n \lambda_k \langle\hat{g}, u_k\rangle u_k \\
	& = \sum_{k=1}^n \lambda_k \Bigl\langle \sum_{j=1}^n \lambda_j \langle g, u_j\rangle u_j, u_k\Bigr\rangle u_k \\
	& = \sum_{k=1}^n \sum_{j=1}^n \lambda_j \lambda_k \langle g, u_j\rangle \langle u_j, u_k\rangle u_k \\
	& = \sum_{k=1}^n \sum_{j=1}^n \lambda_j \lambda_k \langle g, u_j\rangle \delta_{kj} u_k \\
	& = \sum_{k=1}^n \lambda_k^2 \langle g, u_k\rangle u_k
\end{align*}

The projection matrix is idempotent, hence for any eigenvector-eigenvalue pair $(\lambda, u)$ it is true that $\lambda u = Hu = H^2u = H\lambda u = \lambda Hu = \lambda^2 u$. Thus $\lambda$ is either $\pm1$ or $0$.

The solution to the weighted problem is the same Natural Cubic spline.
\[\sum_{i=1}^n \omega_i \brac{t_i - g(x_i)}^2 + \lambda \int \abs{g''(s)}^2 ds \to \min\]
Since again, we can take an NCS with the same first sum but with lesser penalty term.

The solution is $\hat{g} = \brac{W+\lambda K}^{-1} Wt$.

\subsection{Model selection} % (fold)
\label{sub:model_selection}

%% See the handwritten notes

Four adjustments to the likelihood, which incorporate the complexity of the model.

Elements of information theory

Suppose $x$ is a discrete random variable.
The main question is how much information one gets when one observes a realisation of $X$.

The amount of information is quantified by the amount of ``'surprise'':\begin{itemize}
	\item very likely -- not surprising;
	\item very unlikely -- very surprising.
\end{itemize}

$H$ the measure of information has the following properties: \begin{description}
	\item[Monotonicity] $H$ is a monotonic functional of the probability measure function;
	\item[Independence] If events are independent or unrelated, then the information gained from both must be poled together: in the case of densities this means that
\[H(X,Y) = H(X)+H(Y)\]
when $p_{X,Y}(x,y) = p_X(x) p_Y(y)$;
	\item[MISSING]
\end{description}

Average amount of information for a discrete random variable
\[H(X) = \ex\brac{H(X)} = - \sum_x p(x) \log_2 p(x)\]
for a continuous:
\[H(X) = \int \log_2\frac{1}{p(x)} p(x) dx\]

Suppose the $f$ is unknown distribution modelled using $\hat{f}$.

The Kullback-Leibler \textbf{divergence} between $f$ and $\hat{f}$: the additional information need to describe the random variable with $f$ using $\hat{f}$: \[\int f(x) \log_2\frac{1}{\hat{f}(x)} dx - \int f(x) \log_2\frac{1}{f(x)} dx \leq 0 \]

Use Jensen's inequality $\ex g(X) \geq g\brac{\ex X}$ for any convex $g$.
\begin{align*}
	-\int f(x) \log_2\frac{\hat{f}(x)}{f(x)} dx & \geq -\log_2\int f(x) \frac{\hat{f}(x)}{f(x)}  dx = 
\end{align*}

First criterion of model selection (Akaike; 1973).

Suppose $\brac{Y_i}_{i=1}^n$ is some data generate according to some unknown density $g\brac{\induc{y}\theta_0}$. Let there be a parametric family of models
\[M_K \defn \obj{\induc{f\brac{\induc{y}\theta_k}}\,\theta_k\in \Theta}\]
where $\Theta\subseteq \Real^K$ -- the $K$-dimensional parametric family.

Let $\hat{\theta}_k$ be the MLE estimator of the parameter in $\Theta$, and $\hat{f}_k = f\brac{\induc{y}\theta_k}$.

Assume that the models are nested, and are distinguished by their dimension $K$.

How far the $\hat{f}_k$ is from $g$.

The idea is to look a t the Kullback-Leibler divergence between $g$ and $f_k$, and get the $f_k$ with the smallest distance:
\[\text{KL}(g||f_k) = \int g(y) \log g(y) dy - \int g(y) \log f_k(y) dy\]
is the sum of the inherent entropy of $Y$ and the cross entropy.

The problem is to minimize this function, and it is equivalent to minimizing the Kullback-Leibler \textbf{discrepancy}:
\[d(\theta_k) = -2\int g(y) \log f_k(y) dy = - 2 \ex_g\brac{\log f_k(Y)}\]

Instead	let's look at
\[d(\hat{\theta}_k) = - 2\induc{\ex_g\brac{\log f(\induc{y}\theta)}}_{\theta=\theta_k(y)} \]

Since in practice $g$ is unknown, Akaike suggested to use the log likelihood as a proxy for $d(\hat{\theta}_k)$, which is a biased estimator of it.

In fact, using the law of large numbers
\[\frac{1}{N} \sum_{i=1}^n \log f(Y_i|\hat{\theta}_k) \overset{a.s}{\to} \ex_g\brac{\log f(\induc{Y}\hat{\theta}_k)}\]

Therefore
\[-\frac{2}{N} \sum_{i=1}^n \log f(Y_i|\hat{\theta}_k) \approx d(\hat{\theta}_k)\] and Akaike has something great in mind.

In order to compute the bias, it is necessary look an the average Kullback-Leibler distance with respect to the distribution of the MLE estimate $\hat{\theta}_k$
\[\ex_{\hat{\theta}_k} d(\hat{\theta}_k) - \ex_g\brac{- \log f\brac{\induc{Y}\hat{\theta}_k(Y)}}\]

The AIC is given by (up to the terms of the first order):
\[-2\log \hat{f}\brac{\induc{y}\,\hat{\theta}_k(y)} + 2k\]

\textbf{ See the handwritten notes! }



% subsection model_selection (end)

% section lecture_5 (end)

\clearpage
\section{Lecture \# 6} % (fold)
\label{sec:lecture_6}

Akaike criterion combines parameter estimation and model selection

By looking at the Kullback-Leibler divergence between the estimated and the true

\[d(\theta_k) = - 2 \ex_g \ln f\brac{\induc{Y}\theta_k}\]

$g(y\vert \theta_0)$

\[d(\hat{\theta}_k) = - 2 \induc{\ex \ln f\brac{\induc{Y}\theta_k}}_{\theta_k = \hat{\theta}_k(y)}\]

\[\ex_{\hat{\theta}_k} d(\hat{\theta}_k) = - 2 \induc{\ex_z \ex_Y \ln f\brac{\induc{Y}\theta_k}}_{\theta_k = \hat{\theta}_k(z)}\]

The idea is to use the sample twice (law of Large Numbers)

What is the bias here?

Law of Large numbers
\[-2 \ln f\brac{\induc{y}\hat{\theta}_k(y)} \to \ex \]

Modified (Corrected) AIC

Akaike's information criterion in which the contribution of the error term is computed.
This way it is possible to relax the assumption of large sample size $n$.

Look at what happens in the normal linear regression setting.

AIC is is more general, but the bias of the estimate can be really large
$\text{AIC}_c$ requires the model class assumption -- restrictive, but offers greater precision.

\subsection{Derivation} % (fold)
\label{sub:derivation}
Assumptions \begin{description}
	\item[the true model]\hfill\\
	\[y = \underset{n\times p_0}{X}\underset{p_0\times 1}{\beta_0} + \epsilon\]
	where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$
	\item[the candidate model] \hfill\\
	\[y = \underset{n\times p}{X}\underset{p\times 1}{\beta_0} + \epsilon\]
	where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$.
	There are $k=p+1$ parameters to estimate: $\theta_0 = \brac{\beta_0,\sigma_0^2}$ and $\theta_k = \brac{\beta_k,\sigma_k^2}$.
	\item[Nested models] assume that $0\leq p_0 \leq p$ and $\beta_0$ has $(p-p_0)$ zeros at the end. The models are nested $f\brac{\induc{y}\theta_0}\in \Mcal_k$
\end{description}

The LS estimate of $\Mcal_k$:\begin{align*}
	\hat{\beta}_k &= \brac{X'X}^{-1} X'Y\\
	\hat{\sigma}^2_k &= \frac{1}{n}\brac{Y-X\hat{\beta}_k}'\brac{Y-X\hat{\beta}_k}
\end{align*}


the log likelihood is 
\[\ln f\brac{\induc{y}\theta} = -\frac{n}{2} \ln 2\pi -\frac{n}{2}\ln \sigma^2_k - \frac{1}{2\sigma^2_k} \brac{Y-X\beta_k}'\brac{Y-X\beta_k}\]

For $\theta_k$ this can be simplified to 
\[\ln f\brac{\induc{y}\hat{\theta}_k(y)} = -\frac{n}{2} \ln 2\pi -\frac{n}{2}\ln \hat{\sigma}^2_k - \frac{1}{2\hat{\sigma}^2_k} n \hat{\sigma}^2_k \]
\[\ln f\brac{\induc{y}\hat{\theta}_k(y)} = -\frac{n}{2} \brac{ 1 + \ln 2\pi} -\frac{n}{2}\ln \hat{\sigma}^2_k\]

However
\[-2\ln f\brac{\induc{y}\theta_k} = n \ln 2\pi +n\ln \sigma^2_k + \frac{1}{\sigma^2_k} \brac{Y-X\beta_k}'\brac{Y-X\beta_k}\]

Since 
\begin{align*}
	\brac{Y-X\beta_k}'\brac{Y-X\beta_k} &= \brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k} + \\
		& + \brac{\beta_0 - \beta_k}'X'\epsilon + \epsilon'X\brac{\beta_0 - \beta_k}
		& + \epsilon'\epsilon
\end{align*}

Therefore 
\[\ex-2\ln f\brac{\induc{y}\theta_k} = n \ln 2\pi + n\ln \sigma^2_k + \frac{1}{\sigma^2_k} \brac{ \brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k} + n\sigma_0^2 }\]

Now 
\[d\brac{\hat{\theta}_k} = n \ln 2\pi + n\ln \hat{\sigma}^2_k + \frac{1}{\hat{\sigma}^2_k}\brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k} + \frac{n\sigma_0^2}{\hat{\sigma}^2_k}\]

taking expectations
\[\ex d\brac{\hat{\theta}_k} = n \ln 2\pi + n\ex \ln \hat{\sigma}^2_k + n^2\ex \frac{\sigma_0^2}{n\hat{\sigma}^2_k} + n \ex \frac{1}{n\hat{\sigma}^2_k}\brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k}\]

Since $\frac{n\hat{\sigma}^2_k}{\sigma_0^2} \sim \chi^2_{n-p}$ and 
\[\brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k}{\sigma_0^2} \sim \chi^2_p\]
and are independent

Hence
\[\ex d\brac{\hat{\theta}_k} = n \ln 2\pi + n\ex \ln \hat{\sigma}^2_k + \frac{n^2}{n-p-2} + n \ex \frac{1}{n\hat{\sigma}^2_k} \ex \brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k}\]
and thus 
\[\ex d\brac{\hat{\theta}_k} = n \ln 2\pi + n\ex \ln \hat{\sigma}^2_k + \frac{n^2}{n-p-2} + \frac{n p}{n-p-2} \]

Whence 
\[\ex d\brac{\hat{\theta}_k} = n \brac{1+\ln 2\pi} + n\ex \ln \hat{\sigma}^2_k + \frac{2n(p+1)}{n-p-2}\]
\[\ex d\brac{\hat{\theta}_k} = \ex -2 \ln f\brac{\induc{y}\hat{\theta}_k} + \frac{2n(p+1)}{n-p-2}\]

Since 
\[\frac{2n(p+1)}{n-p-2} = \frac{2nk}{n-k-1} = 2k + \frac{2k(k+1)}{n-k-1}\]

the original AIC is
\[\text{AIC} = -2 \ln f\brac{\induc{y}\hat{\theta}_k(y)} + 2k\]

The corrected AIC is 
\[\text{AIC}_c = \text{AIC} + \frac{2k(k+1)}{n-k-1}\]

% subsection derivation (end)

\subsection{Mallow's $C_p$} % (fold)
\label{sub:mallows_cp}

The $C_p$ stands for the conceptual predictive statistic in the context of the multivariate regression.

Assumptions \begin{description}
	\item[the true model]\hfill\\
	The true model is $y = \underset{n\times p_0}{X}\underset{p_0\times 1}{\beta_0} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma_0^2 \underset{n\times 1}{E}}$;
	\item[the candidate model] \hfill\\
	The candidate is $y = \underset{n\times p}{X}\underset{p\times 1}{\beta} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$.
	There are $k=p+1$ parameters to estimate: $\theta_0 = \brac{\beta_0,\sigma_0^2}$ and $\theta_k = \brac{\beta_k,\sigma_k^2}$. THe MLE of $\theta_k$ is $\hat{\theta}_k$.
	\item[the density] $f\brac{\induc{y}\hat{\theta}_k} = f_k$.
\end{description}

The idea is to look athow the RSS behaves

\[\RSS = \sum_{i=1}^n \brac{y_i - x_i \hat{\beta}}^2 = \brac{Y-X\hat{\beta}}'\brac{Y-X\hat{\beta}} = Y'\brac{E-H}Y\]
where $H$ is the projection matrix $X\brac{X'X}^{-1}X'$.

The expected value of the RSS is given by
\begin{align*}
	\ex Y'\brac{E-H}Y &= \ex\tr{Y'\brac{E-H}Y} \\
		&= \beta_0'X_0' \brac{E-H} X_0\beta_0 + \tr\brac{\brac{E-H} \sigma^2_0 I}\\
		&= \beta_0'X_0' \brac{E-H} X_0\beta_0 + (n-p)\sigma^2_0
\end{align*}

Show that if $z\sim \Dcal_m\brac{\mu,\Sigma}$ and $A$ is a non-random $m\times m$ matrix, then
\[\ex\brac{z'Az} = \mu'A\mu + \tr\brac{A\Sigma}\] 

Define the $C_p^0$ as
\[C_p^0 = \frac{\RSS}{\sigma^2_0} - n + 2p\]

The expectation is 
\[\ex C_p^0 = p + \frac{1}{\sigma^2_0} \beta_0'X_0' \brac{E-H} X_0\beta_0\]

If the true model has bee selected then $\ex C_p^0 = p$, otherwise $\ex C_p^0 > p$.

Put $C_p$ to $\frac{\RSS}{\hat{\sigma}^2} - n + 2p$, with the hope, that it leads to the same expectation of $C_p^0$.

But which estimator of the reference $\sigma^2$ to use? Use the smallest -- for the full model
\[\hat{\sigma}^2_*\frac{1}{n-p} \sum_{i=1}^n\brac{y_i - X_i\hat{\beta}}^2\]
where $p$ is the number of parameters in the full model.

% subsection mallows_cp (end)

\subsection{Comparison between the $C_p$ and $\text{AIC}_c$} % (fold)
\label{sub:comparison_between_cp_and_aicc}

These criteria are asymptotically equivalent.

The AIC for (the biased estimator)
\[\text{AIC} = - \ln f\brac{\induc{y}\hat{\theta}_k} + 2k 
	= n \ln \hat{\sigma}^2 + n\brac{1 + \ln 2\pi} + 2(p+1)\]

Return the model with the minimum values of the AIC. For a fixed $n$ it is the same a looking at 
\[n\ln \hat{\sigma}^2 - n\ln \hat{\sigma}^2_* + 2 p \]

But this minimization is equivalent to minimization of the following 
\[ n\ln \frac{\hat{\sigma}^2}{\hat{\sigma}^2_*} + 2p\]

For large $n$ the ratio of $\hat{\sigma}^2$ to $\hat{\sigma}^2_*$ is approximately $1$ due to consistency.
Hence it is possible to expand the logarithm around $1$: $\ln(1+x)\approx x + o(x)$.

Hence
\[\approx b\brac{\frac{\hat{\sigma}^2}{\hat{\sigma}^2_*} - 1} + 2p = C_p\]

\textbf{Exercise}\\
Prove the expected Gauss discrepancy
\[\ex C_p^0 = \ex \frac{1}{\sigma^2_0}\brac{X_0\beta_0 -X \beta}'\brac{X_0\beta_0 -X \beta}\]
Indeed, take a look at the Gauss discrepancy:
\begin{multline*}
	\brac{X_0\beta_0 -X \beta}'\brac{X_0\beta_0 -X \beta} = \\
	= \brac{X_0\beta_0 - X \brac{X'X}^{-1}X'X_0\beta_0 - X \brac{X'X}^{-1}X'\epsilon }'\brac{ \ldots }\\
	= \brac{ (I - H )X_0\beta_0 - H \epsilon }'\brac{ (I - H )X_0\beta_0 - H \epsilon }\\
	= \beta_0'X_0'(I-H)X_0\beta_0 - \epsilon'H(I-H)X_0\beta_0 - \beta_0'X_0'(I-H)H \epsilon + \epsilon'H\epsilon\\
	= \beta_0'X_0'(I-H)X_0\beta_0 + \epsilon'H\epsilon
\end{multline*}
Since $I-H$ and $H$ are projectors onto orthogonal subspaces of $\Real^n$.

Taking expectations yields
\[\ex\brac{\beta_0'X_0'(I-H)X_0\beta_0 + \epsilon'H\epsilon} = \beta_0'X_0'(I-H)X_0\beta_0 + \ex\brac{\epsilon'H\epsilon}\]
while at the same time due to linearity of $\ex$ and $\tr$:

\begin{align*}
\ex(\epsilon'H\epsilon) &= \tr\ex(\epsilon'H\epsilon) = \ex\tr(\epsilon'H\epsilon) \\
&= \ex\tr(H\epsilon\epsilon) = \tr(H\ex\epsilon\epsilon)
\end{align*}

% subsection comparison_between_cp_and_aicc (end)

\subsection{The Bayesian information criterion} % (fold)
\label{sub:the_bayesian_information_criterion}

Also known as the \textbf{B}ayesian \textbf{I}nformation \textbf{C}riterion introduced by Schwartz as the competitor of the AIC.

This looks at the problem from the Bayesian point of view.

Assumptions \begin{description}
	\item[Data] $\brac{y_k}_{k=1}^n$;
	\item[Parametric] The distribution function is fully specified by the vector of parameters $\theta_k\in \Theta_k$.
	\item[the true model]\hfill\\
	The true model is $y = \underset{n\times p_0}{X}\underset{p_0\times 1}{\beta_0} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma_0^2 \underset{n\times 1}{E}}$;
	\item[the candidate model] \hfill\\
	The candidate is $y = \underset{n\times p}{X}\underset{p\times 1}{\beta} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$.
	There are $k=p+1$ parameters to estimate: $\theta_0 = \brac{\beta_0,\sigma_0^2}$ and $\theta_k = \brac{\beta_k,\sigma_k^2}$. THe MLE of $\theta_k$ is $\hat{\theta}_k$.
	\item[the density] $f\brac{\induc{y}\hat{\theta}_k} = f_k$.
\end{description}

\begin{itemize}
	\item There is prior distribution $\pi_k$ on the classes of models $\Mcal_k$;
	\item Each model class has a prior distribution $G_k\brac{\theta_k}$ on the true parameter $\theta_k$;
	\item 
\end{itemize}
The bayes formula yields the following posterior distribution of $\Mcal_k$ given $\mathbf{y}$:
\[p\brac{\induc{\Mcal_k}\mathbf{y}} \propto p\brac{\induc{\mathbf{y}}\Mcal_k} \pi_k\]
up to the factor $\frac{1}{f(\mathbf{y})}$.

Also
\begin{multline*}
p\brac{\induc{\mathbf{y}}\Mcal_k}
= \int p\brac{\induc{\mathbf{y}, \theta_k}\Mcal_k} d\theta_k
= \int p\brac{\induc{\mathbf{y}}\theta_k, \Mcal_k} p\brac{\induc{\theta_k}\Mcal_k}d\theta_k\\
= \int \induc{L}_{\Mcal_k}(\theta_k;\mathbf{y}) g_k(\theta_k) d\theta_k
\end{multline*}

\subsection{Laplace approximation} % (fold)
\label{sub:laplace_approximation}

Idea is to find a Gaussian approximation to a probability density defined over a set of continuous variables.

Univariate case
\[p(z) = \frac{1}{N_z}f(z)\]
where $N_z$ is the normalizing constant, and $f(z)$ is unimodal.
The idea is to approximate $f(z)$ with gaussians $q(z)$.

Consider the Taylor expansion of the logarithm of $f(z)$ around its mode $z_0$, given by $f'(z_0) = 0$.
\[\ln f(z) \approx \ln f(z_0) + \induc{\frac{f'(z)}{f(z)}}_{z=z_0} (z-z_0) - \frac{1}{2} \induc{\frac{d^2}{dz^2} \ln f(z)}_{z=z_0} \brac{z-z_0}^2 \]
the reminder term decays to zero faster than $\brac{z-z_0}^2$. Thus $f(z)\approx e^{ -\frac{1}{2} A {(z-z_0)}^2 }$.

Whence around $z_0$:
\[q(z) = \brac{\frac{A}{2\pi}}^\frac{1}{2}e^{ -\frac{1}{2} A {(z-z_0)}^2 }\]

In the multidimensional case for $A = \induc{-\frac{d^2}{dz'dz}\ln f(z)}_{z=z_0}$
\[q(\mathbf{z}) = \frac{\abs{A}^\frac{1}{2}}{{(2\pi)}^\frac{1}{2}} e^{-\frac{1}{2} (z-z_0)'A(z-z_0)}\]

Therefore
\[N_z = \int f(z) dz = f(z_0) \frac{{(2\pi)}^\frac{1}{2}}{\abs{A}^\frac{1}{2}}\]

% subsection laplace_approximation (end)

The integrand
\[f(\theta_k) = p\brac{\induc{y}\theta_k}g_k(\theta_k)\]

Using the Laplace approximation we can treat the integrand as a normal density.

Let's compute the Laplace approximation.

Let $\theta_k^* = \argmax_{\theta_k\in \Theta_k}\,f(\theta_k)$ and put $A_{\theta_k^*}$ as
\[\induc{- \frac{d^2}{d\theta_kd\theta_k'} \ln f(\theta_k)}_{\theta_k=\theta_k^*}\]

Could be ignore the contribution coming from $g_k$?

The log of
\[\ln p\brac{\induc{\mathbf{y}}\theta_k} = \sum_{i=1}^n p\brac{\induc{y_i}\theta_k}\]
which hints at the fact that $P\brac{\induc{\mathbf{y}}\theta_k}$ dominates the $\ln g_k(\theta_k)$.

Thus we can take $\theta_k^* = \hat{\theta}_k$ -- the MLE and the observed Fisher information matrix
\[I_{\hat{\theta}_k} = \induc{- \frac{\partial^2}{\partial\theta_k \partial\theta_k'}\ln p\brac{\induc{\mathbf{y}} \theta_k}} \]

In fact it is possible to have better Laplace approximations of $\int e^{n\phi(z)}dz = \text{approx} + o(n^{-\frac{1}{2}})$.

The posterior distribution is given by 
\[p\brac{\induc{\mathbf{y}}\Mcal_k} = \int f(\theta_k) d \theta_k = \int p\brac{\induc{\mathbf{y}}\theta_k} g_k(\theta_k) d \theta_k \approx p\brac{\induc{\mathbf{y}}\hat{\theta}_k} g_k(\hat{\theta}_k) \frac{\brac{2\pi}^\frac{k}{2}}{\abs{I_{\hat{\theta}_k}}^\frac{1}{2}}\]
the exponent $\frac{k}{2}$ comes from the dimension of $\theta_k$.

Therefore
\[\ln p\brac{\induc{\mathbf{y}}\Mcal_k} = \ln f\brac{\induc{\mathbf{y}}\hat{\theta}_k} + \ln g_k(\theta_k) + \frac{k}{2} \ln 2 \pi - \frac{1}{2}\ln \abs{I_{\hat{\theta}_k}} \]

For independent observations the full information matrix is just the one-observation information matrix times the number of observations.
Thus 
\[\abs{I_{\hat{\theta}_k}} = n^k \abs{I_1}\]

Hence 
\[\ln p\brac{\induc{\mathbf{y}}\Mcal_k} = \ln f\brac{\induc{\mathbf{y}}\hat{\theta}_k} + \ln g_k(\theta_k) + \frac{k}{2} \ln 2 \pi - \frac{k}{2}\ln n - \ln \abs{I_1} \]

But the terms 
\[ + \ln g_k(\theta_k) + \frac{k}{2} \ln 2 \pi - \ln \abs{I_1}\]
are bound with regardless of the values of $n$ and moreover are independent of it!

Therefore for asymptotic model comparison we can use
\[\text{BIC} = -2\ln f\brac{\induc{\mathbf{y}}\hat{\theta}_k} + k\ln n \]

The BIC tends to favour simpler models, because the penalty term is larger as $n\to \infty$.
In Bayesian modelling one needs to accumulate larger evidence in order to make estimation.
Uncertainty comes from the estimation and from the uncertainty of the underlying parameter.


The BIC is consistent
If the true model is in $\obj{\Mcal_{k_1},\ldots,\Mcal_{k_L}}$ then the the minimal BIC pick the true model with probability 1 as $n$ gets larger.

Heuristics on the values of BIC $\text{BIC}-\text{BIC}_{\min}$
\begin{itemize}
	\item if $\in\clo{0,2}$ -- virtually no evidence against;
	\item if $\in\clo{2,6}$ -- the evidence id positive;
	\item if $\in\clo{6,10}$ -- the evidence is strong;
	\item if $\in\clop{10, +\infty}$ -- the evidence is very strong.
\end{itemize}

% Kass & Raffey (1995)

% subsection the_bayesian_information_criterion (end)

% section lecture_6 (end)

\clearpage
\section{Lecture \# 7} % (fold)
\label{sec:lecture_7}

The goal is to estimate the test error given the small sample we have.

\subsection{Validation techniques} % (fold)
\label{sub:validation_techniques}

Enough data : split the data into two samples and fit on one subsample and test (validate) in the other.

Hold out a set of points in the training set $\brac{x_{2k}}_{k=1}^{n_2}$ on which to test the error of the model.
On the remaining subsample $\brac{x_{1k}}_{k=1}^{n_1}$ fit the model. The splitting is done randomly.
However there are some shortcomings:
\begin{description}
	\item[Variability] \hfill\\
		Depends on which observations are put in both subsamples ;
	\item[Test error] \hfill\\
		The true error might be overestimated due to having less data to estimate the model.
\end{description}

\subsubsection{Leave one out cross validation} % (fold)
\label{ssub:leave_one_out_cross_validation}

The idea is instead of splitting the dataset in two subsets, we test the fit on one single observation.

Is the full training sample is $X=\brac{x_k}_{k=1}^n$, then for each $j=1,\ldots n$ fit the model on $X_{-j} = \brac{X_k}_{k=1,\neq j}^n$.
Let $\hat{T}_j$ be the prediction of the model fit on $X_{-j}$, then the \textbf{M}ean \textbf{S}quare \textbf{E}rror is 
\[\text{MSE} \defn \frac{1}{n}\ \sum_{j=1}^n \big(\hat{T}_j - T_j\big)^2 \]

\begin{description}
	\item[Pros] \hfill\\
		\begin{itemize}
			\item Very general and applicable in many settings;
			\item It is not variable at all -- yields the same results, since the splitting is deterministic;
			\item Compared to validation, the bias of the model error is reduced;
		\end{itemize}
	\item[Cons] \hfill\\
		\begin{itemize}
			\item The variance of the MSE is increased, since there is a substantial overlap in the subsamples;
			\item It inflates the computational complexity (not true for linear estimators -- a linear combination of the response variables);
			\[\text{MSE} = \frac{1}{n}\sum_{j=1}^n \Big(\frac{y_j - \hat{y}_j}{1-h_{jj}}\Big)^2\]
			where $h_{jj}$ is the $j$-th diagonal element of the hat matrix. It is also known as the \emph{leverage}.
			\item Unsuitable for non-independent data.
		\end{itemize}
\end{description}

Covariance function of the residuals $\epsilon = Y - H Y$ is given by 
\[\text{cov}\big(\hat{\epsilon}\big) = \ex\big( (I-H)YY'(I-H) \big) = \sigma^2 (I-H)\]

Thus $\Var(\hat{\epsilon}_j) = \sigma^2 (1-h_{jj})$.

And we arrive at the concept of normalised residuals
\[r_j = \frac{\hat{\epsilon}_j}{\sigma \sqrt{1-h_{jj}}}\]

Since $\sigma$ is usually unknown, the \emph{Studentized} residuals arise:
\[ \hat{t}_j = \frac{\hat{\epsilon}_j}{\hat{\sigma} \sqrt{1-h_{jj}}}\]

% subsubsection leave_one_out_cross_validation (end)

\subsubsection{$k$-fold cross validation} % (fold)
\label{ssub:_k_fold_cross_validation}

Divide the training sample into ``folds'' all of approximately equal size. Estimate the model and the MSE on each fold. The $k$-fold MSE is given by 
\[\text{MSE}_{(k)} = \frac{1}{k}\sum_{j=1}^k \text{MSE}_j \]
where $\text{MSE}_j$ is the MSE of the $j$-th fold with the model estimated on all the remaining data. The LOOCV is a special case of $k$-fold with $k=n$.

Which method and what values of $k$? Using $k=5,10$ is universally recognized.

The mean square error:
\begin{description}
	\item[$k=10$] \hfill\\
		Bias is reduced, with lower variance;
	\item[$k=n$] \hfill\\
		Bias is further reduced at the cost of higher variance.
\end{description}

% subsubsection _k_fold_cross_validation (end)

\subsection{Wrong ways to cross validate} % (fold)
\label{sub:wrong_ways_to_cross_validate}

There are $p=10^4$ predictors, and the task is to keep at most $100$. If the model includes a preprocessing step which finds the $100$ best predictors, then it must be incorporated in x-validation.

In multi step modelling cross validation must be applied to the entire sequence of the modelling steps. This way the we will get the right estimate of the test error (of picking the ``best'' $100$ predictors).

% subsection wrong_ways_to_cross_validate (end)

% subsection validation_techniques (end)

\subsection{Bootstrap} % (fold)
\label{sub:bootstrap}

It is also a resampling technique, where the data is drawn from the already observed data points. The idea is to artificially recreate new samples.

If the training set is $Z=\brac{z_k}_{k=1}^n$, then the bootstrap sample $Z^*_b$ is created by sampling with replacement from $Z$ (non-parametric bootstrap).

This allows to compute the accuracy of any statistic, which is derived from the sample, using the empirical distributions function.

If $S(Z)$ is computed directly form the original sample $Z$, then 
\[\var\big(S(Z^*)\big) = \frac{1}{B}\sum_{b=1}^B S(Z^*_b)\]

Bootstrap seems a bit magical.
% (Peter Hall. Edgeworth's view of the bootstrap). 

Cross validation is simple: fit the model on the bootstrap sample and then compute the MSE on those observations, which did not make it into the bootstrap sample.

The probability that an observation ends up in a bootstrap sample is 
\[\Pr\big(\text{obs}_i\in Z^*_b\big) = 1 - \big(1-\frac{1}{n}\big)^n\approx = 1-e^{-1}\]

The number of distinct observations in a bootstrap sample is $Y$. Then $Y = n (1-\frac{1}{n})$.

Thus the bootstrap cross-validation is approximately equal to the $3$-fold cross validation.
Since bootstrapping is computationally expensive, $k$-fold cross validation is preferred.




% Model selection with penalty of complexity
Estimate the test error directly from the data.
Bootstrap is used to estimate the accuracy of the estimates.



% subsection bootstrap (end)

Classification procedures

% section lecture_7 (end)

\clearpage
\section{Lecture \# 8} % (fold)
\label{sec:lecture_8}

SUpervised learning

an input $X$ and the output $T$ assuming the existence of a link between them. Up to now the case of continuous response $T$ variable has been considered, which led to regression methods.

In classification the value space of the response is finite or countable (a class, category or any discrete value).

Thus there is no special order between the distinct values of $T$. 
Thus the problem of classification arises :
construct a function $y(\cdot)$ of $X$ so that it predicts the value of the target variable $T$.

Training data $\big(x_k, t_k\big)_{k=1}^n$ 

Consider a simple case of a binary $T$ -- a binary classification problem and let $X$ be univariate.

For small $x$ the response is zero, for large -- $T=1$, and in the intermediate region the intervals with $T=0$ and $T=1$ overlap.

Minimizing the expectd sum of squares, the minimizer is the conditional expectation $\ex\big(\big. T\big.\big\rvert X\big.\big)$, which is equal to 
\[\Pr\big(\big. T = 1\big.\big\rvert X = \mathbf{x}\big.\big) = \mathbf{x}\beta\]
and the classification is done through comparison of the probability of $T=1$ versus $T=0$.

%% an estimated slop
In classification the problem is to assign a new class $c\in \Ccal$ to an observation, whereas in the regression setting the goal was to predict the response gicen the new observation.

\subsection{As little misclassification as possible} % (fold)
\label{sub:as_little_misclassification_as_possible}

Minimize the misclassification, by looking at the probability of a mistake:
\[\Pr\big(y(X)\neq T\big)\]
observe that this is a generative approach.

The loss function is
\begin{multline*}
	\Pr\big(\big. y(X) = C_1\big.\big\rvert X\in C_2\big.\big) + \Pr\big(\big. y(X) = C_2\big.\big\rvert X\in C_1\big.\big)\\
	= \int_{R_1} p(x, C_2) dx + \int_{R_2} p(x, C_1) dx \to \min_{R_i}
\end{multline*}

If $p(x, C_2) > p(x, C_1)$ then classify $x$ in $C_2$, otherwise in $C_1$.
This is known as the Bayes classifier, and it is the best possible classification in this setting.

Consider the conditional probabilities and the Bayesian inference rules:
\[\Pr\big(\big. X\in C_1\big.\big\rvert X\big.\big) \Pr( X ) > \Pr\big(\big. X\in C_2\big.\big\rvert X\big.\big) \Pr( X ) > \]

There are two possibilities \begin{itemize}
	\item a generative approach, which models the joint distribution : linear discriminative analysis;
	\item a discriminative approach, with a parametric form, to come up with tractable models;
\end{itemize}

% subsection as_little_misclassification_as_possible (end)

\subsection{Expected loss} % (fold)
\label{sub:expected_loss}

A different perspective: minimize the expected loss, which is suitable under various circumstances where the risk of misclassification of significant.

Introduce a cost of misclassification of a new $x\in C_k$ to a region $R_j$ : $L_{jk}$.
and minimize the expected loss:
\[\text{EL} = \sum_{jk} \int_{R_j} L_{jk} p(x, C_k) dx\]

Thus for each $x$ minimize 
\[\sum_{jk} L_{jk} p(x, C_k) = \sum_{jk} L_{jk} p\big(\big. C_k\big.\big\rvert x\big.\big) p(x)\]

Indeed a similar problem.
% subsection expected_loss (end)

\subsection{Discriminative approach} % (fold)
\label{sub:discriminative_approach}

There is a function $y(\cdot):\mathcal{X}\to \Ccal$ which maps $x$ directly to q decision region.

In the case of two classes, assign $x$ to $C_1$ if $y(x) \geq 0$.

This approach does not have probabilistic motivation, whereas the previous approaches are not.

There is a problem with higher number of classes when using discriminant functions.

\begin{description}
	\item[One-versus-all]\hfill\\
		Classify according to the rule $C_j$ and $\neg C_j$ to get different classification boundaries.
		Basically fits $k$ classifiers. It works in some cases, but may yield undefined regions;
	\item[One-on-one] \hfill\\
		Consider the classes in pairs $C_i$ and $C_j$, neglecting the others. This results in $i-j$ boundaries,
		but requires $\frac{k(k-1)}{2}$ classifiers to fit. Still this can produce undefined regions;
	\item[]
	Consider a single $K$-class discrimiminant given by $K$ linear functions $\brac{y_k(\cdot)}_{i=1}^K$.
	In the $\Real^d$ case these function look like 
	\[y_k(x) = \beta_{0k} + \beta_k'x\]
	Classify $x$ to class $C_k$ if $y_k(x) > y_j(x)$ for all $j\neq k$.
	The boundaries are given by $y_k(x) = y_j(x)$ which define hyperplanes in $\Real^d$:
	\[\big(\beta_j-\beta_k\big)'x + (\beta_{0j}-\beta_{0k}) = 0\]

	This partitions the space into convex decision regions. Indeed:
	If $x_1,x_2\in R_k$, then for any $\lambda\in \clo{0,1}$ consider $x = \lambda x_1 + (1-\lambda x_2)$.
	Then $y_k(x)$ is given by:
	\begin{align*}
		y)k(x) &= \beta_k'x +\beta_{0k} = \lambda\beta_k'x_1 + (1-\lambda)\beta_k'x_2 + \beta_{0k} \\
		& = \lambda\big(\beta_k'x_1 +\beta_{0k}\big) + (1-\lambda)\big(\beta_k'x_2 +\beta_{0k}\big) \\
		& = \lambda y_k(x_1) + (1-\lambda)y_k(x_2)
	\end{align*}
	Since $x_1, x_2\in R_k$ we have $y_k(x_m)>y_j(x_m)$ for all $j\neq k$ and $m=1,2$, whence
	$y_k(x) > y_j(x)$ for all $j\neq k$, which means that $R_k$ is convex. And thus single connected (?)...
\end{description}

% subsection discriminative_approach (end)

\subsection{Logistic regression} % (fold)
\label{sub:logistic_regression}

Suppose there are $K$ distinct categories.

This is a discriminative approach: model $\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big)$ with
\[\sum_{j=1}^K \Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = 1\]

The form of a logistic regression:
\[\ln\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = \beta_{0j} + \sum_{s=1}^p\beta_{js} x_s \]

Take the last category as the reference category: for $j=1,\ldots,K-1$
\[\ln\frac{\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big)}{\Pr\big(\big. T=K\big.\big\rvert X=x\big.\big)}
	= \beta_{0j} + \sum_{s=1}^p\beta_{js} x_s = \beta_j'x \]
where $x_0 = \mathbf{1}$.

The probability that $T=j$ given $x$ is
\[\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = \Pr\big(\big. T=K\big.\big\rvert X=x\big.\big) e^{\beta_j'x}\]

Using the constraint yields
\[1 = \Pr\big(\big. T=K\big.\big\rvert X=x\big.\big) \Big(1 + \sum_{i=1}^{K-1} e^{\beta_i'x}\Big)\]

whence 
\[\Pr\big(\big. T=K\big.\big\rvert X=x\big.\big) = \frac{1}{1 + \sum_{i=1}^{K-1} e^{\beta_i'x}}\]

and
\[\pi_j = \Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = \frac{e^{\beta_j'x}}{1 + \sum_{i=1}^{K-1} e^{\beta_i'x}}\]

\subsubsection{Interpretation} % (fold)
\label{ssub:interpretation}

Valid for the two class classification only.

What might the interpretation of the coefficients in $\beta_j$.

First note that the odds of classifying to $j$
\[\frac{\pi_j}{1-\pi_j} = e^{\beta_j'x}\]
which means that
\[\ln \frac{\pi_j}{1-\pi_j} = \beta_j'x\]
The log of the odds of the posterior behaves linearly.

The \textbf{logit} transform of $x\in \brac{0,1}$ is 
\[\ln\frac{x}{1-x}\]

An increase of one unit in $x_s$ multiplies the odds by $e^\beta_{js}$

% subsubsection interpretation (end)

\subsection{Estimation} % (fold)
\label{sub:estimation}
Consider the case of binary classification of the training sample $\big(x_k,t_k\big)_{k=1}^n$. What is the MLE of $\beta$?
Let $\theta = \brac{\beta_{sj}}_{s=0, j=1}^{p, K}$.

\begin{align*}
	l(\theta) &= - \ln \prod_{i=1}^n p\big(\big. t_i\big.\big\rvert x_i, \theta\big.\big) \\
	&= - \sum_{i=1}^n \ln p\big(\big. t_i\big.\big\rvert x_i, \theta\big.\big)
\end{align*}

Notation: $t_i$ has zero-one values, and $\sigma_i = \sigma\big(\beta'x_i\big)$ where $\sigma(a) = \frac{e^a}{1-e^a}$.

Note that 

Therefore the log-likelihood is 
\begin{align*}
	l(\theta) &= - \sum_{i=1}^n \ln \big( \sigma_i^{t_i} (1-\sigma_i)^{1-t_i} \big) \\
	&= - \sum_{i=1}^n t_i\ln \sigma_i + (1-t_i)\ln (1-\sigma_i)
\end{align*}

The first order conditions are given by
\begin{align*}
	\frac{\partial}{\partial \beta_j}\ln \sigma_i & = \frac{\partial}{\partial \beta_j}-\ln ( 1 + e^{-\beta'x_i} ) \\  
	& = \frac{\partial}{\partial \beta_j} -\ln ( 1 + e^{-\beta'x_i} ) = \frac{x_{ij}e^{-\beta'x_i}}{1 + e^{-\beta'x_i}} \\
	& = x_{ij} (1-\sigma_i)
	\frac{\partial}{\partial \beta_j}\ln (1-\sigma_i) & = - \sigma_i x_{ij}
\end{align*}
notice that $\ln(1-\sigma_i) = -\beta'x_i - \ln(1 + e^{-\beta'x_i})$

therefore
\[\frac{\partial}{\partial \beta_j} l(\theta) = -\sum_{i=1}^n \big( t_i x_{ij} (1-\sigma_i) - (1-t_i)\sigma_i x_{ij} \big) = \sum_{i=1}^n (\sigma_i-t_i)x_{ij} \]

In the matrix notation we have the following:
\begin{align*}
	\nabla_\beta l(\beta) = \underset{n\times p+1}{X}' (\underset{n\times 1}{\sigma} - \underset{n\times 1}{t})
\end{align*}

in appears to be non-linear in $\beta$

In order to employ numerical procedures, the second order conditions must be checked.
\begin{align*}
	\frac{\partial}{\partial \beta_i}\frac{\partial}{\partial \beta_j} l(\theta)
	&= \frac{\partial}{\partial \beta_i} \sum_{i=1}^n (\sigma_i-t_i)x_{ij} \\
	&= \sum_{i=1}^n x_{ij}\frac{\partial}{\partial \beta_i} \sigma_i \\
	&= \sum_{i=1}^n x_{ij} x_{ik} \sigma_i (1 - \sigma_i)
\end{align*}

Introduce a diagonal matrix $B = \text{diag}\big(\sigma_i(1-\sigma_i)\big)_{i=1}^n$.

Therefore
\[\nabla_\beta^2 l(\beta) = X'BX\]
which means that the Hessian is a positive semidefinite matrix.
implying that the optimal $\beta$ is the minimizer. The whole problem is that of convex optimization.

\subsubsection{Newton-Raphson} % (fold)
\label{ssub:newton_raphson}

The idea is to find a second order approximation of $f:\Real^d\to \Real$ in order to find its roots. As usual use the multivariate Taylor expansion
\[f(x)-f(a)  = \nabla f(a)' \big(x-a\big) + \frac{1}{2}\big(x-a\big)'\nabla^2 f(a) \big(x-a\big) + \ldots \]
Thus we approximate $f$ by 
\[q(x) = \frac{1}{2}x'\nabla^2 f(a) x + \alpha' x + \beta\]
where $\alpha = \nabla f(a) - \nabla^2 f(a)a$ and $\beta = \ldots$.

The main ides is to approximate a function near its minimum by a quadratic.

The minimum of $q$ is
\[\nabla q(x) = \nabla^2 f(a) + \alpha = 0\]
whence 
\[ x = -\big(\nabla^2 f(a)\big)^{-1} \alpha \]
and 
\[ x^* = a - \big(\nabla^2 f(a)\big)^{-1} \nabla f(a) \]

Let's apply this idea to the log-likelihood.
\begin{enumerate}
	\item begin with some initial $\beta_0$;
	\item update $\beta\to \beta_{\text{new}}$ using
	\[\beta_{\text{old}} - \big(X'BX\big)^{-1} X'(\sigma - t) = \big(X'BX\big)^{-1} X'B \big( X \beta_{\text{old}} - B^{-1} (\sigma- t ) \big)\]
	the term in brackets is the adjusted response $Z = X \beta_{\text{old}} - B^{-1} (\sigma - t )$ and the $\sigma$ is evaluated at $\beta_{\text{old}}$;
	use the weighted least squares.
\end{enumerate}


% subsubsection newton_raphson (end)

\subsubsection{Probit regression} % (fold)
\label{ssub:probit_regression}

Motivation: there is a latent variable $t^* = \beta'x + \epsilon$ with $\epsilon\sim \Ncal(0,1)$ and the observed binary variable $T = 1_{T^*(x)>0}(x)$.
Now $\Pr\big(\beta'x+\epsilon > 0\big) = \Pr\big(\epsilon > -\beta'x\big) = 1-\Phi(-\beta'x) = \Phi(\beta'x)$


% subsubsection probit_regression (end)

% subsection estimation (end)

% subsection logistic_regression (end)

% section lecture_8 (end)

\clearpage
\section{Lecture \# 9} % (fold)
\label{sec:lecture_9}

Two approaaches discrimimnative and generative.

Discriminative: modelling the posterior (conditional) distribution of the class given the variable; do not necessarily have probabilistic interpretation.
Generative: modelling the joint distribution of the class and the variable;
Direct modelling of the probabilities.
Linear discriminant (quadratic) analysis.
Discriminant functions.
The sample data $(X_k)_{k=1}^n\in \Real^d$ in latent classes $\Ccal$. Want to model the distribution of $X$, conditional on the class to which it belongs.
Once there is the likelihood, ``flip'' it to get the Bayesian posterior. The Bayesian classifier has the lowest error rate $\Pr(C_k\vert x)$.

The probability that 
\[\Pr(T = j\vert X = \mathcal{x}) \propto \Pr(X = \mathcal{x}\vert T = j) \Pr( T = j ) = f_j(x) \pi_j\]
To recover the normalising factor compute $N(x) = \sum_{k=1}^K f_j(x) \pi_j$.

We can model the likelihoods as well as the priors.

Choices for $f_j(x)$: \begin{itemize}
	\item Gaussian yields the LDA or the QDA (access to analytical expression);
	\item each data point comes from a mixture if gaussians gives rise to the MDA (only numerical EM algorithms);
	\item for a large number of predictors assume independence between the components of $X$ within each class (conditionally on the class the features are indepenednent) this results in the na\"ive bayes classifier.
\end{itemize}


\subsection{Linear discriminant analysis} % (fold)
\label{sub:linear_discriminant_analysis}

Each
\[f_j(x) = \frac{1}{\sqrt{2\pi}^p \lvert \Sigma_j\rvert^\frac{n}{2}} \text{exp}\bigg(-\frac{1}{2}(x-\mu_j)'\Sigma_j^{-1}(x-\mu_j)\bigg)\]

all classes have the same covariance matrix $\Sigma_j = \Sigma$.

compare the log of the ration of the probability
\[\log \frac{\Pr\big(T=k\vert X=x\big)}{\Pr\big(T=j\vert X=x\big)} \]
for the posteriors:
\[ = \log \frac{\pi_k}{\pi_j} + \log \frac{\Pr\big(X=x\vert T=k\big)}{\Pr\big(X=x\vert T=j\big)} = \log \frac{\pi_k}{\pi_j} + \log \frac{f_k(x)}{f_j(x)} \]
thus
\[ = \log \frac{\pi_k}{\pi_j} + \Big( -\frac{1}{2}(x-\mu_k)'\Sigma_k^{-1}(x-\mu_k\Big) + \frac{1}{2}(x-\mu_j)'\Sigma_j^{-1}(x-\mu_j) \Big) + \log\frac{\lvert \Sigma_j\rvert}{\lvert \Sigma_k\rvert} \]

using the covariance matrix constance
\[ = \log \frac{\pi_k}{\pi_j} + -\frac{1}{2}\big(\mu_k'\Sigma^{-1}\mu_k - \mu_j'\Sigma^{-1}\mu_j \big) + x'\Sigma^{-1}\big( \mu_k - \mu_j\big) \]
which is linear in $x$ -- the linear discriminant analysis. Define the discriminant function
\[\delta_k(x) = x'\Sigma^{-1}\mu_k - \frac{1}{2} \mu_k'\Sigma^{-1}\mu_k + \log \pi_k\]
Then  classify $x$ as $k$ if $\delta_k(x)>\delta_j$ for $k\neq k$.

Since we have a probabilistic model is is sensible to estimate the parameters using the MLE.

\subsection{The MLE} % (fold)
\label{sub:the_mle}

Summary:
we have got $K$ classes with probabilities $\big(\pi_j\big)_{j=1}^K$ subject to $\sum_{j=1}^K \pi_j = 1$, and a sample $\big(X,T\big)$.


The likelihood of a particular observation $x_i$
\[\prod_{j=1}^K \pi_j^{t_{ij}} f_j(x_i)^{t_{ij}}\]
where $t_{ij}$ is the indicator whether the $i$-th observation belongs to the class $j$ in the training sample.

Thus the complete likelihood of the observed data is
\[L = \prod_{i=1}^n \prod_{j=1}^K \pi_j^{t_{ij}} f_j(x_i)^{t_{ij}}\]
whence
\[\mathcal{L} = \sum_{i=1}^n \sum_{j=1}^K t_{ij} \log \pi_j + t_{ij} \log f_j(x_i)\]
Using teh fact that everything is gaussian and the $\pi_j$ are related:
\[\mathcal{L} = \sum_{i=1}^n t_{i1} \log \big( 1- \sum_{j=2}^k\pi_j \big) +  \sum_{i=1}^n \sum_{j=2}^K t_{ij} \log \pi_j +  \sum_{i=1}^n \sum_{j=1}^K t_{ij} \log f_j(x_i)\]
thus the likelihood is separable in terms of the prior probabilities and the parameters of the gaussians.
Computing the derivatives:
\begin{align*}
	\sum_{i=1}^n \frac{-t_{i1}}{\pi_1} +  \sum_{i=1}^n t_{ij} \frac{t_{i1}}{\pi_j} = 0\\
\end{align*}
whence for all $j\geq 2$:
\[\frac{\pi_j}{\pi_1} = \frac{\sum_{i=1}^n t_{ij}}{\sum_{i=1}^n t_{i1}}\]
thus
\[\hat{\pi_j} = \frac{\sum_{i=1}^n t_{ij}}{\sum_{i=1}^n \sum_{l=1}^K t_{il}}\]

The parameters of the gaussians: maximize with respect to $\mu_j$ and $\Sigma$. The only contribution is
\[\ldots + \sum_{i=1}^n \sum_{j=1}^K t_{ij} \log f_j(x_i) \]
this is equivalent to maximizing
\[ -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^K t_{ij} \big(x_i-\mu_j\big)'\Sigma^{-1}\big(x_i-\mu_j\big)\] 
Thus
\[\sum_{i=1}^n t_{ij} \big(x_i-\hat{\mu}_j\big)'\Sigma^{-1} = 0\]
whence
\[\sum_{i=1}^n t_{ij} x_i = \sum_{i=1}^n t_{ij} \hat{\mu}_j\]
which leads to
\[\hat{\mu}_j = \frac{\sum_{i=1}^n t_{ij} x_i }{\sum_{i=1}^n t_{ij}}\]
the average of the $x_i$ belonging to the class $j$.

And finally the $\Sigma$, which is more tricky.
Collect the terms that depend on $\Sigma$ which depend on it. We get this to minimize:
\[ \sum_{i=1}^n \sum_{j=1}^K t_{ij} (x_i-\mu_j)'\Sigma^{-1}(x_i-\mu_j) + t_{ij} \log \lvert\Sigma\rvert \]
since the trace of a scalar is equal to its trace and $\tr(ABC) = \tr(BCA)$:
\[\sum_{i=1}^n \sum_{j=1}^K t_{ij} \tr\big( \Sigma^{-1}(x_i-\mu_j)(x_i-\mu_j)'\big) + t_{ij} \log \lvert\Sigma\rvert\] 
define $S_j = \frac{1}{n_j}\sum_{i=1}^n(x_i-\mu_j)(x_i-\mu_j)'$ with $n_j=\sum_{i=1}^n t_{ij}$. Therefore
\[\sum_{j=1}^K n_j \tr\big( \Sigma^{-1} S_j \big) - n \log \lvert\Sigma^{-1}\rvert\] 

Now the following could be proved directly:
\[\frac{\partial}{\partial A}\tr(AB) = \frac{\partial}{\partial A}\tr(BA) = B'\]
and
\[\frac{\partial}{\partial A}\log \lvert A\rvert = (A^{-1})'\]
therefore $\sum_{j=1}^K n_j S_j - n \hat{\Sigma} = 0$.

Remarks: \begin{itemize}
	\item if $\Sigma\neq\Sigma_j$ then we have quadratic decision boundaries, and 
	\[\delta_k(x) = - \frac{1}{2} (x-\mu_k)'\Sigma^{-1}(x-\mu_k) + \log \pi_k - \frac{1}{2}\log\lvert\Sigma_k^{-1}\rvert\]
	\item to get quadratice decision boundaries with LDA enrich the feature space by including functions of the predictors;
	\item QDA (and LDA) do not scale well with the number of predictors: use the Na\"ive Bayes;
	The na\"ive Bayes conditional independence. The na\"ive part
	\[\Pr\big(X=x\vert T=j\big) = \prod_{m=1}^p \Pr\big(X_m=x_m\vert T=j\big)\]

	the classification rule is using the Bayes part (maximizing the posterior):
	\[\hat{j}_x = \argmax_{j=1,\ldots, K} \Pr\big(X=x\vert T=j\big) \Pr\big(T=j\big)\]
	This has nothing to do with Bayesian inference. Thus there are Bayesian Na\"ive Bayes classifiers :).
	\item The difference between the logistic regression and the LDA. In logistic we looked at
	the log-ratio of class likelihoods given the data. It also linear in $x$, but the regression
	does not take the structure of the predictors.
	LDA is less flexible:
	\[\delta_k(x) = x'\Sigma^{-1}\mu_k - \frac{1}{2} \mu_k'\Sigma^{-1}\mu_k + \log \pi_k\]
	whereas the logistic had $\beta_0 + x\beta$ -- more general.
	If the true $x$ are Gaussian then it is better to use the LDA.
\end{itemize}

Some bias, but low variance due to simple linear hyper-boundaries.


% subsection the_mle (end)

% subsection linear_discriminant_analysis (end)

\subsection{Mixture of gaussians discriminant analysis} % (fold)
\label{sub:mixture_of_gaussians_discriminant_analysis}

\subsubsection{the Expectation-Maximisation} % (fold)
\label{ssub:the_expectation_maximisation}

the Expectation Maximization algorithm (Dempster et al. (1977)) is used in the case of latent or missing values.

Suppose the there is a probabilistic model with observed variables $X$ or $X,T$ and a set of latent variables $Z$.
In the mixture of distributions the latent variable is the mixture component, from which the observation was drawn.

Gaussian mixture is a superposition of Gaussian distributions:
\[g(x) \sim \sum_{k=1}^K \pi_k \phi_k(x\vert \mu_k, \Sigma_k)\]
such that $\sum_{k=1}^K \pi_k = 1$. It allows multimodality and greater flexibility.
The $\pi_k$ are the mixing coefficients and are the prior probabilities that $x$ comes from $k$-th component.

For the MLE we would have to estimate $\theta=(\pi_k,\mu_k,\Sigma_k)_{k=1}^K$ with the complete log-likelihood
\[l(\theta) = \sum_{i=1}^n \log \big(\sum_{k=1}^K\pi_k \phi_k(x\vert \mu_k, \Sigma_k)\big)\]

A way to get rid of the sum-under-the-log is to add the hidden information (here the original component).
Other examples include \textbf{H}idden \textbf{M}arkov \textbf{M}odels and the \textbf{M}ixture \textbf{D}iscriminant \textbf{A}nalysis.

We need a joint distribution of $(X,Z)$ governed by some $\theta$
\[p(X, Z\vert \theta)\]
the goal is to maximize the likelihood function
\[p(X\vert \theta) = \int p(X, Z\vert \theta) dZ = \sum_{z} p(X, Z=z\vert \theta) \]
This is in general intractable.
Suppose $q(Z\vert X,\theta)$ is an arbitrary density of $Z$ and proceed backwards from the goal.
\[l(\theta) = \log p(x\vert \theta) = \log \sum_z q(z\vert x,\theta) \frac{p(x\vert z, \theta)}{q(z\vert x,\theta)} \]
due to concavity of log, the Jensen's inequality yields:
\[\ldots \geq \sum_z q(z\vert x,\theta) \log \frac{p(x\vert z, \theta)}{q(z\vert x,\theta)} = F(q,\theta) \]

To maximize the $l(\theta)$, first maximize the lower bound. The process is
\begin{description}
	\item[E-step] \[q^{(t+1)} = \argmax_q F(q,\theta^{(t)})\]
	\item[M-step] \[\theta^{(t+1)} = \argmax_\theta F(q^{(t)},\theta)\]
\end{description}
But first we need to initialize the $\theta^{(0)}$ and the EM converges only to a local maximum.
However even in this setting the problem is intractable. We may find a sufficient condition such that the lower bound becomes the equality.

The sufficient condition on $q$ is when for all $z$
\[\frac{p(x\vert z, \theta)}{q(z\vert x,\theta)} = \text{const}\]
whence $q(z\vert x,\theta) \propto p(x\vert z, \theta)$ and 
\[q(z\vert x,\theta) = \frac{p(x, z\vert \theta)}{\sum_z p(x\vert z, \theta)} = p(z\vert x, \theta)\]
which leads to 
\[q^{(t+1)}(z\vert x,\theta) = p(z\vert x, \theta^{(t)})\]

The $M$-step:
\[l(\theta) \geq F(q,\theta) = \sum_z q(z\vert x,\theta) \log p(x\vert z, \theta) - \sum_z q(z\vert x,\theta) \log q(z\vert x,\theta)\]
-- the entropy and the cross entropy of $q$.
Since $q(z\vert x,\theta) = q^{(t+1)}(z\vert x,\theta) = p(z\vert x, \theta^{(t)})$, we have
\[F(q,\theta) = Q(\theta, \theta^{(t)}) + H(q)\]
and
\[Q(\theta, \theta^{(t)}) = \sum_z p(z\vert x, \theta^{(t)} \log p(x\vert z, \theta)
= \ex_{p(z\vert x, \theta^{(t)})} \log p(z\vert x, \theta) \]

In summary: \begin{itemize}
	\item The $E$-step is the computation of $Q(\theta, \theta^{(t)})$;
	\item The $M$-step return (update) the $\theta^{(t+1)} = \argmax_\theta Q(\theta, \theta^{(t)})$;
\end{itemize}
If the density family of the classes is exponential, then it is easy to maximize and compute the $Q$ as well.

\subsubsection{Proof of convergence of EM} % (fold)
\label{ssub:proof_of_convergence_of_em}

Show that $\theta^{(t)}$ converges and $l(\theta^{(t)})$ does not decrease.
Look at the values of $l(\theta)$:
\[l(\theta^{(t)}) = \sum_{i=1}^n \sum_z p(z_i\vert x_i, \theta^{(t)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(x_i\vert z_i, \theta^{(t)})}\]
where we used the chosen good $q$ 
\[\geq \sum_{i=1}^n \sum_z q(z_i\vert x_i, \theta^{(t+1)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(x_i\vert z_i, \theta)}\]
thus
\[\geq \sum_{i=1}^n \sum_z p(z_i\vert x_i, \theta^{(t+1)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(x_i\vert z_i, \theta^{(t+1)})}\]
since $Q(\theta^{(t+1)}, \theta^{(t)}) \geq Q(\theta^{(t)}, \theta^{(t)})$ :
\[\geq \sum_{i=1}^n \sum_z p(z_i\vert x_i, \theta^{(t)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(x_i\vert z_i, \theta^{(t)})}\]
In fact $Q(\theta^{(t)}, \theta^{(t)}) = l(\theta^{(t)})$ and $Q(\theta, \theta^{(t)})$ is convex (quadratic in $\theta$).

% subsubsection proof_of_convergence_of_em (end)

% subsubsection the_expectation_maximisation (end)

% subsection mixture_of_gaussians_discriminant_analysis (end)

% section lecture_9 (end)

\clearpage
\section{Lecture \# 10} % (fold)
\label{sec:lecture_10}

The EM algorithm is in two steps: \begin{description}
	\item[E-step] $Q\brac{\theta, \theta^k} = \ex_{p(z\lvert x, \theta^k) p(x,Z\lvert \theta)}$.
	\item[M-step] $\theta^{k+1}=\argmax_\theta Q(\theta, \theta^k)$.
\end{description}

We need to show that $l(\theta^{k+1})\geq l(\theta^k)$. Two things. The LHS:
\[l(\theta^k) = \sum \sum p(z_i\lvert x_i,\theta^k) \log\frac{p(x_i,z_i\lvert\theta^k)}{p(z_i\lvert x_i,\theta^k)}\]
The RHS:
\[l(\theta^{k+1}) \geq \sum \sum p(z_i\lvert x_i,\theta^k) \log\frac{p(x_i,z_i\lvert\theta^{k+1})}{p(z_i\lvert x_i,\theta^k)}\]
whence 
\[\ldots = Q(\theta^{k+1}, \theta^k) + \text{entropy}\geq Q(\theta^k, \theta^k) + \text{entropy} = l(\theta^k)\]
since $\theta^{k+1} = \argmax_\theta Q(\theta, \theta^k)$ (?) CHECK THIS.

\subsection{A typical application of the EM} % (fold)
\label{sub:a_typical_application_of_the_em}

\subsubsection{The independent mixture problem} % (fold)
\label{ssub:the_independent_mixture_problem}
Consider a binomial response (typically exponential families permit easy maximization). 
Thus the $i$-th observation is $\text{Bi}(n_i, p_j)$ if it is coming from the $j$-th category.

To use the EM we need to consider the hidden variables as well. Suppose $u_j(i)$ is the unobserved
indicator of $i$-th observation coming from $j$-th category.

The $\pi_j$ is the prior probability of $j$-th category.

The complete likelihood is
\[l(\pi, p\lvert x, z) = \log \prod_i \pi_{z_i} p_{z_i}^{x_i}(1-p_{z_i})^{n_i-x_i}\]
in a more convenient form
\[l(\pi, p\lvert x, z) = \sum_i \log \prod_j \big(\pi_j p_j^{x_i}(1-p_j)^{n_i-x_i}\big)^{u_j(i)}\]
whence
\[l(\pi, p\lvert x, z) = \sum_i\sum_j u_j(i)\log \big(\pi_j p_j^{x_i}(1-p_j)^{n_i-x_i}\big)\]
and 
\[l(\pi, p\lvert x, z) = \sum_i \sum_j u_j(i)\log \pi_j + u_j(i)\log p_j^{x_i}(1-p_j)^{n_i-x_i}\]
leading to 
\[l(\pi, p\lvert x, z) = \sum_i \sum_j u_j(i)\log \pi_j + \sum_{i,j} u_j(i)\log p_j^{x_i}(1-p_j)^{n_i-x_i}\]

\textbf{The E-step}. Averaging out of the hidden variables:
\[Q(\theta, \theta^k) =  \ex_{p(z\lvert x, \theta^k) p(x,Z\lvert \theta)}\]
define $\hat{u}_j(i) = \ex u_j(i)$. It is given by 
\[\ex u_j(i) = \Pr\big(Z_i=j\,\lvert\,X=x_i\big)\]
Thus
\[\hat{u}_j(i) = \frac{\Pr\big(X=x_i\,\lvert\,Z_i=j\big)\Pr\big(Z_i=j\big)}{\Pr\big(X=x_i\big)}
= \frac{ \pi_j f(x_i\lvert n_i, p_j)}{\sum_c \pi_c f(x_i\lvert n_i, p_c)}\]

\textbf{The M-step}. Consider the likelihood at $\hat{u}_j(i)$:
\[l(\pi, p\lvert x, z) = \sum_i \sum_j \hat{u}_j(i)\log \pi_j + \sum_{i,j} \hat{u}_j(i)\log p_j^{x_i}(1-p_j)^{n_i-x_i}\]

Maximizing with respect to $\big(\pi_j\big)$ subject to constraint $\sum_j \pi_j = 1$. The First order conditions:
\[\frac{\partial}{\partial \pi_j} l(\cdot) = \sum_i \hat{u}_j(i)\frac{1}{\pi_j} - \lambda = 0\]
whence
\[\sum_i \hat{u}_j(i) = \lambda \pi_j\]
and the ratio for $j\neq c$
\[\frac{\sum_i \hat{u}_j(i)}{\sum_i \hat{u}_c(i)} = \frac{\pi_j}{\pi_c}\]
Therefore
\[\frac{\sum_i \sum_j \hat{u}_j(i)}{\sum_i \hat{u}_c(i)} = \frac{1}{\pi_c}\]
and 
\[\hat{\pi}_c = \frac{\sum_i \hat{u}_c(i)}{\sum_i \sum_j \hat{u}_j(i)} = \frac{1}{n}\sum_i \hat{u}_c(i)\]
since by construction $\sum_j \hat{u}_j(i)=1$.

Now with respect to $p_j$:
\[\frac{\partial}{\partial p_j} l(\cdot) = \sum_i \hat{u}_j(i) \Big(\frac{x_i}{p_j} - \frac{n_i-x_i}{1-p_j}\Big) = 0\]
whence after simplification:
\[\hat{p}_j = \frac{\sum_i \hat{u}_j(i) x_i }{\sum_i \hat{u}_j(i) n_i}\]

\textbf{Initialisation}. Taking the uniform approach to the probability $p_j$ leads to a disaster.
Indeed, the E-step yields:
\[\hat{u}_j(i) = \frac{ \pi_j C_{n_i}^{x_i} p_j^{x_i}(1-p_j)^{n_i - x_i}}{\sum_c \pi_c C_{n_i}^{x_i} p_c^{x_i}(1-p_c)^{n_i - x_i}}\]
whence $\hat{u}_j(i) = \frac{\pi_j}{\sum_c \pi_c}$.

The M-step gives the number of successes:
\[\hat{p}_j = \frac{\sum_i x_i }{\sum_i n_i}\]
and no update whatsoever
\[\hat{\pi}_j = \frac{1}{n}\sum_i \frac{\pi_j}{\sum_c \pi_c} = \frac{\pi_j}{\sum_c \pi_c}\]

% subsubsection the_independent_mixture_problem (end)

% subsection a_typical_application_of_the_em (end)

\subsection{The problem of Mixture Discriminant Analysis} % (fold)
\label{sub:the_problem_of_mixture_discriminant_analysis}

It is a generalization of LDA and the QDA (1996 by Hastie and Tibshirani) . 
Assumptions: \begin{itemize}
	\item $K$ classes with probabilities $\big(\pi_k\big)$ and categories;
	\item given a $k$-th class the observation $X$ is a mixture:
	\[f_k(x) = \sum_j^{n_k}\alpha_{kj} \phi(x\,\lvert\,\mu_{kj}, \Sigma)\]
	\item classes are known in the training data, but the mixture proportion as are latent;
	\item We estimate $\alpha_{kj}$, $\mu_{kj}$, $\pi_k$ and $\Sigma$ (the latter is estimated on the whole data);
	\item To classify use the mode of the posterior:
	\[P(X=x\lvert T=k) \Pr(T=k) = \pi_k \sum_j^{n_k} \hat{\alpha}_{kj} \phi(x\,\lvert\,\hat{\mu}_{kj}, \hat{\Sigma})\]
\end{itemize}

The log-likelihood is:
\[l(\pi, \alpha, \mu, \sigma\lvert x, t) = \sum_i^n \sum_{k=1}^K \log \pi_k^{t_i(k)} \big[f_k(x_i)\big]^{t_i(k)}\]
with $t_i(k)$ indicating if the $i$-th observation belongs to class $k$.

Introduce the latent variables for the mixture $u_{kj}(i)$ which indicates whether the $i$-th observation, which
came from $k$-th class was produced the $j$-th mixture component. Thus
\[f_k(x_i, u_{k\cdot}(i)) = \prod_j^{n_k} \big[ \alpha_{kj} \phi(x_i\,\lvert\,\mu_{kj}, \Sigma)\big]^{u_{kj}(i)}\]

therefore
\[l(\pi, \alpha, \mu, \sigma\lvert x, t)
= \sum_i^n \sum_{k=1}^K t_i(k) \log \pi_k + \sum_i^n \sum_{k=1}^K t_i(k) \log f_k(x_i)\]

Pluggin in the product form of the density, with latent variables yields:
\begin{align*}
l(\pi, \alpha, \mu, \sigma\lvert x, t)
&= \sum_i^n \sum_{k=1}^K t_i(k) \log \pi_k\\
&+ \sum_i^n \sum_{k=1}^K \sum_j^{n_k} t_i(k) u_{kj}(i) \log \alpha_{kj}\\
&+ \sum_i^n \sum_j^{n_k} u_{kj}(i) \log \phi(x_i\,\lvert\,\mu_{kj}, \Sigma)
\end{align*}

The E-step. The optimal prediction of the latent variables is
\[\hat{u}_{kj}(i) = \Pr(X=x_i\lvert Z_i=j, C_i = k) \Pr(Z_i=j\lvert C_i = k) \Pr(C_i = k)\]
whence the posterior probability of being in category $j$ of the class $k$:
\[\hat{u}_{kj}(i) = \frac{\alpha_{kj} \phi(x\,\lvert\,\mu_{kj}, \Sigma)}{\sum_{r=1}^{n_k} \alpha_{kr} \phi(x\,\lvert\,\mu_{kr}, \Sigma) }\]

the M-step. Computing the $\pi_k$ is mundane. Let's compute the $\alpha_{kj}$ and the parameters of the normal density.
First for every $k$ the sum $\sum_{j=1}^{n_k}\alpha_{kj} = 1$. The first derivative with respect to $\alpha_{kj}$, note the traces of Lagrangian multipliers:
\[ \sum_i^n t_i(k) u_{kj}(i) \frac{1}{\alpha_{kj}} = \lambda\]
whence 
\[\hat{\alpha}_{kj} = \sum_i^n t_i(k) u_{kj}(i) \frac{1}{\lambda} \]
and the constraint on $\alpha_{kj}$ yields
\[\sum_{j=1}^{n_k} \sum_i^n t_i(k) u_{kj}(i) = \lambda \]
therefore
\[\hat{\alpha}_{kj}
= \sum_i^n t_i(k) u_{kj}(i) \frac{1}{\sum_i^n t_i(k) \sum_{j=1}^{n_k} u_{kj}(i)}
= \sum_i^n t_i(k) u_{kj}(i) \frac{1}{ \sum_i^n t_i(k) } \]
because $\sum_{j=1}^{n_k} u_{kj}(i) = 1$ as any observation comes from just one mixture component
and $\hat{u}_{jk}(i)$ is the joint posterior probability of category $j$ for observation $i$ in class $k$.

The third term: with respect to $\mu_{jk}$
\[\sum_j^{n_k} u_{kj}(i) \log \phi(x_i\,\lvert\,\mu_{kj}, \Sigma)
= - \frac{1}{2} \sum_j^{n_k} u_{kj}(i) \big(x_i - \mu_{jk}\big)'\Sigma^{-1}\big(x_i - \mu_{jk}\big)\]
which, when differentiating with respect to $\mu_{kj}$, turns into
\[ \Sigma^{-1}\Big( - \frac{1}{2} \sum_j^{n_k} u_{kj}(i) \big(x_i - \mu_{jk}\big) \Big) = 0\]
since $\Sigma^{-1}$ is obviously non-singular:
\[ - \frac{1}{2} \sum_j^{n_k} u_{kj}(i) \big(x_i - \mu_{jk}\big) = 0\]
whence the expression for $\hat{\mu}_{kj}$ follow.

Estimating the $\Sigma$, using the trace:
\begin{multline*}
\frac{\partial }{\partial \Sigma} \sum_j^{n_k} u_{kj}(i) \log \phi(x_i\,\lvert\,\mu_{kj}, \Sigma) = \\
	- \frac{1}{2} \sum_i^n \sum_j^{n_k} u_{kj}(i) \big(x_i - \mu_{jk}\big)'\Sigma^{-1}\big(x_i - \mu_{jk}\big)
	- \frac{1}{2} \sum_i^n \sum_j^{n_k} u_{kj}(i) \log \lvert \Sigma\rvert
\end{multline*}
which yields the estimate of $\Sigma$.

% subsection the_problem_of_mixture_discriminant_analysis (end)

% section lecture_10 (end)

\clearpage
\section*{Tutorial prob \# 7} % (fold)
\label{sec:tutorial_prob_7}

The Gaussian mixture density is 
\[p(x) = \sum_{k=1}^K \pi_k \phi(x\lvert \mu_k, \Sigma_k)\]
where $\phi$ is the $\Ncal_d$ density, $\mu_k\in \Real^d$ is the mean and
$\Sigma_k\in\Real^{d\times d}$ -- a positive definite symmetric covariance matrix.

The log-likelihood of the model with unobserved variable $u_k(i)$ denoting
if the $i$-th observation came form the $k$-th mixture component.
\[
l(\pi, \mu, \Sigma\lvert x, u)
= \sum_{i=1}^N \sum_{k=1}^K u_k(i) \bigl(\log \pi_k + \log \phi(x\lvert \mu_k, \Sigma_k)\bigr)
\]

\textbf{E-step}: find the expectation of the posterior distribution of $u_k(i)$ given $\pi$, $\mu$ and $\Sigma$:
\[
\hat{u}_k(i)
= \pr(C_i=k\lvert X=x_i)
= \pr(X=x_i\lvert C_i=k)\frac{\pr(C_i=k)}{\pr(X=x_i)}
= \frac{\phi(x_i\lvert \mu_k, \Sigma_k) \pi_k}{\sum_{r=1}^K \phi(x_i\lvert \mu_r, \Sigma_r) \pi_r}
\]

\textbf{M-step}: maximize the likelihood with respect to $\pi$, $\mu$ and $\Sigma$ given the estimates
of the latent states $\hat{u}_k(i)$. Note that the likelihood is additively separable in $\pi$ and the $\mu$-$\Sigma$ group
\[
l(\pi, \mu, \Sigma\lvert x, \hat{u})
= \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) \log \pi_k + \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i)\log \phi(x_i\lvert \mu_k, \Sigma_k)
\]
whence maximisation with respect to $\pi$ can be done separately.

\subsection*{$\pi$} % (fold)
\label{sub:_pi_}

Construct the Lagrangian
\[\Lcal = \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) \log \pi_k - \lambda\bigl( \sum_{k=1}^K \pi_k - 1\bigr)\]
The first order condition is
\[\frac{\partial \Lcal}{\partial \pi_k} = \sum_{i=1}^N \hat{u}_k(i) \frac{1}{\pi_k} - \lambda = 0\]
from which it follows that
\[\sum_{i=1}^N \hat{u}_k(i) = \lambda \pi_k\]
since $\sum_{k=1}^K \pi_k - 1$, we must have
\[\sum_{k=1}^K \sum_{i=1}^N \hat{u}_k(i) = \lambda\]
Because $\sum_{k=1}^K \hat{u}_k(i) = 1$it must be true that $\lambda > 0$ which means that
\[
\hat{\pi}_k
= \frac{ \sum_{i=1}^N \hat{u}_k(i) }{ \sum_{k=1}^K \sum_{i=1}^N \hat{u}_k(i) }
= \frac{1}{n} \sum_{i=1}^N \hat{u}_k(i)
\]

% subsection* _pi_ (end)

\subsection*{$\mu$, $\Sigma$} % (fold)
\label{sub:_mu_sigma_}

Consider the right half of $l(\pi, \mu, \Sigma\lvert x, \hat{u})$:
\[\sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i)\log \phi(x_i\lvert \mu_k, \Sigma_k)\]
if fact since $\phi$ is multivariate Gaussian
\[
\phi(x\lvert \mu_k, \Sigma_k)
= \frac{1}{(2\pi)^\frac{d}{2} \lvert \Sigma_k\rvert^\frac{1}{2} } e^{ -\frac{1}{2} (x-\mu_k)' \Sigma_k^{-1} (x-\mu_k) }
\]
its log-density provides the following expression for the is partial log-likelihood
\[
\sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) \frac{1}{2} \log\lvert\Sigma_k^{-1}\rvert 
	- \frac{d}{2} \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) \log2\pi 
	- \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) (x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k)
\]

\subsubsection*{$\mu$} % (fold)
\label{ssub:_mu_}

The part of log-likelihood with $\mu$ is
\[
l(\pi, \mu, \Sigma\lvert x, \hat{u})
= \ldots - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) (x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k) + \ldots
\]
The FOC:
\[
\frac{\partial l}{\partial \mu_k} = - \frac{1}{2} \sum_{i=1}^N \hat{u}_k(i) \Sigma_k^{-1} (x_i-\mu_k) = 0
\]
By assumption $\Sigma_k$ is non-singular (otherwise how would have its inverse existed). Hence
\[\sum_{i=1}^N \hat{u}_k(i) (x_i-\mu_k) = 0\]
and
\[
\hat{\mu}_k = \frac{\sum_{i=1}^N \hat{u}_k(i) x_i }{\sum_{i=1}^N \hat{u}_k(i)}
\]

% subsubsection* _mu_ (end)

\subsubsection*{$\Sigma$} % (fold)
\label{ssub:_sigma_}

$\Sigma$: It is impractical to find the $\Sigma$ directly by brute force differentiation, at least
because the expression involving $\Sigma$ actually depends on its inverse. Thus let's peruse $\Sigma^{-1}$
\[
l(\pi, \mu, \Sigma\lvert x, \hat{u})
= \ldots + \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) \frac{1}{2} \log\lvert\Sigma_k^{-1}\rvert 
	- \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^K \hat{u}_k(i) (x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k)
\]

% subsubsection* _sigma_ (end)

\subsubsection*{Some matrix calculus} % (fold)
\label{ssub:some_matrix_calculus}

First, using Laplace determinant expansion theorem, it is very easy to show that
\[\frac{\partial \lvert A\rvert}{\partial A} = \lvert A\rvert \bigl(A^{-1}\bigr)'\]
Indeed, expanding by $i$-th row
\[
\frac{\partial \lvert A\rvert}{\partial A_{ij}}
= \frac{\partial }{\partial A_{ij}} \sum_{k=1}^n A_{ik} \tilde{A}_{ik}
= \tilde{A}_{ij}
\]
where $\tilde{A_{ik}}$ is the cofactor (\rus{алгебраическое дополнение}) of
the element $A_{ij}$ in the square matrix $A$, and no element of the cofactor
depends on $A_{ij}$, as it basically the $i$,$j$ minor of $A$.

Second, the derivative of a trace of a product $AB$ ($n\times m$,$m\times n$)
with respect to an element $A_{ij}$ is given by
\[
\frac{\partial \tr(AB)}{A_{ij}}
= \frac{\partial }{A_{ij}} \sum_{s=1}^m A_{is} B_{si}
= B_{si}
\]
whence $\frac{\partial \tr(AB)}{A} = B'$.

Finally, there is the cyclical property of traces: $\tr(ABC) = \tr(BCA) = \tr(CAB)$.

% subsubsection* some_matrix_calculus (end)

\subsubsection*{back to the problem} % (fold)
\label{ssub:back_to_the_problem}
Notice that the term $(x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k)$ in the log-likelihood
is in fact a scalar. This means that it is equal to its own trace. Using the cyclical
property one may get
\[
\tr \bigl((x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k)\bigr)
= \tr \bigl(\Sigma_k^{-1} (x_i-\mu_k) (x_i-\mu_k)'\bigr)
\]
Therefore
\[
\frac{\partial }{\partial \Sigma_k^{-1}} (x_i-\mu_k)' \Sigma_k^{-1} (x_i-\mu_k)
= \bigl((x_i-\mu_k) (x_i-\mu_k)'\bigr)'
\]

Using all these observations one gets
\[
\frac{\partial }{\partial \Sigma_k^{-1}} l(\pi, \mu, \Sigma\lvert x, \hat{u})
= \frac{1}{2} \sum_{i=1}^N \hat{u}_k(i) \Bigl(\bigl(\Sigma_k^{-1}\bigr)^{-1}\Bigr)'
- \frac{1}{2} \sum_{i=1}^N \hat{u}_k(i) (x_i-\mu_k) (x_i-\mu_k)' = 0
\]
Since $\Sigma_k$ is a symmetric non-singular matrix, this expression is equivalent to
\[
\Sigma_k \sum_{i=1}^N \hat{u}_k(i) = \sum_{i=1}^N \hat{u}_k(i) (x_i-\mu_k) (x_i-\mu_k)'
\]
whence
\[
\hat{\Sigma}_k = \frac{1}{\sum_{i=1}^N \hat{u}_k(i)} \sum_{i=1}^N \hat{u}_k(i) \Gamma_{ik}
\]
where $\Gamma_{ik} = (x_i-\mu_k) (x_i-\mu_k)'$ is an $d\times d$ matrix of
``intra-cluster covariance''.

% subsubsection* back_to_the_problem (end)

\subsubsection*{Altogether} % (fold)
\label{ssub:altogether}

The E-step gives:
\[
\hat{u}_k^{t+1}(i)
= \frac{\phi(x_i\lvert \hat{\mu}_k^t, \hat{\Sigma}_k^t) \hat{\pi}_k^t}
	{\sum_{r=1}^K \phi(x_i\lvert \hat{\mu}_r^t, \hat{\Sigma}_r^t) \hat{\pi}_r^t}
\]
the M-step:
\begin{align*}
	\hat{\pi}_k^{t+1}
		&= \frac{1}{n} \sum_{i=1}^N \hat{u}_k^{t+1}(i)\\
	\hat{\mu}_k^{t+1}
		&= \frac{1}{\sum_{i=1}^N \hat{u}_k^{t+1}(i)} \sum_{i=1}^N \hat{u}_k^{t+1}(i) x_i\\
	\hat{\Sigma}_k^{t+1}
		&= \frac{1}{\sum_{i=1}^N \hat{u}_k^{t+1}(i)} \sum_{i=1}^N \hat{u}_k^{t+1}(i) \Gamma_{ik}^{t+1}\\
\end{align*}
where $\Gamma_{ik}^{t+1} = (x_i-\mu_k^{t+1}) (x_i-\mu_k^{t+1})'$
% subsubsection* altogether (end)

% subsection* _mu_sigma_ (end)

\subsection*{$k$-means clustering} % (fold)
\label{sub:k_means_clustering}

Consider the minimization problem of the functional
\[ \Lcal = \sum_{i=1}^N \sum_{k=1}^K r_k(i) (x_i-\mu_k)' (x_i-\mu_k) \to \min_{r, \mu} \]
where $r_k(i) = 1_{C_i=k}$ the $0-1$ indicator if the $i$-th observation belongs
to the $k$-th class.

First, suppose the $\mu=\bigl(\mu_k\bigr)_{k=1}^k$ are fixed and it is necessary
to optimize with respect to $r_k(i)$ for some particular observation $i$. The
functional can be rewritten in the following way
\[\Lcal = \ldots + \sum_{k=1}^K r_k(i) (x_i-\mu_k)' (x_i-\mu_k) \to \min_{r, \mu} + \ldots \]
where the dotted parts are independent of $r_k(i)$ for \textbf{this} $i$
provided $\mu$ is fixed.
In order to minimize this partial expression one must set $r_k(i)=0$ for all $k$
but the one which corresponds to the smallest value of $(x_i-\mu_k)' (x_i-\mu_k)$.
Thus, in effect the $i$-th observation is assigned to a class to the centroid $\mu_k$
of which it is the nearest.

Second, given fixed values of $r_k(i)$ for all $k$ and $i$, the minimization of
$\Lcal$ with respect to $\mu_k$ is a trivial exercise in optimization:
\[\frac{\partial \Lcal}{\partial \mu_k} = 2 \sum_{i=1}^N r_k(i) (x_i-\mu_k)\]
whence
\[\mu_k = \frac{\sum_{i=1}^N r_k(i) x_i}{\sum_{i=1}^N r_k(i)}\]

In, conclusion, the two-step maximization procedure must be the following one \begin{description}
	\item[find] Set $r_k(i)=0$ for $k\neq k_i$ and $r_{k_i}(i)=1$ otherwise, where
	$k_i = \argmin_k (x_i-\mu_k)'(x_i-\mu_k)$;
	\item[update] Compute the new centroids with
	\[\mu_k = \frac{\sum_{i=1}^N r_k(i) x_i}{\sum_{i=1}^N r_k(i)}\]
\end{description}
Given $\mu^t = \bigl(\mu_k^t\bigr)_{k=1}^K$, $r_k^{t+1}(i)$ are computed,
and afterwards $\mu^t$ is updated to get $\mu_k^{t+1}$. The stopping criterion
is the stabilization of $r_k(i)$.

Actually one needs to show that this iterative procedure does not increase
the functional $\Lcal$ by the end of each iteration. Indeed, by construction
of $r^t$ and $\mu^t$
\[\Lcal(r^{t+1}, \mu^t) = \min_r \Lcal(r, \mu^t) \leq \Lcal(r^t, \mu^t)\]
and
\[\Lcal(r^{t+1}, \mu^{t+1}) = \min_\mu \Lcal(r^{t+1}, \mu) \leq \Lcal(r^{t+1}, \mu^t)\]
whence $\Lcal$ indeed decreases with the number of iterations $t$.

% subsection* k_means_clustering (end)

%% \subsection*{Equivalence} % (fold)
%% \label{ssub:equivalence}
%% 
%% Consider the Gaussian mixture model with restriction $\Sigma_k = \epsilon I_d$ for $\epsilon>0$.
%% Therefore $\lvert \Sigma_k \rvert = \epsilo ^d$ and the density becomes
%% \[
%% \phi(x\lvert \mu_k, \Sigma_k)
%% = (2\pi \epsilon)^{-\frac{d}{2}} e^{-\frac{1}{2\epsilon} (x-\mu_k)'(x-\mu_k)}
%% \]
%% 
%% Thus
%% \[
%% \hat{u}_k^{t+1}(i)
%% = \frac{ e^{-\frac{1}{2\epsilon} (x-\mu_k)'(x-\mu_k)} \hat{\pi}_k^t}
%% 	{\sum_{r=1}^K e^{-\frac{1}{2\epsilon} (x-\mu_r)'(x-\mu_r)} \hat{\pi}_r^t}
%% \]
%% 
%% \hat{\pi}_k^{t+1}
%% 	&= \frac{1}{n} \sum_{i=1}^N \hat{u}_k^{t+1}(i)
%% \hat{\mu}_k^{t+1}
%% 	&= \frac{1}{\sum_{i=1}^N \hat{u}_k^{t+1}(i)} \sum_{i=1}^N \hat{u}_k^{t+1}(i) x_i
%% \hat{\Sigma}_k^{t+1}
%% 	&= \frac{1}{\sum_{i=1}^N \hat{u}_k^{t+1}(i)} \sum_{i=1}^N \hat{u}_k^{t+1}(i) \Gamma_{ik}^{t+1}
%%% subsection* equivalence (end)

% section* tutorial_prob_7 (end)

\clearpage
\section*{Mid-term test review} % (fold)
\label{sec:mid_term_test_review}
Ridge regression protects from the poor estimation in the direction of the least variance.
% section* mid_term_test_review (end)

\section{Revision} % (fold)
\label{sec:revision}

\subsection{EM-algorithm} % (fold)
\label{sub:em_algorithm}

Consider a model with $X$ observable variables and $Z$ -- latent described by
the likelihood function $L(X,Z;\theta)$.

For all $\theta$ it is true that 
\[\log p(X\;\theta) = \log p(X,Z;\theta) - \log p(Z\lvert X;\theta)\]
Let $q$ be some distribution of the latent variables $Z$, then
\[ \ex_q\log p(X\;\theta) = \ex_q\log p(X,Z;\theta) - \ex_q \log q + \ex_q\log {q}{p(Z\lvert X;\theta)} \]
whence 
\[
\ex_q\log p(X,Z;\theta)
= \log p(X\;\theta) + \ex_q \log q - \ex_q\log {q}{p(Z\lvert X;\theta)}
= - KL\bigl(q\lvert\rvert p(Z\lvert X;\theta)\bigr) + \ex_q \log q + \log p(X\;\theta)
\]

% subsection em_algorithm (end)

% section revision (end)

\section{Lecture \# 11} % (fold)
\label{sec:lecture_11}

An overview of module 4:
\begin{itemize}
	\item linear classifiers (Fisher approach, and perceptron);
	\item Decision trees, (bagginng, boosting, adaboost at c.);
	\item Support Vector Machine (convex optimization);
	\item Reproducing Kernel Hilbert Spaces (classification and regression);
	\item Neural Networks;
	\item (optional) Bayesian regression and classification.
\end{itemize}

\subsection{Classifiers} % (fold)
\label{sub:classifiers}
Probabilistic and discriminative approaches.

One-to-one or one-to-all may result in undefined regions in multi-class problems.

\subsubsection{A reminder on hyperplanes} % (fold)
\label{ssub:a_reminder_on_hyperplanes}
Consider a linear space $V$ with $\dim(V)=p$ and an inner product $\langle\rangle$.
Then a hyperplane is a flat affine subspace $H$ of $V$ of dimension $p-1$. The equation
of a hyperplane $H$ is given by the equation for $x\in V$
\[
\beta_0 + \sum_{k=1}^p \beta_k x_k
= \beta_0 + \langle x, \beta\rangle
= 0
\]
for some $\beta\in V$ -- a normal vector. Let $\beta^* = \frac{\beta}{\|\beta\|}$.

The signed distance of $y\in V$ to the hyperplane $H$ is given by
\[\]
Indeed, for $x_1, x_2\in H$
\begin{align*}
	\langle x_1, \beta\rangle &= -\beta_0\\
	\langle x_2, \beta\rangle &= -\beta_0
\end{align*}
whence
\[\langle x_2-x_1, \beta\rangle = 0\]

Next, for any $y\in V$ there exist a decomposition
\[ y = x^\perp + x^*\]
where $x^*\in H$ and $x^\perp = r\beta^*$. Therefore
\[
\langle\beta,y\rangle
= \langle\beta,x^*\rangle + \langle\beta,r\beta^*\rangle
= 0 + r \frac{\|\beta\|^2}{\|\beta\|}
\]
Thus $\inf_{x\in H}\|y-x\| = r\|\beta\|$ (?).

% subsubsection a_reminder_on_hyperplanes (end)

\subsubsection{The leas sqaure apporach} % (fold)
\label{ssub:the_leas_sqaure_apporach}

The problem of classification is to choose a class among $(c_k)_{k=1}^K$ by using
$K$ linear discriminant functions: for $x\in \Real^d$ and $k=1,\ldots, K$
\[y_k(x) = \beta_{k0} + \beta_k'x\]
Classify by choosing the class with the largest $y_k(x)$.

Let $B$ be a matrix defiend as 
\begin{align*}
\tilde{B}' &= \begin{pmatrix}
	\beta_{10} 	& \beta_{11} 		& \beta_{12} 		& \cdots & \beta_{1d} \\
	\vdots 	& \vdots	& \vdots 	& \ddots & \vdots \\
	\beta_{K0} 	& \beta_{K1} 		& \beta_{K2} 		& \cdots & \beta_{Kd} \\
	\end{pmatrix}_{K\times (D+1)}
\end{align*}
Let $\tilde{\beta}_k' = (\beta_{k0}\, \beta_k)$ where $\beta_k = (\beta_{km})_{m=1}^d$.

Furthermore let $t$ be used in a 1-of-K notation $t = e_k$ vector $K\times 1$.

The training data (?)
\begin{align*}
T &= \begin{pmatrix}
	\beta_{10} 	& \beta_{11} 		& \beta_{12} 		& \cdots & \beta_{1d} \\
	\vdots 	& \vdots	& \vdots 	& \ddots & \vdots \\
	\beta_{K0} 	& \beta_{K1} 		& \beta_{K2} 		& \cdots & \beta_{Kd} \\
	\end{pmatrix}_{n\times K}
\end{align*}

\textbf{Check other's notes!}(?)
\begin{align*}
\tilde{X} &= \begin{pmatrix}
	\beta_{10} 	& \beta_{11} 		& \beta_{12} 		& \cdots & \beta_{1d} \\
	\vdots 	& \vdots	& \vdots 	& \ddots & \vdots \\
	\beta_{K0} 	& \beta_{K1} 		& \beta_{K2} 		& \cdots & \beta_{Kd} \\
	\end{pmatrix}_{n\times (D+1)}
\end{align*}

Consider a problem of discriminating between two classes. The goal is to match the $t$
to $y$ by choosing a proper set of coefficients $\tilde{B}$. We want to minimize the
error function
\[\text{error} = \frac{1}{2} \tr \bigl(\tilde{X}\tilde{B} - T\bigr)'\bigl(\tilde{X}\tilde{B} - T\bigr)\]
\textbf{see the picture}
the solution is given by
\[
\bigl(\tilde{X}\tilde{B} - T\bigr)
+ \bigl(\tilde{X}\tilde{B} - T\bigr)'
= 0
\]
whence 
\[\tilde{B} = (\tilde{X}'\tilde{X})^{-1}(\tilde{X}'T)\]

Least squares estimation in not robust. If there is a third class, on the same
side of the bi-class hyperplane, then the outlying class will be much heavily
penalized, whence the hyperplane in this case will fail to properly distinguish
the first two classes (the plane shifts towards outlier).

The LS solution has the following property: it preserves the linear constraints
on the target vector. If every observation in the training set satisfies $a't_i + b = 0$
for all $i=1,\ldots N$ then $a'y(x) + b = 0$ for all $x\in \Real^d$.

Proof:
The notation $\tilde{B}'\in \Real^{K\times (d+1)}$:
\[\tilde{B}' = \bigl(\beta_0 \rvert B'\bigr)\]
with $\beta_0\in \Real^{K\times 1}$ and $B\in \Real^{K\times K}$.
The contribution of the bias can hereafter be extracted from the product
\[\tilde{X}\tilde{B} = \one\beta_0' + \tilde{X}B\]

The LS estimate (the first step):
\[
\text{error} = \frac{1}{2} \tr\bigl\{
\beta_0\one'\one\beta_0' + \beta_0\one'\tilde{X}B - \beta_0\one'T
+ B\tilde{X}'\one\beta_0' + B\tilde{X}'\tilde{X}B - B\tilde{X}'T
- T'\one\beta_0' - T'\tilde{X}B + T'T
\bigr\}
\]
differentiating with respect to $\beta_0$ and equating to zero yields
\[
\frac{\partial}{\partial \beta_0} \text{error} 
= n\beta_0 + (XB-T)'\one
= 0
\]
which implies that 
\[
\beta_0 = (\one'\one)^{-1} (T-XB)'\one
\]
if $\bar{t} = (\one'\one)^{-1}T'\one$ and $\bar{x} = (\one'\one)^{-1}X'\one$ 
then
\[\beta_0 = \bar{t} - B'\bar{x}\]

Therefore
\[
\text{error}
= \frac{1}{2} \tr\bigl\{ (XB + \one( \bar{t} - B'\bar{x} )' )' \times ( \ldots ) \bigr\}
\]
Letting $\bar{T} = \one \bar{t}'$ and $\bar{X} = \one \bar{x}'$ gives
\[
\text{error}
= \frac{1}{2} \tr\Bigl\{ \bigl( (X-\bar{X}) B - [T-\bar{T}] \bigr)' \times (\ldots) \Bigr\}
\]
whence finally the target criterion becomes
\[
\text{error}
= \frac{1}{2} \tr\Bigl\{ ( \hat{X} B - \hat{T} )' \times ( \hat{X} B - \hat{T} ) \Bigr\}
\]
which implies that 
\[B = (\hat{X}'\hat{X})^{-1}\hat{X}'\hat{T}\]

The second step: apply the linear constraints to $\bar{t}$.
\[
a'\bar{t} + b
= a'\frac{1}{N}T'\one + b
= \frac{1}{N} a'T'\one + b
= -\frac{1}{N} b\one' \one + b
\]
since $a'T' = -b\one'$ by the constraints.

The last step: compute the $y(x)$.
IT sis given by
\[
y(x^*)
= \beta_0 + B'x^*
= \bar{t} - B'\bar{x} + B'x^*
= \bar{t} + B'(x^*-\bar{x})
= \bar{t} + \hat{T}'\hat{X}(\hat{X}'\hat{X})^{-1}(x^*-\bar{x})
\]
now \begin{align*}
a'y(x^*)
	&= a'\bar{t} + a'\hat{T}'\hat{X}(\hat{X}'\hat{X})^{-1}(x^*-\bar{x})\\
\end{align*}
Simplifying gives
\[
a'\hat{T}'
= a'(T-\bar{T})'
= a'T'-a'\bar{T}'
= - b\one' - a'\bar{t}\one'
= - b\one' - b\one'
=0
\]
QED

% subsubsection the_leas_sqaure_apporach (end)

\subsubsection{Fisher's linear discriminant} % (fold)
\label{ssub:fisher_s_linear_discriminant}

Suppose $x\in \Real^{p}$ but there are only two classes $K=2$. The main idea is
to try to reduce the dimensionality of the data, to project it in a ``smart'' way
to enable easy classification. Use the following projection onto $[\beta]$:
\[y = \beta'x\]
usually $x$ do not include the intercept.
The direction of $\beta$ is fine but, there has to be a decision made on the
threshold $\beta_0$ of the discriminant hyperplane.
The classification rule is 
\[\begin{cases}
	\text{class}_1, &\text{ if } y(x)\geq -\beta_0\\
	\text{class}_2, &\text{ otherwise}
\end{cases}\]
The rationale : classes are well separated if the mean values are well separated.
Thus we need to maximize the separation of the projected means onto the flat world
inside the hyperplane.

The means and their projections are given by
\[
\bar{m}_k = \frac{1}{|C_k|}\sum_{x\in C_k} x,
\text{ and }
m_k = \beta'\bar{m}_k
\]
respectively. Fix the magnitude of $\beta$ since the separation increases to infinity
with $\|\beta\|$. The lagrangian
\[\Lcal = \beta'(\bar{m}_2-\bar{m}_1) + \lambda (\beta'\beta - 1)\to\max\]
whence
\[\bar{m}_2-\bar{m}_1 + 2\lambda\beta = 0\]
It turns out to be that the optimal $\beta$ must be parallel to the line between
$\bar{m}_1$ and $\bar{m}_2$, which is geometrically intuitive:
\[\beta \propto \bar{m}_1-\bar{m}_2\]

However, it seems that using the class variance might give better discrimination.
The ideas is the same: project in smart way, but also try to balance the separation
with the within class variance:
\[S_k = \sum_{x\in C_k} (\beta'x-\beta'\bar{m}_k)^2\]

Consider the following criterion (Fisher's)
\[J(\beta) =\frac{(m_1-m_2)^2}{S_1+S_2} \]
Let's make the dependence on $\beta$ explicit by computing the $S_k$.
By definition 
\[
(m_2-m_1)^2
= (m_2-m_1)(m_2-m_1)'
= \beta'(\bar{m}_2-\bar{m}_1)(\bar{m}_2-\bar{m}_1)'\beta
= \beta'S_B\beta
\]
where $S_B$ -- the between class variance matrix (scatter). As for the denominator
\[
S_2+S_1
= \sum_{x\in C_1} (\beta'x-\beta'\bar{m}_1)^2 + \sum_{x\in C_2} (\beta'x-\beta'\bar{m}_2)^2
= \beta'\Bigl(\sum_{x\in C_1} (x-\bar{m}_1)(x-\bar{m}_1)' + \sum_{x\in C_2} (x-\bar{m}_2)(x-\bar{m}_2)'\Bigr) \beta
= \beta'S_W\beta
\]
Thus the quotient we are trying to minimize is the Rayleigh quotient
\[J(\beta) = \mathcal{R} = \frac{\beta'S_B\beta}{\beta'S_W\beta}\]
Reexpress this as an optimization problem with respect to $\beta^* = \beta c$
to see that $c$ cancel. Therefore it is possible to fix the denominator and
then maximize the numerator. Thus we will be trying to maximize the 
\[\beta'S_B\beta\to\max_\beta\]
subject to $\beta'S_W\beta = 1$. The Lagrangian is
\[\Lcal = \beta'S_B\beta - \lambda(\beta'S_W\beta-1)\to\max_{\beta,\lambda}\]
The first order conditions
\begin{align*}
	\frac{\partial}{\partial\beta} \Lcal &= S_B\beta - \lambda S_W\beta = 0\\
	\frac{\partial}{\partial\lambda} \Lcal &= 1 - \beta'\lambda S_W\beta
\end{align*}
Which reduces to a generalized eigenvalue-eigenvector problem
\[
S_B\beta = \lambda S_W\beta
\]
But since $\beta'\lambda S_W\beta=1$ it turns out that
\[\beta'S_B\beta = \lambda \beta'S_W\beta\]
whence $\lambda = \beta'S_B\beta$. Therefore in the generalized eigenvalue-eigenvector
problem the eigenvector $\beta$ with the largest eigenvalue must be chosen. In fact
it is possible to reduce this generalized problem to the usual eugen-problem.
If $S_W$ is invertible then the problem can be transformed into
\[S_W^{-1}S_B \beta = \lambda \beta\]
otherwise it is necessary to regularize the $S_W$ with $S_W + cI_p$ for $c>0$.

Now notice that
\[
S_B\beta
= (\bar{m}_2-\bar{m}_1)(\bar{m}_2-\bar{m}_1)'\beta
= (\bar{m}_2-\bar{m}_1)\bigl((\bar{m}_2-\bar{m}_1)'\beta\bigr)_{1\times 1}
\propto \bar{m}_2-\bar{m}_1
\]
whence from the FOC
\[\beta\propto S_W^{-1} (\bar{m}_2-\bar{m}_1)\]

In order to find the threshold, one could fit the ML mean and variance.

% subsubsection fisher_s_linear_discriminant (end)

\subsubsection{Relation to the LDA} % (fold)
\label{ssub:relation_to_the_lda}

Again the case of $2$ classes. The LDA was
\[
\frac{\Pr\bigl(T=k\lvert X=x\bigr)}{\Pr\bigl(T=j\lvert X=x\bigr)}
= x'\Sigma^{-1}(\mu_k-\mu_j) + \text{terms independent of } x
\]
which is a hyperplane with $\beta = \Sigma^{-1}(\mu_k-\mu_j)$.
The MLE of $\hat{\Sigma}$ is given by
\[\hat{\Sigma} = \frac{1}{N}\sum_{k=1}^K \sum_{x\in C_k} (x - \mu_k)(x - \mu_k)'\]
where $n\hat{\Sigma} = S_W$.

Exercise: show that LDA and Fisher's approach are identical in one special case.


% subsubsection relation_to_the_lda (end)

% subsection classifiers (end)

% section lecture_11 (end)

\section{Lecture \# 12} % (fold)
\label{sec:lecture_12}

\subsection{The perceptron classifier} % (fold)
\label{sub:perceptron}

Recall that the signed distance form a hyperplane $\beta_0 + \beta'x = 0$ to a
point $p$ is given by
\[\nrm{p-H} = \frac{\beta_0 + \beta'x}{\sqrt{\beta'\beta}}\]

A target variable $t_i$ is given by $t_i = +1$ if $x_i\in C_1$ and $t_i = -1$
if $x_i\in C_2$ (otherwise). The convenience of such encoding is that the sign
coincides with the misclassification by the hyperplane. If $x_i$ is misclassified
as $C_2$ then $t_i>0$, but $\beta_0+\beta'x_i < 0$ and vice versa. Thus every time
a point is misclassified, we have $t_i(\beta_0 +\beta'x_i) < 0$, and the last term in
brackets is proportional to the distance of $x_i$ from the hyperplane. The following
criterion minimizes the overall distance of the points:
\[
E = - \sum_{i\in \Mcal} t_i( \beta_0 + \beta'x_i ) \to \min_{\beta_0, \beta}
\]
where $\Mcal$ is a the set of misclassified points and $\|\beta\| = 1$ -- this restricts
the problem to well-defined hyperplanes only.

The gradient of $E$ with respect to $\beta_0$ is given by:
\[\frac{\partial}{\partial \beta_0} E = - \sum_{i\in \Mcal} t_i\]
and the 
\[\frac{\partial}{\partial \beta} E = - \sum_{i\in \Mcal} t_i x_i \]

The batch algorithm is using all the datapoints at the same time
\[
\bigl(\begin{smallmatrix} \beta_0\\ \beta \end{smallmatrix}\bigr)
\leftarrow \bigl(\begin{smallmatrix} \beta_0\\ \beta \end{smallmatrix}\bigr)
+\eta \nabla E
\]
where $\eta$ is the convergence parameter, usually set to $1$.

Stochastic gradient descent is going to update the estimate of $\beta_0$ and $\beta$ 
by going through the dataset point by point:
\[
\bigl(\begin{smallmatrix} \beta_0\\ \beta \end{smallmatrix}\bigr)
\leftarrow \bigl(\begin{smallmatrix} \beta_0\\ \beta \end{smallmatrix}\bigr)
+\eta \nabla \bigl(\begin{smallmatrix} t_i\\ t_i x_i \end{smallmatrix}\bigr)
\]
Update if the point is currently misclassified.

If the data is linearly separable, then the perceptron algorithm converges in a finite
number of iterations.

Depending on a starting values the convergence will take place to a possibly different local minimum.

If the data is not linearly separable then the algorithm will loop (go in cycles).

% subsection perceptron (end)

\subsubsection{Proof of the algorithm} % (fold)
\label{ssub:proof_of_the_algorithm}
Suppose the data is linearly separable. then there exits some $\beta$ and $\beta_0$
such that $t_i ( \beta_0 + \beta'x_i) > 0$ for all $i=1,\ldots, n$.

Denote by $b$ the vector built by stacking $\beta_0$ atop $\beta$ and let
$x_i^* = \bigl(\begin{smallmatrix} 1\\ x_i \end{smallmatrix}\bigr)$. Furthermore let
$z_i = \frac{x_i^*}{\|x_i^*\|}$. Thus $t_i b'z_i > 0 $ for all $i$, whence there must
exist $m>0$ with $t_ib'z_i \geq m$ (since there are finitely many points in the dataset).
Let $b_\text{sep} = \frac{b}{m}$. For such $b_\text{sep}$ it is true that for all $i$
\[t_i b_\text{sep}' z_i \geq 1\]

The perceptron algorithm updates the estimate by (force $eta_i = 1$)
\[b^\text{new} = b_\text{old} + \eta_i t_i z_i\]
Hence
\[
b^\text{new} - b_\text{sep} = b_\text{old} - b_\text{sep} + \eta_i t_i z_i
\]
and the point $i$ is misclassified. The squared norm of the updated parameters is
\[
\|b^\text{new} - b_\text{sep}\|^2
= \|b_\text{old} - b_\text{sep}\|^2 + \eta_i^2 \|t_i z_i\|^2
+ 2 t_i (b_\text{old} - b_\text{sep})'z_i
= \|b_\text{old} - b_\text{sep}\|^2 + |t_i|^2 \|z_i\|^2
+ 2 \eta_i t_i (b_\text{old} - b_\text{sep})'z_i
\]
Since $i$ is misclassified we have $t_i b_\text{old}' z_i < 0$ whereas by construction $t_i b_\text{sep}' z_i \geq 1$. In other words:
\[2 t_i b_\text{old}'z_i - 2 t_i b_\text{sep}'z_i \leq 2 t_i b_\text{old}'z_i - 2\leq -2\]
whence
\[
\|b^\text{new} - b_\text{sep}\|^2 \leq \|b_\text{old} - b_\text{sep}\|^2 + \eta_i^2 - 2 \eta_i
\]

% subsubsection proof_of_the_algorithm (end)

\subsection{Perceptron and Neural networks} % (fold)
\label{sub:neural_networks}

A simple mathematical model of a neuron is that of an input integrator, which fires
when a certain threshold is surpassed.
\begin{description}
	\item[Dendrites] collect the input from other neurons;
	\item[Soma] is responsible for aggregating the input from the dendrites
	and releases an electric spike upon saturation;
	\item[Axon] is the conduit through which the signal from soma is propagated
	further through the network.
\end{description}

Mathematically the model is :
\[x\to f\bigl( w_j'x + w_{j0}\bigr)\]
where $w_{j0}$ is the bias (shift, baseline weight), and $w_j$ is the vector of
dendrite ``weights''. The argument of the activation function $f$ is known as the
soma potential Can be extended to stochastic neural networks by treating the result
as the rate of the Poisson process, governing the firing of a neuron.

Typical activation functions are \begin{description}
	\item[Threshold] $f(z) = 1_{z\geq \theta}$;
	\item[Sigmoid] $f(z) = \frac{1}{1+e^{-z}}$.
\end{description}
A simple perceptron cannot realise a bitwise XOR function, since it is not linearly
separable.

The idea is to group perceptrons in layers and stack layer upon each other and
then use that to construct non-linear decision boundaries.

% subsection neural_networks (end)

\subsection{Decision trees} % (fold)
\label{sub:decision_trees}

This is a completely another topic: nonlinear classification. The idea is to partition
the input space into hyper-rectangles. The question is how to construct the regions
and then to use them for classification purposes.

\subsubsection{Classification and Regression Trees} % (fold)
\label{ssub:classification_and_regression_trees}

The construction methods:\textbf{C}lassification \textbf{a}nd \textbf{R}egression
\textbf{T}rees (Brieman;1984), ID3, C4.5.

The set of rules for classification can be summarized in a tree: easy interpretation
and highly transparent decision procedure.
\begin{itemize}
	\item How to choose the splitting variable;
	\item how to predict and classify in each region.
\end{itemize}

Consider the usual training data $(x_i,t_i)$ with $x_i = (x_{ij})_{j=1}^p$. If the
non-overlapping regions $(R_m)_{m=1}^M$ are constructed, then the regression function
can be defined as the following step function
\[f(x) = \sum_{m=1}^M c_m 1_{R_m}(x)\]
The criterion for minimization is 
\[\sum_{i=1}^N \bigl(t_i - f(x_i)\bigr)^2\]
whence the optimal values of $c_m$ are given by local means within the region:
\[\hat{c}_m = \frac{\sum_{i=1}^N t_i 1_{R_m}(x_i)}{|R_m|}\]
this answers the second question.


The first question is harder, since there are infinitely many way to partition the
space into $M$ regions. The algorithm which is going to be used is the following greedy
algorithm.

Suppose $j$ is the splitting feature (variable) and $s$ id the splitting point.
Let 
\[R_1(j,s)
= \bigl\{x_i\rvert x_{ij}\leq s\bigr\}
= X \bigcap \Bigl( (-\infty, s] \times \prod_{m\neq j} \Real \Bigr)
\]
and $R_2(j,s)$ be the compliment of $R_1(j,s)$.

Let's look for $j$ and $s$ such that 
\[
  \min_{c_1}\sum_{x_i\in R_1(j,s)} (t_i - c_1)^2
+ \min_{c_2}\sum_{x_i\in R_2(j,s)} (t_i - c_2)^2
\to \min_{j,s}
\]
For a given choice of $j,s$ the parameters $c_1$ and $c_2$ are given by the local
averages. An important observation is that the space of effective $s$ for a given $j$
is finite and is given by the set of all possible values of the $j$-th feature.
Therefore exhaustive search is possible, since there are $p$ dimensions and finitely
many data points.

The tree size can be regarded as the model complexity. Possible approaches:
\begin{itemize}
	\item Myopic approach: continue until the reward from splitting is less than
	some threshold;
	\item Pruning approach: start with a large tree $T_0$ and consider a subtree
	$T\subseteq T_0$ obtained by collapsing the branching points, until some
	optimality criterion is met;
\end{itemize}

Trade off between the goodness of fit and the complexity odf hte tree.
Define the \textbf{cost-complexity pruning}
\[C_\lambda(T) = \sum_{m=1}^M\sum_{x_i\in R_m} (t_i - \hat{c}_m)^2 + \lambda |T| \]
where $|T|$ is the number of leaves in tree $T$. If $\lambda=0$ then the optimal
tree is $T_0$, otherwise for $\lambda\to \infty$ the optimal trees would have
fewer and fewer leaves.

Rewrite the criterion as
\[C_\lambda(T) = \sum_{m=1}^M |R_m| Q_m(T)+ \lambda |T| \]
for the \textbf{measure of impurity}
\[Q_m(T) = \frac{1}{|R_m|}\sum_{x_i\in R_m} (t_i - \hat{c}_m)^2\]

Choosing the values of $\lambda$? Use cross-validation! Construct $K$ folds, withhold
one fold and on the remaining $K-1$ folds grow the largest tree $T_0$.
Use the cost-complexity pruning to obtain a sequence of trees $(T_\lambda)$, and then
evaluate the means squared error of the withheld fold. Repeat $K$ times and then average
the $K$ estimates. Find the $\hat{\lambda}$ -- the minimum of the \textbf{MSE}.
For the optimal $\lambda$ grow the most complex tree $T_0$ and then do the cost-complexity
pruning to return an optimal $T_{\hat{\lambda}}\subseteq T_0$.

But how to find $T_\lambda$ from $T_0$? How is branch collapsing done?

THe idea is to iteratively collapse just one node, which creates the smallest increase
in the goodness-of-fit score. This is known as the weakest link pruning:
\begin{itemize}
	\item for a given $\lambda$ collapse the node which gives the smallest increase in
	\[\sum_{m=1}^M |R_m| Q_m(T)\]
	\item Stop at root.
\end{itemize}
It can be shown, that the optimal tree $T_\lambda$ is necessary among the sequence of
single-node collapses.

% subsubsection classification_and_regression_trees (end)

\subsection{Classification trees} % (fold)
\label{sub:classification_trees}

The idea is similar, but a better criterion for splitting and fitting is wanted.
Let $\hat{p}_{mk}$ the proportion of point in region $R_m$ that belong to class $k$:
\[\hat{p}_{km} = \frac{1}{|R_m|} \sum_{x_i\in R_m} 1_{\{k\}}(t_i) \]
classify an observation in $R_m$ according to the majority vote:
\[k(m) = \argmax_k \hat{p}_{km}\]

The choice of the impurity measure $Q_m(T)$
\subsubsection{The misclassification error} % (fold)
\label{ssub:the_misclassification_error}

The misclassification error:
\[
Q_m(T)
= \sum_{i \in R_m} 1_{\{\neq k\}}(t_i)
= 1 - \hat{p}_{km}
\]
But this is not the best measure there is.
% subsubsection the_misclassification_error (end)

\subsubsection{Gini's impurity index} % (fold)
\label{ssub:gini_s_impurity_index}

Not to confuse with the inequality measure. In ecological applications, where it
emerged, it measures the biodiversity. Lots of minority (non-dominant) species means
greater diversity$\sim_{s\in\mathcal{S}} s (1-p_s)$. Literally this translates to a measure
of how many other classes are present at a node $m$:
\[Q_m(T) = \sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\]
where $K$ is the number of classes. This measure is employed by CART.

% subsubsection gini_s_impurity_index (end)

\subsubsection{Impurity measure based on entropy} % (fold)
\label{ssub:impurity_measure_based_on_entropy}

Consider the cross-entropy (ID3, C4.5). The idea to weight the least represented
classes not by the residual proportion, but by the logarithm:
\[Q_m(T) = -\sum_{k=1}^K\hat{p}_{mk} \log \hat{p}_{mk}\]
(Maybe this is not cross-, but a usual entropy (?))

% subsubsection impurity_measure_based_on_entropy (end)

\subsection{Binary classification} % (fold)
\label{sub:binary_classification}

Suppose $p$ is the proportion of observations in class $C_1$
\begin{align*}
	\text{ME} &= 1 - \max\{p, 1-p\}\\
	\text{Gini} &= 2 p (1-p)\\
	\text{X-entropy} &= - p\log p - (1-p)\log (1-p)
\end{align*}

% subsection binary_classification (end)

\subsubsection{Interpreation} % (fold)
\label{ssub:interpreation}

Decision and classification trees are interpretable and explicable. However, in
general trees have poor prediction accuracy. There are various method for improving
the accuracy by aggregating many poor classifiers. Simple aggregation is not robust,
and highly variable.

% subsubsection interpreation (end)

% subsection classification_trees (end)

% subsection decision_trees (end)

\subsection{Bagging and boosting} % (fold)
\label{sub:bagging_and_boosting}

\subsubsection{\emph{B}ootstrap \emph{agg}regation} % (fold)
\label{ssub:bootstrap_aggregation}

This is a general procedure for reducing the variance of a statistical classification
method. The idea is to use bootstrap samples toward this reduction. There are two types
of bootstrap: parametric and non-parametric.

Generate $B$ bootstrap samples, and use each sample as input for the statistical
learning regression method $\hat{f}^*_b(x)$. The bagging prediction for a regression
problem is given by
\[\hat{f}_\text{bag}(x) = \frac{1}{B} \sum_b \hat{f}^*_b(x)\]
for the classification problem the bagging classification is just the ``majority'' vote in
favour of a certain class:
\[\hat{f}_\text{bag}(x) = \argmax_k \sum_b 1_{\{=k\}}(\hat{f}^*_b(x))\]
As $B$ grows larger, the sum under the argmax operator is converging to the
theoretical share of the class $k$.

% There are deep theoretical issues with simple majority voting

Assign each data point in the dataset a probability $\hat{q}_i$. In a non-parametric
bootstrap setting $\hat{q}_i = \frac{1}{n}$. Use $\hat{q}$ to simulate a sample
\[\hat{f}^*(x) \to (x^*_i, t^*_i)_{i=1}^n\]
where $(x^*_i, t^*_i)\sim \hat{q}$. The ``true'' bagging estimate is given by the 
\[\ex_{\hat{q}} \bigl(\hat{f}^*(x)\bigr)\]
-- the expected value of the empirical process. Since it is hard to study the
properties of a random estimate, we are going to focus on its limit.

% subsubsection bootstrap_aggregation (end)

% subsection bagging_and_boosting (end)

% section lecture_12 (end)
\end{document}
