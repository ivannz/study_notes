{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, gc\n",
    "os.chdir( '/Users/innazarov/ownCloud/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings ; warnings.simplefilter( \"ignore\" )\n",
    "import numpy as np, pandas as pd, flex as fl, scipy as sp\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_npz = '/Users/innazarov/Github/study_notes/year_14_15/spring_2015/machine_learning/data/mldata/mnist_scikit.npz'\n",
    "assert( os.path.exists( mnist_npz ) )\n",
    "with np.load( mnist_npz, 'r' ) as npz :\n",
    "    mnist_labels, mnist_data = np.asarray( npz[ 'labels' ], np.int ), npz[ 'data' ] / 255.0\n",
    "mnist_classes = np.unique( mnist_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_plot( data, title, **kwargs ) :\n",
    "    axis = plt.figure( figsize = ( 16, 9 ) ).add_subplot( 111 )\n",
    "    axis.set_title( title )\n",
    "    fl.plot( axis, data, shape = ( 28, -1 ), cmap = plt.cm.hot, interpolation = \"nearest\", **kwargs )\n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion( actual, predicted ) :\n",
    "    tbl_ = sp.sparse.coo_matrix( ( np.ones_like( predicted ), ( predicted, actual ) ) )\n",
    "    tbl = pd.DataFrame( tbl_.todense( ), index = mnist_classes, columns = mnist_classes )\n",
    "    tbl.index.name = \"Predicted\"\n",
    "    tbl.columns.name = \"Actual\"\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState( None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_ = cross_validation.train_test_split( mnist_data, mnist_labels, test_size = 0.75,\n",
    "                                            random_state = random_state )\n",
    "X_train, X_test, y_train, y_test = split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw and plot some random subset of the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_perm_ = np.random.permutation( X_train.shape[ 0 ] )\n",
    "X = X_train[ index_perm_[ : 400 ] ]\n",
    "fast_plot( X, u\"200 random digits MNIST train dataset\", n = 200 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple average pattern classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate averaged images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_digits_ = { label_ : X_train[ np.flatnonzero( y_train == label_ ) ]\n",
    "                           for label_ in mnist_classes }\n",
    "\n",
    "avg_patterns_ = { label_ : samples_.mean( axis = 0 )[ np.newaxis ]\n",
    "                  for label_, samples_ in train_digits_.iteritems() }\n",
    "\n",
    "## Construct an aligned pair of arrays\n",
    "average_digits = np.concatenate( [ array_ for array_ in avg_patterns_.itervalues() ], axis = 0 )\n",
    "average_labels = np.array( [ label_ for label_ in avg_patterns_.iterkeys() ], dtype = np.int )\n",
    "del avg_patterns_\n",
    "\n",
    "## Sort them\n",
    "order_ = np.argsort( average_labels )\n",
    "average_labels = average_labels[ order_ ]\n",
    "average_digits = average_digits[ order_ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot the patterns\n",
    "fast_plot( average_digits, u\"Averaged digits on train\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple minimal error calssifier based on mean patterns.\n",
    "$\\newcommand{\\argmin}{\\mathop{\\mathtt{argmin}}} \\newcommand{\\Dcal}{\\mathcal{D}}$\n",
    "Basically this is a simple gaussian model:\n",
    "$$ \\log \\mathcal{L} = - \\frac{|\\Dcal|}{2} \\log 2\\pi - \\frac{|\\Dcal|}{2} \\log \\sigma^2\n",
    "                      - \\frac{1}{2 \\sigma^2} \\sum_{x\\in \\Dcal} ( x - \\mu_{k_x} )^2 \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is done with this rule:\n",
    "\\begin{align*}\n",
    "    \\hat{y}(x)\n",
    "        & = \\argmin_{k=1,\\ldots,K} \\, \\sum_{n=1}^N \\sum_{m=1}^M ( x_{nm} - \\mu_{knm} )^2 \\\\\n",
    "        & = \\argmin_{k=1,\\ldots,K} \\, \\sum_{n=1}^N \\sum_{m=1}^M - 2 x_{nm} \\mu_{knm} + \\mu_{knm}^2 \\\\\n",
    "        & = \\argmin_{k=1,\\ldots,K} \\, \\|\\mu_k\\|^2 - 2 \\langle x, \\mu_k \\rangle \\,,\n",
    "\\end{align*}\n",
    "where\n",
    "$$ \\langle f, g \\rangle  = \\sum_{n=1}^N \\sum_{m=1}^M f_{nm} g_{nm}\\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute the squared error ...\n",
    "yp_ = - 2 * np.tensordot( X_test, average_digits, axes = [ -1, -1 ] ) \\\n",
    "      + average_digits.dot( average_digits.T )[:1]\n",
    "## ... and based on it find the closest label.\n",
    "pred_labels_ = average_labels[ yp_.argmin(axis = 1) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion( y_test, pred_labels_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Achieved accuracy is %.3f%%\" % ( np.mean( y_test == pred_labels_ ) * 100.0, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest FTW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize and fit a random forest to the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_ = ensemble.RandomForestClassifier( n_estimators = 50, n_jobs = -1,\n",
    "                                        random_state = random_state ).fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_labels_ = rfc_.classes_[ rfc_.predict_proba( X_test ).argmax( axis = 1 ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the accuracy ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Achieved accuracy is %.3f%%\" % ( np.mean( y_test == pred_labels_ ) * 100.0, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion( y_test, pred_labels_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the pixel-feature importances of the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_plot( rfc_.feature_importances_[ np.newaxis ],\n",
    "           u\"Random Forest feature importances\",\n",
    "           n_row = 1, n_col = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use PCA to learn a linear manifold from the data. Scipy's SVD returns $U$, $\\Sigma$ and $V'$ (not $V$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scl_ = preprocessing.StandardScaler( ).fit( X_train )\n",
    "U, S, V = sp.linalg.svd( scl_.transform( X_train ), full_matrices = False )\n",
    "order_ = S.argsort( )[::-1]\n",
    "U, S, V = U[:,order_], S[ order_ ], V[order_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute variance within each principal direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var_ = S**2\n",
    "leverage_ = np.cumsum( var_ ) / np.sum( var_ )\n",
    "n_components = np.min( np.flatnonzero( leverage_ > 0.95 ) )\n",
    "\n",
    "print \"\"\"The least number of principal components required to\"\"\" \\\n",
    "    + \"\"\"guarantee at least 95%% recostruction is %d.\"\"\" % ( n_components, ) \n",
    "\n",
    "plt.plot( leverage_ )\n",
    "plt.axhline( y = 0.95, color = 'k', lw = 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chose the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_ = U[:,:n_components] * S[ :n_components ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed the compnents in the original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ = scl_.inverse_transform( np.dot( pc_, V[:n_components] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_plot( X_, u\"Principal components of train dataset\", n = 512 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how well it fares on the test dataset: reconstruct ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_ = np.dot( scl_.transform( X_test ), V[:n_components].T )\n",
    "X_ = scl_.inverse_transform( np.dot( pc_, V[:n_components] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...  and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_plot( np.abs(X_-X_test), u\"Low rank reconstruction error of the test dataset\", n = 32 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a Gaussian naive bayes classifier: $\\newcommand\\given{\\:\\vert\\:}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes assumes that the features are conditionally independent, provided the class label $y$ is known.\n",
    "In particular for any $i\\in \\{1,\\ldots,d\\}$ one has:\n",
    "$$ p\\bigl( x_1,\\ldots,x_p \\:\\big\\vert\\: y, \\Theta\\bigr) = \\prod_{i=1}^d p\\bigl( x_i \\:\\big\\vert\\: y, \\Theta\\bigr) \\,, $$\n",
    "whence using the Bayes' rule :\n",
    "\\begin{align*}\n",
    "    p\\bigl( y \\:\\big\\vert\\: x, \\Theta\\bigr)\n",
    "            &= p\\bigl( y \\:\\big\\vert\\: x, \\Theta\\bigr) \\frac{p(y\\:\\big\\vert\\: \\Theta)}{p(x\\:\\big\\vert\\: \\Theta)} \\\\\n",
    "            &\\propto p\\bigl(y\\:\\big\\vert\\: \\Theta\\bigr)\n",
    "                    \\prod_{i=1}^d p\\bigl( x_i \\:\\big\\vert\\: y, \\Theta\\bigr) \\,,\n",
    "\\end{align*}\n",
    "where $\\Theta$ is the parameter vector of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is done using the maximum posterior porbability principle: $\\newcommand{\\argmax}{\\mathop{\\mathtt{argmax}}}$\n",
    "$$ \\hat{y}(x) = \\argmax_{y} \\log p\\bigl(y\\:\\big\\vert\\: \\Theta\\bigr)\n",
    "                            + \\sum_{i=1}^d \\log p\\bigl( x_i \\:\\big\\vert\\: y, \\Theta\\bigr) \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the Naive Bayes classifier is done unsing the ML approach:\n",
    "$$  \\hat{\\Theta} = \\argmax_\\Theta \\sum_{j=1}^J \\log p\\bigl(y_j\\:\\big\\vert\\: \\Theta\\bigr)\n",
    "                            + \\sum_{j=1}^J \\sum_{i=1}^d \\log p\\bigl( x_{ji} \\:\\big\\vert\\: y_j, \\Theta\\bigr) \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian NB classifier assumes that \n",
    "$$ p\\bigl( x_i \\:\\big\\vert\\: y, \\Theta\\bigr) = \\bigl(2\\pi \\sigma^2_{yi}\\bigr)^{-\\frac{1}{2}} \\mathtt{exp}\\biggl( -\\frac{1}{2\\sigma^2_{yi}} (x_i - \\mu_{yi} ) \\biggr) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gnb_ = naive_bayes.GaussianNB( ).fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the labels ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_labels_ = gnb_.classes_[ gnb_.predict_proba( X_test ).argmax( axis = 1 ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean( pred_labels_ == y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion( y_test, pred_labels_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated mean of each pixel form the following patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_plot( gnb_.theta_, u\"Estimated mean values of each pixel feature given a class\", n = 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem identical to the average patterns and indeed they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.allclose( gnb_.theta_, average_digits )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the estimated variablility of each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_plot( gnb_.sigma_, u\"GNB Estimated standard deviation of individual pixels\", n = 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression : $l_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's employ logisitc regression to classify the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_ = scl_.transform( X_train )\n",
    "lorl2_ = linear_model.LogisticRegression( C = 1.0, penalty = \"l2\" ).fit( X_train_, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_labels_ = lorl2_.predict( scl_.transform( X_test ) )\n",
    "print \"Accuracy score is %.3f%%\" % ( 100*np.mean( pred_labels_ == y_test ), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion( y_test, pred_labels_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the weights of the estimated logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_plot( lorl2_.coef_, u\"Weights of the $l_2$ logistic regression\", n = 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test an $l_1$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_ = scl_.transform( X_train )\n",
    "lorl1_ = linear_model.LogisticRegression( C = 1.0, penalty = \"l1\" ).fit( X_train_, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_labels_ = lorl1_.predict( scl_.transform( X_test ) )\n",
    "print \"Accuracy score is %.3f%%\" % ( 100*np.mean( pred_labels_ == y_test ), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion( y_test, pred_labels_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and display the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_plot( lorl1_.coef_, u\"Weights of the $l_1$ logistic regression\", n = 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2d_ = np.reshape( X_train_, ( -1, 28, 28 ) )\n",
    "fft2_2d_ = np.fft.fft2( X_train_2d_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fft2_2d_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_plot( fft2_2d_.real, u\"2D FFT of some MNIST digits\", n = 400 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast_plot( fft2_2d_.imag, u\"2D FFT of some MNIST digits\", n = 400 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a higher coefficient for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_ = scl_.transform( X_train )\n",
    "lorl1_3_ = linear_model.LogisticRegression( C = 2.0, penalty = \"l1\" ).fit( X_train_, y_train )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_labels_ = lorl1_3_.predict( scl_.transform( X_test ) )\n",
    "print \"Accuracy score is %.3f%%\" % ( 100*np.mean( pred_labels_ == y_test ), )\n",
    "print confusion( y_test, pred_labels_ )\n",
    "fast_plot( lorl1_3_.coef_, u\"Weights of the $l_1$ logistic regression\", n = 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a Grid seach over the penalty and the degree of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_lor_ = grid_search.GridSearchCV( linear_model.LogisticRegression( ),\n",
    "                                   param_grid = {\n",
    "                                        \"C\" : np.logspace( -2, 2, num = 5 ),\n",
    "                                        \"penalty\" : [ \"l2\", \"l1\" ],\n",
    "                                  }, cv = 7, n_jobs = -1, verbose = 50 ).fit( X_train_, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the cross-validation consider the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_lor_.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA + $k$-nearest neighbour classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensionality using PCA to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scl_ = preprocessing.StandardScaler( ).fit( X_train )\n",
    "X_train_ = scl_.transform( X_train )\n",
    "pca_ = decomposition.PCA( n_components = 15 ).fit( X_train_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $k$-nn implementation in Scikit-Learn, appears to prohibit chaning the number of neighbors on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Don't set the number of neighbours just yet.\n",
    "PC_train = pca_.transform( X_train_ )\n",
    "knn_ = neighbors.KNeighborsClassifier( n_neighbors = None, n_jobs = -1 ).fit( PC_train, y_train )\n",
    "\n",
    "def knn_call( knn, method, k, *args, **kwargs ) :\n",
    "    knn.n_neighbors = k\n",
    "    return getattr( knn, method )( *args, **kwargs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the labels using the majority voting procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_neighbors = np.asarray( np.linspace( 1, 13, num = 5 ), np.int )\n",
    "\n",
    "PC_test = pca_.transform( scl_.transform( X_test ) )\n",
    "pred_labels_ = { k : knn_call( knn_, \"predict\", k = k, X = PC_test )\n",
    "                   for k in n_neighbors }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display accuracy of the $k$-nn classifiers and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "for k in n_neighbors :\n",
    "    tbl = confusion( y_test, pred_labels_[ k ] )\n",
    "    print \"%d-nn : accuracy %0.3f%%\\n\" % ( k, 100*np.mean( pred_labels_[ k ] == y_test ), ), tbl, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe_ = pipeline.Pipeline( [\n",
    "            ( \"scaler1\", preprocessing.StandardScaler( ) ),\n",
    "            ( \"pca\", decomposition.PCA( n_components = 100 ) ),\n",
    "            ( \"scaler2\", preprocessing.StandardScaler( ) ),\n",
    "            ( \"lore\", linear_model.LogisticRegression( C = 1.0, penalty = \"l2\" ) ),\n",
    "        ] ).fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_labels_ = pipe_.predict( X_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display the summary scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Accuracy score is %.3f%%\" % ( 100*np.mean( pred_labels_ == y_test ), )\n",
    "confusion( y_test, pred_labels_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne, theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement as simple neural network with lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4( 'inputs' )\n",
    "target_var = T.ivector( 'targets' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The input layer\n",
    "network = lasagne.layers.InputLayer( shape = ( None, 1, 28, 28 ),\n",
    "                                     input_var = input_var )\n",
    "\n",
    "## The 2D 5x5 conv layer with ReLU and 2x2 max-pooling\n",
    "## (28-7+1) // 1\n",
    "network = lasagne.layers.Conv2DLayer( network, num_filters = 32, filter_size = ( 5, 5 ),\n",
    "                                      nonlinearity = lasagne.nonlinearities.rectify,\n",
    "                                      border_mode = \"valid\", W = lasagne.init.GlorotUniform( ) )\n",
    "network = lasagne.layers.MaxPool2DLayer( network, pool_size = ( 2, 2 ) )\n",
    "\n",
    "## The 2D 3x3 conv layer with ReLU and 2x2 max-pooling\n",
    "network = lasagne.layers.Conv2DLayer( lasagne.layers.dropout( network, p = .2 ),\n",
    "                                      num_filters = 64, filter_size = ( 3, 3 ),\n",
    "                                      nonlinearity = lasagne.nonlinearities.rectify,\n",
    "                                      border_mode = \"valid\", W = lasagne.init.GlorotUniform( ) )\n",
    "network = lasagne.layers.MaxPool2DLayer( network, pool_size = ( 2, 2 ) )\n",
    "\n",
    "## FC layer with dropout\n",
    "network = lasagne.layers.DenseLayer( lasagne.layers.dropout( network, p = .5 ),\n",
    "                                     num_units = 256, nonlinearity = lasagne.nonlinearities.rectify )\n",
    "\n",
    "network = lasagne.layers.DenseLayer( lasagne.layers.dropout( network, p = .5 ),\n",
    "                                     num_units = 10, nonlinearity = lasagne.nonlinearities.softmax )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output( network )\n",
    "loss = lasagne.objectives.categorical_crossentropy( prediction, target_var ).mean( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params( network, trainable = True )\n",
    "updates = lasagne.updates.nesterov_momentum( loss, params, learning_rate = 0.01, momentum = 0.9 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prediction = lasagne.layers.get_output( network, deterministic = True )\n",
    "test_loss = lasagne.objectives.categorical_crossentropy( test_prediction, target_var ).mean( )\n",
    "\n",
    "test_acc = T.mean( T.eq( T.argmax( test_prediction, axis = 1 ), target_var ), dtype = theano.config.floatX )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
