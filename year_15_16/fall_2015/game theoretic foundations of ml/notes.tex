\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\err}{\text{err}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\pr}{\mathbb{P}}

% \usepackage[english, russian]{babel}
% \newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
% \newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Game theoretic foundations of machine learning}
\author{Nazarov Ivan, \rus{201мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section{Lecture \# 1} % (fold)
\label{sec:lecture_1}

Consider a source set of objects $X = \{x\}$ with $Y=\{-1,+1\}$ and a classification function $h:X\mapsto Y$.

A training sample is $\amthcal{S}=((x_i, y_i))_{i=1}^l$. And we want to estimate a classifier $\hat{h}$ on $S$:
$S\leadsto h = h_S$.

The sample is generated by $P$, $(x,y)\sim P$ -- independent and identically distributed.

Since we have a probability, let's consider the chance that the trained classifier makes a mistake
\[ \err_P(h) = P( \{(x,y) : h(x) \neq y\} )\,. \]

On the sample $S$ the measure is empitical:
\[ \err_S(h) = \frac{1}{n}\sum_{j=1}^l 1_{h(x_j)\neq y_j} \,.\]

We want to estimate $h_S$ so that (?)
\[\err_P(h) < \err_S(h) + \epsilon \,.\]


The classifier $h$ is constructed on a sample $S$, and we need a guarantee for all $h\in H$. SO
we seek to estimate
\[
P( S : \exists h\in H\text{ s.t. } \err_S(h) = 0\text{ and } \err_P(h) > \epsilon )
\leq |H| e^{-l\epsilon}
\,. \]
If $h\in H$ is given, then
\[
P( \err_S(h) = 0\text{ and } \err_P(h) > \epsilon )
	= (1-p)^l \leq (1-\epsilon)^l \leq e^{-l\epsilon}
\,,\]
whence the inequality above follows.

% section lecture_1 (end)

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}
The growth function of $VC$-dimensionality.

Consider 
\[ B_H(l) = \max_{(x_j)_{j=1}^l \in X}} \bigl| \bigl\{ ( h(x_j))_{j=1}^l\,: h\in H \bigr\} \bigl| \,.\]
or in another notation
\[ B_H(l) = \max_{X^l} |H\rvert_{X^l}| \leq 2^l \,, \]
-- the projection of $H$ onto $l$-size samples.

Consider an example. Suppose $X\subseteq \Real^2$, and let's consider $H$ to be
the set of linear discriminant calssification rules:
\[ h(x) = \begin{cases}
		+1, &\text{ if } wx+b \geq 0\,;\\
		-1, &\text{ otherwise }
	\end{cases} \,. \]
The more points the less likely it is to be able to get close to the upper bound of $2^l$.

Sauer-Scheloch, Vapnik-Chervonenkis theorem\hifil \\
If $H$ any class of binary classification functions $h:X\mapsto\{-1,+1\}$,
then either \begin{itemize}
	\item $B_H(l) = 2^l$;
	\item or $\exists d$ such that $B_H(l) = 2^l$ for all $l\leq d$ and 
		for any $l>d$
		\[ B_H(l) \leq \sum_{i=0}^d C_l^i \leq e^d \biggl(\frac{l}{d}\biggr)^d \,. \]
\end{itemize}
This $d$ is known as VC-dimensionality of class $H$. From $d$ on $\log B_H(l) \approx O(\log l)$.
The misclassification error is bounded above by $B_H(l) e^{-l} \approx e^{-\epsilon l + c\log l} $.

Another exmaple: if $\mathcal{H} = \{\text{sign}\bigr(\sin(tx)\bigl)\,:t\in \Real\}$,
then there exists a sample, such that there would exist $t\in \Real$ which separates two classes.
For $x_i = 2\pi 10^{-i}$, and for all $l$ and any $(\delta_i)_{i\geq 0}\in \{0,1\}$, there exists
$h\in \mathcal{H}$ such that $\forall i\geq0\, h(x_i)=\delta_i$. Show that such $t$ is given by 
\[ t = \frac{1}{2}\sum_{i\geq 0} (1-\delta_i)^10^{-i} + 1\,.\]

Proof:\hfill\\
Let $h(l,d) = \sum_{i=0}^d C_l^i$. Now $C_l^i = C_{l-1}^i + C_{l-1}^{i-1}$, whence
we hypthesize
\[ h(l,d) = h(l-1,d) + h(l-1,d-1) \,. \]
Let's use induction on $(l,d)$ by supposing that for all ``lower'' pairs $(l,d)$ the decomposition
is true. Now assume the upper bound on $B_H(l)$ is true.
Then for $(l-1,d)$ and $(l-1,d-1)$ it is true. Now, let $X_1=(x_j)_{j=1}^l$
be such a sample, that $B_H(l) = |H\rvert_{X_1}|$. Remove the first element of
$X_1$, and let $H_2 = H\rvert_{X_2}$. Thus
\[ |H_2| \leq B_{H_2}(l-1) \leq h(l-1, d) \,. \]
Let $H_3$ be the set of all classifiers from $h\in H_2$, such that there exists 
$g\in H$ with $h(x_1) \ne g(x_1)$. Then $|H_2|+|H_3| = |H_1|$.

Now 
\[ |H_3| \leq B_{H_3}(l-1) \leq h(l-1, d)\,.\]

Suppoe $\text{VC}_{H_3} = d$, then there is $X^d\in X$ which is ``completely'' separable by 
classifiers from $H_3$. Adding $x_1$ to this $X^d$, then it will also be spearable by
functions form the class $H$. Thus $\text{VC}_{H_1} = d + 1$. (?)

Next,
\[ B_H(l) = |H_1| = |H_2|+|H_3| \leq h(l-1,d)+h(l-1,d-1) = h(l,d) \,.\]
Work this at home!
%%%%%%%

An upper bound on the sum of binomal coefficients :
\begin{align*}
	\sum_{i=0}^d C_l^i &\leq \bigl(\frac{l}{d}\bigr)^d \sum_{i=0}^d C_l^i \bigl(\frac{d}{l}\bigr)^i \\
					   &\leq \bigl(\frac{l}{d}\bigr)^d \sum_{i=0}^l C_l^i \bigl(\frac{d}{l}\bigr)^i \\
					   &= \bigl(\frac{l}{d}\bigr)^d \bigl(1+\frac{d}{l}\bigr)^l \\
					   &\leq \bigl(\frac{l}{d}\bigr)^d e^{d} \,.
\end{align*}
%%%%%%%%%%%%%%%
The main theorem\hfill\\
The upper bound on the probability of misclassification is 
\[ P\bigl( S\,: \exists h\in H\, \err_S(h) = 0\, \wedge \err_P(h) > \epsilon\, \bigr)
	\leq 2B_H(2l) e^{-\frac{l\epsilon}{4}}\,, \]
for $l>\frac{2}{\epsilon}$. The upper bound depends on neither $h$ nor $P$, but
on $H$ the class itself. This is analogous to the law of large numbers.

Symmetrization lemma\hfill\\
\begin{align*}
	P\bigl( S\,: \exists h\in H\, \err_S(h) = 0\, \wedge \err_P(h) > \epsilon\, \bigr)
		&\leq 2 P\bigl( S\|S'\,: \exists h\in H\, \err_S(h) = 0\, \wedge \err_{S'}(h) > \frac{\epsilon}{2}\, \bigr) \,.
\end{align*}
Proof:\hfill\\
For any sample from the left hand side $S$, denote by $h_S\in H$ the appropriate classifier
with
\[ \err_S(h_S) = 0 \text{ and } \err_P( h_S ) > \epsilon \,. \]
Then it is always ( for any $f\in H$) true that 
\[\one\bigl( \err_S(f) = 0 \wedge \err_P(f) > \epsilon \bigr)
	\times \one\bigl( \err_S(f) = 0 \wedge \err_P(f) - \err_{S'}(f) \leq \frac{\epsilon}{2} \bigr)
	\leq \one\bigl( \err_S(f) = 0 \wedge \err_{S'}(f) > \epsilon \bigr)
\,.\]
Thus for a fixed $S$
\[ \one\bigl( \err_S(h_S) = 0 \wedge \err_P(h_S) > \epsilon \bigr)
   \ex_{S'} \one\bigl( \err_S(h_S) = 0 \wedge \err_P(h_S) - \err_{S'}(h_S) \leq \frac{\epsilon}{2} \bigr)
   \leq P\bigl( \err_S(h_S) = 0 \wedge \err_{S'}(h_S) > \epsilon \bigr) \,. \]
Next, for $p=\err_P(h_S)$
\[ P\bigl( \err_S(h_S) = 0 \wedge  \err_{S'}(h_S) \geq p - \frac{\epsilon}{2} \bigr) 
	= \sum_{j\in\{k\,:\frac{k}{l}\geq p - \frac{\epsilon}{2}\}} C_l^k p^k(1-p)^{l-k} \,. \]

Indeed, for $l>\frac{2}{\epsilon}$, one has $p-\frac{\epsilon}{2} < p- \frac{1}{l}$, whence 
\[ \ldots \leq \sum_{j\in\{k\,:k\geq pl - 1\}} C_l^k p^k(1-p)^{l-k} \,,\]
which follows from the fact that $[lp]$ is the median of the binomial distribution.

Thus
\[ \frac{1}{2} \one\bigl( \err_S(h_S) = 0 \wedge \err_P(h_S) > \epsilon \bigr)
   \leq P\bigl( \err_S(h_S) = 0 \wedge \err_{S'}(h_S) > \epsilon \bigr) \,, \]
and for $l > \frac{2}{\epsilon}$
\[ \one\bigl( \err_S(h_S) = 0 \wedge \err_P(h_S) > \epsilon \bigr)
   \leq 2 P\bigl( \err_S(h_S) = 0 \wedge \err_{S'}(h_S) > \epsilon \bigr) \,. \]
Taking expectation over $S$ yields:
\[ P\bigl( S\,: \exists h\in H\, \err_S(h) = 0\, \wedge \err_P(h) > \epsilon\, \bigr)
   \leq 2 P\bigl( S\|S'\,: \exists h\in H\, \err_S(h) = 0\, \wedge \err_{S'}(h) > \frac{\epsilon}{2}\, \bigr) \,. \]

The second lemma, which gives an estimate for the right hand side.
\[ 
P\bigl( S\|S'\,: \exists h\in H\, \err_S(h) = 0\, \wedge \err_{S'}(h) > \frac{\epsilon}{2}\, \bigr)
	\leq 2B_H(l) e^{-\frac{\epsilon l}{2}}\,. \]

Fix the composition $Y$ of a sample of size $2l$ and some classifier $h$, which makes $m$
misclassifications on any sample ``equivalent'' to $Y$.
The probability of a classifier making no mistakes in the the first-half sample, and 
making $m$ errors in the second is given by:
\[ \frac{ C_l^m \cdot 1 }{C_{2l}^m}
		= \frac{ l! m! (2l-m)!}{ (2l)! m! (l-m)! }
		= \prod_{j=l+1}^{2l} \frac{j-m}{j}
		\leq (1-\frac{m}{2l})^l \leq e^{-\frac{l \epsilon}{2}} \,. \]

Thus for $m\geq \epsilon l$
\[ P\bigl( S\|S'\,: \exists h\in H\, \err_S(h) = 0\,
					\wedge \err_{S'}(h) > \frac{\epsilon}{2}\, \rvert SS'\sim Y\bigr)
		\leq |H\rvert_Y| e^{-\frac{l \epsilon}{2}} \,, \]
but $|H\rvert_Y| \leq B_H(2l)\leq 2B_H(l)$.

%% Rademacher complexity is more modern.

% section lecture_2 (end)

\section{Lecture \# 3} % (fold)
\label{sec:lecture_#_3}

Previously on the Game Theory

Consider a sample $S = (x_i, y_i)_{i=1}^l\in \mathcal{X}\times\{-1,1\}$ -- iid.
A classifier $h\in H$ is a map $h:X\to\{-1,1\}$.
The theorectial misclassification rate is 
\[\err_\pr(h) = \pr\{ h(x) \neq y \} \,,\]
and the empirical is
\[\err_{\hat{P}}(h) = \frac{1}{l} \sum_{i=1}^l \one\{ h(x_i) \neq y_i \} \,.\]


\notation Theorem:\hfill\\
The probability of overfitting
\[ \pr\{ S : \exists h\in H\, \err_S(h) = 0\,\wedge\, \err_\pr(h) > \epsilon \}\,,\]
is bounden from above by 
\[  2B_H(2l) e^{-\frac{\epsilon l}{4}} \,, \]
where the cpmplexity of class $H$ 
\[B_H(l) = \max_{(x_i)_{i=1}^l} \bigl|\{ (h(x_i))_{i=1}^l\,:\,h\in H \}\bigr| \,.\]

Put
\[ \delta = 2B_H(2l) e^{-\frac{\epsilon l}{4}} \,\]
whence
\[ \epsilon = \frac{4}{l} \log \frac{2B_H(2l)}{\delta} \,\]

\noindent Corollary\hfill\\
The complement of the event in the probability above:
\[ \pr\{ S : \forall h\in H\, \err_S(h) = 0\,\implies\, \err_\pr(h) \geq \epsilon \} \geq 1-\delta \,. \]
In other terms: for all $\delta>0$ with probability $\geq 1-\delta$
for all $h\in H$ if $\err_S(h) = 0$, then
\[\err_\pr(h)\leq \frac{4}{l}\log \frac{2B_H(2l)}{\delta}\,.\]
If $d = \text{VC}_H$, then in this inequality on can use 
\[\err_\pr(h) \leq \frac{4}{l} \bigl( d\log \frac{e \cdot l}{d} + \log\frac{1}{\delta} \bigr)\,.\]

\noindent Another theorem\hfill\\
\[ \pr\{ S : \exists h\in H\,\text{s.t.} \err_\pr(h) \leq  \err_S(h) + \epsilon \}
	\geq 4 B_H(2l) 2^{-\frac{\epsilon^2 l}{2}} \,, \]
for $l\geq \frac{2}{\epsilon^2}$.

\subsection{VC dimensionality of linear classifiers} % (fold)
\label{sub:vc_dimensionality_of_linear_classifiers}

Consider the vectors of features $x\in \Real^n$ and $y\in \{-1,+1\}$.
Then for any $\w\in \Real^n$ the equation $w'x + b = 0$ defines a hyperplane
in $\Real^n$. A linear classifier is the folloeing function:
\[ h(x) = \begin{cases}
	+1, &\text{ if } w'x + b \geq 0\\
	-1, &\text{ o/w }
\end{cases}\,, \]

Theorem
The VC dimensionality of the class of all linear classifiers is equal
to $n+1$, whereas the dimensionality of homogeneous linear classifiers
is $n$.

Proof:
Let $I = (e_i)_{i=1}^n$ -- be the unit orthogonal vectors in $\Real^n$. Then
for any $E\subseteq I$ on has $E = \{ e_{i_1}, e_{i_k}\}$ and $I\setminus E$
enumerated by $\{e_{i_{k+1}}, e_{i_n}\}$. Then, use weights $w$ given by
\[ w_j = \begin{cases}
	1, &\text{ if } j \leq k\\
	-1, &\text{ o/w }
\end{cases}\,.\]

Suppose there exist $n+1$ vectors $(u_i)_{j=1}^{n+1}\in \Real^n$ which
are completely separated into $2^{n+1}$ subclasses.

Then the matrix $A_{ij}$ with $i=1, \ldots, n+1$ and $j=1,\ldots,2^{n+1}$
such that 
\[ A_{ij} = u_i' a_j\,, \]
all columns have all possible combinations of $\{-1,+1\}^{n+1}$ signs.

Since the dot product in continuous it is always possible to make
the elements nonzero (just nudge them a bit).

Now, any $n+1$ vectors in an $n$-dimensional linear space are linear ly dependent.
Thus there exist non trivial coefficients $(\lambda_j)_{j=1}^{n+1}$, such that
\[ \sum_{j=1}^{n+1} \lambda_j u_j = \mathbf{0} \,. \]
Let's use $j=1,\ldots,2^{n+1}$ such that the sign of $A_{ij}$ coincides
with $\lambda_i$. Then $\lambda_i a_j'u_i \geq 0$ for all $i=1,\ldots,n+1$ and there
is one $k$ such that $\lambda_i a_j'u_i > 0$, which leads to a contradiction. 


% subsection vc_dimensionality_of_linear_classifiers (end)

% section lecture_#_3 (end)

\section{Lecture \# 4} % (fold)
\label{sec:lecture_#_4}

Recall that in SVM the dataset partcipates only thorugh the inner porduct.
INdeed in the dual formulation
\[ \sum_{i=1}^n \lambda_i -
   \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j y_i y_j \langle x_i, x_j \rangle
   \to \max_{\lambda_i\in [0,C] } \,, \]
to maximum with respect to $\lambda$. This makes it possible to substitute
$\Phi(x_i)$ instead on $x_i$ for each $i$, such that 
\[ \langle \Phi(x_i), \Phi(x_j) \rangle = K(x_i, x_j) \,.\]

the decision function is given by
\[ f(x) = \sum_{i=1}^n \lambda_i K(x, x_i) \,. \]

\subsection{Kernels} % (fold)
\label{sub:kernels}

Polynomial kernel of degree $p$ is given by $K(x,y)$ for $x,y\in \Real^d$ as
\[ K(x,y) = \bigl( 1 + \sum_{k=1}^d x_i y_i \bigr)^p\,. \]

Consider the map $\Phi:X\to Y$ and $K(x,y) = \langle\Phi(x),\Phi(y)\rangle$.
Then obviously $K$ is symmetric. And for all $n\geq 1$, $(\alpha_i)_{i=1}^n\in \Real$
and $(x_i)_{i=1}^n\in X$
\begin{align*}
	\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j K(x_i,x_j)
		&= \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \langle \Phi(x_i),\Phi(x_j)\rangle \\
		&= \langle \sum_{i=1}^n \alpha_i \Phi(x_i), \sum_{j=1}^n \alpha_j \Phi(x_j)\rangle \\
		&= \Bigl\| \sum_{i=1}^n \alpha_i \Phi(x_i) \bigr\| \geq 0
\end{align*}

A positive definite kernel is a map $K:X\times X \to \Real^+$ with 
\[ \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j K(x_i,x_j) \geq 0 \,, \]
for all $n\geq 1$, $(\alpha_i)_{i=1}^n\in \Real$ and $(x_i)_{i=1}^n\in X$
and such that $k$ is symmteric.

\textbf{RKHS} -- a Hilbert space generated by a reproducing kernel.
A Hilbert space is an inner product space over $\Cplx$ which is complete with
respect to the induced norm. We are interested in a functionas Hilbert space:
$ H\subseteq \Real^X $. Predictors need to be non-linear, that is why functional
spaces are desirable.

Let $\Hcal$ be a functional Hilbert space. The evalutaion functional is defined
by $\epsilon_x(f) = f(x)$ for some $x\in X$. It is obviously linear in $f$ due
to $\Real$ being a field. Then $\Hcal$ is an RKHS if the evalutaion functional
$\epsilon_x$ is bounded (continuous wrt the natural norm) for any $x\in X$.
Bounded: for any $x\in X$ there exists $M\geq 0$ such that for all $f\in \Hcal$
\[ | \epsilon_x(f)| \leq M \|f\| \,. \]
Then according to Fisher-Rietz theorem for any $x\in X$ there exists a unique
element $h_x\in \Hcal$ such that for all $f\in \Hcal$
\[ \epsilon_x(f) = \langle f, K_x\rangle \,. \]
Since $K_x$ is unique, then it is a map $K:x\to \Hcal$ and thus a bivariate map
$K:X\times X\to\Real$. Thus $K(x,y) = \langle K_y, K_x \rangle$, whence
$K_x(y) = K_y(x)$.

\noindent An example\hfill \\
Take a linear homogeneous functional. Consider $\phi:X\to \Real^n$, $X=\Real^n$
and $\Hcal$ is given by
\[ \{ f\,:\, f(x) = w'\phi(x)\, w\in \Real^n \} \,.\]
The inner product is defined as
\[  \langle f, g\rangle = w_f'w_g \,, \]
in $\Real^n$. Let's show that $\epsilon_x$ is continuous in this space. Indeed,
Let $x\in X$, then
\[ |\epsilon_x(f)| = f(x) = |w'\phi(x)| \leq \|w\| \|\phi(x)\| \,. \]
since $\|w\| = \|f\|$ by definition. Here $K_x$ is given by 
\[ K_x(y) = \phi(x)' \phi(y) \,. \]
Indeed, $\langle f, K_x \rangle = w'\phi(x)$.

In RKHS convergence in norm implies pointwise convergence, since the evaluation
functional is continuous.

\noindent Another exmaple\hfill\\
A Sobolev space $H^1([0,1])$ consists of absolutely continuous functions $f:[0,1]\to \Real$
with finite norm given by
\[ \| f \| = \sqrt{ \int_0^1 |f(x)|^2 dx + \int_0^1 |f'(x)|^2 dx } \,.\]
As is usual in machine learning, the predictors are required to be well-behaved,
meaning that they do not oscillate too much.

\textbf{Question}: is $L^2$ an RKHS (check the continuity of the evaluation functional)?

% subsection kernels (end)

\subsection{The canonical RKHS} % (fold)
\label{sub:the_canonical_rkhs}

Consider a symmteric and positive definete function $K:X\timex X\to \Real$.
The construction goes as follows. Let $\Phi:X\to \Real^X$ be defined as 
\[ \phi(x) = K(x, \cdot) \,. \]

Let $\Hcal_0$ be given by
\[ \Hcal_0 = \{ \sum_{i=1}^n \alpha_i K(x_i,\cdot) \,:\,
				n\geq 1,\,(\alpha_i)_{i=1}^n \in \Real,\, (x_i)_{i=1}^n \in x\} \,, \]
with the scalar product given by
\[ \langle f, g\rangle = \sum_{i=1}^n \sum_{j=1}^m \alpha_i \beta_j K(x_i, y_j) \,. \]
This does not depened on the representation of function, since $K(x_i, y_j) = K(y_j, x_i)$.

Consider a Cauchy sequence in $\Hcal_0$. Then for any $\epsilon>0$ there exists
$N\geq1$ such that for all $n,m \geq N$ one has $\|f_n-f_m\| < \epsilon$. Since
the reproducibility property holds
\[ |f_n(x) - f_m(x)| = \langle f_n-f_m, K_x\rangle \leq \|f_n-f_m\| \sqrt{ K(x,x)} \,, \]
which implies that $f_n(x)$ is Cauchy in $\Real$ ans thus there exists $f(x)\in \Real$
such that $f_n(x)\overset{\Real}{\to}f(x)$.

Complete $\Hcal_0$ with the limiting function of every Cauchy sequence to $\Hcal$.
Then the linearity properties hold for the limits. The inner product in $\Hcal$
is defined as
\[ \langle f, g \rangle = \lim_{n\to \infty} \langle f_n,g_n\rangle_0 \,. \]
This is a simple and canonical construction of an RKHS.

% subsection the_canonical_rkhs (end)

\noindent Lab problem\hfill \\
Take a UCI dataset. And fit an SVM with cross-validation.

\section{Rademacher complexity} % (fold)
\label{sec:rademacher_complexity}

Consider a probability space $(\Omega, \Fcal, P)$ and a class $\Hcal$ of measurable
functions on it. Let $Z^l = (z_i)_{i=1}^l\in \Omega$ be the sample. Let
$\sigma=(\sigma_i)_{i=1}^l\sim\text{Ber}(\frac{1}{2})$ iid, which are known as
Rademacher random variables. Then \begin{itemize}
	\item the Rademacher sample mean of the class $\Hcal$ is
	\[ \hat{R}_l(\Hcal) = \ex_\sigma \sup_{h\in \Hcal} \frac{1}{l} \sum_{i=1}^l \sigma_i h(z_i) \,,\]
	for some fixed sample $Z^l$;
	\item the Rademacher mean of the class $\Hcal$ is 
	\[ R_l(\Hcal) = \ex_{(z_i)_{i=1}^l\sim P}\hat{R}_l(\Hcal) \,; \]
\end{itemize}

The empirical mean
\[ \hat{\ex}_{Z^l} f = \frac{1}{l} \sum_{i=1}^n f(z_i) \,, \]
and the population mean is
\[ \ex_P f = \int f dP \,. \]
We are intereset in a uniform estimate of the convergence rate, although the
probability theory offers us pointwise (in $\Hcal$) convergence guarantees.

\noindent Theorem \hfill\\
\[ \ex_{Z^l\sim P} \sup_{h\in \Hcal} \bigl| \ex_P(h) - \hat{\ex}_{Z^l}(h) \bigr| \leq 2 R_l(f) \,.\]

\noindent Corollary \hfill\\
If $\Hcal$ is a class of functions $X\to[0,1]$ then for all $\delta>0$ then with
probability not less than $1-\delta$ for all $h\in \Hcal$
\[  \ex_P(f) \leq \hat{\ex}_{z^l} h + 2 R_l(f) + \sqrt{ \frac{1}{2}\log \frac{1}{\delta}} \,. \]

% section rademacher_complexity (end)

% section lecture_#_4 (end)

\end{document}