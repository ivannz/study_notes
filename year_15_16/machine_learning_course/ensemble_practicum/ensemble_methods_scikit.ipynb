{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A half-baked tutorial on ensemble methods\n",
    "#### <center>by Ivan Nazarov<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial covers both introductiory level theory underpinning\n",
    "each ensemble method, as well as the tools available in Scikit-Learn\n",
    "and XGBoost. We also cover the topic of Stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials\n",
    "\n",
    "* **T. Hastie, R. Tibshirani, and J. Friedman.** `The Elements of Statistical Learning: Data  Mining, Inference, and  Prediction.` Springer Series in Statistics. Springer New York, 2013.\n",
    "    1. Bagging: ch. 8.7;\n",
    "    2. Random Forests: ch. 15;\n",
    "    3. Boosting: ch. 10;\n",
    "    4. Stacking: ch 8.8;\n",
    "    5. Ensemble methods: ch. 16;\n",
    "\n",
    "\n",
    "* **A. J. Izenman.** `Modern Multivariate Statistical Techniques: Regression, Classiffcation, and Manifold Learning.` Springer Texts in Statistics. Springer NewYork, 2009.\n",
    "    1. Committee methods: ch. 14, pp. 506-510, 530-532;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary modules and fix the RNG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy data from **HTF, p. 339**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def htf_p339(n_samples=2000, p=10, random_state=None):\n",
    "    random_state=check_random_state(random_state)\n",
    "    ## Inputs\n",
    "    X = random_state.normal(size=(n_samples, max(10, p)))\n",
    "    ## Response: \\chi^2_10 0.5-prob outliers\n",
    "    y = (np.sum(X[:, :10]**2, axis=1) > 9.34).astype(int).reshape(-1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the RNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(0xC01DC0DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate four samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = htf_p339(2000, 10, random_state)\n",
    "X_test, y_test = htf_p339(10000, 10, random_state)\n",
    "\n",
    "X_valid_1, y_valid_1 = htf_p339(2000, 10, random_state)\n",
    "X_valid_2, y_valid_2 = htf_p339(2000, 10, random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general any ensemble methods can be broken down into the following two\n",
    "stages, possibly overlapping:\n",
    "1. Populate a dictionary of base learners;\n",
    "2. Combine them to get a composite predictor.\n",
    "\n",
    "Many ML estimators can be considerd ensemble methods:\n",
    "1. Regression is a linear ensemble of basis functions: predictors $x\\in \\mathbb{R}^{p\\times 1}$;\n",
    "2. Any model with additive structure, like regression/classificatio trees;\n",
    "3. Feedforward Neural network is a bunch of layers of nonlinear predictors stacked one atop\n",
    "the other, in a specific DAG-like manner;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees\n",
    "\n",
    "A regression tree is a piecewise constant function $T:\\mathcal{X} \\mapsto \\mathbb{R}$\n",
    "having the following expression\n",
    "\n",
    "$$ T(x) = \\sum_{j=1}^J w_j 1_{R_j}(x) \\,, $$\n",
    "where $(R_j)_{j=1}^J$, $J\\geq 1$, is a tree-partition of the input space,\n",
    "and $(w_j)_{j=1}^J$ are estimated values at terminal nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multiclass problem, a classification tree is a composition of a **majority\n",
    "voting** decision function\n",
    "\n",
    "$$ \\mathtt{MAJ}(y) = \\mathop{\\text{argmax}}_{k=1\\,\\ldots, K} y_k \\,, $$\n",
    "with a scoring funciton $T:\\mathcal{X} \\mapsto \\mathbb{R}^K$ of similar structure\n",
    "as in the regression case\n",
    "\n",
    "$$ T(x) = \\sum_{j=1}^J w_j 1_{R_j}(x) \\,, $$\n",
    "where $(w_j)_{j=1}^J\\in\\mathbb{R}^K$ are vectors of class likelihoods (probabilities)\n",
    "at the terminal nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree-partition $(R_j)_{j=1}^J$ and node values $(w_j)_{j=1}^J$ result from running\n",
    "a variant of the **standard greedy top-down tree-induction algorithm** (CART, C.45, et c.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree (1 levels) error: 0.4648\n",
      "Decision tree (3 levels) error: 0.3932\n",
      "Decision tree (7 levels) error: 0.3354\n",
      "Decision tree (max levels) error: 0.2765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf1_ = DecisionTreeClassifier(max_depth=1,\n",
    "                               random_state=random_state).fit(X_train, y_train)\n",
    "clf2_ = DecisionTreeClassifier(max_depth=3,\n",
    "                               random_state=random_state).fit(X_train, y_train)\n",
    "clf3_ = DecisionTreeClassifier(max_depth=7,\n",
    "                               random_state=random_state).fit(X_train, y_train)\n",
    "clf4_ = DecisionTreeClassifier(max_depth=None,\n",
    "                               random_state=random_state).fit(X_train, y_train)\n",
    "\n",
    "print \"Decision tree (1 levels) error:\", 1 - clf1_.score(X_test, y_test)\n",
    "print \"Decision tree (3 levels) error:\", 1 - clf2_.score(X_test, y_test)\n",
    "print \"Decision tree (7 levels) error:\", 1 - clf3_.score(X_test, y_test)\n",
    "print \"Decision tree (max levels) error:\", 1 - clf4_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is meta algortihm that aims at constructing an esimator by averaging \n",
    "many noisy–± but approximately unbiased models. The general idea is that averaging\n",
    "a set of unbiased estimates, yields an estimate with much reduced variance\n",
    "(provided the base estimates are uncorrelated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging works poorly on models, that linearly depend on the data (like linear\n",
    "regression), and best performs on nonlinear base estimators (like trees). In\n",
    "other terms bagging succeeds in building a better combined estimator, if the\n",
    "base estimator is unstable. Indeed, if the learning procedure is stable, and\n",
    "random perturbation of the train dataset do not affect it by much, the bagging\n",
    "estimator will not differ much from a single predictor, and may even weak its\n",
    "performance somewhat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "\n",
    "Consider a train sample $Z = (X, y) = (x_i, y_i)_{i=1}^n \\in \\mathcal{X}\\times \\mathcal{Y}$,\n",
    "samplesed form a distribution $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bootstrap sample $Z^* = (z^*_i)_{i=1}^n$ is a subsample of $Z = (z_j)_{j=1}^n$ with\n",
    "each element drawn with **replacement** from $Z$. More technically, a bootstrap sample\n",
    "of size $l$ is a sample from the empirical distribution of the training data $Z$, denoted\n",
    "by $\\hat{P}$. So $Z^*\\sim \\hat{P}^l$ means that $(z^*_i)_{i=1}^l \\sim \\hat{P}$ iid, or,\n",
    "similarly,\n",
    "$$ z^*_i = \\bigl\\{ z_j \\text{ w. prob. } \\frac{1}{n}\\,,\\, j=1, \\ldots, n\\bigr.\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting property of a bootstraped sample, is that on average $36.79\\%$ of\n",
    "the original sample are left out of each $Z^{*b}$. Indeed, the probability that a\n",
    "given sample is present in $Z^*$ is\n",
    "$$ 1 - \\bigl(1 - \\frac{1}{n}\\bigr)^n = 1 - e^{-1} + o(n) \\approx 63.21\\%\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the observations not selected for the $b$-th bootstrap sample $Z^{*b}$,\n",
    "denoted by $Z\\setminus Z^{*b}$, $b=1,\\ldots,B$, can be used as an independent test set.\n",
    "The **out-of-bag** sample, $Z\\setminus Z^{*b}$, and for estimating the generalization\n",
    "error, and for defining an **OOB**-predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given collection of bootstrap samples $(Z^{*b})_{b=1}^B$ define the set of samples\n",
    "the $i$-th observation **does not** belong to as $\\Gamma_i = \\{b=1,\\ldots, n\\,:\\, z_i \\notin Z^{*b} \\}$,\n",
    "$i=1,\\ldots, n$. For a fixed observation $i$ the set $\\Gamma_i$ is empty, meaning that\n",
    "$z_i$ is **never** out-of-bag, occurs with probability $\\bigl(1 - (1-n^{-1})^n\\bigr)^B\n",
    "\\approx (1-e^{-1})^B$, which is negligible for $B \\geq 65$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "Let $\\mathcal{A}$ is a learning algorithm, taking a learning sample, that learns\n",
    "regression models $\\hat{f}:\\mathcal{X} \\mapsto \\mathbb{R}$, like Regression Tree,\n",
    "$k$-NN, multi-layer neural netowrk et c. The bagged regression estimator is constructed\n",
    "as follows:\n",
    "1. Draw $B$ independent bootstrap samples $(Z^{*b})_{b=1}^B$;\n",
    "2. On each bootstrap sample $Z^{*b}$ learn an estimator $\\hat{f}^{*b} = \\hat{f}^{*b}(\\cdot; Z^{*b})\n",
    "= \\mathcal{A}(Z^{*b})(\\cdot)$;\n",
    "3. Construct the bagged estimator:\n",
    "$$ \\hat{f}^{\\text{bag}}_B(x) = B^{-1} \\sum_{b=1}^B \\hat{f}^*(x) \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagged estimator $\\hat{f}^{\\text{bag}}_B$ is different from the original-sample\n",
    "estimator $\\hat{f}=\\hat{f}(\\cdot; Z)$ if the ML algorithm is nonlinear on the data,\n",
    "or adaptive. Bagged estimator $\\hat{f}^{\\text{bag}}_B$ is a Monte-Carlo approximation\n",
    "of the ideal Bagging estimator, given by the function\n",
    "\n",
    "$$ \\hat{f}^{\\text{bag}}(x) = \\mathop{\\mathbb{E}}\\nolimits_{Z^*} \\hat{f}^*(x; Z^*) \\,.$$\n",
    "\n",
    "By the law of large numbers we have $\\hat{f}^{\\text{bag}}_B \\to \\hat{f}^{\\text{bag}}$\n",
    "with probability one (over the empirical distribution $\\hat{P}$) as $B\\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOB samples can be used to construct the OOB-predictor -- an estimator, defined only\n",
    "for the training samples:\n",
    "$$\\hat{f}^{\\text{oob}}_b (x_i) = \\frac{1}{|\\Gamma_i|} \\sum_{b\\in \\Gamma_i} \\hat{f}^{*b}(x_i) \\,, $$\n",
    "and based on it the OOB mean squared error:\n",
    "$$ \\text{oob-MSE} = n^{-1} \\sum_{i=1}^n \\bigl(y_i - \\hat{f}^{\\text{oob}}_B(x_i)\\bigr)^2 \\,, $$\n",
    "where observations with $\\Gamma_i=\\emptyset$ are omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "In case of classification the baggin estimator is constructed similarly, but there\n",
    "are important caveats. In this case the ML algorithm learns a class-score function\n",
    "$\\hat{f}:\\mathcal{X} \\mapsto \\mathbb{R}^K$, and then the class label is predicted\n",
    "by $\\mathtt{MAJ}$ (majority voting) on $\\hat{f}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority vote over $K$ candidates with weights $(w_k)_{k=1}^K\\in \\mathbb{R}$ is defined as\n",
    "$$ \\mathtt{MAJ}(w) = \\mathop{\\text{argmax}}_{k=1\\,\\ldots, K} w_k \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option is to define the bagged estimator as\n",
    "$$ \\hat{g}^{\\text{bag}}_B(x)\n",
    "    = \\mathtt{MAJ}\\Bigl( B^{-1}\\sum_{b=1}^B e_{k^{*b}(x)} \\Bigr)\n",
    "    \\,, $$\n",
    "where $e_k$ is the $k$-th unit vector in $\\{0,1\\}^{K\\times 1}$, and\n",
    "$k^{*b}(x)=\\mathtt{MAJ}\\bigl(\\hat{f}^{*b}(x)\\bigr)$.\n",
    "Basically, this ensemble classifies according to voting proportions of the population\n",
    "of bootstrapped classifiers. However, when most calssifiers within the population\n",
    "classify some class correctly, then its voting poportion will overestimate the\n",
    "class probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better option, especially for well-calibrated classfiers is to use their scores directly:\n",
    "$$ \\hat{g}^{\\text{bag}}_B(x)\n",
    "    = \\mathtt{MAJ}\\bigl( B^{-1}\\sum_{b=1}^B \\hat{f}^{*b}(x) \\bigr)\n",
    "    \\,, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can construct an OOB-classifier (or generally an OOB-predictor) using the following\n",
    "idea:\n",
    "$$ \\hat{g}^{\\text{oob}}_B(x_i)\n",
    "    = \\mathtt{MAJ}\\Bigl(\n",
    "        \\frac{1}{|\\Gamma_i|} \\sum_{b\\in \\Gamma_i} e_{k^{*b}(x_i)}\n",
    "        \\Bigr)\\,, $$\n",
    "or\n",
    "$$ \\hat{g}^{\\text{oob}}_B(x_i)\n",
    "    = \\mathtt{MAJ}\\Bigl(\n",
    "        \\frac{1}{|\\Gamma_i|} \\sum_{b\\in \\Gamma_i} \\hat{f}^{*b}(x_i)\n",
    "        \\Bigr)\\,. $$\n",
    "Obviously, this classifier is defined only for the observed samples data, and for only those\n",
    "examples, for which $\\Gamma_i\\neq\\emptyset$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging a good classifier (one with misclassification rate **less** than $0.5$) can\n",
    "improve its accuracy, while bagging a poor one (with higher than $0.5$ error rate)\n",
    "can seriously degrade predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Bagging Calssifier and Regressor have similar parameters:\n",
    "- **n_estimators** -- the number of estimators in the ensemble;\n",
    "- **base_estimator** -- the base estimator from which the bagged ensemble is\n",
    "built;\n",
    "- **max_samples** -- the fraction of samples to be used to train each\n",
    "individual base estimator. Choosing `max_samples < 1.0` leads to a reduction\n",
    "of variance and an increase in bias.\n",
    "- **max_features** -- The number of features to draw from X to train each\n",
    "base estimator;\n",
    "- **bootstrap** -- determines whether samples are drawn with replacement;\n",
    "- **bootstrap_features** -- determines whether features are drawn with replacement;\n",
    "- **oob_score** -- determines whether to use out-of-bag samples to estimate\n",
    "    the generalization error;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged (10) decision tree (3 levels) error: 0.3228\n",
      "Bagged (10) decision tree (max levels) error: 0.2007\n",
      "Bagged (100) decision tree (3 levels) error: 0.2895\n",
      "Bagged (100) decision tree (max levels) error: 0.1704\n"
     ]
    }
   ],
   "source": [
    "clf1_ = BaggingClassifier(n_estimators=10,\n",
    "                          base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "                          random_state=random_state).fit(X_train, y_train)\n",
    "clf2_ = BaggingClassifier(n_estimators=10,\n",
    "                          base_estimator=DecisionTreeClassifier(max_depth=None),\n",
    "                          random_state=random_state).fit(X_train, y_train)\n",
    "clf3_ = BaggingClassifier(n_estimators=100,\n",
    "                          base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "                          random_state=random_state).fit(X_train, y_train)\n",
    "clf4_ = BaggingClassifier(n_estimators=100,\n",
    "                          base_estimator=DecisionTreeClassifier(max_depth=None),\n",
    "                          random_state=random_state).fit(X_train, y_train)\n",
    "\n",
    "print \"Bagged (10) decision tree (3 levels) error:\", 1 - clf1_.score(X_test, y_test)\n",
    "print \"Bagged (10) decision tree (max levels) error:\", 1 - clf2_.score(X_test, y_test)\n",
    "print \"Bagged (100) decision tree (3 levels) error:\", 1 - clf3_.score(X_test, y_test)\n",
    "print \"Bagged (100) decision tree (max levels) error:\", 1 - clf4_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Essentially, a random forest is an bagging ensemble constructed from a large collection\n",
    "of **decorrelated** regression/decision trees. The algorithm specifially modifies\n",
    "the tree induction procedure to produce trees with as low correlation  as possible.\n",
    "1. for $b=1,\\ldots, B$ do:\n",
    "    1. Draw a bootstrap sample $Z^{*b} = (z^{*b}_i)_{i=1}^P$, of size $P = \\lfloor \\eta n\\rfloor$ from $Z$;\n",
    "    2. Grow a tree $T^{*b}$ in a specialized manner: the greedy recursive algorithm\n",
    "    is the same, but each time split candidates are chosen from **a random subset of\n",
    "    features**, and the tree is grown until a minimum node size is reached;\n",
    "2. Take the tree ensemble $(\\hat{T}^{*b})_{b=1}^B$, and return the bagged estimator;\n",
    "\n",
    "Trees benefit the most from bagging and random forest ensembles due to their high nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Bagging, Random Forest Classifier and Regressor accept similar parametrs:\n",
    "- **criterion** -- the function to measure the quality of a split. Supported criteria\n",
    "are:\n",
    "    * _\"gini\"_ -- Gini impurity (classification only);\n",
    "    * _\"entropy\"_ -- the information gain (classification only);\n",
    "    * _\"mse\"_ -- mean squared error (regression only);\n",
    "- **max_features** -- The number of features to consider when looking for the\n",
    "best split: `sqrt`, `log2` and share in $(0,1]$ are accepted (choosing `max_features < n_features`\n",
    "leads to a reduction of variance and an increase in bias);\n",
    "- **max_depth** -- maximum depth of the individual regression tree estimators\n",
    "(the maximum depth limits the number of nodes in the tree, the best value depends\n",
    "on the interaction of the input variables);\n",
    "- **min_samples_split** -- The minimum number of samples required to split an\n",
    "internal node;\n",
    "- **min_samples_leaf** -- The minimum number of samples required to be at a\n",
    "leaf node;\n",
    "- **min_weight_fraction_leaf** -- The minimum weighted fraction of the input\n",
    "samples required to be at a leaf node;\n",
    "- **max_leaf_nodes** -- Grow trees with ``max_leaf_nodes`` in best-first\n",
    "fashion, determined by the relative reduction in impurity;\n",
    "- **bootstrap** -- determines whether samples are drawn with replacement;\n",
    "- **oob_score** -- determines whether to use out-of-bag samples to estimate\n",
    "    the generalization error.\n",
    "\n",
    "Note that in Scikit-learn the bootstrap sample size is the same as teh original\n",
    "sample ($\\eta=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomForestClassifier** also handles imbalanced classification problems via\n",
    "the `class_weight` parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **class_weight** --  weights associated with classes given in the form of a\n",
    "dictionary with elements `{class_label: weight}`, or a rebalancing mode:\n",
    "    * _\"balanced\"_ -- mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data;\n",
    "    * _\"balanced__subsample\"_ -- mode is the same as \"balanced\", except that\n",
    "    weights are re-computed based on the bootstrap sample for every tree grown.\n",
    "    These weights will be used to adjust the sample weight (passed\n",
    "    through the fit method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (10, 3 levels) error: 0.2799\n",
      "Random Forest (100, 3 levels) error: 0.2544\n",
      "Random Forest (10, max levels) error: 0.2\n",
      "Random Forest (100, max levels) error: 0.1525\n"
     ]
    }
   ],
   "source": [
    "clf1_ = RandomForestClassifier(n_estimators=10, max_depth=3,\n",
    "                               random_state=random_state).fit(X_train, y_train)\n",
    "clf2_ = RandomForestClassifier(n_estimators=100, max_depth=3,\n",
    "                               random_state=random_state).fit(X_train, y_train)\n",
    "clf3_ = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "                               random_state=random_state).fit(X_train, y_train)\n",
    "clf4_ = RandomForestClassifier(n_estimators=100, max_depth=None,\n",
    "                               random_state=random_state).fit(X_train, y_train)\n",
    "\n",
    "print \"Random Forest (10, 3 levels) error:\", 1 - clf1_.score(X_test, y_test)\n",
    "print \"Random Forest (100, 3 levels) error:\", 1 - clf2_.score(X_test, y_test)\n",
    "print \"Random Forest (10, max levels) error:\", 1 - clf3_.score(X_test, y_test)\n",
    "print \"Random Forest (100, max levels) error:\", 1 - clf4_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying idea of boosting is to combine a collection of weak predictors,\n",
    "into one strong powerful committee model. Most commonly a dictionary of nonlinear\n",
    "base predictors, like decision trees (regression/classification), is used weak\n",
    "predictors in boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following classification problem: learn a hypothesis (algorithm)\n",
    "$h:\\mathcal{X}\\mapsto \\{-1,1\\}$ that is able to generalize well beyond the given\n",
    "learning sample $Z = (X, y) = (x_i, y_i)_{i=1}^n \\in \\mathcal{X}\\times \\{-1, +1\\}$.\n",
    "The empirical risk is the sample average loss\n",
    "$$ \\hat{\\mathcal{R}}_Z(h(\\cdot))\n",
    "    = n^{-1} \\sum_{i=1}^n L(h(x_i), y_i)\n",
    "    = \\mathbb{E}_{(x,y)\\sim Z} L(h(x), y) \\,, $$\n",
    "where $\\mathbb{E}_Z$ denotes the expectation over the empirical measure induced\n",
    "by $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, it would be great to learn such a classifier $g:\\mathcal{X}\\mapsto\\{-1,+1\\}$,\n",
    "that minimizes the theoretical risk\n",
    "\n",
    "$$ \\mathcal{R}(h(\\cdot)) = \\mathbb{E}_{(x, y)\\sim D} 1_{y\\neq h(x)} \\,, $$\n",
    "\n",
    "where $D$ is the true unknown distribution on $\\mathcal{X} \\times \\{-1, +1\\}$ of\n",
    "the data. The ideal calssifier given by the Bayes classifier $g^*(x) = \\mathbb{P}_D(y=1|X=x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this functional is unavailable in real life, and thus we have to get\n",
    "by minimizing the empirical risk, which is known to be an approximation of the\n",
    "theoretical risk due to the Law of Large Numbers. We do this, hoping that\n",
    "$$ \\hat{h} \\in\n",
    "    \\mathop{\\text{argmin}}_{g\\in \\mathcal{F}}\n",
    "        \\hat{\\mathcal{R}}_Z(g(\\cdot)) \\,, $$\n",
    "also more-or-less minimizes the theoretical risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore for a general class of hypotheses $h:\\mathcal{X}\\mapsto \\{-1,+1\\}$, the\n",
    "empirical risk minimization problem cannot be solved efficiently due to non-convexity\n",
    "of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FSAM\n",
    "**F**orward **S**tagewise **A**dditive **M**odelling is a general greedy approach\n",
    "to modelling **additive enesembles** (generalized additive models). The basic idea\n",
    "of this approach is to construct a suboptimal model incrementally in a greedy fashion.\n",
    "The goal is to minimize $\\sum_{i=1}^n L(y_i, f(x_i)) + \\Omega(f)$ over some class\n",
    "$f\\in \\mathcal{F}$, where $\\Omega(\\cdot)$ is an additive complexity regularizer.\n",
    "\n",
    "**Algorithm**:\n",
    "1. set $F_0 = 0$;\n",
    "2. for $k = 1,\\ldots, K$ do:\n",
    "    1. using some efficient method find at least a good approximation to the following:\n",
    "    $$ f_k \n",
    "        \\leftarrow \\mathop{\\mathtt{argmin}}\\limits_{f\\in \\mathcal{F}}\n",
    "            \\sum_{i=1}^n L\\bigl( y_i, F_{k-1}(x_i) + f(x_i)\\bigr)\n",
    "            + \\Omega(F_{k-1}) + \\Omega(f)\n",
    "        \\,; $$\n",
    "    2. set $ F_k = F_{k-1} + f_k$;\n",
    "3. Return $F_K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **AdaBoost** algorithm is based on the **F**orward-**S**tagewise **A**dditive **M**odelling\n",
    "approach, which implements a greedy strategy of constructing an additive model, such as\n",
    "an ensemble (or even a tree), from a rich dictionary of basis functions. In classification,\n",
    "it is a particular example of a convex relaxation of the empirical risk minimization problem:\n",
    "**AdaBoost** dominates the $0-1$ loss $(y, p)\\mapsto 1_{y p < 0}$ with exp-loss $(y,p)\\mapsto e^{-yp}$\n",
    "and minimizes a convex upper bound of the classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AdaBoost.M1\n",
    "1. initialize $\\omega_{1i} \\leftarrow \\frac{1}{n}$, $i=1\\ldots, n$;\n",
    "2. for $m=1,\\ldots, M$ do:\n",
    "    1. fit a classifier $\\hat{g}_m$ to $(X, y)$ with sample weights $(\\omega_{mi})_{i=1}^n$;\n",
    "    2. get the miscassification error $\\epsilon_m = W_m^{-1} \\sum_{i\\,:\\,y_i\\neq \\hat{g}_m(x_i)} \\omega_{mi}$,\n",
    "    for $W_m = \\sum_{i=1}^n \\omega_{mi}$;\n",
    "    3. compute the log-odds ratio $\\alpha_m = \\log \\frac{1-\\epsilon_m}{\\epsilon_m}$;\n",
    "    4. update the weights:\n",
    "    $\\omega_{m+1,t} \\leftarrow \\omega_{mi} \\text{exp}\\bigl( \\alpha_m 1_{\\{i\\,:\\,y_i\\neq \\hat{g}_m(x_i)\\}} \\bigr)$;\n",
    "3. Output the ensemble $\\hat{g} = \\mathop{\\text{sign}}\\bigl\\{\\sum_{m=1}^m \\alpha_m \\hat{g}_m\\bigr\\}$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **AdaBoost.M1** algorithm employs an adversarial teaching approach to strengthen\n",
    "the ensemble. As is visible from the algorithm, the teacher tries to maximize the\n",
    "classification error of the learner by amplifying the weights of the difficult to\n",
    "classify examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the ensemble $M$ serves as a regularization parameter, since\n",
    "the greater the $M$, the more boosting overfits. An `optimal` $M$ can be\n",
    "chosen by cross-validation (preferably on a single common validation set).\n",
    "\n",
    "A recent development, called **DeepBoost** [Mohri et al.; 2014](http://research.google.com/pubs/pub42856.html),\n",
    "proposes a new ensemble learning algorithm, that is similar in spirit to\n",
    "**AdaBoost**. Its key feature is that the algorithm incorporates a complexity\n",
    "penalty for convex combinations of models into the convex relaxation of\n",
    "the loss criterion. This enables selection of better hypotheses that minimize\n",
    "the upper bound on the theoretical risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common parameters:\n",
    "- **n_estimators** -- the maximum number of estimators at which boosting is\n",
    "terminated (in **case of perfect fit**, the learning procedure is stopped early);\n",
    "- **base_estimator** -- the base estimator, which supports **sample weighting**,\n",
    "from which the boosted ensemble is built;\n",
    "- **learning_rate** -- learning rate shrinks the contribution of each classifier\n",
    "by `learning_rate`.\n",
    "\n",
    "AdaBoostClassifier only:\n",
    "- **algorithm** -- the AdaBoost version to use:\n",
    "    * _\"SAMME.R\"_ -- the SAMME.R real boosting algorithm;\n",
    "    * _\"SAMME\"_ -- the SAMME (**M1**) discrete boosting algorithm;\n",
    "    The `SAMME.R` algorithm typically converges faster than `SAMME`,\n",
    "    achieving a lower test error with fewer boosting iterations.\n",
    "\n",
    "AdaBoostRegressor only:\n",
    "- **loss** -- the loss function to use when updating the weights after each\n",
    "    boosting iteration:\n",
    "    * _\"linear\"_ -- absolute loss $L(y, p) = |y-p|$;\n",
    "    * _\"square\"_ -- squared loss $L(y, p) = |y-p|^2$;\n",
    "    * _\"exponential\"_ -- Exponential loss $L(y, p) = 1-e^{-|y-p|}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost.M1 (10, stumps) error: 0.3116\n",
      "AdaBoost.M1 (100, stumps) error: 0.0804\n",
      "AdaBoost.M1 (10, 3 levels) error: 0.1791\n",
      "AdaBoost.M1 (100, 3 levels) error: 0.1491\n"
     ]
    }
   ],
   "source": [
    "clf1_ = AdaBoostClassifier(n_estimators=10, \n",
    "                           base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                           random_state=random_state).fit(X_train, y_train)\n",
    "clf2_ = AdaBoostClassifier(n_estimators=100, \n",
    "                           base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                           random_state=random_state).fit(X_train, y_train)\n",
    "clf3_ = AdaBoostClassifier(n_estimators=10, \n",
    "                           base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "                           random_state=random_state).fit(X_train, y_train)\n",
    "clf4_ = AdaBoostClassifier(n_estimators=100,\n",
    "                           base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "                           random_state=random_state).fit(X_train, y_train)\n",
    "\n",
    "print \"AdaBoost.M1 (10, stumps) error:\", 1 - clf1_.score(X_test, y_test)\n",
    "print \"AdaBoost.M1 (100, stumps) error:\", 1 - clf2_.score(X_test, y_test)\n",
    "print \"AdaBoost.M1 (10, 3 levels) error:\", 1 - clf3_.score(X_test, y_test)\n",
    "print \"AdaBoost.M1 (100, 3 levels) error:\", 1 - clf4_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain circumstances in order to minimize a convex twice-differentiable\n",
    "function $f:\\mathbb{R}^p \\mapsto \\mathbb{R}$ one uses Newton-Raphson iterative\n",
    "procedure, which repeats until convergence this update step:\n",
    "$$ x_{m+1} \\leftarrow x_m - \\bigl(\\nabla^2 f(x_m)\\bigr)^{-1} \\nabla f(x_m) \\,, $$\n",
    "where $\\nabla^2 f(x_m)$ is the hessian of $f$ at $x_m$ and $\\nabla f(x_m)$ is its\n",
    "gradient.\n",
    "\n",
    "In a more general setting, if the function is not twice differentiable, or if\n",
    "the hessian is expensive to compute, then one resorts to a **gradient descent**\n",
    "procedure, which moves in the direction os teh steepest descent and updates\n",
    "according to\n",
    "$$ x_{m+1} \\leftarrow x_m - \\eta \\nabla f(x_m) \\,,$$\n",
    "for some step $\\eta > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is, to a certain extent, a gradient descent procedure aimed at\n",
    "minimizing an expected loss functional $\\mathcal{L}: \\mathcal{F}\\mapsto \\mathbb{R}$\n",
    "on some function space $\\mathcal{F} \\subset \\mathbb{R}^{\\mathcal{X}}$. In particular,\n",
    "it the underlying distribution of the data were known, Gradient Boosting would\n",
    "attempt to find a minimizer $x\\mapsto \\hat{f}(x)$ such that for all $x\\in \\mathcal{X}$\n",
    "$$ \\hat{f}(x)\n",
    "    = \\mathop{\\text{argmin}}_{f\\in\\mathcal{F}}\n",
    "        \\mathbb{E}_{y \\sim P|x} L(y, f(x)) \\,. $$\n",
    "\n",
    "At each itration it would update the current estimate of the minimizer $\\hat{f}_m$\n",
    "in the direction of the steepest-descent towards $\\hat{f}$:\n",
    "$$ \\hat{f}_{m+1} \\leftarrow \\hat{f}_m - \\rho \\hat{g}_m \\,, $$\n",
    "where $\\hat{g}_m \\in \\mathcal{F}$ is given by\n",
    "$$ \\hat{g}_m(x) = \\biggl. \\frac{\\partial}{\\partial f(x)}\n",
    "    \\Bigl( \\mathbb{E}_{y \\sim P|x} L\\bigl(y, f(x)\\bigr) \\Bigr)\n",
    "    \\biggr\\rvert_{f=\\hat{f}_m}\n",
    "     = \\biggl.\n",
    "    \\mathbb{E}_{y \\sim P|x} \\frac{\\partial}{\\partial f(x)}L\\bigl(y, f(x)\\bigr)\n",
    "    \\biggr\\rvert_{f=\\hat{f}_m} \\,, $$\n",
    "(under some regularity conditions it is possible to interchange the expectation\n",
    "and differentiation operation). In turn $\\rho$ is determined by\n",
    "$$ \\rho = \\mathop{\\text{argmin}}_\\rho \\mathbb{E}_{(x,y) \\sim P} L(y, \\hat{f}_m(x) - \\rho \\hat{g}_m(x)) \\,. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in practice the expectaions are not known, one approximates them with their\n",
    "empirical counterparts, which makes the gradient undefined outside the observed\n",
    "sample points. That is why one needs a class of basis functions, which can \n",
    "generalize the gradient from a point to its neighbourhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting procedure**\n",
    "1. Initialize the ensemble with $\\hat{f}_0 \\leftarrow \\mathop{\\text{argmin}}_\\gamma \\sum_{i=1}^n L(y_i, \\gamma)$;\n",
    "2. for $m=1,\\ldots, M$ do:\n",
    "    1. **Gradient approximation**: Compute the current sample descent direction (negative\n",
    "    gradient) using the current ensmeble:\n",
    "    $$ r_{mi} = \\biggl.\n",
    "            - \\frac{\\partial}{\\partial f(x_i)} L\\bigl(y_i, f(x_i)\\bigr)\n",
    "        \\biggr\\rvert_{f=f_{m-1}} \\,, $$\n",
    "    this can be thought of as a finte-dimensional approximation of a functional\n",
    "    gradient $\\delta \\mathcal{L}$ of the loss functional $\\mathcal{L}$;\n",
    "    2. Fit an MSE minimizing parametric basis function $h(x;\\theta)$ to the approximation\n",
    "    of the gradient $(r_{mi})_{i=1}^n$:\n",
    "    $$ (\\theta_m, \\beta_m)\n",
    "        \\leftarrow \\mathop{\\text{argmin}}_{\\theta, \\beta}\n",
    "            \\sum_{i=1}^n \\bigl(r_{mi} - \\beta h(x_i;\\theta) \\bigr)^2\\,; $$\n",
    "    basically we hope that $h(\\cdot;\\theta)$ approximates the functional gradient well\n",
    "    enought and extrapolates beyond the point estimates to their immediate neighbourhoods;\n",
    "    3. **Line search**: determine the optmial `step` in the direction of the `functional`\n",
    "    `gradient` that minimizes the loss functional:\n",
    "    $$ \\gamma_m \\leftarrow \\mathop{\\text{argmin}}_\\gamma\n",
    "       \\sum_{i=1}^n L\\bigl(y_i, f_{m-1}(x_i) + \\gamma h(x_i;\\theta_m)\\bigr)\\,;$$\n",
    "    4. Update the ensemble $f_m = f_{m-1} + \\eta \\, \\gamma_m h(\\cdot;\\theta_m)$;\n",
    "3. Return $\\hat{f}(x) = f_M(x)$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\eta$>0 is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boost algorithm uses basis functions $h(\\cdot; \\theta)$ from some class\n",
    "to approximate the gradient. For example, one can use regression splines, or more\n",
    "generally fit a kernel ridge regression for gradient interpolation, or use regression\n",
    "trees. Regression trees do not assume a predetermined parametric form, and instead\n",
    "are constructed according to information derived from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a given tree-partition structure $(R_j)_{j=1}^J$, it is really straightforward\n",
    "to find optimal estimates $(w_j)_{j=1}^J\\in \\mathbb{R}$.\n",
    "\n",
    "Now finding an optimal partition $(R_j)_{j=1}^J$ is entirely different matter: exhaustive\n",
    "search is out of question, so the algorithm to go is the greedy top-down recursive\n",
    "partitioning procedure.\n",
    "\n",
    "Boosted trees is an ensemble $\\hat{f}(x) = \\sum_{m=1}^M \\hat{f}_m(x)$, with weights\n",
    "incorporated in each base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GBRT**\n",
    "1. Initialize the ensemble with $\\hat{f}_0 \\leftarrow \\mathop{\\text{argmin}}_\\gamma \\sum_{i=1}^n L(y_i, \\gamma)$;\n",
    "2. for $m=1,\\ldots, M$ do:\n",
    "    1. Compute the current sample descent direction (negative gradient) using the current ensmeble:\n",
    "    $$ r_{mi} = \\biggl.\n",
    "            - \\frac{\\partial}{\\partial f(x_i)} L\\bigl(y_i, f(x_i)\\bigr)\n",
    "        \\biggr\\rvert_{f=f_{m-1}} \\,, $$\n",
    "    this is a finte-dimensional version of the first **variation** $\\delta J$\n",
    "    of a functional $J:\\mathbb{R}^{\\mathcal{X}}\\mapsto \\mathbb{R}$ on some\n",
    "    function space;\n",
    "    2. Fit an MSE minimizing regression tree $\\hat{T}_m = \\sum_{j=1}^J \\beta_j 1_{R_{mj}}(x)$\n",
    "    to the current gradient $(r_{mi})_{i=1}^n$ and keep its partition structure;\n",
    "    basically, we want to `generalize` the point estimates of the variation to \n",
    "    some neighbourhood of each sample point (here the heighbourhoods are the tree\n",
    "    partitions);\n",
    "    3. **Line search**: determine the optmial node-weights\n",
    "    $$w_{mj} \\leftarrow \\mathop{\\text{argmin}}_w\n",
    "       \\sum_{i\\,:\\,x_i\\in R_{mj}} L(y_i, f_{m-1}(x_i) + w)\\,;$$\n",
    "    4. Update the ensemble $f_m = f_{m-1} + \\sum_{j=1}^J w_{mj} 1_{R_{mj}}$;\n",
    "3. Return $\\hat{f}(x) = f_M(x)$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Gradient boosting ensembles in scikit accept the following paramters:\n",
    "- **loss** -- loss function to be optimized:\n",
    "    * Classification:\n",
    "        * _'deviance'_ -- refers logistic regression with probabilistic outputs;\n",
    "        * _'exponential'_ -- gradient boosting recovers the AdaBoost algorithm;\n",
    "    * Regression:\n",
    "        * _'ls'_ -- refers to least squares regression;\n",
    "        * _'lad'_ -- (least absolute deviation) is a highly robust loss function solely\n",
    "        based on order information of the input variables;\n",
    "        * _'huber'_ -- is a combination of the two;\n",
    "        * _'quantile'_ -- allows quantile regression (use `alpha` to specify the\n",
    "        quantile);\n",
    "- **learning_rate** -- learning rate shrinks the contribution of each tree\n",
    "by `learning_rate`;\n",
    "- **n_estimators** -- The number of boosting stages to perform. Gradient boosting\n",
    "is fairly robust to over-fitting so a large number usually results in better performance;\n",
    "- **max_depth** -- maximum depth of the individual regression tree estimators (the\n",
    "maximum depth limits the number of nodes in the tree, the best value depends on the\n",
    "interaction of the input variables);\n",
    "- **min_samples_split** -- The minimum number of samples required to split an\n",
    "internal node;\n",
    "- **min_samples_leaf** -- The minimum number of samples required to be at a\n",
    "leaf node;\n",
    "- **min_weight_fraction_leaf** -- The minimum weighted fraction of the input\n",
    "samples required to be at a leaf node;\n",
    "- **subsample** -- The fraction of samples to be used for fitting the individual\n",
    "base learners (choosing `subsample < 1.0` results in Stochastic Gradient Boosting\n",
    "and leads to a reduction of variance and an increase in bias);\n",
    "- **max_features** -- The number of features to consider when looking for the\n",
    "best split: `sqrt`, `log2` and share in $(0,1]$ are accepted (choosing `max_features < n_features`\n",
    "leads to a reduction of variance and an increase in bias);\n",
    "- **max_leaf_nodes** -- Grow trees with `max_leaf_nodes` in best-first fashion,\n",
    "with best nodes are defined as relative reduction in impurity;\n",
    "- **alpha** -- the alpha-quantile of the huber loss function and the quantile\n",
    "loss function (**only if** `loss='huber'` or `loss='quantile'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High learning Rate, small ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBRT (10, stumps) error: 0.3143\n",
      "GBRT (100, stumps) error: 0.0909\n",
      "GBRT (10, 3 levels) error: 0.172\n",
      "GBRT (100, 3 levels) error: 0.09\n"
     ]
    }
   ],
   "source": [
    "clf1_ = GradientBoostingClassifier(n_estimators=10,\n",
    "                                   max_depth=1, learning_rate=0.75,\n",
    "                                   random_state=random_state).fit(X_train, y_train)\n",
    "clf2_ = GradientBoostingClassifier(n_estimators=100,\n",
    "                                   max_depth=1, learning_rate=0.75,\n",
    "                                   random_state=random_state).fit(X_train, y_train)\n",
    "clf3_ = GradientBoostingClassifier(n_estimators=10,\n",
    "                                   max_depth=3, learning_rate=0.75,\n",
    "                                   random_state=random_state).fit(X_train, y_train)\n",
    "clf4_ = GradientBoostingClassifier(n_estimators=100,\n",
    "                                   max_depth=3, learning_rate=0.75,\n",
    "                                   random_state=random_state).fit(X_train, y_train)\n",
    "\n",
    "print \"GBRT (10, stumps) error:\", 1 - clf1_.score(X_test, y_test)\n",
    "print \"GBRT (100, stumps) error:\", 1 - clf2_.score(X_test, y_test)\n",
    "print \"GBRT (10, 3 levels) error:\", 1 - clf3_.score(X_test, y_test)\n",
    "print \"GBRT (100, 3 levels) error:\", 1 - clf4_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large ensemble, small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBRT (100, stumps) error: 0.1874\n",
      "GBRT (1000, stumps) error: 0.0829\n",
      "GBRT (100, 3 levels) error: 0.122\n",
      "GBRT (1000, 3 levels) error: 0.0862\n"
     ]
    }
   ],
   "source": [
    "clf1_ = GradientBoostingClassifier(n_estimators=100,\n",
    "                                   max_depth=1, learning_rate=0.1,\n",
    "                                   random_state=random_state).fit(X_train, y_train)\n",
    "clf2_ = GradientBoostingClassifier(n_estimators=1000,\n",
    "                                   max_depth=1, learning_rate=0.1,\n",
    "                                   random_state=random_state).fit(X_train, y_train)\n",
    "clf3_ = GradientBoostingClassifier(n_estimators=100,\n",
    "                                   max_depth=3, learning_rate=0.1,\n",
    "                                   random_state=random_state).fit(X_train, y_train)\n",
    "clf4_ = GradientBoostingClassifier(n_estimators=1000,\n",
    "                                   max_depth=3, learning_rate=0.1,\n",
    "                                   random_state=random_state).fit(X_train, y_train)\n",
    "\n",
    "print \"GBRT (100, stumps) error:\", 1 - clf1_.score(X_test, y_test)\n",
    "print \"GBRT (1000, stumps) error:\", 1 - clf2_.score(X_test, y_test)\n",
    "print \"GBRT (100, 3 levels) error:\", 1 - clf3_.score(X_test, y_test)\n",
    "print \"GBRT (1000, 3 levels) error:\", 1 - clf4_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "Briefly, XGBoost, is a higlhy streamlined open-source gradient boosting library, which\n",
    "supports many useful loss functions and uses second order loss approximation both\n",
    "to increas the ensemble accuracy and speed of convergence:\n",
    "1. learning rate $\\eta>0$ to regulate the convergence;\n",
    "2. offer $l_1$ and $l_2$ regularization on the node-weights and bias-varaince tradeoff and sparsity;\n",
    "3. cost-complexity pruning of the growm trees;\n",
    "4. Employs specialized regression and classification tree growth algorithms\n",
    "with random projections, and bagging;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It important to note, that XGBoost implements binary trees, which does not restrict\n",
    "the model in any way. However this adds the need for an extra preprocessing step for\n",
    "categorical features. Specifically the binary structure requires that such features\n",
    "be $0-1$ encoded, which is likely to use excessive volumes of memory, especially\n",
    "when the set of possible categories is of the order of thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to permit the use of arbitrary convex loss functions --\n",
    "$$ \\sum_{i=1}^n L( y_i, \\hat{y}_i ) + \\sum_{k=1}^K \\Omega(f_k)\n",
    "    \\rightarrow \\mathop{\\mathtt{min}}_{f_k\\in\\mathcal{M} } \\,,$$\n",
    "with prediction $\\hat{y}_i = \\sum_{k=1}^K f_k(x_i)$, the loss $L(y, \\hat{y})$,\n",
    "and the additive complexity regularizer $\\Omega(\\cdot)$, -- and still achieve\n",
    "high preformance during learning, the author of XGBoost, implemented a clever\n",
    "trick: he uses FSAM general approach, but the minimization with respect to the\n",
    "increment $f(\\cdot)$ is performed on the second order Taylor series approximation\n",
    "of the loss $L$ at $(x_i, y_i)$ and $F(\\cdot)$. In particular the minimization\n",
    "over $f(\\cdot)$ is done on a quadratic approximation\n",
    "$$ q_{y, x}\n",
    "    = L(y, F(x))\n",
    "    + \\frac{\\partial L}{\\partial \\hat{y}}\\bigg\\vert_{(y,F(x))}\\!\\! f(x)\n",
    "    + \\frac{1}{2} \\frac{\\partial^2 L}{\\partial \\hat{y}^2}\\bigg\\vert_{(y,F(x))}\\! f(x)^2\n",
    "    \\,, $$\n",
    "rather than $L(y, F(x) + f(x))$.\n",
    "\n",
    "Since $\\Omega(F_{k-1})$ and $L( y_i, F_{k-1}(x_i) )$ are unaffected by the\n",
    "choice of $f\\in\\mathcal{F}$ at iteration $k$, the greedy step can be reduced\n",
    "to:\n",
    "$$ f_k\n",
    "    \\leftarrow \\mathop{\\mathtt{argmin}}\\limits_{f\\in \\mathcal{F}}\n",
    "        \\sum_{i=1}^n g^{k-1}_i f(x_i) + \\frac{1}{2} h^{k-1}_i f(x_i)^2 + \\Omega(f)\n",
    "    \\,, $$\n",
    "where $g^{k-1}_i = \\frac{\\partial l(y, \\hat{y})}{\\partial \\hat{y}}$ and\n",
    "$h^{k-1}_i = \\frac{\\partial^2 l(y, \\hat{y})}{\\partial \\hat{y}^2}$ evaluated\n",
    "at $y=y_i$ and $\\hat{y}=F_{k-1}(x_i)$.\n",
    "\n",
    "The values $g^{k-1}_i$ and $h^{k-1}_i$ are the gradient and hessian statistics\n",
    "on the $i$-th observation, respectively. These statistics have to be recomputed\n",
    "at each stage for the new $\\hat{y}$. The statistics $g^{0}_i$ and $h^{0}_i$ are\n",
    "initialized to values of the first and second derivatives of $L(y_i, c)$ for some\n",
    "fixed $c$ at each $i=1,\\ldots n$ ($c$ is the sample average in the case or\n",
    "regression, or the log-odds of the class ratio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizing the objective\n",
    "\n",
    "XGBoost uses criteria derived from the objective function that permit automatic\n",
    "tree-pruning. Consider some tree $f$ with structure\n",
    "$$ f = \\sum_{j=1}^J w_j 1_{R_j} \\,,$$\n",
    "where $(R_j)_{j=1}^J\\subseteq \\mathcal{X}$ is its partition and $w\\in\\mathbb{R}^J$\n",
    "-- leaf predicted values. For this tree the complexity regularization is\n",
    "$$ \\Omega(f) = \\gamma J + \\frac{\\lambda}{2} \\sum_{j=1}^J w_j^2 + \\alpha \\sum_{j=1}^J \\bigl|w_j\\bigr| \\,. $$\n",
    "As one can see both excessively large leaf values and tree depths are\n",
    "penalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### stage $k\\geq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the map $x\\mapsto j(x)$, which gives the unique leaf index $j=1,\\ldots,J$ such\n",
    "that $x\\in R_j$, the objective function minimized at each stage $k\\geq 1$ is given by\n",
    "\\begin{align*}\n",
    "\\mathtt{Obj}_k(R, w)\n",
    "    &= \\sum_{i=1}^n \\bigl( g^{k-1}_i w_{j(x_i)} + \\frac{1}{2} h^{k-1}_i w_{j(x_i)}^2 \\bigr)\n",
    "     + \\frac{\\lambda}{2} \\sum_{j=1}^J w_j^2 + \\alpha \\sum_{j=1}^J \\bigl|w_j\\bigr| + \\gamma J \\\\\n",
    "    &= \\sum_{j=1}^J \\bigl( w_j G_{k-1}(R_j) + \\frac{1}{2} \\bigl( H_{k-1}(R_j) + \\lambda \\bigr) w_j^2\n",
    "     + \\alpha \\bigl|w_j\\bigr| + \\gamma \\bigr) \\,,\n",
    "\\end{align*}\n",
    "where for any $P\\subseteq X$, the values $G_{k-1}(P) = \\sum_{i\\,:\\,x_i\\in P} g^{k-1}_i$\n",
    "and $H_{k-1}(P) = \\sum_{i\\,:\\,x_i\\in P} h^{k-1}_i$ are called the **first** and the\n",
    "**second** order gradient scores respectively. When $P = R_j$ these are the $j$-th leaf\n",
    "gradinet statistics, which depend only on the ensemble $F_{k-1}$ and  are constant\n",
    "relative to the increment $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **structural score** of an XGBoost regression tree is the minimal value of the\n",
    "objective function for a fixed partition structure $R = (R_j)_{j=1}^J$:\n",
    "$$ \\mathtt{Obj}^*(R)\n",
    "    = \\min_{w_j} \\mathtt{Obj}_k(R, w)\n",
    "    = \\min_{w_j} \\sum_{i=1}^n \\bigl( g^{k-1}_i w_{j(x_i)} + \\frac{1}{2} h^{k-1}_i w_{j(x_i)}^2 \\bigr)\n",
    "    + \\frac{\\lambda}{2} \\sum_{j=1}^J w_j^2 + \\alpha \\sum_{j=1}^J \\bigl|w_j\\bigr| + \\gamma J\n",
    "    \\,. $$\n",
    "This is not an intermediate value of the objective function, but rather its difference\n",
    "against $\\sum_{i=1}^n l(y_i, F_{k-1}(x_i))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting, that since there are no cross interactions between scores $w_j$\n",
    "for different leaves, this minimization problem equivalently reduces to $J$ univariate\n",
    "optimization problems:\n",
    "$$ w_j G_{k-1}(R_j) + \\frac{1}{2} \\bigl( H_{k-1}(R_j) + \\lambda \\bigr) w_j^2\n",
    "                    + \\alpha \\bigl|w_j\\bigr| + \\gamma \\to \\min_{w_j}\\,,$$\n",
    "for $j=1,\\ldots, J$. Let's assume that $H_{k-1}(R_j) + \\lambda > 0$, since otherwise\n",
    "this problem has no solution.\n",
    "\n",
    "The optimal leaf value $w_j^*$ in the general case is given by\n",
    "$$ w^*_j = - \\frac{1}{H_{k-1}(R_j) + \\lambda}\\begin{cases}\n",
    "G_{k-1}(R_j) + \\alpha & \\text{ if } G_{k-1}(R_j) \\leq -\\alpha\\\\\n",
    "0&\\text{ if } G_{k-1}(R_j) \\in [-\\alpha, \\alpha]\\\\\n",
    "G_{k-1}(R_j) - \\alpha & \\text{ if } G_{k-1}(R_j) \\geq \\alpha\n",
    "\\end{cases} \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tree construction process\n",
    "\n",
    "Trees in XGBoost employ a greedy algorithm for recursive tree construction, outlined below:\n",
    " 1. every region $R_j$ in the partition $R$ is probed for the optimal binary split\n",
    "    $R_j\\to R_{j_1}\\!\\|R_{j_2}$ according to the structural gain score\n",
    "    $$ \\mathtt{Gain}\\bigl( R_j\\to R_{j_1}\\!\\| R_{j_2} \\bigr) = \\mathtt{Obj}^*( R ) - \\mathtt{Obj}^*( R' ) \\,, $$\n",
    "    where the partition $R'$ is constructed from $R$ by splitting $R_j\\to R_{j_1}\\|R_{j_2}$;\n",
    " 2. the region $R_j$ with the highest gain from the optimal split is split into $R_{j_1}$ and $R_{j_2}$;\n",
    " 3. the tree growth process continues until no more splits are possible.\n",
    " \n",
    "The first step is the most computatuionally intensive, since it requires $O( J d n\\log n )$\n",
    "operations. This step which is performed by XGBoost in parallel, since FSAM and tree-induction\n",
    "are series by nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tree growth gain\n",
    "For simplicity, let's consider the case when $\\alpha = 0$, $L^2$ regularization.\n",
    "In this case the following weights give optimal leaf scores\n",
    "$$ w^*_j = -\\frac{G_{k-1}(R_j)}{H_{k-1}(R_j) + \\lambda}\\,.$$\n",
    "The strucutral score becomes\n",
    "$$ \\mathtt{Obj}^*(R) = \\gamma J - \\frac{1}{2}\\sum_{j=1}^J \\frac{G_{k-1}^2(R_j)}{H_{k-1}(R_j) + \\lambda} \\,. $$\n",
    "\n",
    "Any split $R_j \\rightarrow R_{j_1}\\!\\| R_{j_2}$ yields the following gain:\n",
    "$$ \\mathtt{Gain} = \\frac{1}{2}\\Biggl(\n",
    "                          \\frac{G_{k-1}^2(R_{j_1})}{H_{k-1}(R_{j_1}) + \\lambda}\n",
    "                        + \\frac{G_{k-1}^2(R_{j_2})}{H_{k-1}(R_{j_2}) + \\lambda}\n",
    "                        - \\frac{G_{k-1}^2(R_j)}{ H_{k-1}(R_j) + \\lambda}\n",
    "                   \\Biggr) - \\gamma\\,.$$\n",
    "Note that $G_{k-1}(\\cdot)$ and $H_{k-1}(\\cdot)$ are additive by construction:\n",
    "$$G_{k-1}(R_j) = G_{k-1}(R_{j_1}) + G_{k-1}(R_{j_2}) \\,,$$\n",
    "and\n",
    "$$H_{k-1}(R_j) = H_{k-1}(R_{j_1}) + H_{k-1}(R_{j_2}) \\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xg\n",
    "\n",
    "seed = random_state.randint(0x7FFFFFFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.407\tvalidation_1-error:0.373\n",
      "Multiple eval metrics have been passed: 'validation_1-error' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-error hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-error:0.333\tvalidation_1-error:0.311\n",
      "[2]\tvalidation_0-error:0.317\tvalidation_1-error:0.285\n",
      "[3]\tvalidation_0-error:0.3375\tvalidation_1-error:0.295\n",
      "[4]\tvalidation_0-error:0.33\tvalidation_1-error:0.281\n",
      "[5]\tvalidation_0-error:0.303\tvalidation_1-error:0.2755\n",
      "[6]\tvalidation_0-error:0.2945\tvalidation_1-error:0.262\n",
      "[7]\tvalidation_0-error:0.287\tvalidation_1-error:0.261\n",
      "[8]\tvalidation_0-error:0.2785\tvalidation_1-error:0.2475\n",
      "[9]\tvalidation_0-error:0.2695\tvalidation_1-error:0.248\n",
      "[10]\tvalidation_0-error:0.2795\tvalidation_1-error:0.252\n",
      "[11]\tvalidation_0-error:0.268\tvalidation_1-error:0.241\n",
      "[12]\tvalidation_0-error:0.258\tvalidation_1-error:0.2315\n",
      "[13]\tvalidation_0-error:0.26\tvalidation_1-error:0.2435\n",
      "[14]\tvalidation_0-error:0.2455\tvalidation_1-error:0.233\n",
      "[15]\tvalidation_0-error:0.245\tvalidation_1-error:0.227\n",
      "[16]\tvalidation_0-error:0.2395\tvalidation_1-error:0.2275\n",
      "[17]\tvalidation_0-error:0.231\tvalidation_1-error:0.2235\n",
      "[18]\tvalidation_0-error:0.2225\tvalidation_1-error:0.2125\n",
      "[19]\tvalidation_0-error:0.2195\tvalidation_1-error:0.2035\n",
      "[20]\tvalidation_0-error:0.2205\tvalidation_1-error:0.2035\n",
      "[21]\tvalidation_0-error:0.2165\tvalidation_1-error:0.2085\n",
      "[22]\tvalidation_0-error:0.2145\tvalidation_1-error:0.199\n",
      "[23]\tvalidation_0-error:0.2165\tvalidation_1-error:0.1975\n",
      "[24]\tvalidation_0-error:0.2075\tvalidation_1-error:0.1985\n",
      "[25]\tvalidation_0-error:0.2015\tvalidation_1-error:0.187\n",
      "[26]\tvalidation_0-error:0.199\tvalidation_1-error:0.184\n",
      "[27]\tvalidation_0-error:0.2\tvalidation_1-error:0.1815\n",
      "[28]\tvalidation_0-error:0.1975\tvalidation_1-error:0.18\n",
      "[29]\tvalidation_0-error:0.1945\tvalidation_1-error:0.1825\n",
      "[30]\tvalidation_0-error:0.197\tvalidation_1-error:0.181\n",
      "[31]\tvalidation_0-error:0.196\tvalidation_1-error:0.182\n",
      "[32]\tvalidation_0-error:0.191\tvalidation_1-error:0.172\n",
      "[33]\tvalidation_0-error:0.195\tvalidation_1-error:0.1725\n",
      "[34]\tvalidation_0-error:0.189\tvalidation_1-error:0.1675\n",
      "[35]\tvalidation_0-error:0.1935\tvalidation_1-error:0.167\n",
      "[36]\tvalidation_0-error:0.1865\tvalidation_1-error:0.1685\n",
      "[37]\tvalidation_0-error:0.1805\tvalidation_1-error:0.165\n",
      "[38]\tvalidation_0-error:0.1835\tvalidation_1-error:0.17\n",
      "[39]\tvalidation_0-error:0.179\tvalidation_1-error:0.1635\n",
      "[40]\tvalidation_0-error:0.1755\tvalidation_1-error:0.163\n",
      "[41]\tvalidation_0-error:0.173\tvalidation_1-error:0.159\n",
      "[42]\tvalidation_0-error:0.178\tvalidation_1-error:0.1575\n",
      "[43]\tvalidation_0-error:0.176\tvalidation_1-error:0.1585\n",
      "[44]\tvalidation_0-error:0.1735\tvalidation_1-error:0.1575\n",
      "[45]\tvalidation_0-error:0.1685\tvalidation_1-error:0.1575\n",
      "[46]\tvalidation_0-error:0.161\tvalidation_1-error:0.1525\n",
      "[47]\tvalidation_0-error:0.1695\tvalidation_1-error:0.1535\n",
      "[48]\tvalidation_0-error:0.167\tvalidation_1-error:0.1535\n",
      "[49]\tvalidation_0-error:0.1675\tvalidation_1-error:0.145\n"
     ]
    }
   ],
   "source": [
    "clf_ = xg.XGBClassifier(\n",
    "## Boosting:\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    objective=\"binary:logistic\",\n",
    "    base_score=0.5,\n",
    "## Regularization: tree growth\n",
    "    max_depth=3,\n",
    "    gamma=0.5,\n",
    "    min_child_weight=1.0,\n",
    "    max_delta_step=0.0,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    colsample_bylevel=1.0,\n",
    "## Regularization: leaf weights\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "## Class balancing\n",
    "    scale_pos_weight=1.0,\n",
    "## Service parameters: missing=None, makes use np.nan as missing.\n",
    "    seed=seed,\n",
    "    missing=None,\n",
    "    nthread=2,\n",
    "    silent=False)\n",
    "\n",
    "clf_.fit(\n",
    "    X_train, y_train,\n",
    "    early_stopping_rounds=5,\n",
    "    eval_set=[(X_valid_1, y_valid_1),\n",
    "              (X_valid_2, y_valid_2),])\n",
    "\n",
    "y_pred_ = clf_.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally XGBoost relies heavily on a custom dataset format **DMatrix**.\n",
    "The interface, which is exposed into python has three capabilities:\n",
    "- load datasets in libSVM compatible format;\n",
    "- load SciPy's sparse matrices;\n",
    "- load Numpy's ndarrays.\n",
    "\n",
    "The DMatrix class is constructed with the following parameters:\n",
    "- **data** : Data source of DMatrix. When data is string type, it represents\n",
    "the path `libsvm` format `txt` file, or binary file that xgboost can read from,\n",
    "or a matrix of observed features $X$ in a numpy or scipy matrix;\n",
    "- **label** : the observation labels $y$ (could be categorical or numeric);\n",
    "- **missing** : a vector of values that encode missing observations, if `None` defaults to `np.nan`;\n",
    "- **feature_names** : the columns names of $X$;\n",
    "- **feature_types** : defines the python types of each column of $X$, in case of heterogeneous data;\n",
    "- **weight** : the vector of nonnegative weights of each observation in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xg.DMatrix(X_train, label=y_train, missing=np.nan)\n",
    "dtest = xg.DMatrix(X_test, missing=np.nan)\n",
    "\n",
    "dvalid1 = xg.DMatrix(X_valid_1, label=y_valid_1, missing=np.nan)\n",
    "dvalid2 = xg.DMatrix(X_valid_2, label=y_valid_2, missing=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same XGBoost classifier as in the Scikit-learn example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tv1-error:0.407\tv2-error:0.373\n",
      "Multiple eval metrics have been passed: 'v2-error' will be used for early stopping.\n",
      "\n",
      "Will train until v2-error hasn't improved in 5 rounds.\n",
      "[1]\tv1-error:0.333\tv2-error:0.311\n",
      "[2]\tv1-error:0.317\tv2-error:0.285\n",
      "[3]\tv1-error:0.3375\tv2-error:0.295\n",
      "[4]\tv1-error:0.33\tv2-error:0.281\n",
      "[5]\tv1-error:0.303\tv2-error:0.2755\n",
      "[6]\tv1-error:0.2945\tv2-error:0.262\n",
      "[7]\tv1-error:0.287\tv2-error:0.261\n",
      "[8]\tv1-error:0.2785\tv2-error:0.2475\n",
      "[9]\tv1-error:0.2695\tv2-error:0.248\n",
      "[10]\tv1-error:0.2795\tv2-error:0.252\n",
      "[11]\tv1-error:0.268\tv2-error:0.241\n",
      "[12]\tv1-error:0.258\tv2-error:0.2315\n",
      "[13]\tv1-error:0.26\tv2-error:0.2435\n",
      "[14]\tv1-error:0.2455\tv2-error:0.233\n",
      "[15]\tv1-error:0.245\tv2-error:0.227\n",
      "[16]\tv1-error:0.2395\tv2-error:0.2275\n",
      "[17]\tv1-error:0.231\tv2-error:0.2235\n",
      "[18]\tv1-error:0.2225\tv2-error:0.2125\n",
      "[19]\tv1-error:0.2195\tv2-error:0.2035\n",
      "[20]\tv1-error:0.2205\tv2-error:0.2035\n",
      "[21]\tv1-error:0.2165\tv2-error:0.2085\n",
      "[22]\tv1-error:0.2145\tv2-error:0.199\n",
      "[23]\tv1-error:0.2165\tv2-error:0.1975\n",
      "[24]\tv1-error:0.2075\tv2-error:0.1985\n",
      "[25]\tv1-error:0.2015\tv2-error:0.187\n",
      "[26]\tv1-error:0.199\tv2-error:0.184\n",
      "[27]\tv1-error:0.2\tv2-error:0.1815\n",
      "[28]\tv1-error:0.1975\tv2-error:0.18\n",
      "[29]\tv1-error:0.1945\tv2-error:0.1825\n",
      "[30]\tv1-error:0.197\tv2-error:0.181\n",
      "[31]\tv1-error:0.196\tv2-error:0.182\n",
      "[32]\tv1-error:0.191\tv2-error:0.172\n",
      "[33]\tv1-error:0.195\tv2-error:0.1725\n",
      "[34]\tv1-error:0.189\tv2-error:0.1675\n",
      "[35]\tv1-error:0.1935\tv2-error:0.167\n",
      "[36]\tv1-error:0.1865\tv2-error:0.1685\n",
      "[37]\tv1-error:0.1805\tv2-error:0.165\n",
      "[38]\tv1-error:0.1835\tv2-error:0.17\n",
      "[39]\tv1-error:0.179\tv2-error:0.1635\n",
      "[40]\tv1-error:0.1755\tv2-error:0.163\n",
      "[41]\tv1-error:0.173\tv2-error:0.159\n",
      "[42]\tv1-error:0.178\tv2-error:0.1575\n",
      "[43]\tv1-error:0.176\tv2-error:0.1585\n",
      "[44]\tv1-error:0.1735\tv2-error:0.1575\n",
      "[45]\tv1-error:0.1685\tv2-error:0.1575\n",
      "[46]\tv1-error:0.161\tv2-error:0.1525\n",
      "[47]\tv1-error:0.1695\tv2-error:0.1535\n",
      "[48]\tv1-error:0.167\tv2-error:0.1535\n",
      "[49]\tv1-error:0.1675\tv2-error:0.145\n"
     ]
    }
   ],
   "source": [
    "param = dict(\n",
    "## Boosting:\n",
    "    eta=0.1,\n",
    "    objective=\"binary:logistic\",\n",
    "    base_score=0.5,\n",
    "## Regularization: tree growth\n",
    "    max_depth=3,\n",
    "    gamma=0.5,\n",
    "    min_child_weight=1.0,\n",
    "    max_delta_step=0.0,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    colsample_bylevel=1.0,\n",
    "## Regularization: leaf weights\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "## Class balancing\n",
    "    scale_pos_weight=1.0,\n",
    "## Service parameters:\n",
    "    seed=seed,\n",
    "    nthread=2,\n",
    "    silent=1)\n",
    "\n",
    "evals_result = dict()\n",
    "xgb_ = xg.train(\n",
    "## XGboost settings\n",
    "    param,\n",
    "## Train dataset\n",
    "    dtrain,\n",
    "## The size of the ensemble\n",
    "    num_boost_round=50,\n",
    "## Early-stopping\n",
    "    early_stopping_rounds=5,\n",
    "    evals=[(dvalid1, \"v1\"),\n",
    "              (dvalid2, \"v2\"),],\n",
    "    evals_result=evals_result)\n",
    "\n",
    "pred_ = xgb_.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the sklearn-compatible and basic python interfaces have the similar\n",
    "parameters. Except they are passed slightly differently.\n",
    "\n",
    "Gradient boosting parameters:\n",
    "- **eta**, **learning_rate** ($\\eta$) -- step size shirinkage factor;\n",
    "- **n_estimators**, **num_boost_round** ($M$) -- the size of the ensemble, number of boosting rounds;\n",
    "- **objective** -- objective functions:\n",
    "    * _\"reg:linear\"_ -- Linear regression: $(x_i, y_i)_{i=1}^n \\in \\mathcal{X} \\times \\mathbb{R}$,\n",
    "    $\\hat{p}:\\mathcal{X} \\mapsto \\mathbb{R}$;\n",
    "    * _\"reg:logistic\"_ -- Logistic regression for probability regression task: $(x_i, y_i)_{i=1}^n\n",
    "    \\in \\mathcal{X} \\times [0, 1]$, $\\hat{p}:\\mathcal{X} \\mapsto [0, 1]$;\n",
    "    * _\"binary:logistic\"_ -- Logistic regression for binary classification task: $(x_i, y_i)_{i=1}^n\n",
    "    \\in \\mathcal{X} \\times \\{0, 1\\}$, $\\hat{p}:\\mathcal{X} \\mapsto \\{0, 1\\}$;\n",
    "    * _\"binary:logitraw\"_ -- Logistic regression for binary classification, output score\n",
    "    before logistic transformation: $\\hat{p}:\\mathcal{X} \\mapsto \\mathbb{R}$;\n",
    "    * _\"multi:softmax\"_ -- Softmax for multi-class classification, output class index:\n",
    "    $\\hat{p}:\\mathcal{X} \\mapsto \\{1,\\ldots,K\\}$;\n",
    "    * _\"multi:softprob\"_ -- Softmax for multi-class classification, output probability\n",
    "    distribution:  $\\hat{p}:\\mathcal{X} \\mapsto \\{\\omega\\in [0,1]^K\\,:\\, \\sum_{k=1}^K \\omega_k = 1 \\}$;\n",
    "- **base_score** -- global bias of the model: in linear regression (\"reg:linear\") sets\n",
    "the bias of the regression function, in binary classification (\"reg:logistic\",\n",
    "\"binary:logistic\" and \"binary:logitraw\") sets the base class ratio (transformed to log-odds\n",
    "and added to logistic score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization - related to tree growth and decorrelation:\n",
    "- **max_depth** -- this parameters limits the size of the tree, by setting a\n",
    "hard bound on the number of tree layers (limits the recursion depth);\n",
    "- **min_child_weight** -- the minimal value of the hessian statistic of a leaf\n",
    "required for it to be considered a candidate for splitting;\n",
    "- **gamma** ($\\gamma$) -- the complexity cost parameter, imposes minimal structural\n",
    "score gain for splitting a leaf of the currnt tree;\n",
    "- **subsample** -- the share of the training data to use for growing a tree:\n",
    "determines the size bootstrap smaples $Z^{*b}$;\n",
    "- **colsample_bytree** -- the size of the random subset of features, that\n",
    "cam be used in the growth of the **whole** tree (accessible features);\n",
    "- **colsample_bylevel** -- subsample ratio of features when considering a split:\n",
    "determines the size of the random subset of accessible features considered as\n",
    "candidates for node splitting at each level of every tree.\n",
    "\n",
    "Regularization - tree leaf weights:\n",
    "- **reg_alpha** ($\\alpha$) -- the importance of the $L^1$ regularizer;\n",
    "- **reg_lambda** ($\\lambda$) -- the weight of the $L^2$ regularization term;\n",
    "- **max_delta_step** -- clips the absolute value of each leaf's score, thereby\n",
    "making the tree growth step more conservative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class balancing (not used in multiclass problems as of commit **c9a73fe2a99300aec3041371675a8fa6bc6a8a72**):\n",
    "- **scale_pos_weight** -- a uniform upscale/downscale factor for the weights of\n",
    "positive examples ($y=+1$); Useful in imbalanced binary classification problems.\n",
    "\n",
    "Early-stopping\n",
    "- **early_stopping_rounds** -- the validation error on the last validation dataset needs\n",
    "to decrease at least every `early_stopping_rounds` round(s) to continue training; If\n",
    "equal to None, then early stopping is deactivated.\n",
    "- **eval_set** -- validation datasets given as a list of tuples (DMatrix, name);\n",
    "- **evals_result** -- a dictionary to store the validation results; the keys are the names\n",
    "of the validation datasets, and values are the dictionaries of key-values pairs:\n",
    "    loss -- list of scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (10, stumps) error: 0.3199\n",
      "XGBoost (1000, stumps) error: 0.0822\n",
      "XGBoost (10, 3 levels) error: 0.122\n",
      "XGBoost (1000, 3 levels) error: 0.0862\n"
     ]
    }
   ],
   "source": [
    "clf1_ = xg.XGBClassifier(n_estimators=10,\n",
    "                         max_depth=1, learning_rate=0.1,\n",
    "                         seed=seed).fit(X_train, y_train)\n",
    "\n",
    "clf2_ = xg.XGBClassifier(n_estimators=1000,\n",
    "                         max_depth=1, learning_rate=0.1,\n",
    "                         seed=seed).fit(X_train, y_train)\n",
    "\n",
    "clf2_ = xg.XGBClassifier(n_estimators=10,\n",
    "                         max_depth=3, learning_rate=0.1,\n",
    "                         seed=seed).fit(X_train, y_train)\n",
    "\n",
    "clf2_ = xg.XGBClassifier(n_estimators=1000,\n",
    "                         max_depth=3, learning_rate=0.1,\n",
    "                         seed=seed).fit(X_train, y_train)\n",
    "\n",
    "print \"XGBoost (10, stumps) error:\", 1 - clf1_.score(X_test, y_test)\n",
    "print \"XGBoost (1000, stumps) error:\", 1 - clf2_.score(X_test, y_test)\n",
    "print \"XGBoost (10, 3 levels) error:\", 1 - clf3_.score(X_test, y_test)\n",
    "print \"XGBoost (1000, 3 levels) error:\", 1 - clf4_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (10, stumps) error: 0.3131\n",
      "XGBoost (1000, stumps) error: 0.1026\n",
      "XGBoost (10, 3 levels) error: 0.122\n",
      "XGBoost (1000, 3 levels) error: 0.0862\n"
     ]
    }
   ],
   "source": [
    "clf1_ = xg.XGBClassifier(n_estimators=10,\n",
    "                         max_depth=1, learning_rate=0.5,\n",
    "                         seed=seed).fit(X_train, y_train)\n",
    "\n",
    "clf2_ = xg.XGBClassifier(n_estimators=1000,\n",
    "                         max_depth=1, learning_rate=0.5,\n",
    "                         seed=seed).fit(X_train, y_train)\n",
    "\n",
    "clf2_ = xg.XGBClassifier(n_estimators=10,\n",
    "                         max_depth=3, learning_rate=0.5,\n",
    "                         seed=seed).fit(X_train, y_train)\n",
    "\n",
    "clf2_ = xg.XGBClassifier(n_estimators=1000,\n",
    "                         max_depth=3, learning_rate=0.5,\n",
    "                         seed=seed).fit(X_train, y_train)\n",
    "\n",
    "print \"XGBoost (10, stumps) error:\", 1 - clf1_.score(X_test, y_test)\n",
    "print \"XGBoost (1000, stumps) error:\", 1 - clf2_.score(X_test, y_test)\n",
    "print \"XGBoost (10, 3 levels) error:\", 1 - clf3_.score(X_test, y_test)\n",
    "print \"XGBoost (1000, 3 levels) error:\", 1 - clf4_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.4615\tvalidation_1-error:0.4555\n",
      "Multiple eval metrics have been passed: 'validation_1-error' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-error hasn't improved in 20 rounds.\n",
      "[1]\tvalidation_0-error:0.4245\tvalidation_1-error:0.407\n",
      "[2]\tvalidation_0-error:0.3835\tvalidation_1-error:0.3725\n",
      "[3]\tvalidation_0-error:0.3645\tvalidation_1-error:0.3505\n",
      "[4]\tvalidation_0-error:0.3515\tvalidation_1-error:0.346\n",
      "[5]\tvalidation_0-error:0.328\tvalidation_1-error:0.3315\n",
      "[6]\tvalidation_0-error:0.322\tvalidation_1-error:0.317\n",
      "[7]\tvalidation_0-error:0.3225\tvalidation_1-error:0.302\n",
      "[8]\tvalidation_0-error:0.32\tvalidation_1-error:0.299\n",
      "[9]\tvalidation_0-error:0.317\tvalidation_1-error:0.301\n",
      "[10]\tvalidation_0-error:0.3075\tvalidation_1-error:0.292\n",
      "[11]\tvalidation_0-error:0.2995\tvalidation_1-error:0.283\n",
      "[12]\tvalidation_0-error:0.2825\tvalidation_1-error:0.2675\n",
      "[13]\tvalidation_0-error:0.2685\tvalidation_1-error:0.2545\n",
      "[14]\tvalidation_0-error:0.2395\tvalidation_1-error:0.235\n",
      "[15]\tvalidation_0-error:0.226\tvalidation_1-error:0.226\n",
      "[16]\tvalidation_0-error:0.213\tvalidation_1-error:0.2155\n",
      "[17]\tvalidation_0-error:0.198\tvalidation_1-error:0.2025\n",
      "[18]\tvalidation_0-error:0.208\tvalidation_1-error:0.2055\n",
      "[19]\tvalidation_0-error:0.199\tvalidation_1-error:0.206\n",
      "[20]\tvalidation_0-error:0.196\tvalidation_1-error:0.2015\n",
      "[21]\tvalidation_0-error:0.1845\tvalidation_1-error:0.186\n",
      "[22]\tvalidation_0-error:0.186\tvalidation_1-error:0.1865\n",
      "[23]\tvalidation_0-error:0.188\tvalidation_1-error:0.1875\n",
      "[24]\tvalidation_0-error:0.186\tvalidation_1-error:0.1855\n",
      "[25]\tvalidation_0-error:0.1855\tvalidation_1-error:0.1835\n",
      "[26]\tvalidation_0-error:0.1885\tvalidation_1-error:0.191\n",
      "[27]\tvalidation_0-error:0.1925\tvalidation_1-error:0.1955\n",
      "[28]\tvalidation_0-error:0.1885\tvalidation_1-error:0.1925\n",
      "[29]\tvalidation_0-error:0.1885\tvalidation_1-error:0.1825\n",
      "[30]\tvalidation_0-error:0.189\tvalidation_1-error:0.179\n",
      "[31]\tvalidation_0-error:0.1845\tvalidation_1-error:0.182\n",
      "[32]\tvalidation_0-error:0.185\tvalidation_1-error:0.1675\n",
      "[33]\tvalidation_0-error:0.176\tvalidation_1-error:0.1645\n",
      "[34]\tvalidation_0-error:0.174\tvalidation_1-error:0.169\n",
      "[35]\tvalidation_0-error:0.1675\tvalidation_1-error:0.1605\n",
      "[36]\tvalidation_0-error:0.168\tvalidation_1-error:0.161\n",
      "[37]\tvalidation_0-error:0.161\tvalidation_1-error:0.154\n",
      "[38]\tvalidation_0-error:0.156\tvalidation_1-error:0.1565\n",
      "[39]\tvalidation_0-error:0.1575\tvalidation_1-error:0.155\n",
      "[40]\tvalidation_0-error:0.161\tvalidation_1-error:0.1545\n",
      "[41]\tvalidation_0-error:0.1595\tvalidation_1-error:0.1505\n",
      "[42]\tvalidation_0-error:0.1545\tvalidation_1-error:0.1525\n",
      "[43]\tvalidation_0-error:0.154\tvalidation_1-error:0.1505\n",
      "[44]\tvalidation_0-error:0.151\tvalidation_1-error:0.1475\n",
      "[45]\tvalidation_0-error:0.1545\tvalidation_1-error:0.1475\n",
      "[46]\tvalidation_0-error:0.149\tvalidation_1-error:0.147\n",
      "[47]\tvalidation_0-error:0.1535\tvalidation_1-error:0.1445\n",
      "[48]\tvalidation_0-error:0.15\tvalidation_1-error:0.1465\n",
      "[49]\tvalidation_0-error:0.156\tvalidation_1-error:0.1455\n",
      "[50]\tvalidation_0-error:0.151\tvalidation_1-error:0.144\n",
      "[51]\tvalidation_0-error:0.151\tvalidation_1-error:0.143\n",
      "[52]\tvalidation_0-error:0.146\tvalidation_1-error:0.138\n",
      "[53]\tvalidation_0-error:0.1505\tvalidation_1-error:0.135\n",
      "[54]\tvalidation_0-error:0.1475\tvalidation_1-error:0.1385\n",
      "[55]\tvalidation_0-error:0.1435\tvalidation_1-error:0.1365\n",
      "[56]\tvalidation_0-error:0.147\tvalidation_1-error:0.138\n",
      "[57]\tvalidation_0-error:0.1505\tvalidation_1-error:0.137\n",
      "[58]\tvalidation_0-error:0.148\tvalidation_1-error:0.1405\n",
      "[59]\tvalidation_0-error:0.1475\tvalidation_1-error:0.1425\n",
      "[60]\tvalidation_0-error:0.142\tvalidation_1-error:0.1395\n",
      "[61]\tvalidation_0-error:0.1405\tvalidation_1-error:0.1355\n",
      "[62]\tvalidation_0-error:0.1355\tvalidation_1-error:0.135\n",
      "[63]\tvalidation_0-error:0.135\tvalidation_1-error:0.134\n",
      "[64]\tvalidation_0-error:0.13\tvalidation_1-error:0.132\n",
      "[65]\tvalidation_0-error:0.1345\tvalidation_1-error:0.131\n",
      "[66]\tvalidation_0-error:0.1365\tvalidation_1-error:0.1315\n",
      "[67]\tvalidation_0-error:0.138\tvalidation_1-error:0.13\n",
      "[68]\tvalidation_0-error:0.1355\tvalidation_1-error:0.1275\n",
      "[69]\tvalidation_0-error:0.133\tvalidation_1-error:0.1295\n",
      "[70]\tvalidation_0-error:0.1285\tvalidation_1-error:0.1265\n",
      "[71]\tvalidation_0-error:0.132\tvalidation_1-error:0.1225\n",
      "[72]\tvalidation_0-error:0.1275\tvalidation_1-error:0.122\n",
      "[73]\tvalidation_0-error:0.1255\tvalidation_1-error:0.1205\n",
      "[74]\tvalidation_0-error:0.127\tvalidation_1-error:0.1195\n",
      "[75]\tvalidation_0-error:0.1265\tvalidation_1-error:0.117\n",
      "[76]\tvalidation_0-error:0.127\tvalidation_1-error:0.117\n",
      "[77]\tvalidation_0-error:0.1215\tvalidation_1-error:0.1155\n",
      "[78]\tvalidation_0-error:0.1215\tvalidation_1-error:0.114\n",
      "[79]\tvalidation_0-error:0.117\tvalidation_1-error:0.116\n",
      "[80]\tvalidation_0-error:0.121\tvalidation_1-error:0.118\n",
      "[81]\tvalidation_0-error:0.1185\tvalidation_1-error:0.117\n",
      "[82]\tvalidation_0-error:0.12\tvalidation_1-error:0.117\n",
      "[83]\tvalidation_0-error:0.119\tvalidation_1-error:0.115\n",
      "[84]\tvalidation_0-error:0.12\tvalidation_1-error:0.1105\n",
      "[85]\tvalidation_0-error:0.1185\tvalidation_1-error:0.113\n",
      "[86]\tvalidation_0-error:0.117\tvalidation_1-error:0.1165\n",
      "[87]\tvalidation_0-error:0.1155\tvalidation_1-error:0.1135\n",
      "[88]\tvalidation_0-error:0.119\tvalidation_1-error:0.1165\n",
      "[89]\tvalidation_0-error:0.1175\tvalidation_1-error:0.1115\n",
      "[90]\tvalidation_0-error:0.117\tvalidation_1-error:0.111\n",
      "[91]\tvalidation_0-error:0.116\tvalidation_1-error:0.108\n",
      "[92]\tvalidation_0-error:0.116\tvalidation_1-error:0.1055\n",
      "[93]\tvalidation_0-error:0.114\tvalidation_1-error:0.107\n",
      "[94]\tvalidation_0-error:0.1115\tvalidation_1-error:0.1035\n",
      "[95]\tvalidation_0-error:0.112\tvalidation_1-error:0.1035\n",
      "[96]\tvalidation_0-error:0.1115\tvalidation_1-error:0.105\n",
      "[97]\tvalidation_0-error:0.11\tvalidation_1-error:0.1\n",
      "[98]\tvalidation_0-error:0.1085\tvalidation_1-error:0.1005\n",
      "[99]\tvalidation_0-error:0.111\tvalidation_1-error:0.1035\n",
      "[100]\tvalidation_0-error:0.111\tvalidation_1-error:0.103\n",
      "[101]\tvalidation_0-error:0.1105\tvalidation_1-error:0.1065\n",
      "[102]\tvalidation_0-error:0.1085\tvalidation_1-error:0.104\n",
      "[103]\tvalidation_0-error:0.106\tvalidation_1-error:0.103\n",
      "[104]\tvalidation_0-error:0.1055\tvalidation_1-error:0.103\n",
      "[105]\tvalidation_0-error:0.11\tvalidation_1-error:0.1035\n",
      "[106]\tvalidation_0-error:0.1105\tvalidation_1-error:0.1035\n",
      "[107]\tvalidation_0-error:0.11\tvalidation_1-error:0.0995\n",
      "[108]\tvalidation_0-error:0.112\tvalidation_1-error:0.1025\n",
      "[109]\tvalidation_0-error:0.1075\tvalidation_1-error:0.105\n",
      "[110]\tvalidation_0-error:0.105\tvalidation_1-error:0.104\n",
      "[111]\tvalidation_0-error:0.1035\tvalidation_1-error:0.1005\n",
      "[112]\tvalidation_0-error:0.101\tvalidation_1-error:0.1035\n",
      "[113]\tvalidation_0-error:0.1035\tvalidation_1-error:0.1\n",
      "[114]\tvalidation_0-error:0.101\tvalidation_1-error:0.1005\n",
      "[115]\tvalidation_0-error:0.1\tvalidation_1-error:0.0985\n",
      "[116]\tvalidation_0-error:0.0995\tvalidation_1-error:0.0985\n",
      "[117]\tvalidation_0-error:0.1005\tvalidation_1-error:0.093\n",
      "[118]\tvalidation_0-error:0.1025\tvalidation_1-error:0.0955\n",
      "[119]\tvalidation_0-error:0.0995\tvalidation_1-error:0.094\n",
      "[120]\tvalidation_0-error:0.098\tvalidation_1-error:0.0935\n",
      "[121]\tvalidation_0-error:0.0985\tvalidation_1-error:0.095\n",
      "[122]\tvalidation_0-error:0.095\tvalidation_1-error:0.094\n",
      "[123]\tvalidation_0-error:0.096\tvalidation_1-error:0.0935\n",
      "[124]\tvalidation_0-error:0.0965\tvalidation_1-error:0.0935\n",
      "[125]\tvalidation_0-error:0.0985\tvalidation_1-error:0.094\n",
      "[126]\tvalidation_0-error:0.1\tvalidation_1-error:0.092\n",
      "[127]\tvalidation_0-error:0.1\tvalidation_1-error:0.0945\n",
      "[128]\tvalidation_0-error:0.1005\tvalidation_1-error:0.093\n",
      "[129]\tvalidation_0-error:0.097\tvalidation_1-error:0.0935\n",
      "[130]\tvalidation_0-error:0.097\tvalidation_1-error:0.0925\n",
      "[131]\tvalidation_0-error:0.0945\tvalidation_1-error:0.0905\n",
      "[132]\tvalidation_0-error:0.095\tvalidation_1-error:0.0905\n",
      "[133]\tvalidation_0-error:0.093\tvalidation_1-error:0.088\n",
      "[134]\tvalidation_0-error:0.096\tvalidation_1-error:0.089\n",
      "[135]\tvalidation_0-error:0.097\tvalidation_1-error:0.092\n",
      "[136]\tvalidation_0-error:0.1005\tvalidation_1-error:0.095\n",
      "[137]\tvalidation_0-error:0.1\tvalidation_1-error:0.094\n",
      "[138]\tvalidation_0-error:0.1005\tvalidation_1-error:0.0945\n",
      "[139]\tvalidation_0-error:0.095\tvalidation_1-error:0.089\n",
      "[140]\tvalidation_0-error:0.0945\tvalidation_1-error:0.087\n",
      "[141]\tvalidation_0-error:0.0955\tvalidation_1-error:0.087\n",
      "[142]\tvalidation_0-error:0.0975\tvalidation_1-error:0.087\n",
      "[143]\tvalidation_0-error:0.097\tvalidation_1-error:0.0855\n",
      "[144]\tvalidation_0-error:0.0965\tvalidation_1-error:0.085\n",
      "[145]\tvalidation_0-error:0.097\tvalidation_1-error:0.084\n",
      "[146]\tvalidation_0-error:0.097\tvalidation_1-error:0.0855\n",
      "[147]\tvalidation_0-error:0.0975\tvalidation_1-error:0.0845\n",
      "[148]\tvalidation_0-error:0.098\tvalidation_1-error:0.0845\n",
      "[149]\tvalidation_0-error:0.099\tvalidation_1-error:0.0855\n",
      "[150]\tvalidation_0-error:0.0975\tvalidation_1-error:0.084\n",
      "[151]\tvalidation_0-error:0.0925\tvalidation_1-error:0.082\n",
      "[152]\tvalidation_0-error:0.094\tvalidation_1-error:0.08\n",
      "[153]\tvalidation_0-error:0.094\tvalidation_1-error:0.0815\n",
      "[154]\tvalidation_0-error:0.0965\tvalidation_1-error:0.0825\n",
      "[155]\tvalidation_0-error:0.097\tvalidation_1-error:0.082\n",
      "[156]\tvalidation_0-error:0.097\tvalidation_1-error:0.082\n",
      "[157]\tvalidation_0-error:0.096\tvalidation_1-error:0.081\n",
      "[158]\tvalidation_0-error:0.095\tvalidation_1-error:0.0785\n",
      "[159]\tvalidation_0-error:0.0925\tvalidation_1-error:0.0815\n",
      "[160]\tvalidation_0-error:0.0905\tvalidation_1-error:0.082\n",
      "[161]\tvalidation_0-error:0.0915\tvalidation_1-error:0.0795\n",
      "[162]\tvalidation_0-error:0.092\tvalidation_1-error:0.0795\n",
      "[163]\tvalidation_0-error:0.092\tvalidation_1-error:0.081\n",
      "[164]\tvalidation_0-error:0.0905\tvalidation_1-error:0.082\n",
      "[165]\tvalidation_0-error:0.089\tvalidation_1-error:0.0815\n",
      "[166]\tvalidation_0-error:0.0885\tvalidation_1-error:0.082\n",
      "[167]\tvalidation_0-error:0.0875\tvalidation_1-error:0.082\n",
      "[168]\tvalidation_0-error:0.087\tvalidation_1-error:0.0815\n",
      "[169]\tvalidation_0-error:0.0865\tvalidation_1-error:0.084\n",
      "[170]\tvalidation_0-error:0.086\tvalidation_1-error:0.082\n",
      "[171]\tvalidation_0-error:0.088\tvalidation_1-error:0.0805\n",
      "[172]\tvalidation_0-error:0.0865\tvalidation_1-error:0.0805\n",
      "[173]\tvalidation_0-error:0.0885\tvalidation_1-error:0.0815\n",
      "[174]\tvalidation_0-error:0.0895\tvalidation_1-error:0.081\n",
      "[175]\tvalidation_0-error:0.0885\tvalidation_1-error:0.0805\n",
      "[176]\tvalidation_0-error:0.088\tvalidation_1-error:0.081\n",
      "[177]\tvalidation_0-error:0.0865\tvalidation_1-error:0.0815\n",
      "[178]\tvalidation_0-error:0.0875\tvalidation_1-error:0.08\n",
      "Stopping. Best iteration:\n",
      "[158]\tvalidation_0-error:0.095\tvalidation_1-error:0.0785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf1_ = xg.XGBClassifier(n_estimators=1000,\n",
    "                         max_depth=1, learning_rate=0.5,\n",
    "                         seed=seed).fit(X_train, y_train,\n",
    "                                        early_stopping_rounds=20,\n",
    "                                        eval_set=[(X_valid_1, y_valid_1),\n",
    "                                                  (X_valid_2, y_valid_2),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other methods\n",
    "\n",
    "### Stacking\n",
    "\n",
    "Every ensemble method comprises of essentially two phases:\n",
    "1. population of a dictionary of base learners (models, like classification\n",
    "trees in AdaBoost, or regression trees in GBRT);\n",
    "2. aggregation of the dictionary into a sinlge estimator;\n",
    "\n",
    "These phases are not necessarily separated: in Bagging and Random Forests\n",
    "they are (and so these can be done in parallel), in GBRT and AdaBoost they\n",
    "are not. In hte latter, the procedure is path dependent (serial), i.e. the\n",
    "dictionary is populated sequentially, so that each successive base estimator\n",
    "is learnt conditional on the current dictonary.\n",
    "\n",
    "Stacking is a method which allows to corectly construct second-level meta\n",
    "features using ML models atop the first level inputs. By `correctly` we\n",
    "mostly mean that there is little train-test leakeage, the resultng meta-\n",
    "features though not i.i.d, can still to a certain degree comply to the\n",
    "standard ML assumtions, and allow to focus on the `aggregation` step of\n",
    "ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General pipeline\n",
    "\n",
    "Let $Z = (X, y) = (x_i, y_i)_{i=1}^n$ be a dataset. The model construction\n",
    "and verification pipeline goes as follows:\n",
    "1. Split the dataset into **nonoverlapping** train and test datasets:\n",
    "$Z^{\\text{train}}$ and $Z^{\\text{test}}$;\n",
    "2. Apply `stacking` to get meta features, $\\mathcal{P}^{\\text{train}}$ (it is\n",
    "possible to include the first-level features as well);\n",
    "3. Split the meta-features, $\\mathcal{P}^{\\text{train}}$, into train and validation\n",
    "sets: fit on the former, test and select models on the latter;\n",
    "4. Use regularization at each stage to choose the best strategy against\n",
    "overfitting;\n",
    "\n",
    "For the final prediction:\n",
    "1. learn a regularized model on the whole $Z^{\\text{train}}$;\n",
    "2. get the meta-features, $\\mathcal{P}^{\\text{test}}$, on the inputs of $Z^{\\text{test}}$;\n",
    "3. fit a regularized aggregaton model on the whole train sample of\n",
    "meta-fetatures $\\mathcal{P}^{\\text{train}}$;\n",
    "4. use the fitted aggregation model to compute final prediction on\n",
    "the $\\mathcal{P}^{\\text{test}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave-one-out stacking\n",
    "\n",
    "The idea is to compute the meta feature of each example based on\n",
    "a base estimator learnt on the sample with that observation **knocked out**.\n",
    "\n",
    "Let $\\hat{f}^{-i}_m$ the $m$-th base estimator learnt on the sample $Z_{-i}$\n",
    "(without observation $z_i$). Then the meta-features $(\\hat{p}_{mi})_{i=1}^n$\n",
    "are given by\n",
    "$$ \\hat{p}_{mi} = \\hat{f}^{-i}_m(x_i) \\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $K$-fold stacking\n",
    "\n",
    "Leave-one-out stacking is in general computationally intensive, unless the\n",
    "base estimator is linear in the targets, in which case this can be done\n",
    "quite fast. A possible solution to this is inspiured by $K$-fold cross\n",
    "validation technique.\n",
    "\n",
    "Let $C_k\\subset\\{1,\\ldots, n\\}$ be the $k$-th fold in $K$-fold, and let\n",
    "$C_{-k}$ be the rest of the dataset $Z$: $C_{-k} = \\{i\\,:\\,i\\notin C_k\\}$.\n",
    "$C_k$ has approximately $\\frac{n}{K}$ observations. The dataset is **randomly\n",
    "shuffled** before being partitioned into $K$ folds.\n",
    "\n",
    "Define $\\hat{f}^{-k}_m$ as the $m$-th base estimator learnt on $Z^{-k}$\n",
    "given by $(z_i)_{i\\in C_{-k}}$. then the metafeatures are computed using\n",
    "\n",
    "$$ \\hat{p}_{mi} = \\hat{f}^{-k_i}_m(x_i) \\,, $$\n",
    "where $k_i$ is the unique index $k$ in the $K$-fold such that $i\\in C_k$.\n",
    "Basically we use the data outside the $k$-th fold, $C_{-k}$ to construct\n",
    "the meta-features inside the $k$-th fold, $C_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the meta-features\n",
    "\n",
    "For example, if we want to compute a linear combination of the regression\n",
    "estimators, we must solve the following optimization problem (unrestricted\n",
    "LS):\n",
    "$$ \\sum_{i=1}^n \\bigl(y_i - \\beta'\\hat{p}_i \\bigr)^2\\,, $$\n",
    "where $\\hat{p}_i = (\\hat{p}_{mi})_{m=1}^M$ and $\\beta, \\hat{p}_i \\in \\mathbb{R}^{m\\times1}$\n",
    "for all $i$.\n",
    "If a convex combination is required ($\\beta_m\\geq 0$ and $\\sum_{m=1}^M\\beta_m = 1$),\n",
    "one solves a constrained optimization problem. If pruning is desirable,\n",
    "then one should use either lasso ($L_1$ regularization), or subset-selection\n",
    "methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple $K$-fold stacking procedure. It estimates each model\n",
    "on the $K-1$ folds and predicts (with the specified method) the on the $K$-th\n",
    "fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def kfold_stack(estimators, X, y=None, predict_method=\"predict\",\n",
    "                n_folds=3, shuffle=False, random_state=None,\n",
    "                return_map=False):\n",
    "    \"\"\"\n",
    "    Splits the dataset into `n_folds` (K) consecutive folds (without shuffling\n",
    "    by default). Predictions are made on each fold while the K - 1 remaining\n",
    "    folds form the training set for the predictor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimators : list of estimators\n",
    "        The dictionary of estimators used to construct meta-features on\n",
    "        the dataset (X, y). A cloned copy of each estimator is fitted on\n",
    "        remainind data of each fold.\n",
    "\n",
    "    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "        Training vectors, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape = [n_samples], optional\n",
    "        Target values.\n",
    "        \n",
    "    predict_method : string, default=\"predict\"\n",
    "        The method of each estimator, to be used for predictiong the\n",
    "        meta features.\n",
    "\n",
    "    n_folds : int, default=3\n",
    "        Number of folds. Must be at least 2.\n",
    "\n",
    "    shuffle : boolean, optional\n",
    "        Whether to shuffle the data before splitting into batches.\n",
    "\n",
    "    random_state : None, int or RandomState\n",
    "        When shuffle=True, pseudo-random number generator state used for\n",
    "        shuffling. If None, use default numpy RNG for shuffling.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    meta : array-like, shape = [n_samples, ...]\n",
    "        Computed meta-features of each estimator.\n",
    "        \n",
    "    map : array-like\n",
    "        The map, identifying which estimator each column of `meta`\n",
    "        came from.\n",
    "        \n",
    "    \"\"\"\n",
    "    stacked_, index_ = list(), list()\n",
    "    folds_ = KFold(X.shape[0], n_folds=n_folds,\n",
    "                   shuffle=shuffle, random_state=random_state)\n",
    "    for rest_, fold_ in folds_:\n",
    "        fitted_ = [clone(est_).fit(X[rest_], y[rest_])\n",
    "                   for est_ in estimators]\n",
    "\n",
    "        predicted_ = [getattr(fit_, predict_method)(X[fold_])\n",
    "                      for fit_ in fitted_]\n",
    "        stacked_.append(np.stack(predicted_, axis=1))\n",
    "        index_.append(fold_)\n",
    "\n",
    "    stacked_ = np.concatenate(stacked_, axis=0)\n",
    "    meta_ = stacked_[np.concatenate(index_, axis=0)]\n",
    "    if not return_map:\n",
    "        return meta_\n",
    "\n",
    "    map_ = np.repeat(np.arange(len(estimators)),\n",
    "                 [pred_.shape[1] for pred_ in predicted_])\n",
    "    return meta_, map_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining base classifiers using Logistic Regression is a typical example of how\n",
    "first level features $x\\in \\mathcal{X}$ are transformed by $\\hat{f}_m:\\mathcal{X}\\mapsto \\mathbb{R}$\n",
    "into second-level meta features $(\\hat{f}_m(x))_{m=1}^M \\in \\mathbb{R}^M$, that\n",
    "are finally fed into a logistic regression, that does the utlimate prediction.\n",
    "\n",
    "Here $K$-fold stacking allows proper estimation of the second-level model for a\n",
    "classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = random_state.randint(0x7FFFFFFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the first-level predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "estimators_ = [\n",
    "    RandomForestClassifier(n_estimators=200, max_features=0.5, n_jobs=-1, random_state=seed),\n",
    "\n",
    "    GradientBoostingClassifier(n_estimators=200, max_depth=3, learning_rate=0.75, random_state=seed),\n",
    "\n",
    "    BaggingClassifier(n_estimators=200, base_estimator=DecisionTreeClassifier(max_depth=None),\n",
    "                      max_samples=0.5, n_jobs=-1, random_state=seed),\n",
    "\n",
    "    xg.XGBClassifier(n_estimators=200, max_depth=3, learning_rate=0.5, nthread=-1, seed=seed),\n",
    "\n",
    "## Both SVM and AdaBoost (on stumps) are very good here\n",
    "    SVC(kernel=\"rbf\", C=1.0, probability=True, gamma=1.0),\n",
    "\n",
    "    AdaBoostClassifier(n_estimators=200, base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                       random_state=seed),\n",
    "\n",
    "]\n",
    "\n",
    "estimator_names_ = [est_.__class__.__name__ for est_ in estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create meta features for the train set: using $K$-fold stacking estimate the class-1\n",
    "probabilities $\\hat{p}_i = (\\hat{p}_{mi})_{m=1}^M = (\\hat{f}^{-k_i}_m(x_i))_{m=1}^M$\n",
    "for every $i=1,\\ldots, n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_train_ = kfold_stack(estimators_, X_train, y_train,\n",
    "                          n_folds=5, predict_method=\"predict_proba\")[..., 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the whole train, create test set meta features: $p_j = (\\hat{f}_m(x_j))_{m=1}^M$\n",
    "for $j=1,\\ldots, n_{\\text{test}}$. Each $\\hat{f}_m$ is estimated on the whole train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fitted_ = [clone(est_).fit(X_train, y_train) for est_ in estimators_]\n",
    "meta_test_ = np.stack([fit_.predict_proba(X_test) for fit_ in fitted_], axis=1)[..., 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction error of each individual classifier (trained on the whole train dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier        0.1586\n",
       "GradientBoostingClassifier    0.0824\n",
       "BaggingClassifier             0.1554\n",
       "XGBClassifier                 0.0886\n",
       "SVC                           0.1672\n",
       "AdaBoostClassifier            0.0621\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1 - fit_.score(X_test, y_test) for fit_ in fitted_],\n",
    "          index=estimator_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using $10$-fold cross validation on the train dataset $(\\hat{p}_i, y_i)_{i=1}^n$,\n",
    "find the best $L_1$ regularization coefficient $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.50400, std: 0.00050, params: {'C': 0.001},\n",
       " mean: 0.91900, std: 0.01203, params: {'C': 0.01},\n",
       " mean: 0.92450, std: 0.00801, params: {'C': 0.10000000000000001},\n",
       " mean: 0.92650, std: 0.01101, params: {'C': 1.0},\n",
       " mean: 0.93300, std: 0.00927, params: {'C': 10.0},\n",
       " mean: 0.93300, std: 0.00844, params: {'C': 100.0},\n",
       " mean: 0.93350, std: 0.00803, params: {'C': 1000.0}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "grid_cv_ = GridSearchCV(LogisticRegression(penalty=\"l1\"),\n",
    "                        param_grid=dict(C=np.logspace(-3, 3, num=7)),\n",
    "                        n_jobs=-1, cv=5).fit(meta_train_, y_train)\n",
    "\n",
    "log_ = grid_cv_.best_estimator_\n",
    "grid_cv_.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights chosen by logisitc regression are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [-30.09115177] \n",
      "Base probability: 8.54237893895e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier        -3.933312\n",
       "GradientBoostingClassifier     2.765867\n",
       "BaggingClassifier              2.461472\n",
       "XGBClassifier                  2.072624\n",
       "SVC                            4.120341\n",
       "AdaBoostClassifier            52.288876\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import exp\n",
    "print \"Intercept:\", log_.intercept_, \"\\nBase probability:\", 1.0/(1+exp(-log_.intercept_))\n",
    "pd.Series(log_.coef_[0], index=estimator_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the final model works on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (l1) error: 0.0631\n"
     ]
    }
   ],
   "source": [
    "print \"Logistic Regression (l1) error:\", 1 - log_.score(meta_test_, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very basic method of constructing an aggregated classifier from a finite dictionary.\n",
    "\n",
    "Let $\\mathcal{V}$ be the set of classifiers (voters), with each calssifier's class probablilites\n",
    "given by $\\hat{f}_v:\\mathcal{X}\\mapsto\\mathbb{[0,1]}^K$ and prediction\n",
    "$\\hat{g}_v(x) = \\mathtt{MAJ}(\\hat{f}_v(x))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority vote over $K$ candidates with weights $(w_k)_{k=1}^K\\in \\mathbb{R}$ is defined as\n",
    "$$ \\mathtt{MAJ}(w) = \\mathop{\\text{argmax}}_{k=1\\,\\ldots, K} w_k \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard voting collects label-prediction of each voter, counts the voting proportions and,\n",
    "then predict the label with the most votes. Mathematically the following aggregation is\n",
    "used:\n",
    "$$ \\hat{g}^{\\text{maj}}_\\mathcal{V}(x)\n",
    "    = \\mathtt{MAJ}\\Bigl( W^{-1} \\sum_{v\\in \\mathcal{V}} w_v e_{\\hat{g}_v(x)} \\Bigr)\n",
    "    \\,, $$\n",
    "where $e_k$ is the $k$-th unit vector in $\\{0,1\\}^{K\\times 1}$, and $W = \\sum_{v\\in \\mathcal{V}} w_v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft voting uses the class-probabilities functions directly: it computes the weighted\n",
    "average probability of each class over all voters, and then selects the class with the\n",
    "highest posterior probability. Namely,\n",
    "$$ \\hat{g}^{\\text{maj}}_\\mathcal{V}(x)\n",
    "    = \\mathtt{MAJ}\\bigl( W^{-1} \\sum_{v\\in \\mathcal{V}} w_v \\hat{f}_v(x) \\bigr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Bagging, if the base classifiers are well calibrated, then `hard` voting\n",
    "will ovrestimate probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VotingClassifier** options:\n",
    "- **estimators** -- The list of classifiers;\n",
    "- **voting** -- Vote aggregation strategy:\n",
    "    * _\"hard\"_ -- use predicted class labels for majority voting;\n",
    "    * _\"soft\"_ -- use sums of the predicted probalities for determine\n",
    "    the most likely class;\n",
    "- **weights** -- weight the occurrences of predicted class labels (`hard` voting)\n",
    "or class probabilities while averaging (`soft` voting);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard voting classifier error: 0.0754\n",
      "Soft voting classifier error: 0.073\n"
     ]
    }
   ],
   "source": [
    "clf1_ = VotingClassifier(list(zip(estimator_names_, estimators_)),\n",
    "                         voting=\"hard\", weights=None).fit(X_train, y_train)\n",
    "\n",
    "clf2_ = VotingClassifier(list(zip(estimator_names_, estimators_)),\n",
    "                         voting=\"soft\", weights=None).fit(X_train, y_train)\n",
    "print \"Hard voting classifier error:\", 1 - clf1_.score(X_test, y_test)\n",
    "print \"Soft voting classifier error:\", 1 - clf2_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the test error as a function of the size of the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stump_ = DecisionTreeClassifier(max_depth=1).fit(X_train, y_train)\n",
    "t224_ = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=224).fit(X_train, y_train)\n",
    "\n",
    "ada_ = AdaBoostClassifier(n_estimators=400, random_state=random_state).fit(X_train, y_train)\n",
    "bag_ = BaggingClassifier(n_estimators=400, random_state=random_state,\n",
    "                         n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "rdf_ = RandomForestClassifier(n_estimators=400, random_state=random_state,\n",
    "                              n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the prediction as a function of the memebers in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_staged_accuracy(ensemble, X, y):\n",
    "    prob_ = np.stack([est_.predict_proba(X)\n",
    "                      for est_ in ensemble.estimators_],\n",
    "                     axis=1).astype(float)\n",
    "    pred_ = np.cumsum(prob_[..., 1] > 0.5, axis=1).astype(float)\n",
    "    pred_ /= 1 + np.arange(ensemble.n_estimators).reshape((1, -1))\n",
    "    return np.mean((pred_ > .5).astype(int) == y[:, np.newaxis], axis=0)\n",
    "\n",
    "bag_scores_ = get_staged_accuracy(bag_, X_test, y_test)\n",
    "rdf_scores_ = get_staged_accuracy(rdf_, X_test, y_test)\n",
    "\n",
    "ada_scores_ = np.array(list(ada_.staged_score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x110c36bd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFwCAYAAAB3kDgfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYYqDhUwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1107c8210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_ylim(0, 0.50) ; ax.set_xlim(-10, ada_.n_estimators)\n",
    "ax.plot(1+np.arange(ada_.n_estimators), 1-ada_scores_, c=\"k\", label=\"AdaBoost\")\n",
    "ax.plot(1+np.arange(bag_.n_estimators), 1-bag_scores_, c=\"m\", label=\"Bagged DT\")\n",
    "ax.plot(1+np.arange(bag_.n_estimators), 1-rdf_scores_, c=\"c\", label=\"RF\")\n",
    "ax.axhline(y=1 - stump_.score(X_test, y_test), c=\"r\", linestyle=\"--\", label=\"stump\")\n",
    "ax.axhline(y=1 - t224_.score(X_test, y_test), c=\"b\", linestyle=\"--\", label=\"DT $J=224$\")\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An excerpt from **HTF, p. 340**\n",
    "> Here the weak classifier is just a two terminal-node classification tree.\n",
    "Applying this classifier alone to the training data set yields a very poor\n",
    "test set error rate of 46.5%, compared to 50% for random guessing. However,\n",
    "as boosting iterations proceed the error rate steadily decreases, reaching\n",
    "5.8% after 400 iterations. Thus, boosting this simple very weak classifier\n",
    "reduces its prediction error rate by almost a factor of four. It also\n",
    "outperforms a single large classification tree (error rate 26.7%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see that the Bagged Decision Tree and the Random Forest Classifiers\n",
    "converge much faster to their asymptotic test error: it takes less than 25\n",
    "trees for Bagging, and approximately 25 for Random forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
