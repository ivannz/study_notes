@inproceedings{burnaevV14,
author={Burnaev, Evgeny and Vovk, Vladimir},
title={Efficiency of conformalized ridge regression},
booktitle={Proceedings of The 27th Conference on Learning Theory, {COLT} 2014, Barcelona, Spain, June 13-15, 2014},
pages={605--622},
year={2014},
crossref={DBLP:conf/colt/2014},
url={http://jmlr.org/proceedings/papers/v35/burnaev14.html},
timestamp={Thu, 11 Sep 2014 07:28:56 +0200},
biburl={http://dblp.uni-trier.de/rec/bib/conf/colt/BurnaevV14},
bibsource={dblp computer science bibliography, http://dblp.org}
}

@article{shafervovk2008,
author={Shafer, Glenn and Vovk, Vladimir},
title={A Tutorial on Conformal Prediction},
journal={The Journal of Machine Learning Research},
issue_date={6/1/2008},
volume={9},
month={jun},
year={2008},
issn={1532-4435},
pages={371--421},
numpages={51},
url={http://dl.acm.org/citation.cfm?id=1390681.1390693},
acmid={1390693},
publisher={JMLR.org},
} 

@book{vovk2005,
title={Algorithmic Learning in a Random World},
author={Vovk, V. and Gammerman, A. and Shafer, G.},
isbn={9780387001524},
lccn={2005042556},
year={2005},
publisher={Springer}
}

@article{steinwart2002,
title={On the influence of the kernel on the consistency of support vector machines},
author={Steinwart, Ingo},
journal={The Journal of Machine Learning Research},
volume={2},
pages={67--93},
year={2002},
publisher={JMLR. org}
}

@incollection{pcw20005a7,
title={Estimating predictive variances with kernel ridge regression},
author={Cawley, Gavin C and Talbot, Nicola LC and Chapelle, Olivier},
booktitle={Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment},
pages={56--77},
year={2006},
publisher={Springer}
}

@inproceedings{tarassenko1995,
title={Novelty detection for the identification of masses in mammograms},
author={Tarassenko, Lionel and Hayton, P and Cerneaz, N and Brady, M},
booktitle={Artificial Neural Networks, 1995., Fourth International Conference on},
pages={442--447},
year={1995},
organization={IET}
}

@incollection{quinn2007,
title={Known unknowns: Novelty detection in condition monitoring},
author={Quinn, John A and Williams, Christopher KI},
booktitle={Pattern Recognition and Image Analysis},
pages={1--6},
year={2007},
publisher={Springer}
}

@inproceedings{clifton2011,
title={Identification of patient deterioration in vital-sign data using one-class support vector machines},
author={Clifton, Lei and Clifton, David A and Watkinson, Peter J and Tarassenko, Lionel},
booktitle={Computer Science and Information Systems (FedCSIS), 2011 Federated Conference on},
pages={125--131},
year={2011},
organization={IEEE}
}

@article{tarassenko2009,
title={Novelty detection},
author={Tarassenko, Lionel and Clifton, David A and Bannister, Peter R and King, Steve and King, Dennis},
journal={Encyclopedia of Structural Health Monitoring},
year={2009},
publisher={Wiley Online Library}
}

@article{surace2010,
title={Novelty detection in a changing environment: a negative selection approach},
author={Surace, Cecilia and Worden, Keith},
journal={Mechanical Systems and Signal Processing},
volume={24},
number={4},
pages={1114--1128},
year={2010},
publisher={Elsevier}
}

@article{patcha2007,
title={An overview of anomaly detection techniques: Existing solutions and latest technological trends},
author={Patcha, Animesh and Park, Jung-Min},
journal={Computer networks},
volume={51},
number={12},
pages={3448--3470},
year={2007},
publisher={Elsevier}
}

@article{jyothsna2011,
title={A review of anomaly based intrusion detection systems},
author={Jyothsna, V and Prasad, VV Rama and Prasad, K Munivara},
journal={International Journal of Computer Applications},
volume={28},
number={7},
pages={26--35},
year={2011},
publisher={Citeseer}
}

@inproceedings{dutta2007,
title={Distributed Top-K Outlier Detection from Astronomy Catalogs using the DEMAC System.},
author={Dutta, Haimonti and Giannella, Chris and Borne, Kirk D and Kargupta, Hillol},
booktitle={SDM},
pages={473--478},
year={2007},
organization={SIAM}
}

@article{chandola2009,
title={Anomaly detection: A survey},
author={Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
journal={ACM computing surveys (CSUR)},
volume={41},
number={3},
pages={15},
year={2009},
publisher={ACM}
}

@book{vic1994,
title={Outliers in Statistical Data},
author={Barnett, Vic and Lewis, Toby},
isbn={9780471930945},
lccn={93029289},
series={Wiley Series in Probability \& Statistics},
year={1994},
publisher={Wiley},
pages={293}
}

@article{markou2003,
title={Novelty detection: a review -- part 1: statistical approaches},
author={Markou, Markos and Singh, Sameer},
journal={Signal processing},
volume={83},
number={12},
pages={2481--2497},
year={2003},
publisher={Elsevier}
}

@book{duda2012,
title={Pattern classification},
author={Duda, Richard O and Hart, Peter E and Stork, David G},
isbn={9780471056690},
year={2012},
publisher={John Wiley \& Sons}
}

@inproceedings{miljkovic2010,
title={Review of novelty detection methods},
author={Miljkovi{\'c}, Dubravko},
booktitle={MIPRO, 2010 Proceedings of the 33rd International Convention},
pages={593--598},
year={2010},
organization={IEEE}
}

@article{grubbs1969,
title={Procedures for detecting outlying observations in samples},
author={Grubbs, Frank E},
journal={Technometrics},
volume={11},
number={1},
pages={1--21},
year={1969},
publisher={Taylor \& Francis}
}

@inproceedings{aggarwal2008,
title={Outlier Detection with Uncertain Data.},
author={Aggarwal, Charu C and Philip, S Yu},
booktitle={Proceedings of the SIAM International Conference on Data Mining},
pages={483--493},
year={2008},
organization={SIAM}
}

@article{chow1970,
title={On optimum recognition error and reject tradeoff},
author={Chow, Chao K},
journal={Information Theory, IEEE Transactions on},
volume={16},
number={1},
pages={41--46},
year={1970},
publisher={IEEE}
}

@article{scott2008,
title={Kernel density estimators},
author={Scott, David W},
journal={Multivariate Density Estimation: Theory, Practice, and Visualization},
pages={125--193},
year={2008},
publisher={Wiley Online Library}
}

@article{breunig2000,
author={Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, J\"{o}rg},
title={LOF: Identifying Density-based Local Outliers},
journal={SIGMOD Rec.},
issue_date={June 2000},
volume={29},
number={2},
month=may,
year={2000},
issn={0163-5808},
pages={93--104},
numpages={12},
url={http://doi.acm.org/10.1145/335191.335388},
doi={10.1145/335191.335388},
acmid={335388},
publisher={ACM},
address={New York, NY, USA},
keywords={database mining, outlier detection},
}

@inproceedings{kriegel2009,
author={Kriegel, Hans-Peter and Kr\"{o}ger, Peer and Schubert, Erich and Zimek, Arthur},
title={LoOP: Local Outlier Probabilities},
booktitle={Proceedings of the 18th ACM Conference on Information and Knowledge Management},
series={CIKM '09},
year={2009},
isbn={978-1-60558-512-3},
location={Hong Kong, China},
pages={1649--1652},
numpages={4},
url={http://doi.acm.org/10.1145/1645953.1646195},
doi={10.1145/1645953.1646195},
acmid={1646195},
publisher={ACM},
address={New York, NY, USA},
keywords={outlier detection},
}

@inproceedings{hautamaki2004,
author={Hautamaki, Ville and Karkkainen, Ismo and Franti, Pasi},
title={Outlier Detection Using k-Nearest Neighbour Graph},
booktitle={Proceedings of the Pattern Recognition, 17th International Conference on (ICPR'04) Volume 3 - Volume 03},
series={ICPR '04},
year={2004},
isbn={0-7695-2128-2},
pages={430--433},
numpages={4},
url={http://dx.doi.org/10.1109/ICPR.2004.671},
doi={10.1109/ICPR.2004.671},
acmid={1021000},
publisher={IEEE Computer Society},
address={Washington, DC, USA},
}

@article{zhang2006,
author={Zhang, Ji and Wang, Hai},
title={Detecting Outlying Subspaces for High-dimensional Data: The New Task, Algorithms, and Performance},
journal={Knowl. Inf. Syst.},
issue_date={October 2006},
volume={10},
number={3},
month=oct,
year={2006},
issn={0219-1377},
pages={333--355},
numpages={23},
url={http://dx.doi.org/10.1007/s10115-006-0020-z},
doi={10.1007/s10115-006-0020-z},
acmid={1165872},
publisher={Springer-Verlag New York, Inc.},
address={New York, NY, USA},
keywords={Dynamic subspace search, High-dimensional data, Outlier detection, Outlying subspace},
}

@inproceedings{srivastava2005,
author={A. N. Srivastava and B. Zane-Ulman}, 
booktitle={2005 IEEE Aerospace Conference}, 
title={Discovering recurring anomalies in text reports regarding complex space systems}, 
year={2005}, 
pages={3853-3862}, 
abstract={Many existing complex space systems have a significant amount of historical maintenance and problem data bases that are stored in unstructured text forms. The problem that we address in this paper is the discovery of recurring anomalies and relationships between problem reports that may indicate larger systemic problems. We illustrate our techniques on data from discrepancy reports regarding software anomalies in the Space Shuttle. These free text reports are written by a number of different people, thus the emphasis and wording vary considerably. We test four automatic methods of anomaly detection in text that are popular in the current literature on text mining. The first method that we describe is k-means or Gaussian mixture model and its application to the term-document matrix. The second method is the Sammon nonlinear map, which projects high dimensional document vectors into two dimensions for visualization and clustering purposes. The third method is based on an analysis of the results of applying a clustering method, expectation maximization on a mixture of von Mises Fisher distributions that represents each document as a point on a high dimensional sphere. In this space, we perform clustering to obtain sets of similar documents. The results are derived from a new method known as spectral clustering, where vectors from the term-document matrix are embedded in a high dimensional space for clustering. The paper concludes with recommendations regarding the development of an operational text mining system for analysis of problem reports that arise from complex space systems. We also contrast such systems with general purpose text mining systems, illustrating the areas in which this system needs to be specified for the space domain}, 
keywords={aerospace engineering;aircraft maintenance;classification;text analysis;word processing;Gaussian mixture model;Sammon nonlinear map;Space Shuttle;anomaly detection;automatic methods;complex space systems;discrepancy reports;expectation maximization;historical maintenance;k-means method;problem data bases;problem reports;recurring anomalies;software anomalies;spectral clustering;term-document matrix;text mining;text reports;von Mises Fisher distributions;Aerospace testing;Algorithm design and analysis;Data analysis;Data mining;Functional analysis;Information analysis;Manufacturing processes;Sensor systems;Text mining;Thermal sensors}, 
doi={10.1109/AERO.2005.1559692}, 
ISSN={1095-323X}, 
month={March},}

@inproceedings{srivastava2006,
author={A. N. Srivastava}, 
booktitle={2006 IEEE Aerospace Conference}, 
title={Enabling the discovery of recurring anomalies in aerospace problem reports using high-dimensional clustering techniques}, 
year={2006}, 
pages={17 pp.-}, 
abstract={This paper describes the results of a significant research and development effort conducted at NASA Ames Research Center to develop new text mining algorithms to discover anomalies in free-text reports regarding system health and safety of two aerospace systems. We discuss two problems of significant import in the aviation industry. The first problem is that of automatic anomaly discovery concerning an aerospace system through the analysis of tens of thousands of free-text problem reports that are written about the system. The second problem that we address is that of automatic discovery of recurring anomalies, i.e., anomalies that may be described in different ways by different authors, at varying times and under varying conditions, but that are truly about the same part of the system. The intent of recurring anomaly identification is to determine project or system weakness or high-risk issues. The discovery of recurring anomalies is a key goal in building safe, reliable, and cost-effective aerospace systems. We address the anomaly discovery problem on thousands of free-text reports using two strategies: (1) as an unsupervised learning problem where an algorithm takes free-text reports as input and automatically groups them into different bins, where each bin corresponds to a different unknown anomaly category; and (2) as a supervised learning problem where the algorithm classifies the free-text reports into one of a number of known anomaly categories. We then discuss the application of these methods to the problem of discovering recurring anomalies. In fact, because recurring anomalies tend to have very small cluster sizes, we explore new methods and measures to enhance the original approach for anomaly detection. We present our results on the identification of recurring anomalies in problem reports concerning two aerospace systems as well as benchmark data sets that are widely used in the field of text mining. The first system is the Aviation Safety Reporting Sys- - tem (ASRS) database, which contains several hundred-thousand free text reports filed by commercial pilots concerning safety issues on commercial airlines. The second aerospace system we analyze is the NASA Space Shuttle problem reports as represented in the CARS data set, which consists of 7440 NASA Shuttle problem reports. We show significant classification accuracies on both of these systems as well as compare our results with reports classified into anomaly categories by field experts}, 
keywords={data mining;database management systems;space research;aerospace problem reports;aerospace systems;anomaly discovery problem;aviation industry;high dimensional clustering;recurring anomalies identification;supervised learning problem;text mining algorithms;unsupervised learning problem;Aerospace industry;Aerospace safety;Air safety;Clustering algorithms;Health and safety;NASA;Research and development;Supervised learning;Text mining;Unsupervised learning}, 
doi={10.1109/AERO.2006.1656136}, 
ISSN={1095-323X}, 
month={},}

@inproceedings{scholkopf1999,
author={Sch{\"o}lkopf, Bernhard and Williamson, Robert C and Smola, Alexander J and Shawe-Taylor, John and Platt, John C and others},
title={Support Vector Method for Novelty Detection},
booktitle={NIPS},
volume={12},
pages={582--588},
year={1999}
}

@article{augusteijn2002,
author={Augusteijn, M. F. and Folkert, B. A.},
title={Neural network classification and novelty detection},
journal={International Journal of Remote Sensing},
volume={23},
number={14},
pages={2891-2902},
year={2002},
doi={10.1080/01431160110055804},
URL={http://www.tandfonline.com/doi/abs/10.1080/01431160110055804},
eprint={http://www.tandfonline.com/doi/pdf/10.1080/01431160110055804},
abstract={Novel data are data belonging to classes not included in the training set of the classifier. Neural network classifiers tend to put these data into the category of patterns that most resemble the novel ones, rather than label them as novel or unknown. This work investigates the ability of the back-propagation neural network architecture to detect novel patterns and concludes that this method is unsuitable for this task. It also explores the applicability of a different neural network architecture, the probabilistic neural network, and finds that this method shows superior performance as an overall classifier when compared to back-propagation and, in addition, is able to identify novel patterns.}
}

@inproceedings{hawkins2002,
author={Hawkins, Simon and He, Hongxing and Williams, Graham J. and Baxter, Rohan A.},
editor={Kambayashi, Yahiko and Winiwarter, Werner and Arikawa, Masatoshi},
title={Outlier Detection Using Replicator Neural Networks},
booktitle={Data Warehousing and Knowledge Discovery: 4th International Conference, DaWaK 2002 Aix-en-Provence, France, September 4--6, 2002 Proceedings},
series={DaWaK 2000},
year={2002},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={170--180},
isbn={978-3-540-46145-6},
doi={10.1007/3-540-46145-0_17},
url={http://dx.doi.org/10.1007/3-540-46145-0_17}
}

@inproceedings{williams2002,
author={G. Williams and R. Baxter and Hongxing He and S. Hawkins and Lifang Gu}, 
booktitle={Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International Conference on}, 
title={A comparative study of RNN for outlier detection in data mining}, 
year={2002}, 
pages={709-712}, 
abstract={We have proposed replicator neural networks (RNNs) for outlier detection. We compare RNN for outlier detection with three other methods using both publicly available statistical datasets (generally small) and data mining datasets (generally much larger and generally real data). The smaller datasets provide insights into the relative strengths and weaknesses of RNNs. The larger datasets in particular test scalability and practicality of application.}, 
keywords={data mining;neural nets;statistical databases;very large databases;data mining datasets;outlier detection;replicator neural networks;scalability;statistical datasets;Australia;Data mining;Databases;Feedforward systems;Helium;Intelligent networks;Multi-layer neural network;Neural networks;Recurrent neural networks;Testing}, 
doi={10.1109/ICDM.2002.1184035}, 
month={},}

@article{hoffmann2007,
author={Hoffmann, Heiko},
title={Kernel PCA for Novelty Detection},
journal={Pattern Recogn.},
issue_date={March, 2007},
volume={40},
number={3},
month=mar,
year={2007},
issn={0031-3203},
pages={863--874},
numpages={12},
url={http://dx.doi.org/10.1016/j.patcog.2006.07.009},
doi={10.1016/j.patcog.2006.07.009},
acmid={1219565},
publisher={Elsevier Science Inc.},
address={New York, NY, USA},
keywords={Breast cancer, Handwritten digit, Kernel method, Novelty detection, PCA},
} 

@techreport{shyu2003,
title={A novel anomaly detection scheme based on principal component classifier},
author={Shyu, Mei-Ling and Chen, Shu-Ching and Sarinnapakorn, Kanoksri and Chang, LiWu},
year={2003},
institution={DTIC Document}
}

@inbook{jolliffe2014,
title={Principal Component Analysis},
author={Jolliffe, Ian},
author={},
publisher={John Wiley \& Sons, Ltd},
isbn={9781118445112},
url={http://dx.doi.org/10.1002/9781118445112.stat06472},
doi={10.1002/9781118445112.stat06472},
keywords={dimension reduction, factor analysis, multivariate analysis, variance maximization},
booktitle={Wiley StatsRef: Statistics Reference Online},
year={2014},
abstract={When large multivariate datasets are analyzed, it is often desirable to reduce their dimensionality. Principal component analysis is one technique for doing this. It replaces the p original variables by a smaller number, q, of derived variables, the principal components, which are linear combinations of the original variables. Often, it is possible to retain most of the variability in the original variables with q very much smaller than p. Despite its apparent simplicity, principal component analysis has a number of subtleties, and it has many uses and extensions. A number of choices associated with the technique are briefly discussed, namely, covariance or correlation, how many components, and different normalization constraints, as well as confusion with factor analysis. Various uses and extensions are outlined.},
}

@article{scholkopf1998,
author={Sch\"{o}lkopf, Bernhard and Smola, Alexander and M\"{u}ller, Klaus-Robert},
title={Nonlinear Component Analysis As a Kernel Eigenvalue Problem},
journal={Neural Comput.},
issue_date={July 1, 1998},
volume={10},
number={5},
month=jul,
year={1998},
issn={0899-7667},
pages={1299--1319},
numpages={21},
url={http://dx.doi.org/10.1162/089976698300017467},
doi={10.1162/089976698300017467},
acmid={295960},
publisher={MIT Press},
address={Cambridge, MA, USA},
} 

@article{tax1999,
author={Tax, David M.J and Duin, Robert P.W},
title={Support vector domain description},
journal={Pattern Recognition Letters},
volume={20},
number={11–13},
pages={1191 - 1199},
year={1999},
note={},
issn={0167-8655},
doi={http://dx.doi.org/10.1016/S0167-8655(99)00087-2},
url={http://www.sciencedirect.com/science/article/pii/S0167865599000872},
keywords={Data domain description},
keywords={Outlier detection},
keywords={One-class classification},
keywords={Support vector machines},
abstract={This paper shows the use of a data domain description method, inspired by the support vector machine by Vapnik, called the support vector domain description (SVDD). This data description can be used for novelty or outlier detection. A spherically shaped decision boundary around a set of objects is constructed by a set of support vectors describing the sphere boundary. It has the possibility of transforming the data to new feature spaces without much extra computational cost. By using the transformed data, this \{SVDD\} can obtain more flexible and more accurate data descriptions. The error of the first kind, the fraction of the training objects which will be rejected, can be estimated immediately from the description without the use of an independent test set, which makes this method data efficient. The support vector domain description is compared with other outlier detection methods on real data.}
}

@article{manevitz2002,
author={Manevitz, Larry M. and Yousef, Malik},
title={One-class Svms for Document Classification},
journal={J. Mach. Learn. Res.},
issue_date={3/1/2002},
volume={2},
month={March},
year={2002},
issn={1532-4435},
pages={139--154},
numpages={16},
url={http://dl.acm.org/citation.cfm?id=944790.944808},
acmid={944808},
publisher={JMLR.org},
}

@book{scholkopf2002,
title={Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
author={Sch{\"o}lkopf, B. and Smola, A.J.},
isbn={9780262194754},
lccn={2001095750},
series={Adaptive computation and machine learning},
year={2002},
publisher={MIT Press}
}

@book{shawe2004,
title={Kernel Methods for Pattern Analysis},
author={Shawe-Taylor, J. and Cristianini, N.},
isbn={9780521813976},
lccn={30695900},
series={Kernel Methods for Pattern Analysis},
year={2004},
publisher={Cambridge University Press}
}

@book{rasmussen2006,
title={Gaussian Processes for Machine Learning},
author={Rasmussen, C.E. and Williams, C.K.I.},
isbn={9780262182539},
lccn={2005053433},
series={Adaptative computation and machine learning series},
year={2006},
publisher={University Press Group Limited}
}

@incollection{Laxhammar2014,
title = "Chapter 4 - Anomaly Detection ",
editor = "Balasubramanian, Vineeth N. and , and Ho, Shen-Shyang and ,  and Vovk, Vladimir ",
booktitle = "Conformal Prediction for Reliable Machine Learning ",
publisher = "Morgan Kaufmann",
edition = "",
address = "Boston",
year = "2014",
pages = "71 - 97",
isbn = "978-0-12-398537-8",
doi = "http://dx.doi.org/10.1016/B978-0-12-398537-8.00004-3",
url = "http://www.sciencedirect.com/science/article/pii/B9780123985378000043",
author = "Rikard Laxhammar",
keywords = "Anomaly Detection",
keywords = "One-Class Classification",
keywords = "Unsupervised Learning",
keywords = "Semi-supervised Learning",
keywords = "Inductive Conformal Predictors",
keywords = "Hausdorff Distance",
keywords = "k-Nearest Neighbor",
keywords = "Sequential Data ",
abstract = "Abstract This chapter presents an extension of conformal prediction for anomaly detection applications. It includes the presentation and discussion of the Conformal Anomaly Detector (CAD) and the computationally more efficient Inductive Conformal Anomaly Detector (ICAD), which are general algorithms for unsupervised or semi-supervised and offline or online anomaly detection. One of the key properties of \{CAD\} and \{ICAD\} is that the rate of detected anomalies is well calibrated in the online setting under the randomness assumption. Similar to conformal prediction, the choice of Nonconformity Measure (NCM) is of central importance for the classification performance of \{CAD\} and ICAD. A novel \{NCM\} for examples that are represented as sets of points is presented. One of the key properties of this NCM, which is known as the directed Hausdorff k -nearest neighbors (DH-kNN) NCM, is that the p-value for an incomplete test example monotonically decreases as more data points are observed. An instance of \{CAD\} based on DH-kNN NCM, known as the sequential Hausdorff nearest neighbor conformal anomaly detector (SHNN-CAD), is presented and discussed for sequential anomaly detection applications. We also investigate classification performance results for the unsupervised online SHNN-CAD on a public dataset of labeled trajectories. "
}
