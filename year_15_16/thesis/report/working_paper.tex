\documentclass{ITaSconf}
% \documentclass[a4paper]{article}
% \documentclass[a4paper,14pt]{extarticle}

\usepackage[utf8]{inputenc}

\usepackage{geometry}
\usepackage{fullpage}

\usepackage[mathcal]{euscript}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}
\usepackage{algorithm2e}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{float}

\newcommand{\ex}{\mathop{\mathbb{E}}\nolimits}
\newcommand{\pr}{\mathop{\mathbb{P}}\nolimits}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\BigO}{\mathcal{O}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\nil}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\diag}{\mathop{\text{diag}}\nolimits}

\title{Conformalized Kernel Ridge Regression}
\author{Nazarov, I. N., Burnaev, E. V.}

\begin{document}
\maketitle
% \tableofcontents
% \clearpage


\section{Introduction} % (fold)
\label{sec:introduction}

Regression methods are concerned with studying pattern of dependence between the
target variable $y$ and the input variable $x$ based on a finite train sample of 
possibly noisy observations $(X, y)$ from $\Omega\times \Ycal \subseteq \Xcal\times\Real$.
These methods postulate that there exits a stable unobserved relationship $y_x = f(x)$
and observations of $y$ at any given $x\in\Omega$ are corrupted by additive noise:
\begin{equation} \label{eq:signal_model}
  y_x = f(x) + \xi_x \,,
\end{equation}
with $\ex \xi_x = 0$ for all $x\in \Omega$. The estimated relation $\hat{f}$ of $f$
is used to produce point predictions of $y$ at any $x$. However, without additional
distributional assumptions regression methods do note readily yield any measure of
uncertainty in the prediction.

\noindent \textbf{SOMETHING}

In this study, we demonstrate the applicability of conformal prediction in the case
of offline batch learning, and show that when model assumptions do hold, the conformal
confidence sets of the kernel ridge regression do not perform worse than the prediction
confidence intervals of the Gaussian Process Regression with the same kernel (Bayesian
interpretation of the Kernel ridge regression).

% section introduction (end)

\section{Kernel ridge regression} % (fold)
\label{sec:kernel_ridge_regression}

Suppose $\Xcal = \Real^{d\times 1}$. Ordinary least squares regression assumes that
there exists $\beta\in \Real^{d\times 1}$ such that $f(x) = x'\beta$. If the underlying
relationship is non-linear, the trained linear model will have high bias, i.e.
it will make statistically non-negligible systematic errors in its predictions.
However, the linear formulation does not limit the applicability of the ordinary
regression, since by choosing a richer input-feature mapping $\phi: \Omega \mapsto
\Real^{p\times 1}$ it is possible to cover the case of nonlinear target-observation
relationships. The map $\phi$ defines a basis with respect to which the unknown
underlying $f$ is approximated. For instance, the mapping $x\mapsto (\prod_{i=1}^d
x_i^{m_i})_{m\in M}$, with integers $M=(m_i)_{i=1}^d\geq 0$ and $\sum_{i=1}^d m_i \leq p$,
permits approximation of polynomial $f$.

In general, the larger the feature space (i.e. the more diverse ) the more expressive
the regression model is and thus the wider the class of problems it is applicable
to. However, more coefficients in the feature expansion of $f$ make the regression
problem overdetermined and, thus prone to over-fitting. This increases the variance
of the trained regression, thereby reducing it generalization ability. This can be
remedied by restricting the set of functions available for approximation by regularizing,
or shrinking the coefficients $\beta$ in the expansion.

Given train-set observations $(x_i, y_i)_{i=1}^n \in \Real^{d\times 1}\times \Real$,
with $x_i$ collected row-wise in a design matrix $X\in \Real^{n\times d}$, the target
vector -- $y=(y_i)_{i=1}^n \in \Real^{n\times 1}$, and a regularization parameter
$\lambda > 0$, the sample-space \textbf{R}idge \textbf{R}egression (RR) problem solves
this minimization problem
\begin{equation}
  \| y - X\beta - \one\beta_0 \|^2 + \lambda \|\beta\|^2
    \to \min_{\beta_0\in \Real, \beta \in \Real^{d\times 1}} \,,
\end{equation}
where $\one \in \Real^{n\times 1}$ is a vector of ones. Basically ridge regression
is linear least squares regression with $L_2$-norm regularization.

The intercept term is given by
\begin{equation}
  \hat{\beta}_0 = \bar{y} - \bar{X} \hat{\beta} \,,
\end{equation}
where $\bar{y}$ and $\bar{X}$ are the mean target and observation values respectively,
and $\hat{\beta}$ solves the intercept-free ridge regression problem run on centered
observations and targets. Therefore in the following we omit the intercept term and
assume that the relevant columns in the design matrix have been dealt with, and both
the design matrix and the targets have been centered.

Since this one is problem of convex optimization, there exists a unique solution
given by 
\begin{align*}
  \hat{\beta}
    &= (X'X + a I_p)^{-1} X' y \\
    &= X' (a I_n + XX')^{-1} y \,,
\end{align*}
in the direct and the equivalent dual forms, respectively. Predictions on the test
set $X^* = (x_i)_{i=n+1}^{n+m}\in \Real^{m\times d}$, given the train-set, are
\begin{align}
  \hat{y}^*_{\vert(X,y), X^*} = X^* \hat{\beta}
    &= X^* (X'X + a I_p)^{-1} X' y \nonumber \\ 
    &= X^* X' (a I_n + XX')^{-1} y \label{eq:rr_dual_sol}\,,
\end{align}
again in the direct and dual form, respectively.

The so called ``Kernel-trick'', \cite{kernel_trick}, generalizes any ML algorithm,
provided it could be reduced to a form, which depends on the training sample data
only thorough inner-products, or equivalents, between the objects (the sample Gram
matrix). The generalization is done by replacing the products with the products of
the feature maps $\langle\phi(x_i),\phi(x_j)\rangle$ in some RKHS $\Hcal$, or, equivalently,
by the gram matrix of some Mercer-type kernel (see sec.~\ref{sub:kernel_methods}).
It should be noted that care must be taken to ensure additional conditions on the
input domain imposed by this ``trick'' are met.

For instance, the \textbf{RR} prediction in its dual form, eq.~\ref{eq:rr_dual_sol},
depends only on the Gram matrix if inter-object inner-products $X X'$ in the input
domain, which is why it is possible to restate it as the Kernel ridge regression (KRR)
problem.

\subsection{Kernel methods} % (fold)
\label{sub:kernel_methods}

In this section we are going to give a brief recap of the foundations of kernel methods
and their applications.

A tuple $(\Hcal, \langle\cdot, \cdot\rangle)$ is an inner-product space over $\Real$
if $\Hcal$ is a vector space over $\Real$, and $\langle\cdot, \cdot\rangle : \Hcal
\times \Hcal \mapsto \Real$ is an inner product in $\Real$ -- a map linear in both
arguments, symmetric, and possessing the following properties:
\begin{enumerate}
  \item $\langle f, f\rangle\geq 0$ for any $f\in \Hcal$;
  \item $\langle f, f\rangle = 0$ if and only if $f = \nil_\Hcal$.
\end{enumerate}
Over the field of complex numbers $\Cplx$ the requirements of symmetry and bi-
linearity are swapped with being Hermitian, and linear with respect to the first
argument. In the following presentation all considered vector spaces are over the
field of reals.

A Hilbert space is an inner product space, which is complete with respect to the
natural metric induced by the norm $ \|f\| = \sqrt{\langle f, f\rangle} $. A \textbf{R}eproducing
\textbf{K}ernel \textbf{H}ilbert \textbf{S}pace is a complete inner-product vector
space of maps $\Xcal \mapsto \Real$ over the field $\Real$, in which the evaluation
functional defined by $\epsilon_x(f) = f(x)$ is $(\Hcal, \|\cdot\|) \mapsto (\Real, |\cdot|)$
bounded for any $x\in \Xcal$ (in normed spaces bounded linearity is equivalent to
continuity). Equivalently, an \textbf{RKHS} is a Hilbert space for which there exists
a map $\phi:\Xcal\to\Hcal$, and a \textbf{P}ositive \textbf{D}efinite kernel function
$K:\Xcal \times \Xcal \mapsto \Real$ such that \begin{enumerate}
  \item $K(x,y) = \langle \phi(x), \phi(y) \rangle$ for all $x, y\in \Xcal$;
  \item $g(x) = \langle g, K(x, \cdot)\rangle$ for all $x\in \Xcal$ and every
  $g\in \Hcal$.
\end{enumerate}
A function $K:\Xcal \times \Xcal \mapsto \Real$ is positive definite if for any
$n\geq1$, $(x_i)_{i=1}^n \in \Xcal$ and $\alpha \in \Real^{n\times 1}$
\begin{equation*}
  \alpha'K\alpha
    = \sum_{i=1}^n \sum_{j=1}^n \alpha_j \alpha_i K(x_i, x_j)
    \geq 0
    \,,
\end{equation*}
or, in other words the Gram matrix $K$ is positive semi-definite, where $K$ is an
$n \times n$ matrix of $K(x_i, x_j)$. Note that unlike linear algebra, ``positive
definite'' here permits such non-zero weights, for which the product is exactly
zero. Positive definite kernels are also known as Mercer kernels, or covariance
kernels.

Finally, any \textbf{PD} kernel $K$ gives rise to a so called \textbf{canonical RKHS}
associated with $K$ constructed by a standard metric-completion argument for a pre-Hilbert
space (a non-complete inner-product space)
\begin{equation*}
\Hcal_0
  = \bigl\{
    \sum_{i=1}^n \alpha_i K(x_i, \cdot)
    \,:\, n\geq1, (x_i)_{i=1}^n \in \Xcal, (\alpha_i)_{i=1}^n\in \Real
  \bigr\} \,,
\end{equation*}
with an inner-product 
\begin{equation*}
  \langle f, g \rangle = \sum_{i=1}^n \sum_{j=1}^m \alpha_i K(x_i, z_j) \beta_j \,,
\end{equation*}
where $f, g\in \Hcal_0$ with representations $f = \sum_{i=1}^n \alpha_i K(x_i, \cdot)$
and $g = \sum_{j=1}^m \beta_j K(z_j, \cdot)$. In the completed RKHS $\Hcal = \bigl[\Hcal_0\bigr]$
the canonical feature map $\phi$ is given by the canonical map $x\mapsto K(x, \cdot)$
and $K(\cdot, \cdot)$ is the reproducing kernel. The inner-product is extended to
$\Hcal$ by continuity as a step in completion of $\Hcal_0$. It is worth noting, that
the pre-Hilbert space $\Hcal_0$ is by construction dense everywhere in $\Hcal$ with
respect to the metric induced by the completed inner-product.

Given a train sample $(x_i, y_i)_{i=1}^n \in X\times \Real$ a general kernel-based
machine learning regularized loss minimization problem is to solve
\begin{equation}
  \Lcal\bigl((x_i, y_i)_{i=1}^n, f\bigr)
    + \lambda \Omega\bigl(\|f\|\bigr)
    \to \min_{f\in \Hcal} \,,
\end{equation}
where $\Hcal$ is a Hilbert Space with Reproducing Kernel $K$, $\Lcal$ is a loss
function which depends on $f$ through $(f(x_i))_{i=1}^n$, and $\Omega:\Real\mapsto\Real$
is a monotone increasing function used for regularization. In this setting the Representer
theorem, \cite{ref:rkhs_representer}, states that the optimal solution is to be sought
in a finite dimensional linear span of the canonical feature maps $\phi: x\mapsto K(x, \cdot)$
evaluated at the sample objects:
\begin{equation*}
  f^* \in \bigl\{ \beta'\Phi\,:\, \beta \in \Real^{n\times 1} \bigr\} \,,
\end{equation*}
where $\Phi = \bigl(\phi(x_i)\bigr)_{i=1}^n \in \Hcal^{n\times 1}$.

Kernel ridge regression (KRR) combines ridge regression with the kernel trick, and
learns a linear function in the feature space induced by the selected kernel and
sample data. Given a training sample $X = (x_i)_{i=1}^n$, $X\in \Xcal^{n\times 1}$
and $y \in \Real^{n\times 1}$, KRR solves the following problem
\begin{equation*}
  \|y - f\|^2 + \lambda \|f\|^2 \to \min_{f \in \Hcal} \,,
\end{equation*}
with $f \in \Real^{n\times 1}$ being a column-vector of $f$ evaluated at every input
in $X$. Here $\Hcal$ is the canonical RKHS associated with a Mercer-type kernel $K$.
Note that with slight abuse of notation the inner-products and norms over $\Hcal$
and $\Real^{n\times 1}$ are used without denoting which is which, since it is always
clear from both the context and the type of the argument of the norm as a function.

The Representer theorem states that the solution to this minimization is of the form
$f = \Phi'\beta$, for $\Phi = (\phi(x_i))_{i=1}^n \in \Hcal^{n\times 1}$. By definition
of the Gram matrix $K$ and due to its symmetry for any $j=1,\ldots,n$
\begin{equation*}
  f(x_j)
    = \Phi(x_j)' \beta
    = k_{x_j}' \beta
    = (K e_j)' \beta
    = e_j' K \beta
    \,,
\end{equation*}
where $k_x = \Phi(x) = (\phi(x_i)(y))_{i=1}^n \in \Real^{n\times 1}$ and $e_j$ is
the $j$-th unit vector in $\Real^{n\times 1}$. Furthermore
\begin{equation*}
  \| f \|^2
    = \sum_{i=1}^n\sum_{j=1}^n\beta_i\beta_j \langle\phi(x_i), \phi(x_j)\rangle
    % = \sum_{i=1}^n\sum_{j=1}^n\beta_i\beta_j K(x_i, x_j)
    = \beta' K \beta \,.
\end{equation*}
Thus the kernel ridge regression problem is reduced to the following finite-dimensional
convex minimization problem:
\begin{equation*}
  \|y - K \beta \|^2 + \lambda \beta' K \beta
    \to \min_{\beta\in \Real^{n\times 1}} \,.
\end{equation*}
The first order conditions imply that
\begin{equation} \label{eq:krr_approx}
  \hat{\beta} = (\lambda I_n + K)^{-1} y \,\text{ and }\, \hat{y}_x = k_x' \beta \,,
\end{equation}
-- the estimated weight vector and an optimal prediction at $x\in \Xcal$, respectively.

Consider a problem of estimating the prediction accuracy of a test sample $(X^*, y^*)$
given a training sample $(X, y)$, where $X = (x_i)_{i=1}^n$, $y\in \Real^{n\times 1}$,
$X^* = (x^*_j)_{j=1}^m = (x_i)_{i=n+1}^{n+m}$ and $y^*\in \Real^{m\times 1}$. 
The out-of-train-sample prediction residuals are given by
\begin{equation*}
  y^* - \hat{y}^*_{|(X, y), X^*}
    = y^* - K_{XX^*}' (\lambda I_n + K_{XX})^{-1} y
    \,,
\end{equation*}
with $K_{XX^*} = (K_X(x^*_j))_{j=1}^m \in \Real{n\times m}$.
Now an in-sample prediction based on the optimal weights over the pooled sample
$(\tilde{X}, \tilde{y})$ given by $((X, X^*), (y, y^*))$. First, since the matrix
$\lambda I_n + K_{XX}$ is always invertible for $\lambda > 0$, block-matrix inversion
formula implies that $Q_{\tilde{X}} = (\lambda I_{n+m} + K_{\tilde{X}\tilde{X}})^{-1}$
is given by:
\begin{equation*}
  % & = \begin{pmatrix}
  %   \lambda I_n + K_{XX} & K_{XX^*} \\
  %   K_{X^*X} & \lambda I_m + K_{X^*X^*}
  % \end{pmatrix}^{-1} \\
  \begin{pmatrix}
    Q_X + Q_X K_{XX^*} M^{-1} K_{X^*X} Q_X & - Q_X K_{XX^*} M^{-1} \\
    - M^{-1} K_{X^*X} Q_X & M^{-1}
  \end{pmatrix}
  \,,
\end{equation*}
where $Q_X = \bigl( \lambda I_n + K_{XX} \bigr)^{-1}$ and
\begin{equation*}
  M = \lambda I_m + K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*} \,.
\end{equation*}
Since $\lambda I_{n+m} + K_{\tilde{X}\tilde{X}}$ is an invertible positive definite
matrix ($M\succeq 0$), its inverse is \textbf{PD} as well, whence $M$ is positive
definite and invertible as well. If $H_{\tilde{X}} = K_{\tilde{X}\tilde{X}} Q_{\tilde{X}}$,
then $I_{n+m} - H_{\tilde{X}} = \lambda Q_{\tilde{X}}$, whence
\begin{align}
  y^* - \hat{y}^*_{|(\tilde{X}, \tilde{y})}
    &= \lambda \begin{pmatrix} 0\\ I_m \end{pmatrix} Q_{\tilde{X}}
        \begin{pmatrix} y\\ y^* \end{pmatrix} \nonumber \\
    % &= \lambda \begin{pmatrix} - M^{-1} K_{X^*X} Q_X & M^{-1} \end{pmatrix}
    %   \begin{pmatrix} y\\ y^* \end{pmatrix} \\
    &= \lambda M^{-1} \bigl( y^* - K_{X^*X} Q_X y \bigr) %\nonumber \\
    % &= \lambda M^{-1} \bigl( y^* - \hat{y}^*_{|(X, y), X^*} \bigr)
    \label{eq:holdout_resid}
    \,.
\end{align}
It is possible to show that the matrix $K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*}$ is also
\textbf{PD}. Therefore, we have that $M \succeq \lambda I_m$ whence $\lambda M^{-1}
\preceq I_m$, or that the matrix $M$ defines an expanding linear transformation
$\Real^{n\times 1} \mapsto \Real^{n\times 1}$. This shows that the out-of-sample
residuals are generally larger than the corresponding in-sample ones.

In particular, eq.~\ref{eq:holdout_resid} enables fast computation of the leave-one-out
residuals of the KRR, without refitting all the way. The full sample residuals of a
fitted KRR are given by a vector $\hat{r}_{\text{in}}(X, y) = (I_n - K_{XX} Q_X) y$,
and the $i$-th, $i=1,\ldots, n$, leave-one-out residual the train sample is
\begin{equation}
  e_i' \hat{r}_{\text{loo}}(X, y)
  = y_i - K_{-i}(x_i)' Q_{-i} y_{-i} \,,
\end{equation}
where $K_{-i} = K_{X_{-i}}: \Xcal \mapsto \Real^{(n-1)\times1}$ is the canonical map
vector, and $Q_{-i}$ is given by $(\lambda I_{n-1} + K_{X_{-i}X_{-i}})^{-1}$.
Therefore, from eq.~\ref{eq:holdout_resid} it follows that
\begin{align}
  e_i' \hat{r}_{\text{in}}(X, y)
  % = \lambda e_i' P_{i:n} P_{i:n} Q_X P_{i:n} P_{i:n} y
  % = \lambda e_n'
  %   \begin{pmatrix}
  %     Q_{-i} + Q_{-i} K_{-i}(x_n) m_i^{-1} K_{-i}(x_n)' Q_{-i} & - Q_{-i} K_{-i}(x_n) m_i^{-1} \\
  %     - m_i^{-1} K_{-i}(x_n)' Q_{-i} & m_i^{-1} \\
  %   \end{pmatrix}
  %   \begin{pmatrix} y_{-i} \\ y_i \end{pmatrix} \\
  &= \lambda m_i^{-1} \bigl(y_i - K_{-i}(x_i)' Q_{-i} y_{-i} \bigr) \nonumber \\
  &= \lambda m_i^{-1} e_i' \hat{r}_{\text{loo}}(X, y) \label{eq:loo_resid} \,,
\end{align}
where $\lambda m_i^{-1}$ is the KRR analogue of the LS ``leverage'' score of the
$i$-th observation, with
\begin{equation} \label{eq:krr_leverage}
  m_i = \lambda + K(x_i, x_i) - K_{-i}(x_i)' Q_{-i} K_{-i}(x_i) \,.
\end{equation}
The main diagonal of the KRR hat matrix $H_X$ is given by $(\lambda m_i^{-1})_{i=1}^n$,
as can be easily seen by applying the block-inversion formula to the matrix $Q_X$
with symmetrically permuted rows and columns. This ``leverage'' depends only on the
input part, $X$, of the dataset and is always within $[0, 1]$, since the kernel $K$
is \textbf{PD} (see argument after eq.~\ref{eq:holdout_resid}).

% subsection kernel_ridge_regression (end)

\subsection{Bayesian KRR confidence interval} % (fold)
\label{sub:bayesian_krr_confidence_interval}

It has been mentioned above that the Gaussian Process interpretation of the KRR
readily yields predictive confidence regions. In this subsection we are going to
derive the Bayesian confidence interval, against which the conformal confidence
regions will be compared in the numerical study.

%% Brief intro into Gaussian processes
A random process $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$, with mean $m : \Xcal
\mapsto \Real$ and \textbf{PD} function $\Kcal : \Xcal \times \Xcal \mapsto \Real$
is a Gaussian Process if all its finite dimensional distributions are multivariate
Gaussian. In particular, for any $n\geq1$ and any $X = (x_i)_{i=1}^n \in \Xcal$ the
vector $f_X = (f(x_i))_{i=1}^n \in \Real^{n\times 1}$ has Gaussian distribution with
mean $m_X = (m(x_i))_{i=1}^n$ and covariance matrix $K_{XX} = (\Kcal(x_i,x_j))_{ij}$ --
the $n\times n$ Gram matrix of $\Kcal(\cdot,\cdot)$ evaluated at points $(x_i)_{i=1}^n$:
\begin{equation*}
  f_X \sim \Ncal_n(m_X, K_{XX}) \,.
\end{equation*}

In this formulation the model \ref{eq:signal_model} represents the addition of an
independent white noise process $(\epsilon_x)_{x\in \Xcal}$ with variance $\lambda$
to $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$. Indeed, for any $n\geq1$ and
any $X = (x_i)_{i=1}^n \in \Xcal$ the finite dimensional distribution of $(y_x)_{x\in\Xcal}$
is $y_X\sim \Ncal_n(m_X, \lambda I_n + K_{XX})$, because
\begin{equation*}
  y_X
    = \begin{pmatrix} I_n & I_n \end{pmatrix}
    \begin{pmatrix} f_X \\ \epsilon_X \end{pmatrix}
    \,,
\end{equation*}
-- a linear combination of $f_X$ and $\epsilon_X$ with joint distribution
\begin{equation*}
  \begin{pmatrix} f \\ \epsilon \end{pmatrix}
    \sim \Ncal_{n+n}\begin{pmatrix}
        \begin{pmatrix} m \\ 0 \end{pmatrix},
        \begin{pmatrix}
          K_{XX} & 0 \\
          0 & \lambda I_n
        \end{pmatrix}
      \end{pmatrix}
    \,.
\end{equation*}
Therefore $(y_x)_{x\in\Xcal} \sim GP(m(x), \Kcal(x,x'))$ with $\Kcal$ given by a PD
kernel
\begin{equation*}
  \Kcal(x,x') = \lambda \delta_{x,x'} + K(x,x') \,,
\end{equation*}
where $\delta_{x,x'}$ is the delta-function on $\Xcal\times \Xcal$.

%% Conditional distribution
The conformal procedure constructs confidence intervals for test observations $y_x$,
contaminated by the additive noise $\epsilon_x$, as opposed to true values $f_x$ in
the model \ref{eq:signal_model} (see sec.~\ref{sec:conformal_prediction}). In the
Bayesian setting this means that it is necessary to get a conditional distribution
of observations in a test sample $y_{X^*} = (y_{x^*_j})_{j=1}^l$, with respect to
the train sample $y_X = (y_{x_i})_{i=1}^n$. Gaussianity makes it especially easy
to derive conditional distributions. Indeed, if
\begin{equation*}
  \begin{pmatrix}z_1 \\ z_2\end{pmatrix}
    \sim \Ncal_{d_1+d_2}\Biggl(
      \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix},
      \begin{pmatrix}
        \Sigma_{11} & \Sigma_{12} \\
        \Sigma_{21} & \Sigma_{22}
      \end{pmatrix}
    \Biggr)
    \,,
\end{equation*}
then the conditional distribution of $z_1$ given $z_2$ is Gaussian with
\begin{equation*}
  {z_1}_{|z_2}
    \sim \Ncal_{d_1}\bigl(
      \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2),
      \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
    \bigr)
    \,,
\end{equation*}
provided the inverse of $\Sigma_{22}$ exits. Therefore the conditional distribution
is given by
\begin{equation} \label{eq:cond_distr}
  y_{X^*}\vert_{y_X}
    \sim \Ncal_l\bigl(
      m_{X^*} + K_{X^*X} Q_X (y_X - m_X),
      \Sigma_K(X^*)
    \bigr)
    \,,
\end{equation}
where $\Sigma_K(X^*) = K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*}$, and
$Q_X = \bigl(K_{XX}\bigr)^{-1}$, since $(y_x)_{x\in\Xcal}$ is a GP and
\begin{equation*}
  \begin{pmatrix} y_X \\ y_{X^*} \end{pmatrix}
    \sim \Ncal_{n+l}\begin{pmatrix}
      \begin{pmatrix} m_X \\ m_{X^*} \end{pmatrix},
      \begin{pmatrix}
        K_{XX} & K_{XX^*} \\
        K_{X^*X} & K_{X^*X^*}
      \end{pmatrix}
    \end{pmatrix}
    \,,
\end{equation*}
with $K_{XX^*} = (\Kcal(x_i, x^*_j))\in \Real^{n\times l}$.

The Bayesian prediction of $y_{X^*}$ conditional on observing $y_X$ is given by
the Maximum Posterior prediction (which in the Gaussian case coincides with the
true Bayesian prediction $\ex\bigl(y_{X^*}\,|\, y_X\bigr)$):
\begin{equation*}
  \hat{y}_{y_X}(X^*) = m_{X^*} + K_{X^*X} Q_X \bigl(y_X - m_X\bigr) \,.
\end{equation*}

Gaussian process regression (Kriging) posits that the observed data is drawn from
$GP(m(x), \Kcal(x, x'))$ with
\begin{equation} \label{eq:krig_kernel}
  \Kcal(x,x') = \sigma^2( \lambda \delta_{x, x'} + K(x, x') ) \,,
\end{equation}
for a PD kernel $K$, and $m(x) = \Phi(x)'\beta$, where $\Phi: \Xcal\mapsto \Real^{d\times 1}$
is the regression feature map. This model naturally enables adaptive estimation of
parameters of the kernel $K$ by maximizing the joint likelihood of the train data
$(X, y)$:
\begin{align} \label{eq:bkrr_likelihood}
  \Lcal
    &= -\frac{n}{2} \log 2\pi
    - \frac{n}{2}\log \sigma^2
    - \frac{1}{2}\log \lvert R_X \rvert \nonumber\\
    &- \frac{1}{2\sigma^2} (y - F\beta)' R_X^{-1} (y - F\beta)
    \,,
\end{align}
where $R_X = \lambda I_n + K_{XX}$, and $F \in\Real^{n\times d}$ is the design matrix
of $X$ under $\Phi$. The optimal $\beta$ is given by the generalized least squares
solution
\begin{equation*}
  \hat{\beta} = (F'R_X^{-1}F)^{-1} F'R_X^{-1} y \,.
\end{equation*}
Minimization of \ref{eq:bkrr_likelihood} by alternating between kernel parameter
and $\beta$ steps, makes it possible to gradually estimate the optimal parameters.

Another alternative is to select the parameters based on the data that has not been
used in fitting the conditional mean of the kernel ridge regression. One could use
leave-one-out cross-validation and minimize the RMSE of the deleted residuals
(eq.~\ref{eq:loo_resid}):
\begin{equation}
  \hat{\theta}
    = \mathop{\text{argmin}}_{\theta \in\Theta}
      n^{-1} \| \hat{r}_{\text{loo}}(X, y; \theta) \|^2
    \,,
\end{equation}
where the dependence on $\theta$ is through the sample kernel Gram matrix $K_{XX}$.
It is worth noting that the leave-one-out residuals are not affected by the scale
$\sigma^2$ of the kernel $\Kcal$ (eq.~\ref{eq:krig_kernel}). Similarly to the case
of predictive variances in \cite{pcw20005a7}, using leave-one-out cross-validation
eliminates the bias inherent in estimates of the kernel parameters.

Bayesian kernel ridge regression confidence interval for an observed values $y_{x^*}$
at $x^*\in \Xcal$ can be obtained from eq.~\ref{eq:cond_distr} by fixing $\beta = 0$.
Indeed, the distribution of $y_{x^*}$, conditional on the train data $y_X$, is
\begin{equation} \label{eq:gp_cond_dist}
{y_{x^*}}_{|y_X}
  \sim \Ncal\bigl( \hat{y}_{y_X}(x^*), \sigma^2 \sigma_K^2(x^*) \bigr) \,,
\end{equation}
with $\hat{y}_{y_X}(x^*) = K_X(x^*)' Q_X y_X$, and
\begin{equation*}
  \sigma_K^2(x^*)
    = \lambda + K(x^*, x^*) - K_X(x^*)' Q_X K_X(x^*) \,,
\end{equation*}
where $Q_X = \bigl(\lambda I_n + K_{XX}\bigr)^{-1}$, $K_{XX} = (K(x_i,x_j))_{ij}$,
and $K_X = (K(x_i, \cdot))_{i=1}^n: \Xcal \mapsto \Real^{n\times1}$. Thus, the $1 - \alpha$
confidence interval is thus given by
\begin{equation} \label{eq:gp_conf_int}
\Gamma^\alpha_{y_X}(x^*)
  = \hat{y}_{y_X}(x^*)
  + \sigma \sqrt{\sigma_K^2(x^*)}
  \times [z_{\frac{\alpha}{2}}, z_{1-\frac{\alpha}{2}}]
  \,,
\end{equation}
where $z_\gamma$ is the $\gamma$ quantile of $\Ncal(0, 1)$.

% subsection bayesian_krr_confidence_interval (end)

% section kernel_ridge_regression (end)

\section{Conformal prediction} % (fold)
\label{sec:conformal_prediction}

Conformal prediction is a distribution-free technique designed to yield a statistically
valid confidence sets for predictions made by machine learning algorithms. The key
advantage of the method is that it offers coverage probability guarantees under standard
IID assumptions, even in cases when assumptions of the underlying prediction algorithm
fail to be satisfied. The method was introduced in \cite{vovk2005} and is applicable
in both the supervised and unsupervised online learning settings. In the following
we consider supervised setting with $\Zcal$ denoting the object-target space $\Xcal \times \Ycal$.

The nature of the underlying ML predictive algorithm is irrelevant to conformal
prediction, as the core of the procedure is a measurable map $A: \Zcal^*\times \Zcal \mapsto \Real$,
a \textbf{N}on-\textbf{C}onformity \textbf{M}easure, \textbf{NCM}, which for a training
sample $Z_{:n}=(z_i)_{i=1}^n\in\Zcal$ and a test object $z_* \in \Zcal$ quantifies
how much different $z_*$ is relative to the sample $Z_{:n}$.

A conformal predictor over the NCM $A$ is a procedure, that for every sample $Z_{:n}$,
a test object $x_{n+1} \in \Xcal$, and a confidence level $\alpha\in(0,1)$, computes
a confidence set $\Gamma_{Z+{:n}}^\alpha(x^*)$ for the target value $y_{n+1}$ corresponding
to $x_{n+1}$:
\begin{equation} \label{eq:conf_pred_set}
  \Gamma_{Z_{:n}}^\alpha(x_{n+1})
    = \bigl\{ y\in \Ycal \,:\, p_{Z_{:n}}(\tilde{z}^y_{n+1}) \geq \alpha \bigr\} \,,
\end{equation}
where $\tilde{z}^y_{n+1} = (x_{n+1}, y)$ a synthetic test observation with target
label $y$. The function $p:\Zcal^*\times (\Xcal\times \Ycal)\mapsto [0,1]$ is given
by
\begin{equation} \label{eq:conf_p_value}
  p_{Z_{:n}}(\tilde{z})
    = (n+1)^{-1} \bigl\lvert\{ i \,:\,
      \eta_i^{\tilde{z}} \geq \eta_{n+1}^{\tilde{z}} \}\bigr\rvert \,,
\end{equation}
where $i=1,\ldots, n+1$, and $\eta_i^{\tilde{z}} = A(S^{\tilde{z}}_{-i}, S^{\tilde{z}}_i)$
-- the degree of non-conformity of the $i$-th observation with respect to the augmented
sample $S^{\tilde{z}} = (Z_{:n}, {\tilde{z}}^y_{n+1}) \in \Zcal^{n+1}$. For any $i$,
$S^{\tilde{z}}_i$ is the $i$-th element of the sample, and $S^{\tilde{z}}_{-i}$ is
the sample with the $i$-th observation omitted. Intuitively, the p-value (eq.~\ref{eq:conf_p_value})
measures the likelihood of $\tilde{z}$ based on its non-conformity, or with $Z_{:n}$.

A distribution $P$ on $\Zcal^n$ is called \textbf{exchangeable} if for any permutation
$\sigma$ of ${1,\ldots,n}$ for any measurable $B\subseteq \Zcal^n$
\begin{multline} \label{eq:exchangeability}
  \pr\bigl(\{z\in\Zcal^n \,:\, (z_i)_{i=1}^n\in B\}\bigr)
  = \\ \pr\bigl(\{z\in\Zcal^n \,:\, (z_{\sigma(i)})_{i=1}^n \in B\} \bigr)\,.
\end{multline}
Any product-measure on $\Zcal^n$ is exchangeable, and therefore exchangeability
generalizes independence. A distribution $P$ on $\Zcal^\infty$ is exchangeable
is all its finite distributions are exchangeable.

In \cite{Vovketal2005}, chapter 2, is has been shown, that if a sequence of examples
$(z_n)_{n \geq1}$ is generated by an exchangeable distribution $P$ on $\Zcal^\infty$,
then the coverage probability of the prediction set $\Gamma^\alpha$, yielded by
the procedure \ref{eq:conf_pred_set} is at least $1-\alpha$ and successive errors
are independent in online learning and prediction setting. Thus the outlined procedure
guarantees unconditional validity of the confidence region (eq.~\ref{eq:conf_pred_set}):
for any $\alpha \in (0,1)$, for any $n\geq1$ and any exchangeable distribution
$P_n$ on $\Zcal^n$ one has
\begin{equation} \label{eq:conservative_coverage}
  \pr_{Z_{:n}\sim P_n} \bigl(
    y_n \notin \Gamma^\alpha_{Z_{:(n-1)}}(x_n)
  \bigr) \leq \alpha \,,
\end{equation} 
where $(x_n, y_n) = z_n$. In a special case, when $z_n$ are independent the measure
$P_n$ is just the product measure $P_{z_1} \otimes \ldots \otimes P_{z_n}$.

Intuitively, the event $y_n \notin \Gamma^\alpha_{Z_{:(n-1)}}(x_n)$ is equivalent
to $\eta_n$ being among the largest $\lfloor n\alpha\rfloor$ values of $\eta_i = A(Z_{-i}, Z_i)$,
which is equal to $\frac{\lfloor n\alpha\rfloor}{n}$, due to exchangeability of
$Z_{:n}$ (this heuristic argument assumes that all $\eta_i$ are distinct and that
probability under $P_{:n}$ of any $(\eta_i)_{i=1}^n$ is positive, for a rigorous
proof see \cite{vovk2005}, chapter 8).

In general, any real-valued jointly measurable \textbf{NCM} could be used, the only
difference will be in the size of the predicted confidence set (efficiency) and the
computational complexity of the conformal procedure. In the general case, despite
theoretical guarantees, computing eq.~\ref{eq:conf_pred_set} requires exhaustive
search through the target space $\Ycal$. This complexity issue is not acute in typical
classification settings, when $\Ycal$ is finite, but is infeasible in regression
settings when $\Ycal$ is $\Real$. Furthermore, the inner eq.~\ref{eq:conf_p_value}
requires looping over all observations in the augmented sample $S^{\tilde{z}}$.
In regression setting for specific non-conformity measures it is possible to come
up with efficient procedure for computing the confidence region (\ref{eq:conf_pred_set}),
as demonstrated in \cite{vovk2005}, chapter 2, and sec.~\ref{sub:conformalized_krr}.

It should be note, that a conformal prediction procedure can also be constructed
based on ``conformity measures'', as opposed to ``non-conformity measures''. As
the name suggests conformity measures quantify the similarity of a test object to
the train sample. Both approaches yield equivalent guarantees and are almost identically
constructed: the ``$\geq$'' sign must be switched to ``$\leq$'' in eq.~\ref{eq:conf_p_value}
if a conformity measure is used.

% section conformal_prediction (end)

\subsection{Confromalized ridge regression} % (fold)
\label{sub:conformalized_ridge_regression}

Consider a sample $Z = (x_i, y_i)_{i=1}^n \in \Zcal$, with $\Xcal = \Real^{d\times 1}$
and $\Ycal=\Real$. Let $X\in\Real^{n\times d}$ be the overall design matrix of $Z$
and $y\in\Real^{n\times1}$ -- the target vector of the sample. Bayesian ridge regression
(BRR) is a particular case of the kernel ridge regression with the kernel being
the inner product in $\Real^{d\times1}$:
\begin{equation*}
  \Kcal(x,x') = \sigma^2(\lambda \delta_{x,x'} + \langle x,x'\rangle) \,.
\end{equation*}
Thus BRR assumes that $y = x'\beta + \epsilon_x$ for any $x\in\Xcal$ as well as
the following:
\begin{enumerate}
  \item $\beta\sim\Ncal_d(0, \sigma^2 \lambda^{-1} I_p)$;
  \item $\epsilon_x\sim \Ncal(0, \sigma^2)$ and iid for all $x\in \Real^{d\times 1}$;
  \item $\beta$ and $\epsilon_x$ are independent.
\end{enumerate}
Assuming a known $\sigma^2$, equation~\ref{eq:gp_cond_dist} implies that the distribution
of $y^*$ at $x^*$ conditional on the observed sample is
\begin{align} \label{eq:brr_cond_dist}
  {y_{x^*}}_{|y_X}
    \sim \Ncal\bigl(&{x^*}'(\lambda I_p + X'X)^{-1}X'y, \nonumber\\
      &\sigma^2 (\lambda + {x^*}' (\lambda I_p + X'X)^{-1} x^* ) \bigr)
    \,.
\end{align}

The properties of conformalized ridge regression (CRR) have been studied in \cite{burnaevV14},
where it has been shown that under more general assumptions (the Gaussianity of $\beta$
is relaxed), the conformal confidence region, eq.~\ref{eq:conf_pred_set}, constructed
with an NCM based on the ridge regression residuals is asymptotically efficient.

The conformal procedure in \cite{burnaevV14} is constructed from a conformity measure
\begin{align} \label{eq:crr_ncm}
  \eta_i &= A(Z_{-i}, Z_i) \nonumber \\
    &= \bigl\lvert\{j\,:\, \hat{r}_j \geq \hat{r}_i \} \bigr\rvert \wedge
       \bigl\lvert\{j\,:\, \hat{r}_j \leq \hat{r}_i \} \bigr\rvert \,,
\end{align}
where $\hat{r} = \hat{r}_Z$ are the in-sample ridge regression residuals given by
\begin{equation} \label{eq:rr_insample_resid}
  \hat{r}_Z = (I_n - X (\lambda I_p + X'X)^{-1} X') y \,,
\end{equation}

In the paper it was also shown that for any $\alpha\in(0,1)$ the confidence
region $\Gamma^\alpha$ produced by CRR procedure for the conformity measure in
eq.~\ref{eq:crr_ncm} is equivalent to the intersection of confidence sets yielded
by conformal procedures with non-conformity measures given by $\eta_i = \hat{r}_i$
and $\eta_i = -\hat{r}_i$ at significance levels $\frac{\alpha}{2}$. Individually,
these NCMs define a \textbf{upper} and \textbf{lower} CRR sets respectively.

The main result of \cite{burnaevV14} states that under relaxed BRR assumptions if
a sequence $(x_n)_{n\geq1}\in\Xcal$ is iid with an existing and non-singular second
moment matrix $\ex x_1x_1' \preceq 0$, then for all sufficiently large $n$ the CRR
produces confidence regions that are intervals, and the difference between the upper
and lower ends of the Bayesian confidence interval and the conformal confidence
intervals are asymptotically Gaussian with zero mean.

Since ridge regression has a dual form solution, it is possible to apply the kernel
trick to it to get a more powerful ML algorithm. The Bayesian confidence intervals
have been derived in sec.~\ref{sec:bayesian_krr_confidence_interval}, so in the next
section the construction of a conformal prediction procedure will be discussed.

% subsection conformalized_ridge_regression (end)

\subsection{Conformalized kernel ridge regression} % (fold)
\label{sub:conformalized_krr}

In this section we describe the construction of confidence regions of the conformal
procedure eq.~\ref{eq:conf_pred_set} for the case of the non-conformity measures
based on kernel ridge regression.

\subsubsection{Ridge Regression Confidence Machine} % (fold)
\label{ssub:ridge_regression_confidence_machine}

We consider two versions of the NCM proposed in \cite{vovk2005}, chapter 2,
for the regression setting: an in-sample and a \textbf{l}eave-\textbf{o}ne-\textbf{o}ut
version. To this end consider a sample $(X, y) = (x_i, y_{x_i})_{i=1}^n$, and for
any $i=1\ldots, n$ put $X = (X_{-i}, x_i)$, and $y = (y_{-i}, y_i)$.
Let $e_i\in \Real^{n\times 1}$ be the $i$-th unit vector.

The ``in-sample'' NCM $A_{\text{in}}$ measures the absolute value of the regression
residual and is given by:
\begin{align}
  A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    &= |y_i - \hat{y}_{|(X, y)}(x_i)| \nonumber\\
    &= |e_i' \hat{r}_{\text{in}}(X, y)| \label{eq:ins_ncm}
    \,,
\end{align}
where $\hat{y}_{|(X, y)}(x_i)$ is the residual of the regression fit on the complete
dataset $(X, y)$. The ``loo'' NCM is defined similarly, but uses ``loo''-residuals
for the task:
\begin{align*}
  A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    &= |y_i - \hat{y}_{|(X_{-i}, y_{-i})}(x_i)| \\
    &= |e_i' \hat{r}_{\text{loo}}(X, y)|
    \,.
\end{align*}
Note that both are interrelated using the formula:
\begin{multline*}
  A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    \\ = \lambda^{-1} \diag(Q_X)^{-1}
    A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    \,,
\end{multline*}
based on eq.~\ref{eq:loo_resid}.

For the NCM $A$ the $1-\alpha$ conformal confidence interval for the $n$-th observation
is given by
\begin{equation} \label{eq:conf_ci}
\Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
  = \bigl\{ z\in \Real \,:\, p_n\bigl((X, \tilde{y}_n^z)\bigr) \geq \alpha \bigr\}
  \,,
\end{equation}
where $\tilde{y}_j^z = (y_{-j}, z)$ -- the augmented sample $y$, with the $j$-th
value replaced by $z$. The ``conformal likelihood'' of the $j$-th observation in
some sample $(X, y)$ is given by
\begin{equation*}
  p_j\bigl((X, y)\bigr)
    = n^{-1} \bigl\lvert \bigl\{
        i = 1,\ldots, n \, : \,
        \eta_i \geq \eta_j
    \bigr\} \bigr\rvert
    \,,
\end{equation*}
for $\eta_i = A\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)$.
Thus it is necessary to obtain a formula, describing dependency of the residuals on
the augmentation of the $n$-th observation. The in-sample KRR residuals of the augmented
dataset $(X, \tilde{y}_n^z)$ are given by
\begin{align}
    \hat{r}_{\text{in}}(X, \tilde{y}_n^z)
    % &= \lambda Q_X \tilde{y}_n^z \nonumber \\
    % &= \lambda
    %   \begin{pmatrix}
    %     Q_{-n} + Q_{-n} K_{-n}(x_n) m_n^{-1} K_{-n}(x_n)' Q_{-n} & - Q_{-n} K_{-n}(x_n) m_n^{-1} \\
    %     - m_n^{-1} K_{-n}(x_n)' Q_{-n} & m_n^{-1} \\
    %   \end{pmatrix}
    %   \begin{pmatrix} y_{-n} \\ z \end{pmatrix} \nonumber \\
    % &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix}
    %  - \lambda \begin{pmatrix} Q_{-n} K_{-n}(x_n) \\ -1 \end{pmatrix}
    %    m_n^{-1} \bigl(z - K_{-n}(x_n)' Q_{-n} y_{-n} \bigr) \nonumber \\
    &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix} \nonumber \\
    &- \lambda B_{-n}(x_n) K_{-n}(x_n)' Q_{-n} y_{-n} \nonumber \\
    &+ \lambda B_{-n}(x_n) z \label{eq:krr_in_resid} \,,
\end{align}
Where the vector $B_{-n}(x_n)\in\Real^{n\times 1}$ is given by
\begin{equation} \label{eq:krr_in_resid_B}
  B_{-n}(x_n)
    = \begin{pmatrix} - Q_{-n} K_{-n}(x_n) \\ 1 \end{pmatrix} m_n^{-1}
    \,.
\end{equation}

In general, the construction of the confidence region (\ref{eq:conf_ci}) requires
searching through all $y\in \Ycal$ that satisfy the significance level requirement,
which is infeasible unless $\Ycal$ is finite. The construction for the NCM (\ref{eq:ins_ncm})
for in-sample residuals in this particular problem relies on the representation
of each residual as
\begin{equation*}
  \hat{r}_i^z
    = e_i' \hat{r}_{\text{in}}(X, \tilde{y}_n^z)
    = \lambda c_i + \lambda b_i z
    \,,
\end{equation*}
with $c_i = e_i' C_{-n}\bigl((X, y), x_n\bigr)$ and
\begin{align*}
  C_{-n}\bigl((X, y), x_n\bigr)
    &= \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix} \\
    &- B_{-n}(x_n) K_{-n}(x_n)' Q_{-n} y_{-n}
    \,.
\end{align*}

Since absolute values of the residuals are compared, it is possible to consistently
manipulate the signs of each entry in $C$ and $B$ to ensure that $e_i'B\geq 0$ for
all $i$. The regions $S_i = \{z\in\Real\,:\, |\hat{r}_i^z| \geq |\hat{r}_n^z|\}$, for
$i=1,\ldots, n$, are either closed intervals, complements of open intervals,
one-side closed half-rays in $\Real$, depending on the values of $C$ and $B$. In
particular, with $p_i$ and $q_i$ denoting the values $-\frac{c_i+c_n}{b_i+b_n}$ and
$\frac{c_i-c_n}{b_n-b_i}$, respectively (whenever each is defined), each region
$S_i$ has one of the representations:
\begin{enumerate}
%% a picture of shifted and scaled x->|x| helps in derivation of this.
  \item $b_i=b_n=0$: $S_i = \Real$ if $|c_i| \geq |c_n|$, or $S_i = \emptyset$
  otherwise;
  \item $b_n = b_i > 0$: $S_i$ is either $(-\infty, p_i]$ if $c_i < c_n$, $[p_i, +\infty)$ if
  $c_i > c_n$, or $\Real$ otherwise;
  \item $b_n > b_i \geq 0$: $S_i$ is either $[p_i, q_i]$ if $c_i b_n \geq c_n b_i$,
  or $[q_i, p_i]$ otherwise;
  \item $b_i > b_n \geq 0$: $S_i$ is $\Real\setminus (q_i, p_i)$ when $c_i b_n \geq c_n b_i$,
  or $\Real\setminus (p_i, q_i)$ otherwise.
\end{enumerate}
Let $P$ and $Q$ be the sets of all well-defined $p_i$ and $q_i$ respectively, and
let $(g_j)_{j=0}^{J+1}$ enumerate distinct values of $\{\pm\infty\} \cup P \cup Q$,
so that $g_j < g_{j+1}$ for all $j$. Then the confidence region is a closed subset
of $\Real$ constructed from sets $G^m_j = [g_j, g_{j+m}]\cap \Real$ for $m=0, 1$:
\begin{equation} \label{eq:rrcm_conf_ci}
  \Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
    = \bigcup_{m\in\{0,1\}} \bigcup_{j\,:\, N^m_j \geq n \alpha} G^m_j
    \,,
\end{equation}
where $N^m_j = |\{i \,:\, G^m_j \subseteq S_i\}|$ is the coverage frequency of $G^m_j$.
In general, the resulting confidence set might contain isolated singletons $G^0_j$.

This set, can be constructed efficiently in $\BigO(n \log{} n)$ time with $\BigO(n)$
memory footprint. Indeed, it is necessary to sort at most $J\leq 2n$ distinct endpoints
of $G_j$, then locate the values $p_i$ and $q_i$ associated with each region $S_i$
($\BigO(n \log{} n)$). Then, since the building blocks $G^m_j$ of $\Gamma^\alpha$
are either singletons ($m=0$), or intervals made up from adjacent singletons ($m=1$),
coverage numbers $N^m_j$ can be computed in at most $\BigO(n)$ time.

% subsubsection ridge_regression_confidence_machine (end)

\subsubsection{Kernel CRR} % (fold)
\label{ssub:kernel_crr}
Another possibility is to use the two-sided NCM proposed in \cite{burnaevV14} and
mentioned in sec.~\ref{sub:conformalized_ridge_regression}. Recall that, in order
to construct a CRR confidence region for significance level $\alpha$ (eq.~\ref{eq:crr_ncm}),
it is necessary to intersect confidence sets yielded by conformal procedures with NCM
scores given by $\hat{r}_i^z$ (upper) and $-\hat{r}_i^z$ (lower) at significance level
$\frac{\alpha}{2}$. Confidence regions based on CRR, much like RRCM, can use any kind
of residual: leave-one-out, or in-sample. In this section we present an efficient
algorithm for constructing this confidence region.

For the upper CRR consider the regions $U_i = \{z\in\Real\,:\, \hat{r}_i^z \geq \hat{r}_n^z\}$,
$i=1,\ldots, n$ are either empty, full $\Real$ or one-side closed half-rays. Since
$\hat{r}_i^z = \lambda c_i + \lambda b_i z$, $U_i$ takes one of the following forms:
\begin{enumerate}
  \item $b_i=b_n$: $U_i = \Real$ if $c_i\geq c_n$, and $\emptyset$ otherwise;
  \item $b_i\neq b_n$: $U_i = [q_i, +\infty)$ if $b_i>b_n$, or
  $U_i = (-\infty, q_i]$ otherwise;
\end{enumerate}
with $q_i = \frac{c_i-c_n}{b_n-b_i}$. The forms of regions $L_i$ for the lower CRR
are computed similarly, but with the signs of $c_i$ and $b_i$ flipped for each $i=1, \ldots, n$.

Both upper and lower confidence regions are built similarly to the kernel RRCM region
eq.~\ref{eq:rrcm_conf_ci} in sec.~\ref{ssub:ridge_regression_confidence_machine}. The
final the Kernel CRR confidence set is:
\begin{equation} \label{eq:crr_conf_ci}
  \Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
    = \Gamma_{X_{-n}, y_{-n}}^{\alpha,\text{u}}(x_n)
    \cap \Gamma_{X_{-n}, y_{-n}}^{\alpha,\text{l}}(x_n)
    \,.
\end{equation}
This intersection can be computed efficiently in $\BigO(n \log{} n)$, since the regions
are built form sets anchored at a finite set $Q$ with at most $n+2$ values. Therefore,
the CRR confidence set for a fixed significance level $\alpha$ has $\BigO(n\log{} n)$
complexity.

% subsubsection kernel_crr (end)

% \subsection{Kernel parameter estimation} % (fold)
% \label{sub:kernel_parameter_estimation}

% The third option is to use a nonparametric method suggested in \cite{Nemirovsky1997}.

% % subsection kernel_parameter_estimation (end)

% subsection conformalized_krr (end)

\section{Numerical study} % (fold)
\label{sec:numerical_study}

Validity of conformal predictors in the online learning setting has been shown in
\cite{vovk2005}, chapter 2, however, no result of this kind is known in the batch
learning setting. Our experiments aim to evaluate the empirical performance of the
conformal prediction in this setting: with dedicated train and test datasets. In
this section we conduct a set of experiments to examine the validity of the regions,
produced by the conformal Kernel Ridge Regression and compare its efficiency to
the Bayesian confidence intervals.

We primarily test the conformalized KRR predictions in cases when the input space
$\Xcal$ is a compact set in $\Real^{d\times 1}$ for either $d=1$ or $d=2$. The rationale
for this is that since conformal procedure is oblivious to the structure of the
input dataset, there is no reason for its validity to deteriorate with increasing
dimensionality of $\Xcal$. Indeed, the conformal confidence region eq.~\ref{eq:conf_pred_set}
the input data from $\Xcal$ in the training and the test sets are fed into the NCM
$A$, which, in general, can be an arbitrary computable function, and never ``leak''
into the procedure itself. The dimensionality of the input data, however, may impact
the efficiency (the width) of the resulting confidence region. And our experiments,
to a certain extent, assess their effect.

We consider the isotropic \textbf{R}adial \textbf{B}asis \textbf{F}unction kernel
for both the Conformal Kernel ridge regression and the Gaussian Process Regression.
The RNF kernel over $\Xcal$ is given by
\begin{equation} \label{eq:rbf}
  K(x,x')
  = \mathop{\text{exp}}\bigl\{-\theta \|x - x'\|^2\bigr\}
  \,,
\end{equation}
where $\theta>0$ is the precision parameter. This kernel is widely used in practice,
because it has very nice properties. In particular, if $\Xcal$ is compact its canonical
RKHS is universal: $\Hcal_K$ is dense in the set $C_0(\Xcal)$ of continuous bounded
maps with respect to the uniform norm $\|\cdot\|_\infty$, see.~\cite{steinwart2002influence}.

% Another example of a universal kernel is the Laplacian kernel 
% \begin{equation*}
%   K(x,x')
%   = \mathop{\text{exp}}\bigl\{-\theta \|x - x'\|_1\bigr\}
%   \,,
% \end{equation*}
% where $\|\cdot\|_1$ is the $L_1$ norm on $\Xcal$. In contrast to the RBF, the Laplacian
% kernel 
% Its spectral density is the Cauchy
% density, as opposed to the Gaussian density in the case of RBF. The

\subsection{Setup} % (fold)
\label{sub:setup}

The off-line validity and efficiency is studied in three settings: fully-Gaussian
case, ``semi''-Gaussian and non-Gaussian cases. In the first case, we study the
validity and efficiency of GPR and conformal confidence regions on a sample path
of a Gaussian process on $\Xcal$. We focus consider $4$ alternatives, depending on
whether the parameter $\theta$ or $\lambda$ of the KRR are equal, or not, to their
counterparts in the kernel of the Gaussian process.

In the second and third settings, we concentrate on deliberately non-Gaussian functions,
and examine the effects of moderate Gaussian additive noise on the performance of
both Bayesian and Conformal confidence regions. In a ``semi''-Gaussian setting,
the assumptions of the confidence regions of Gaussian Process Regression are valid
to a certain extent: a deterministic function is contaminated by moderate Gaussian noise.
We explicitly examine two cases: whether the regularization in the kernel ridge regression
is equal to the theoretical level of the noise in the data, or not.

In the final, completely non-Gaussian setting, we examine the same set of signals
as in the ``semi''-Gaussian case, but with negligible noise in the observed data.

Below is a list of working conjectures we aim to find empirical evidence for: \begin{itemize}
  \item the Bayesian confidence regions perform gradually worse the further the data
  is from the GPR assumptions;
  \item in the fully Gaussian case both Bayesian and conformal confidence intervals
  possess the asymptotic validity guarantees, and the conformal procedure yields
  asymptotically efficient (close to the Bayesian) regions;
  \item the asymptotic validity (at least conservative) of the conformal confidence
  set is expected to hold in all cases.
\end{itemize}

We study the effects of the kernel parameters and the choice of the conformal procedure
on the validity and efficiency of the confidence regions by varying the following
hyper-parameters: \begin{enumerate}
  \item the magnitude of the Gaussian noise-to-signal ratio in the data: negligible
  $\gamma = 10^{-6}$ and moderate $\gamma = 10^{-1}$;
  \item the size $n$ of the train sample: small ($n \in \{ 25, 50, 100\}$),
  moderate, and large ($n \in \{200 k\,:\, k=1, \ldots, 8 \}$);
  \item the NCM $A$ for the confidence region: either the RRCM, or the CRR;
  \item the type of residual $\hat{r}$: either $\hat{r}_{\text{in}}$, or $\hat{r}_{\text{loo}}$;
  \item the noise-to-signal (regularization) ratio $\lambda$ in the kernel ridge
  regression is chosen from two regimes: high $\lambda = 10^{-1}$ (smoothing), and
  low $\lambda=10^{-6}$ (interpolation);
  \item the precision of the RBF kernel $\theta$ is picked from $\{10, 10^2, 10^3\}$,
  since the input domain is the unit cube, for which $\sup_{x,x'\in[0,1]^d}\|x-x'\|^2 = d$;
  \item the fixed $\theta$ or its MLE estimate ($\theta$ that minimizes \ref{eq:bkrr_likelihood};
\end{enumerate}
For a given test function $f:\Xcal \mapsto \Real$ on some compact domain $\Xcal$
each experiment for $n, \theta, \lambda, A$ and $\hat{r}$ consists of the following
steps:
\begin{enumerate}
  \item The test inputs, $X^*$, are given by a grid with constant spacing in $\Xcal$;
  \item Train inputs, $X$, are sampled from a uniform distribution over $\Xcal$;
  \item For all $x\in X_{\text{pool}} = X \cup X^*$, draw target values $y_x = f(x)$
  from the data generation process;
  \item let $\tilde{T} = (x, y_x)_{x\in X}$;
  \item perform $L$ independent replications of the following steps: for $l=1,\ldots, L$
  \begin{enumerate}
    \item draw an independent random sample of size $n$ without replacement from $\tilde{T}$;
    \item fit a Gaussian Process regression with $\beta = 0$ and $\Kcal$ given by
    eq.~\ref{eq:krig_kernel} for the RBF kernel $K$ with the specified precision
    $\theta$ with fixed $\lambda > 0$;
    \item for each $x^* \in X^*$ construct, the Bayesian KRR region, $\Bcal_l^\alpha(x^*)$,
    and the conformal confidence regions, $\Gamma_l^\alpha(x^*; A, \hat{r})$ using
    the specified NCM $A$, the residuals $\hat{r}$, and the kernel $\Kcal$ (with
    estimated $\sigma^2$);
    \item estimate the coverage rate and the width of the convex hull of the region
    over the test sample $X^*$: $p_l(R) = |X^*|^{-1}\sum_{x\in X^*} 1_{y_x\in R_x}$,
    and $w_l(R) = \inf\{b-a\,:\,R \subseteq [a, b]\}$, where $R$ is a confidence
    region;
  \end{enumerate}
\end{enumerate}
For the $1$-D case the domain $\Xcal$ is $[0,1]$ and the test input set is given by
$\{k N^{-1}\,:\,k=0,\ldots, N+1\}$ for $N=1000$. In $2$-D case we consider functions
defined on $\Xcal=[-1,1]^2$, and use $X^* = \{k N^{-1}\,:\,k=0,\ldots, N+1\}^2$
for $N=50$ as the test set.

%% What is the estimate of \sigma^2

% subsection setup (end)

\subsection{Results} % (fold)
\label{sub:results}

We begin with the examination of the one-dimensional fully-Gaussian setup, with
$\Xcal = [0, 1]$. To illustrate the constructed confidence regions, we generated
a sample path of the $1$-d Gaussian process with $y_x\sim GP(0, \gamma \delta_{x,x'} + K_{x,x'})$
for the RBF kernel (eq.~\ref{eq:rbf}) on a regular grid $G=\{500^{-1} k \,:\, k=0,
\ldots, 500\}$, and then use its sub-grid of size $51$ with stepping $0.9\cdot 20^{-1}$
in $[0.05, 0.95]$ for training the Kernel Ridge regression and constructing the
confidence regions. The training sample was deliberately chosen to highlight the
blowup of the size of the confidence bands outside the train region.
Figure~\ref{fig:gauss_1d_prof_gpr} illustrates one realization of $y_x$ and the
constructed Bayesian confidence regions (eq.~\ref{eq:gp_conf_int}). Here the abbreviation
``GPR-p'' stands for a confidence region for the true value $f_x$ in eq.~\ref{eq:signal_model},
and ``GPR-f'' -- for the observed value. Recall that the non-parametric vanilla
conformal procedures yield predictions for the observed values $y_x$ only. That
is why in the following we are going to focus on the ``GPR-f'' confidence intervals,
and the ``GPR-p'' band is presented for illustrative purposes only.
\begin{figure}%[t, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/0.1_0.1/50/profile_gaussian_0,1_0,1_100_5p-GPR_50.pdf}
    %{images/profile/profile_gaussian_0,1_0,1_100_5p-GPR_50.png}
    % \caption{Sample path with $\gamma=10^{-1}$.} \label{fig:gauss_1d_prof_gpr_high}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/1e-06_0.1/50/profile_gaussian_1e-06_0,1_100_5p-GPR_50.pdf}
    %{images/profile/profile_gaussian_1e-06_0,1_100_5p-GPR_50.png}
    % \caption{Sample path with $\gamma=10^{-6}$.} \label{fig:gauss_1d_prof_gpr_low}
  \end{subfigure}
  \caption{A sample path of a $1$-d Gaussian Process on $\Xcal$ ($y_x$,  $\hat{y}_x$),
  and the forecast and prediction confidence bands (GPR-f and GPR-p, respectively).
  \textit{Left:} a sample path with $\gamma=10^{-1}$; \textit{right:} $\gamma=10^{-6}$.}
  \label{fig:gauss_1d_prof_gpr}
\end{figure}

On the provided illustration (fig.~\ref{fig:gauss_1d_prof_gpr}) the confidence bands
on are too wide in the negligible noise case, whereas the bands in seem to have better
coverage rate due to higher noise. Figure~\ref{fig:gauss_1d_prof_conf} shows sample
conformal confidence bands in the same setting. Near the endpoints of the $[0,1]$,
the confidence regions dramatically increase their size, reflecting increasing uncertainty.
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/0.1_0.1/50/profile_gaussian_0,1_0,1_100_5p-RRCM_50.pdf}
    %{images/profile/profile_gaussian_0,1_0,1_100_5p-CRR_50.png}
    % \caption{$\gamma = 10^{-1}$, $\lambda = 10^{-1}$}
    % \label{fig:gauss_1d_prof_conf_high_crr}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/1e-06_0.1/50/profile_gaussian_1e-06_0,1_100_5p-RRCM_50.pdf}
    %{images/profile/profile_gaussian_0,1_0,1_100_5p-RRCM_50.png}
    % \caption{$\gamma = 10^{-6}$, $\lambda = 10^{-1}$}
    % \label{fig:gauss_1d_prof_conf_high_crr}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/0.1_1e-06/200/profile_gaussian_0,1_1e-06_100_5p-RRCM_200.pdf}
    %{images/profile/profile_gaussian_1e-06_0,1_100_5p-CRR_50.png}
    % \caption{$\gamma = 10^{-1}$, $\lambda = 10^{-6}$}
    % \label{fig:gauss_1d_prof_conf_low_crr}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/1e-06_1e-06/200/profile_gaussian_1e-06_1e-06_100_5p-RRCM_200.pdf}
    %{images/profile/profile_gaussian_1e-06_0,1_100_5p-RRCM_50.png}
    % \caption{$\gamma = 10^{-6}$, $\lambda = 10^{-6}$}
    % \label{fig:gauss_1d_prof_conf_low_rrcm}
  \end{subfigure}
  \caption{Sample RRCM confidence bands (eq.~\ref{eq:rrcm_conf_ci}).}
  \label{fig:gauss_1d_prof_conf}
\end{figure}

In can be argued theoretically, that confidence regions for any observation $x_n$
sufficiently far away from the bulk of the training dataset have constant size,
determined only by the train sample (fig.~\ref{fig:limit_1d_ci_size}). Indeed, as
$\|x_n\|^2\to \infty$ ($n$-fixed) the vector $B_{-n}$ in (eq.~\ref{eq:krr_in_resid_B})
approaches the $n$-th unit vector $e_n$, since for the RBF kernel the vector
$\|K_{-n}(x_n)\|^2\to 0$. Since the RBF kernel is bounded, the value $m_n$ (eq.~\ref{eq:krr_leverage})
is a bounded function of $x_n$, which, in turn, implies that eventually all RRCM
regions $S_i$ assume the form of closed intervals $[-|q_i|, |q_i|]$, where
$q_i = m_n e_i'Q_{-n}y_{-n} + o(\|x_n\|^2)$, $i \neq n$. The same is true for the
CRR procedure. Therefore, the conformal procedure essentially reverts to a constant
-size confidence region, determined by the  $n^{-1}\lfloor n(1-\alpha)\rfloor$-th
order statistic of $(|q_i|)_{i=1}^n$. Analogous effects can be observed for the
Gaussian Process confidence interval (eq.~\ref{eq:gp_conf_int}). % In fact, this is
% true for any stationary kernel on $\Real^{d\times 1})$.
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/profile/profile_gaussian_1e-06_0,1_100_1p-RRCM_200.pdf}
  \end{subfigure}\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/profile/profile_heaviside_1e-06_0,1_100_1p-RRCM_200.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/profile/profile_gaussian_1e-06_0,1_100_1p-GPR_200.pdf}
  \end{subfigure}\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/profile/profile_heaviside_1e-06_0,1_100_1p-GPR_200.pdf}
  \end{subfigure}
  \caption{Limiting behaviour of RRCM (\textit{top row}) and GPR (\textit{bottom row})
  confidence regions for a sample path of a Gaussian process (\textit{left column}) and
  the Heaviside function (\textit{right column}).}
  \label{fig:limit_1d_ci_size}
\end{figure}

Now let's assess the performance of the confidence regions in fully-Gaussian setting.
For each tuple of experimental hyper-parameters carry put the experiment, as described
in sec.~\ref{sub:setup}: we generate a large sample path of the Gaussian process
with RBF kernel with fixed precision $\theta_0=10^2$, and then average across $L=25$
resamplings of the train dataset to eliminate possible input sample side-effects.
In each experiment we measure the average ratio of the mean squared error of the
test prediction $\hat{y}_x$ to the test varaince of the observed $y_x$, in order
to monitor the quality of the fit.

Firstly, we consider the case when the $\theta$ hyper-parameter is exactly equal
to $\theta_0$. The performance of Bayesian confidence intervals for increasing training
sample sizes in the case of known high noise-to-signal ratio ($\lambda=\gamma=10^{-1}$)
is presented in fig.~\ref{fig:gauss_1d_asy_gpr_high_width}, whereas the coverage
rates and sizes of conformal confidence intervals are depicted at fig.~\ref{fig:gauss_1d_asy_conf_high}
and fig.~\ref{fig:gauss_1d_asy_conf_high_width}, respectively.
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/gaussian_0,1_0,1_100_ratio.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/0.1_0.1/gaussian_0,1_0,1_100_ratio.png}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/GPR-f/coverage_gaussian_0,1_0,1_100_GPR-f.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/GPR-f/coverage_gaussian_0,1_0,1_100_GPR-f.png}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/GPR-f/width_gaussian_0,1_0,1_100_GPR-f.png}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/GPR-f/width_gaussian_0,1_0,1_100_GPR-f.png}
  \end{subfigure}
  \caption{Asymptotic coverage rate and width of the GPR confidence interval in the fully
  Gaussian case.}
  \label{fig:gauss_1d_asy_gpr_high_width}
\end{figure}
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM/coverage_gaussian_0,1_0,1_100_RRCM.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/0.1_0.1/coverage_gaussian_0,1_0,1_100_RRCM.png}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM-loo/coverage_gaussian_0,1_0,1_100_RRCM-loo.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/0.1_0.1/coverage_gaussian_0,1_0,1_100_RRCM-loo.png}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR/coverage_gaussian_0,1_0,1_100_CRR.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/0.1_0.1/coverage_gaussian_0,1_0,1_100_CRR.png}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR-loo/coverage_gaussian_0,1_0,1_100_CRR-loo.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/0.1_0.1/coverage_gaussian_0,1_0,1_100_CRR-loo.png}
  \end{subfigure}
  \caption{Asymptotic coverage rate of the conformal procedure in the fully Gaussian
  case.}
  \label{fig:gauss_1d_asy_conf_high}
\end{figure}
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM/width_gaussian_0,1_0,1_100_RRCM.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/0.1_0.1/width_gaussian_0,1_0,1_100_RRCM.png}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM-loo/width_gaussian_0,1_0,1_100_RRCM-loo.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/0.1_0.1/width_gaussian_0,1_0,1_100_RRCM-loo.png}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/CRR/width_gaussian_0,1_0,1_100_CRR.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/0.1_0.1/width_gaussian_0,1_0,1_100_CRR.png}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/CRR-loo/width_gaussian_0,1_0,1_100_CRR-loo.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/0.1_0.1/width_gaussian_0,1_0,1_100_CRR-loo.png}
  \end{subfigure}
  \caption{Asymptotic width rate of the conformal confidence intervals in the fully
  Gaussian case. Legend: upward triangles indicate the $5\%$ sample quantile across
  the whole test sample of confidence regions' widths, whereas downward triangles
  indicate the maximal width in the region. The median width is drawn with a slightly
  thicker line. The coloring matches the colors of respective confidence levels as
  in fig.~\ref{fig:gauss_1d_asy_gpr_high_width}}
  \label{fig:gauss_1d_asy_conf_high_width}
\end{figure}
From this particualr expeerimentin this setting, can be conlcued that there is evidence
in favour of the conjectured asymptotic conservative validity of the conformal
confidence regions for all considered NCMs and residual types as well as their
asymptotic efficiency. As expected, the GPR confidence predictions uphold their
theoretical guarantees in this case.

In the second experiment in this batch, we conduct an experiment for negligible
noise-to-signal ratio, fig.\ref{fig:gauss_1d_asy_conf_low}.
Both the conformal and Bayesian confidence sets seem to be too large for exact
coverage, but still offer conservative validity guarantees. The reason for this,
is, judging by the $mse/var$ ratio, the fact that an almost perfect fit was achieved
for samples larger than $n\geq100$. By construction, both GPR and CKRR confidence
intervals incorporate the regularization level $\lambda$, which, first of all,
regularizes the resulting confidence regions, and, second, renders them more
conservative in the case with negligible noise, (see the right column row at
fig.~\ref{fig:gauss_1d_prof_conf}).
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/gaussian_1e-06_1e-06_100_ratio.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/1e-06_1e-06/gaussian_1e-06_1e-06_100_ratio.png}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/GPR-f/coverage_gaussian_1e-06_1e-06_100_GPR-f.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/GPR-f/coverage_gaussian_1e-06_1e-06_100_GPR-f.png}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/RRCM/coverage_gaussian_1e-06_1e-06_100_RRCM.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/1e-06_1e-06/coverage_gaussian_1e-06_1e-06_100_RRCM.png}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/RRCM-loo/coverage_gaussian_1e-06_1e-06_100_RRCM-loo.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/1e-06_1e-06/coverage_gaussian_1e-06_1e-06_100_RRCM-loo.png}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/CRR/coverage_gaussian_1e-06_1e-06_100_CRR.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/1e-06_1e-06/coverage_gaussian_1e-06_1e-06_100_CRR.png}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/CRR-loo/coverage_gaussian_1e-06_1e-06_100_CRR-loo.pdf}
    %{images/output/exp_1d/gaussian/correct_theta/noise_eq_lambda/1e-06_1e-06/coverage_gaussian_1e-06_1e-06_100_CRR-loo.png}
  \end{subfigure}
  \caption{Asymptotic coverage rate of the conformal procedure in the fully Gaussian
  case $\lambda=\gamma=10^{-6}$).}
  \label{fig:gauss_1d_asy_conf_low}
\end{figure}

In the third experiment (fig.~\ref{fig:gauss_1d_asy_gpr_neq}), we check the performance
of the confidence regions in the case when the $\lambda \neq\gamma$, to see how mild
misspecification affects both the Bayesian and the conformal predictions. The asymptotic
coverage rate plots for the CKRR intervals exhibit identical pattern as at fig.~\ref{fig:gauss_1d_asy_conf_high},
are not reproduced.
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/GPR-f/coverage_gaussian_0,1_1e-06_100_GPR-f.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/GPR-f/coverage_gaussian_1e-06_0,1_100_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM/coverage_gaussian_0,1_1e-06_100_RRCM.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/RRCM/coverage_gaussian_1e-06_0,1_100_RRCM.pdf}
  \end{subfigure}
  \caption{GPR (\title{top}) and RRCM (\title{bottom}) confidence inteval coverage
  rate dynamics for the case of mismatched parameters in the fully Gaussian setting.}
  \label{fig:gauss_1d_asy_gpr_neq}
\end{figure}

Figure~\ref{fig:gauss_1d_asy_cov_mis_eq_auto} shows the results of the fourth experiment,
in which either $\theta\neq \theta_0$ is fixed, or automatically selected using the GPR
ML estimate (maximizing eq.~\ref{eq:bkrr_likelihood}). The results for $\lambda=\gamma=10^{-1}$
are mostly similar to the first experiment: both GPR and CKRR confidence intervals seem
have comparable validity and performance. It is worth noting, however, that in the case
when the kernel in KRR is imprecise ($\theta << \theta_0$), thre resulting KRR estimate
(eq.~\ref{eq:krr_approx}) the more smoothed, and thus biased, which is why the coverage
of rate of all confidence intervals might be slightly off (fig.~\ref{fig:gauss_1d_asy_cov_mis_eq}).
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM/coverage_gaussian_0,1_0,1_auto_RRCM.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM-loo/coverage_gaussian_0,1_0,1_auto_RRCM-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR/coverage_gaussian_0,1_0,1_auto_CRR.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR-loo/coverage_gaussian_0,1_0,1_auto_CRR-loo.pdf}
  \end{subfigure}
  \caption{GPR (\title{left column}) and RRCM (\title{right columns}) confidence
  inteval coverage rate dynamics for the case of ML estimate of $\theta$ in the
  fully Gaussian setting.}
  \label{fig:gauss_1d_asy_cov_mis_eq_auto}
\end{figure}
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM/coverage_gaussian_0,1_0,1_10_RRCM.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM-loo/coverage_gaussian_0,1_0,1_10_RRCM-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR/coverage_gaussian_0,1_0,1_10_CRR.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR-loo/coverage_gaussian_0,1_0,1_10_CRR-loo.pdf}
  \end{subfigure}
  \caption{GPR (\title{left column}) and RRCM (\title{right columns}) confidence
  inteval coverage rate dynamics for the case of $\theta << \theta_0$ in the fully
  Gaussian setting.}
  \label{fig:gauss_1d_asy_cov_mis_eq}
\end{figure}
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/GPR-f/width_gaussian_0,1_0,1_10_GPR-f.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM/width_gaussian_0,1_0,1_10_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/GPR-f/width_gaussian_0,1_0,1_auto_GPR-f.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM/width_gaussian_0,1_0,1_auto_RRCM.pdf}
  \end{subfigure}
  \caption{GPR (\title{left column}) and RRCM (\title{right columns}) confidence
  inteval coverage rate dynamics for the case of $\theta\neq \theta_0$ in the fully
  Gaussian setting.}
  \label{fig:gauss_1d_asy_mis_eq}
\end{figure}

Semi- and non- Gaussian $1$-d settings, fig.~\ref{fig:non_gauss_1d_prof_gpr}. 
\begin{figure}%[t, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/heaviside/0.1_0.1/50/profile_heaviside_0,1_0,1_100_10p-GPR_50.pdf}
    %{images/profile/profile_heaviside_0,1_0,1_100_10p-GPR_50.png}
    \caption{$\gamma=10^{-1}$} \label{fig:non_gauss_1d_prof_gpr_high}
  \end{subfigure}~
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/heaviside/1e-06_1e-06/50/profile_heaviside_1e-06_1e-06_100_10p-GPR_50.pdf}
    %{images/profile/profile_heaviside_1e-06_1e-06_100_10p-GPR_50.png}
    \caption{$\gamma=10^{-6}$} \label{fig:non_gauss_1d_prof_gpr_low}
  \end{subfigure}
  \caption{A sample path of $f(x)+\gamma \epsilon_x$, for $\epsilon_x\sim \Ncal(0,1)$.}
  \label{fig:non_gauss_1d_prof_gpr}
\end{figure}

Sample confidence intervals in the current non-Gaussian setting are provided in
fig.~\ref{fig:non_gauss_1d_prof_conf}.
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/heaviside/0.1_0.1/50/profile_heaviside_0,1_0,1_100_10p-CRR_50.pdf}
    %{images/profile/profile_heaviside_0,1_0,1_100_10p-CRR_50.png}
    \caption{CRR, $\gamma=10^{-1}$.}
    \label{fig:non_gauss_1d_prof_conf_high_crr}
  \end{subfigure}\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/heaviside/0.1_0.1/50/profile_heaviside_0,1_0,1_100_10p-RRCM_50.pdf}
    %{images/profile/profile_heaviside_0,1_0,1_100_10p-RRCM_50.png}
    \caption{RRCM, $\gamma=10^{-1}$.}
    \label{fig:non_gauss_1d_prof_conf_high_crr}
  \end{subfigure}
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/heaviside/1e-06_1e-06/50/profile_heaviside_1e-06_1e-06_100_10p-CRR_50.pdf}
    %{images/profile/profile_heaviside_1e-06_1e-06_100_10p-CRR_50.png}
    \caption{CRR, $\gamma=10^{-6}$.}
    \label{fig:non_gauss_1d_prof_conf_low_crr}
  \end{subfigure}\
  \begin{subfigure}[b]{0.475\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/heaviside/1e-06_1e-06/50/profile_heaviside_1e-06_1e-06_100_10p-RRCM_50.pdf}
    %{images/profile/profile_heaviside_1e-06_1e-06_100_10p-RRCM_50.png}
    \caption{RRCM, $\gamma=10^{-6}$.}
    \label{fig:non_gauss_1d_prof_conf_low_rrcm}
  \end{subfigure}
  \caption{Sample CCR (eq.~\ref{eq:crr_conf_ci}) and RRCM (eq.~\ref{eq:rrcm_conf_ci}) confidence bands.}
  \label{fig:non_gauss_1d_prof_conf}
\end{figure}
%% Describe the 1d profiles (best looking)
%%%% one gaussian case (50: 10%/5%, 200: 5%)
%%%% two nongaussian cases (50: 10%/5%, 200: 5%)
%%%%%% heaviside
%%%%%% and f6
%% Describe the 1d convergence results (best/worst)
%%%% gaussian case with exact paramters
%%%% gaussian case with wrong paramters
%%%%%% theta - exact:
%%%%%%%% lambda = noise:
%%%%%%%% lambda  noise:
%%%%%% theta - wrong:
%%%%%%%% lambda = noise:
%%%%%%%% lambda  noise:
%%%% non-gaussian case

%% Show some 2d sample test plots
%% Show two tables (gaussian/non-gaussian case).
%%%% gaussian case with exact paramters
%%%% non-gaussian case

Convergence in Fig.~\ref{fig:asy1d} is typical to all conducted experiments, and
gives empirical support to the conjecture that conformal intervals are asymptotically
conservative in batch learning setting. Convergence can also e observed in $2$-D
setting as shown on fig.~\ref{fig:asy2d} for a <<function>> with kernel parameters
<<$\Theta$>>.

We make the following empirical conclusions based on the conditional numerical study
in the $1$-d case: \begin{enumerate}
  \item the choice of the NCM does not appear to affect the validity of the confidence
  regions;
  \item Bayesian confidence regions, based on the Gaussian Process regression, seem
  to perform poorly relatively to the Conformal regions in non-gaussian cases;

\end{enumerate}

% Gaussian Process interpretation of the KRR (\ref{sub:bayesian_krr_confidence_interval}) also
% permits adaptive selection of the precision parameter $\theta$ of the RBF kernel
% by maximizing the joint likelihood of the training data $(X, y)$. This options has
% also been investigates, but yielded conclusions which are qualitatively identical
% to the ones summarized above.

% subsection results (end)

% section numerical_study (end)

\section{Conclusion and further work} % (fold)
\label{sec:conclusion_and_further_work}

In the non-gaussian experiments we found empirical evidence sugge that the GPR confidence
region is not valid when the noise gaussianity assumptions fail to hold. Identical
experiments on the conformal procedures demonstrated that indeed, theres is empirical
evidence supporting the conjectured conservative validity in the batch setting.
At the same time, confidence intervals of the conformalized kernel ridge regression
seem to perform not worse than the confidence rgions of the Gaussian Process regression,
when the Gaussianity assumptions hold.

Further work on the validity of conformal prrocedures in batch setting, with application
to anomaly detection, inculdes: \begin{itemize}
  \item establishing theoretical foundations for the proposed conjectures for the KRR
  with RBF kernel, or isolating special cases when it holds, and studying the cases
  when it fails;
  \item obtaining a generalization to the asymptotic efficiency result in \cite{burnaevV14},
  in for a kernel ridge regression with genreal kernels.
\end{itemize}

% section conclusion_and_further_work (end)

% \clearpage

% \bibliographystyle{amsplain}
\bibliographystyle{ugost2008ls}
\bibliography{references}

\end{document}

