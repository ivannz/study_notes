% \documentclass{ITaSconf}
% \documentclass[a4paper]{article}
\documentclass[a4paper,14pt]{extarticle}

\usepackage[utf8]{inputenc}

\usepackage{geometry}
\usepackage{fullpage}

\usepackage[mathcal]{euscript}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}
\usepackage{algorithm2e}
\usepackage{subcaption}
\usepackage{caption}

\newcommand{\ex}{\mathop{\mathbb{E}}\nolimits}
\newcommand{\pr}{\mathop{\mathbb{P}}\nolimits}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\nil}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\diag}{\mathop{\text{diag}}\nolimits}

\title{Conformalized Kernel Ridge Regression}
\author{Burnaev, E. V., Nazarov, I. N.}

\begin{document}
\maketitle
% \tableofcontents
% \clearpage

\section{Introduction} % (fold)
\label{sec:introduction}

In general, regression problems are concerned with prediction of a continuous-valued
attribute $y$ associated with an predetermined input sample $x$. This goal is achieved
by accurate estimation of the conditional mean of the target distribution. However
in certain practical tasks the point estimates of the predicted values are insufficient,
and must be supplemented by a measure of predictive uncertainty. One possible solution
is provided by introducing additional distributional assumption into the regression
model.


For example, on the space of target values $\Ycal$.
For example, suppose one is provided with a train dataset $(X, y)$ of size $n\geq 1$,
with the rows of the design matrix $X\in \Real^{n\times d}$ given by $x_i\in \Real^{d\times 1}$,
and the vector of target values $y = (y_i)_{i=1}^n \in \Real^{n\times 1}$. The ordinary
linear regression assumes that $\ex(y|x) = \beta'x$ for some $\beta\in \Real^{d\times1}$,
which is equivalent to
\begin{equation}\label{eq:signal_model}
  y_i = \beta'x_i + \epsilon_i\,,
\end{equation}
for iid zero-mean random variables $(\epsilon_i)_{i=1}^n$. The OLS estimate of $\beta$
is $\hat{\beta} = (X'X)^{-1} X'y$

If furthermore $\epsilon_i
\sim \Ncal(0, \sigma^2)$, then the predictive distribution of the target value $y*$
at a test input sample $x^*$ is $y^* \sim \Ncal$

If the train data were treated as a realization of some underlying Gaussian Process,
then one readily recovers the predictive distribution of the unobserved 
   given the data $(X, y)$

\subsection{Problem statement} % (fold)
\label{sub:problem_statement}

The following are a set of methods intended for regression in which the target value
is expected to be a linear combination of the input variables. In mathematical notion,
if $\hat{y}$ is the predicted value, then it is given by a linear combination of
the input features.

Ridge regression addresses some of the problems of Ordinary Least Squares by imposing
a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual
sum of squares,

Kernel ridge regression (KRR) [M2012] combines Ridge Regression (linear least squares
with l2-norm regularization) with the kernel trick. It thus learns a linear function
in the feature space induced by the respective kernel and the data.

Given train-set observations $(x_i, y_i)_{i=1}^n \in \Real^{d\times 1}\times \Real$,
with $x_i$ collected row-wise in a design matrix $X\in \Real^{n\times d}$, the target
vector -- $y=(y_i)_{i=1}^n \in \Real^{n\times 1}$, and a regularization parameter
$a > 0$, the sample-space \textbf{R}idge \textbf{R}egression problem solves this
minimization problem
$$ \Lcal(X, y, a)
  = (y - X\beta - \one\beta_0)'(y - X\beta - \one\beta_0) + a\beta'\beta
    \to \min_{\beta_0\in \Real, \beta\in\Real^{d\times 1}}
  \,. $$
In fact the intercept term is estimated with
$$ \hat{\beta}_0 = \bar{y} - \bar{X} \hat{\beta} \,, $$
where $\bar{y}$ and $\bar{X}$ are the mean target and observation values respectively,
and $\hat{\beta}$ solves the intercept-free Ridge regression problem run on centered
observations and targets. Therefore int the following we omit the intercept term and
assume that the relevant columns in the design matrix have been dealt with, and both
the design matrix and the targets have been centered.

% сть различие между терминами: "приближение функций", "аппроксимация", "интерполяция", и "регрессия". Оно заключается в следующем.
% Приближение функций. Дана функция u дискретного или непрерывного аргумента. Требуется найти функцию f из некоторого параметрическую семейства, например, среди алгебраических полиномов заданной степени. Параметры функции  f должны доставлять минимум некоторому функционалу, например,
% \rho(u,f)=\left(\frac{1}{b-a}\int_a^b|f(x)-g(x)|^2dx\right)^{\frac{1}{2}}.
% Термин аппроксимация — синоним термина "приближение функций". Чаще используется тогда, когда речь идет о заданной функции, как о функции дискретного аргумента. Здесь также требуется отыскать такую функцию f, которая проходит наиболее близко ко всем точкам заданной функции. При этом вводится понятие невязки — расстояния между точками непрерывной функции f и соответствующими точками функции u дискретного аргумента.
% Интерполяция функций — частный случай задачи приближения, когда требуется, чтобы в определенных точках, называемых узлами интерполяции совпадали значения функции u и приближающей ее функции f. В более общем случае накладываются ограничения на значения некоторых производных f производных. То есть, дана функция u дискретного аргумента. Требуется отыскать такую функцию f, которая проходит через все точки u. При этом метрика обычно не используется, однако часто вводится понятие "гладкости" искомой функции.
% Регрессия и классификация тесно связаны друг с другом. Термин алгоритм в классификации мог бы стать синонимом термина модель в регрессии, если бы алгоритм не оперировал с дискретным множеством ответов-классов, а модель — с непрерывно-определенной свободной переменной.


The Kernel Ridge Regression is formulated as the following problem:
% \begin{equation*}
% \end{equation*}

Mention the kernel trick.
Briefly say something about RKHS.

The Kernel Ridge Regression for a given PD kernel $K:\Xcal\times \Xcal \to \Real$
solves the following problem:
\begin{equation*}
  \|y - K_{XX}\beta\|^2 + \lambda \beta' K_{XX} \beta
    \to \min_{\beta\in \Real^{n\times 1}}
    \,,
\end{equation*}
with $\|\cdot\|$ -- the euclidean norm, and $K_{XX}$ defined as in \ref{eq:gp_cond_dist}.
Assuming $K_{XX}$ is invertible, elementary matrix algebra yields the following
equation for the minimizer $\beta$:
\begin{equation*}
  (\lambda I_n + K_{XX}) \beta - y = 0 \,.
\end{equation*}

% subsection problem_statement (end)


\subsubsection{RKHS} % (fold)
\label{ssub:rkhs}

A tuple $(\Hcal, \langle\cdot, \cdot\rangle)$ is an inner-product space over $\Real$
if $\Hcal$ is a vector space over $\Real$, and $\langle\cdot, \cdot\rangle : \Hcal
\times \Hcal \mapsto \Real$ is an inner product in $\Real$ -- a map linear in both
arguments, symmetric, and possessing the following properties:
\begin{enumerate}
  \item $\langle f, f\rangle\geq 0$ for any $f\in \Hcal$;
  \item $\langle f, f\rangle = 0$ if and only if $f = \nil_\Hcal$.
\end{enumerate}
Over the field of complex numbers $\Cplx$ the requirements of symmetry and bi-
linearity are swapped with being Hermitian, and linear with respect to the first
argument. In the following presentation all considered vector spaces are over the
field of reals.

A Hilbert space is an inner product space, which is complete with respect to the
natural metric induced by the norm $ \|f\| = \sqrt{\langle f, f\rangle} $. A \textbf{R}eproducing
\textbf{K}ernel \textbf{H}ilbert \textbf{S}pace is a complete inner-product vector
space of maps $\Xcal \mapsto \Real$ over the field $\Real$, in which the evaluation
functional defined by $\epsilon_x(f) = f(x)$ is $(\Hcal, \|\cdot\|) \mapsto (\Real, |\cdot|)$
bounded for any $x\in \Xcal$ (in normed spaces bounded linearity is equivalent to
continuity). Equivalently, an \textbf{RKHS} is a Hilbert space for which there exists
a map $\phi:\Xcal\to\Hcal$, and a \textbf{P}ositive \textbf{D}efinite kernel function
$K:\Xcal \times \Xcal \mapsto \Real$ such that \begin{enumerate}
  \item $K(x,y) = \langle \phi(x), \phi(y) \rangle$ for all $x, y\in \Xcal$;
  \item $g(x) = \langle g, K(x, \cdot)\rangle$ for all $x\in \Xcal$ and every
  $g\in \Hcal$.
\end{enumerate}
A function $K:\Xcal \times \Xcal \mapsto \Real$ is positive definite if for any
$n\geq1$, $(x_i)_{i=1}^n \in \Xcal$ and $\alpha \in \Real^{n\times 1}$
$$ \alpha'K\alpha
  = \sum_{i=1}^n \sum_{j=1}^n \alpha_j \alpha_i K(x_i, x_j)
  \geq 0
  \,, $$
or, in other words the Gram matrix $K$ is positive semi-definite, where $K$ is an
$n \times n$ matrix of $K(x_i, x_j)$. Note that unlike linear algebra, ``positive
definite'' here permits such non-zero weights, for which the product is exactly zero.
Positive definite kernels are also known as Mercer kernels, or covariance kernels.

Finally, any \textbf{PD} kernel $K$ gives rise to a so called \textbf{canonical RKHS}
associated with $K$ constructed by a standard metric-completion argument for a pre-Hilbert
space (a non-complete inner-product space)
$$ \Hcal_0
  = \bigl\{
    \sum_{i=1}^n \alpha_i K(x_i, \cdot)
    \,:\, n\geq1, (x_i)_{i=1}^n \in \Xcal, (\alpha_i)_{i=1}^n\in \Real
  \bigr\}
  \,, $$
with an inner-product $ \langle f, g \rangle = \sum_{i=1}^n \sum_{j=1}^m \alpha_i K(x_i, z_j) \beta_j $,
where $f, g\in \Hcal_0$ with representations $f = \sum_{i=1}^n \alpha_i K(x_i, \cdot)$
and $g = \sum_{j=1}^m \beta_j K(z_j, \cdot)$. In the completed RKHS $\Hcal = \bigl[\Hcal_0\bigr]$
the map $\phi$ is given by the canonical map $x\mapsto K(x, \cdot)$ and $K$ is the
reproducing kernel. The inner-prodcut is extended to $\Hcal$ by contonuity as a step
in completion of $\Hcal_0$. It is worth noting, that the pre-Hilbert space $\Hcal_0$
is by construction dense everywhere in $\Hcal$ with respect to the norm-mertic
induced by the completed inner-product.

% subsubsection rkhs (end)

\subsubsection{Kernel methods} % (fold)
\label{ssub:kernel_methods}

Given a train sample $(x_i, y_i)_{i=1}^n \in X\times \Real$ a general kernel-based
machine learning regularized loss minimization problem is to solve
$$ \Lcal\bigl((x_i, y_i)_{i=1}^n, f\bigr)
  + \lambda \Omega\bigl(\|f\|\bigr)
  \to \min_{f\in \Hcal}\,, $$
where $\Hcal$ is a Hilbert Space with Reproducing Kernel $K$, $\Lcal$ is a loss
function which depends on $f$ through $(f(x_i))_{i=1}^n$, and $\Omega:\Real\mapsto\Real$
is a monotone increasing function used for regularization. In this setting the Representer
theorem states that the optimal solution is to be sought in a finite dimensional
linear span of the canonical feature maps $\phi: x\mapsto K(x, \cdot)$ evaluated at
the sample objects:
$$ f^* \in \bigl\{ \beta'\Phi\,:\, \beta \in \Real^{n\times 1} \bigr\} \,, $$
where $\Phi = \bigl(\phi(x_i)\bigr)_{i=1}^n \in \Hcal^{n\times 1}$.

Another principle, the so called ``Kernel-trick'' states that whenever an ML algorithm
could be reduced to a form, which depends on the training sample data only thorough
inner-products between the objects (a or Gram matrix), then it is possible to construct
another ML algorithm by simply replacing the products with an arbitrary \textbf{PD}
kernel. It should be noted that care must be taken to ensure additional conditions
on the input domain imposed by this ``trick'' are met.

In particular, the \textbf{RR} solution in its dual form depends only on the Gram
matrix if inter-object inner-products $X X'$ in the input domain, which is why it is
possible to restate it as the Kernel \textbf{RR} problem: given a training sample
$X = (x_i)_{i=1}^n\in\Xcal^{n\times 1}$ and $y\in\Real^{n\times 1}$
$$ \|y - f\|^2 + a \|f\|^2 \to \min_{f \in \Hcal} \,, $$
with $f \in \Real^{n\times 1}$ being a column-vector of $f$ evaluated at every input
in $X$. Here $\Hcal$ is the canonical RKHS associated with a Mercer kernel $K$. Note
that with slight abuse of notation the inner-products and norms over $\Hcal$ and
$\Real^{n\times 1}$ are used without denoting which is which, since it is always
clear from both the context and the type of the argument of the norm as a function.

The Representer theorem states that the solution to this minimization is of the form
$f = \Phi'\beta$, for $\Phi = (\phi(x_i))_{i=1}^n \in \Hcal^{n\times 1}$. By definition
of the Gram matrix $K$ and due to its symmetry for any $j=1,\ldots,n$
$$ f(x_j)
  = \Phi(x_j)' \beta
  = k_{x_j}' \beta
  = (K e_j)' \beta
  = e_j' K \beta
  \,, $$
where $k_x = \Phi(x) = (\phi(x_i)(y))_{i=1}^n \in \Real^{n\times 1}$ and $e_j$ is
the $j$-th unit vector in $\Real^{n\times 1}$. Furthermore
$$ \| f \|^2
  = \sum_{i=1}^n\sum_{j=1}^n\beta_i\beta_j \langle\phi(x_i), \phi(x_j)\rangle
  = \sum_{i=1}^n\sum_{j=1}^n\beta_i\beta_j K(x_i, x_j)
  = \beta' K \beta \,. $$
Thus the kernel Ridge Regression problem is reduced to the following finite-dimensional
convex minimization problem:
$$ \|y - K \beta \|^2 + \lambda \beta' K \beta \to \min_{\beta\in \Real^{n\times 1}} \,. $$
The First order conditions imply that
$$ \hat{\beta} = (aI_n + K)^{-1} y \,\text{ and }\, \hat{y}_x = k_x' \beta \,, $$
-- the estimated weight vector and an optimal prediction at $x\in \Xcal$, respectively.

% subsubsection kernel_methods (end)



\subsection{Conformal prediction} % (fold)
\label{sub:conformal_prediction}

Conformal prediction is a technique designed to yield a statistically valid measure
of confidence for individual predictions made by a machine learning algorithm and
to be applicable in both the supervised and unsupervised online learning settings.
In the supervised case, let $\Zcal$ denote the object-target space $\Xcal \times \Ycal$.
The core of this technique is a measurable map $A: \Zcal^+\times \Zcal \mapsto \Real$
-- a \textbf{N}on-\textbf{C}onformity \textbf{M}easure, which for a training sample
$Z = (z_i)_{i=1}^n$ and a test object $z_*\in \Zcal$ returns a value $A(Z, z_*)$,
which quantifies how much different $z_*$ is relative to a sample $Z$. A conformal
predictor is a procedure, that takes in a sample $Z_{:n}=(z_i)_{i=1}^n\in\Zcal$,
a test object $x_*\in\Xcal$, and a confidence level $\epsilon\in(0,1)$, and outputs
a confidence set $\Gamma^\epsilon(Z, x_*) \subseteq Y$ for the corresponding target
value $y_*$ (see algorithm~\ref{alg:conf_predictor}). The central idea is to compute
an empirical estimate the p-value of a test object-target pair $(x_*, y_*)$ using
some convenient \textbf{NCM} with respect to the reference sample $Z_{:n}$.

\begin{algorithm}%[H]
  \caption{Conformal predictor} \label{alg:conf_predictor}
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{
    $A$ -- \textbf{NCM}, $\epsilon \in (0,1)$ -- significance level, training sample
    $Z_{:n}=(z_i)_{i=1}^n \in \Zcal^+$ and a test object $x_*\in \Xcal$.}
  \Output{Confidence set $\Gamma^\epsilon$ for the test target $y_*$.}
  \BlankLine
  $\Gamma^\epsilon \leftarrow \emptyset$\;
  \For{$y \in Y$}{
    $z_{n+1} \leftarrow (x_*, y)$\;
    \For{$i = 1,\ldots, n, n+1$}{
      $Z_{-i} \leftarrow \bigl(z_j\bigr)_{j=1, j\neq i}^{n+1}$\;
      $\eta_i \leftarrow A(Z_{-i}, z_i)$\;
    }
    $ p^y \leftarrow (n+1)^{-1} \bigl\lvert \{
        i \,:\, \eta_i \geq \eta_{n+1}
      \} \bigr\rvert $\;
    \If{$p^y > \epsilon$}{
      $\Gamma^\epsilon \leftarrow \Gamma^\epsilon \cup\{y\}$\;
    }
  }
  Return $\Gamma^\epsilon$\;
\end{algorithm}

In \cite{Vovketal2005} is has been shown, that if a sequence $(z_n)_n{\geq1}$ is
generated by an exchangeable distribution $P$, then the coverage probability of
the prediction set $\Gamma^\epsilon$, yielded by procedure \ref{alg:conf_predictor},
is at least $1-\epsilon$ and successive errors are independent in online learning
and prediction setting. Thus the outlined procedure guarantees that unconditionally
$$ \pr\bigl( y_* \notin \Gamma^\epsilon(Z_{:n}, x_*)\bigr) \leq \epsilon \,, $$
where $\pr = P_{Z_{:n}, {z_*}}$, $(x_*, y_*)=z_*$, and $Z_{:n}$ denotes a sample of
size $n$. In a special case, when $z_n$ are iid the measure is just the product measure
$P_{z_1} \otimes \ldots \otimes P_{z_n} \otimes P_{z_*}$.

In general, any real-valued jointly measurable \textbf{NCM} could be used, the only
difference will be in the size of the predicted confidence set (efficiency) and its
informativeness. Despite attractive theoretical guarantees, the procedure suffers
from a significant drawback: in the general case it is very computationally inefficient,
since it exhaustively searches over $\Ycal$ (generally infeasible in regression
settings) and loops over all observations in the extended sample $Z_{:n}\cup\{z_*\}$
to get the calibration set of $(\eta_i)_{i=1}^{n+1}$.

% subsection conformal_prediction (end)

\subsection{Goals} % (fold)
\label{sub:goals}

The goal of this paper is study to perform numerical a study of the validity of
the conformal confidence intervals in the batch learning setting with Kernel Ridge
Regression.

% subsection goals (end)

% section introduction (end)

\section{Derivation} % (fold)
\label{sec:derivation}
In this section we derive the keys formulae, which are necessary for the construction
of the studied confidence regions. The first half is concerned with the Bayesian 
confidence regions, which arise if the kernel ridge regression is looked at from
the perspective of a Gaussian Process. In the second half we describe the construction
of confidence regions based on the conformal procedure alg.~\ref{alg:conf_predictor}.

\subsection{Bayesian KRR interval} % (fold)
\label{sub:bayesian_krr_interval}

Confidence interval in the Bayesian setting can easily be obtained if the KRR
problem is examined from a Gaussian Process point of view.

%% Brief intro into Gaussian processes
A random process $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$, with mean $m : \Xcal
\mapsto \Real$ and \textbf{PD} function $\Kcal : \Xcal \times \Xcal \mapsto \Real$
is a Gaussian Process if all its finite dimensional distributions are multivariate
Gaussian. In particular, for any $n\geq1$ and any $X = (x_i)_{i=1}^n \in \Xcal$ the
vector $f_X = (f(x_i))_{i=1}^n \in \Real^{n\times 1}$ has Gaussian distribution with
mean $m_X = (m(x_i))_{i=1}^n$ and covariance matrix $K_{XX} = (\Kcal(x_i,x_j))_{ij}$ --
the $n\times n$ Gram matrix of $\Kcal(\cdot,\cdot)$ evaluated at points $(x_i)_{i=1}^n$:
\begin{equation*}
  f_X \sim \Ncal_n(m_X, K_{XX}) \,.
\end{equation*}

In this formulation the model \ref{eq:signal_model} represents the addition of an
independent white noise process $(\epsilon_x)_{x\in \Xcal}$ with variance $\lambda$
to $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$. Indeed, for any $n\geq1$ and
any $X = (x_i)_{i=1}^n \in \Xcal$ the finite dimensional distribution of $(y_x)_{x\in\Xcal}$
is $y_X\sim \Ncal_n(m_X, \lambda I_n + K_{XX})$, because
\begin{equation*}
  y_X
    = \begin{pmatrix} I_n & I_n \end{pmatrix}
    \begin{pmatrix} f_X \\ \epsilon_X \end{pmatrix}
    \,,
\end{equation*}
-- a linear combination of $f$ and $\epsilon$ with joint distribution
\begin{equation*}
  \begin{pmatrix} f \\ \epsilon \end{pmatrix}
    \sim \Ncal_{n+n}\begin{pmatrix}
        \begin{pmatrix} m \\ 0 \end{pmatrix},
        \begin{pmatrix}
          K_{XX} & 0 \\
          0 & \lambda I_n
        \end{pmatrix}
      \end{pmatrix}
    \,.
\end{equation*}
Therefore $(y_x)_{x\in\Xcal} \sim GP(m(x), \Kcal(x,x'))$ with $\Kcal$ given by a PD
kernel
\begin{equation*}
  \Kcal(x,x') = \lambda \delta_{x,x'} + K(x,x') \,,
\end{equation*}
where $\delta_{x,x'}$ is the delta-function on $\Xcal\times \Xcal$.

%% Conditional distribution
The conformal procedure constructs confidence intervals for test observations $y_x$,
contaminated by the additive noise $\epsilon_x$, as opposed to true values $f_x$ in
the model \ref{eq:signal_model}. In the Bayesian setting this means that it is necessary
to get a conditional distribution of a test sample $y_{X^*} = (y_{x^*_j})_{j=1}^l$,
with respect to the train sample $y_X = (y_{x_i})_{i=1}^n$. Gaussianity makes it
especially easy to derive conditional distributions. Indeed, if
\begin{equation*}
  \begin{pmatrix}z_1 \\ z_2\end{pmatrix}
    \sim \Ncal_{d_1+d_2}\Biggl(
      \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix},
      \begin{pmatrix}
        \Sigma_{11} & \Sigma_{12} \\
        \Sigma_{21} & \Sigma_{22}
      \end{pmatrix}
    \Biggr)
    \,,
\end{equation*}
then the conditional distribution of $z_1$ given $z_2$ is Gaussian with
\begin{equation*}
  {z_1}_{|z_2}
    \sim \Ncal_{d_1}\bigl(
      \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2),
      \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
    \bigr)
    \,,
\end{equation*}
provided the inverse of $\Sigma_{22}$ exits. Therefore the conditional distribution
is given by
\begin{equation} \label{eq:cond_distr}
  y_{X^*}\vert_{y_X}
    \sim \Ncal_l\bigl(
      m_{X^*} + K_{X^*X} Q_X (y_X - m_X),
      \Sigma_K(X^*)
    \bigr)
    \,,
\end{equation}
where $\Sigma_K(X^*) = K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*}$, and
$Q_X = \bigl(K_{XX}\bigr)^{-1}$, since $(y_x)_{x\in\Xcal}$ is a GP and
\begin{equation*}
  \begin{pmatrix} y_X \\ y_{X^*} \end{pmatrix}
    \sim \Ncal_{n+l}\begin{pmatrix}
      \begin{pmatrix} m_X \\ m_{X^*} \end{pmatrix},
      \begin{pmatrix}
        K_{XX} & K_{XX^*} \\
        K_{X^*X} & K_{X^*X^*}
      \end{pmatrix}
    \end{pmatrix}
    \,,
\end{equation*}
with $K_{XX^*} = (\Kcal(x_i, x^*_j))\in \Real^{n\times l}$.

The Bayesian prediction of $y_{X^*}$ conditional on observing $y_X$ is given by
the Maximum Posterior prediction (which in the Gaussian case coincides with the
true Bayesian prediction $\ex\bigl(y_{X^*}\,|\, y_X\bigr)$):
\begin{equation*}
  \hat{y}_{y_X}(X^*) = m_{X^*} + K_{X^*X} Q_X \bigl(y_X - m_X\bigr) \,.
\end{equation*}

The Gaussian Process Kernel Ridge Regression is usually formulated with $m=0$ and
$\Kcal(x,x') = \sigma^2(\lambda \delta_{x,x'} + K(x,x'))$, for some PD kernel $K$,
whence the distribution of an observation at a test object $x^*\in \Xcal$, conditional
on the train data $y_X$ is
\begin{equation} \label{eq:gp_cond_dist}
{y_{x^*}}_{|y_X}
  \sim \Ncal\bigl( \hat{y}_{y_X}(x^*), \sigma^2 \sigma_K^2(x^*) \bigr) \,,
\end{equation}
with $\hat{y}_{y_X}(x^*) = K_X(x^*)' Q_X y_X$, and
\begin{equation*}
  \sigma_K^2(x^*)
    = \lambda + K(x^*, x^*) - K_X(x^*)' Q_X K_X(x^*) \,,
\end{equation*}
where $Q_X = \bigl(\lambda I_n + K_{XX}\bigr)^{-1}$, $K_{XX} = (K(x_i,x_j))_{ij}$,
and $K_X = (K(x_i, \cdot))_{i=1}^n: \Xcal \mapsto \Real^{n\times1}$. The $1-\alpha$
confidence interval is thus given by
\begin{equation} \label{eq:gp_conf_int}
\Gamma^\alpha_{y_X}(x^*)
  = \hat{y}_{y_X}(x^*)
  + \sigma \sqrt{\sigma_K^2(x^*)}
  \times [z_{\frac{\alpha}{2}}, z_{1-\frac{\alpha}{2}}]
  \,,
\end{equation}
where $\hat{y}_{y_X}(x^*) = K_X(x^*)' Q_X y_X$, and $z_\alpha$ is the $\alpha$
quantile of $\Ncal(0, 1)$.

% subsection bayesian_krr_interval (end)

\subsection{Conformal KRR interval} % (fold)
\label{sub:conformal_krr_interval}

% Consider a problem of estimating the prediction accuracy of a test sample $(X^*, y^*)$
% given a training sample $(X, y)$, where $X = (x_i)_{i=1}^n$, $y\in \Real^{n\times 1}$,
% $X^* = (x^*_j)_{j=1}^m = (x_i)_{i=n+1}^{n+m}$ and $y^*\in \Real^{m\times 1}$. 
% The out-of-train-sample prediction residuals are given by
% \begin{equation*}
%   y^* - \hat{y}^*_{|(X, y), X^*}
%     = y^* - K_{XX^*}' (\lambda I_n + K_{XX})^{-1} y
%     \,,
% \end{equation*}
% with $K_{XX^*} = (K_X(x^*_j))_{j=1}^m \in \Real{n\times m}$.
% Now an in-sample prediction based on the optimal weights over the pooled sample
% $(\tilde{X}, \tilde{y})$ given by $((X, X^*), (y, y^*))$. First, since the matrix
% $\lambda I_n + K_{XX}$ is always invertible for $\lambda > 0$, block-matrix inversion
% formula implies:
% \begin{align*}
%   Q_{\tilde{X}}
%     &% = \bigl( \lambda I_{n+m} + K_{\tilde{X}\tilde{X}} \bigr)^{-1}
%     = \begin{pmatrix}
%       \lambda I_n + K_{XX} & K_{XX^*} \\
%       K_{X^*X} & \lambda I_m + K_{X^*X^*}
%     \end{pmatrix}^{-1} \\
%     &= \begin{pmatrix}
%       Q_X + Q_X K_{XX^*} M^{-1} K_{X^*X} Q_X & - Q_X K_{XX^*} M^{-1} \\
%       - M^{-1} K_{X^*X} Q_X & M^{-1}
%     \end{pmatrix}
%     \,,
% \end{align*}
% where $Q_X = \bigl( \lambda I_n + K_{XX} \bigr)^{-1}$ and
% \begin{equation*}
%   M = \lambda I_m + K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*} \,.
% \end{equation*}
% Since $\lambda I_{n+m} + K_{\tilde{X}\tilde{X}}$ is an invertible positive definite
% matrix ($M\succeq 0$), its inverse is \textbf{PD} as well, whence $M$ is positive
% definite and invertible as well. If $H_{\tilde{X}} = K_{\tilde{X}\tilde{X}} Q_{\tilde{X}}$,
% then $I_{n+m} - H_{\tilde{X}} = \lambda Q_{\tilde{X}}$, whence
% \begin{align*}
%   y^* - \hat{y}^*_{|(\tilde{X}, \tilde{y})}
%     &= \lambda \begin{pmatrix} 0\\ I_m \end{pmatrix} Q_{\tilde{X}}
%         \begin{pmatrix} y\\ y^* \end{pmatrix} \\
%     % &= \lambda \begin{pmatrix} - M^{-1} K_{X^*X} Q_X & M^{-1} \end{pmatrix}
%     %   \begin{pmatrix} y\\ y^* \end{pmatrix} \\
%     &= \lambda M^{-1} \bigl( y^* - K_{X^*X} Q_X y \bigr)
%     = \lambda M^{-1} \bigl( y^* - \hat{y}^*_{|(X, y), X^*} \bigr)
%     \,.
% \end{align*}
% It is possible to show that the matrix $K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*}$ is also
% \textbf{PD}. Therefore, we have that $M \succeq \lambda I_m$ whence $\lambda M^{-1}
% \preceq I_m$, or that the matrix $M$ defines an expanding linear transformation $\Real^{n\times 1} \mapsto
% \Real^{n\times 1}$. This means that the out-of-sample residuals are generally larger
% than the corresponding in-sample ones.

Conformal Confidence Region (\ref{sub:conformal_prediction}) requires a non-conformity
measure. We consider two versions of the NCM proposed in \cite{vovk2005}, chapter 2,
for the regression setting: an in-sample and a \textbf{l}eave-\textbf{o}ne-\textbf{o}ut
version. To this end consider a sample $(X, y) = (x_i, y_{x_i})_{i=1}^n$, and for
any $i=1\ldots, n$ put $X = (X_{-i}, x_i)$, and $y = (y_{-i}, y_i)$.
Let $e_i\in \Real^{n\times 1}$ be the $i$-th unit vector.

The ``in-sample'' NCM $A_{\text{in}}$ measures the absolute value of the regression
residual and is given by:
\begin{align}
  A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    &= |y_i - \hat{y}_{|(X, y)}(x_i)| \nonumber\\
    &= |e_i' \hat{r}_{\text{in}}(X, y)| \label{eq:ins_ncm}
    \,,
\end{align}
where $\hat{y}_{|(X, y)}(x_i)$ is the residual of the regression fit on the complete
dataset $(X, y)$. The ``loo'' NCM is defined similarly, but uses loo-residuals for
the task:
\begin{align*}
  A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    &= |y_i - \hat{y}_{|(X_{-i}, y_{-i})}(x_i)| \\
    &= |e_i' \hat{r}_{\text{loo}}(X, y)|
    \,.
\end{align*}
Note that both are interrelated using the formula:
\begin{multline*}
  A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    \\ = \lambda^{-1} \diag(Q_X)^{-1}
    A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    \,.
\end{multline*}
Indeed, for all $i=1,\ldots,n$, the full sample residual of a fitted KRR is given
by $\hat{r}_{\text{in}}(X, y) = (I_n - K_{XX} Q_X) y$. Since $Q_X$ is invertible,
block matrix inversion yields the following expression: for all $i=1,\ldots, n$
\begin{align}
  e_i' \hat{r}_{\text{in}}(X, y)
  % = \lambda e_i' P_{i:n} P_{i:n} Q_X P_{i:n} P_{i:n} y
  % = \lambda e_n'
  %   \begin{pmatrix}
  %     Q_{-i} + Q_{-i} K_{-i}(x_n) m_i^{-1} K_{-i}(x_n)' Q_{-i} & - Q_{-i} K_{-i}(x_n) m_i^{-1} \\
  %     - m_i^{-1} K_{-i}(x_n)' Q_{-i} & m_i^{-1} \\
  %   \end{pmatrix}
  %   \begin{pmatrix} y_{-i} \\ y_i \end{pmatrix} \\
  &= \lambda m_i^{-1} \bigl(y_i - K_{-i}(x_i)' Q_{-i} y_{-i} \bigr) \nonumber \\
  &= \lambda m_i^{-1} e_i' \hat{r}_{\text{loo}}(X, y) \label{eq:loo_resid} \,,
\end{align}
where $K_{-i} = K_{X_{-i}}: \Xcal \mapsto \Real^{(n-1)\times1}$ (see eqn.~\ref{eq:gp_cond_dist})
is the canonical map vector, $Q_{-i}$ is given by $(\lambda I_{n-1} + K_{X_{-i}X_{-i}})^{-1}$,
and $\lambda m_i^{-1}$ is the KRR leverage of the $i$-th observation, with
\begin{equation*}
  m_i = \lambda + K(x_i, x_i) - K_{-i}(x_i)' Q_{-i} K_{-i}(x_i) \,.
\end{equation*}
This ``leverage'' is always within $[0,1]$, since $K$ is PD, and depends only on
the input part, $X$, of the dataset. Furthermore, it can be shown that the main
diagonal of $Q_X$ is given by $(m_i^{-1})_{i=1}^n$. Indeed, to see this apply the
block-inversion formula to the matrix $Q_X$ with symmetrically permuted rows and
columns.

For the NCM $A$ the $1-\alpha$ conformal confidence interval for the $n$-th observation
is given by
\begin{equation} \label{eq:conf_ci}
\Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
  = \bigl\{ z\in \Real \,:\, p_n\bigl((X, \tilde{y}_n^z)\bigr) \geq \alpha \bigr\}
  \,,
\end{equation}
where $\tilde{y}_j^z = (y_{-j}, z)$ -- the augmented sample $y$, with the $j$-th
value replaced by $z$. The ``conformal likelihood'' of the $j$-th observation in
some sample $(X, y)$ is given by
\begin{equation*}
  p_j\bigl((X, y)\bigr)
    = n^{-1} \bigl\lvert \bigl\{
        i = 1,\ldots, n \, : \,
        \eta_i \geq \eta_j
    \bigr\} \bigr\rvert
    \,,
\end{equation*}
for $\eta_i = A\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)$.
Thus it is necessary to obtain a formula, describing dependency of the residuals on
the augmentation of the $n$-th observation. The in-sample KRR residuals of the augmented
dataset $(X, \tilde{y}_n^z)$ are given by
\begin{align}
    \hat{r}_{\text{in}}(X, \tilde{y}_n^z)
    % &= \lambda Q_X \tilde{y}_n^z \nonumber \\
    % &= \lambda
    %   \begin{pmatrix}
    %     Q_{-n} + Q_{-n} K_{-n}(x_n) m_n^{-1} K_{-n}(x_n)' Q_{-n} & - Q_{-n} K_{-n}(x_n) m_n^{-1} \\
    %     - m_n^{-1} K_{-n}(x_n)' Q_{-n} & m_n^{-1} \\
    %   \end{pmatrix}
    %   \begin{pmatrix} y_{-n} \\ z \end{pmatrix} \nonumber \\
    % &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix}
    %  - \lambda \begin{pmatrix} Q_{-n} K_{-n}(x_n) \\ -1 \end{pmatrix}
    %    m_n^{-1} \bigl(z - K_{-n}(x_n)' Q_{-n} y_{-n} \bigr) \nonumber \\
    &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix} \nonumber \\
    &- \lambda B_{-n}(x_n) K_{-n}(x_n)' Q_{-n} y_{-n} \nonumber \\
    &+ \lambda B_{-n}(x_n) z \label{eq:krr_in_resid} \,,
\end{align}
Where the vector $B_{-n}(x_n)\in\Real^{n\times 1}$ is given by
\begin{equation*}
  B_{-n}(x_n)
    = \begin{pmatrix} - Q_{-n} K_{-n}(x_n) \\ 1 \end{pmatrix} m_n^{-1}
    \,.
\end{equation*}

In general, the construction of the confidence region (\ref{eq:conf_ci}) requires
swiping through all $y\in \Ycal$ that satisfy the significance level requirement,
which is infeasible unless $\Ycal$ is finite. The construction for the NCM (\ref{eq:ins_ncm})
for in-sample residuals in this particular problem relies on the representation
of each residual as
\begin{equation*}
  \hat{r}_i^z
    = e_i' \hat{r}_{\text{in}}(X, \tilde{y}_n^z)
    = \lambda a_i + \lambda b_i z
    \,,
\end{equation*}
with $c_i = e_i' C_{-n}\bigl((X, y), x_n\bigr)$ and
\begin{align*}
  C_{-n}\bigl((X, y), x_n\bigr)
    &= \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix} \\
    &- B_{-n}(x_n) K_{-n}(x_n)' Q_{-n} y_{-n}
    \,.
\end{align*}

Since absolute values of the residuals are compared, it is possible to consistently
manipulate the signs of each entry in $C$ and $B$ to ensure that $e_i'B\geq 0$ for
all $i$. The regions $S_i = \{z\in\Real\,:\, |\hat{r}_i^z| \geq |\hat{r}_n^z|\}$, for
$i=1,\ldots, n$, are either closed intervals, complements of open intervals,
one-side closed half-rays in $\Real$, depending on the values of $C$ and $B$. In
particular, with $p_i$ and $q_i$ denoting the values $-\frac{c_i+c_n}{b_i+b_n}$ and
$\frac{c_i-c_n}{b_n-b_i}$, respectively (whenever each is defined), each region
$S_i$ has one of the representations:
\begin{enumerate}
%% a picture of shifted and scaled x->|x| helps in derivation of this.
  \item $b_i=b_n=0$: $S_i = \Real$ if $|c_i| \geq |c_n|$, or $S_i = \emptyset$
  otherwise;
  \item $b_n = b_i > 0$: $S_i$ is either $(-\infty, p_i]$ if $c_i < c_n$, $[p_i, +\infty)$ if
  $c_i > c_n$, or $\Real$ otherwise;
  \item $b_n > b_i \geq 0$: $S_i$ is either $[p_i, q_i]$ if $c_i b_n \geq c_n b_i$,
  or $[q_i, p_i]$ otherwise;
  \item $b_i > b_n \geq 0$: $S_i$ is $\Real\setminus (q_i, p_i)$ when $c_i b_n \geq c_n b_i$,
  or $\Real\setminus (p_i, q_i)$ otherwise.
\end{enumerate}
Let $P$ and $Q$ be the sets of all well-defined $p_i$ and $q_i$ respectively, and let
$(g_i)_{j=1}^{J+1}$ be an enumeration of $\{\pm\infty\}\cup P \cup Q$ in ascending
order. Then the conformal confidence region $\Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)$ is
constructed from intervals $G_j = [g_j, g_{j+1}] \cap \Real$:
\begin{equation*}\label{eq:rrcm_conf_ci}
  \Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
    = \bigcup_{j\,:\, N_j \geq \alpha n} G_j
    \,,
\end{equation*}
where $N_j = |\{i\,:\,G_j \subseteq S_i\}|$ is the number of times $G_j$ is covered by
any region $S_i$.

For a single test point $x_n$ the worst-case complexity of this construction is
$\mathcal{O}(n \log n)$: it is necessary to sort at most $2n$ distinct endpoints
of $G_j$, then locate the values $p_i$ and $q_i$ associated with each region $S_i$
($\mathcal{O}(n\log n)$), and finally in at most $\mathcal{O}(2n)$ time compute
the coverage numbers $N_j$. The memory footprint is $\mathcal{O}(n)$.

% Another possibility is to use the NCM proposed in \cite{BurVovk2014}

% subsection conformal_krr_interval (end)

\subsection{Kernel parameter estimation} % (fold)
\label{sub:kernel_parameter_estimation}

Gaussian interpretation of the Kernel Ridge Regression naturally enables adaptive
estimation of the parameters $\theta$ of the kernel $K$ by maximizing the joint
likelihood of the train data $(X, y)$, given by the likelihood of the multivariate
Gaussian distribution with zero mean and covariance $\sigma^2 (\lambda I_n + K_{XX}))$.

%% Write out the likelihood of the train sample.
%% 

Another alternative is to use leave-one-out cross-validation and minimize the RMSE
of the deleted residuals (eq.~\ref{eq:loo_resid}). In this case, the ``optimal''
parameters are given by
$$ \hat{\theta}
  = \mathop{\text{argmin}}_{\theta \in\Theta}
    \|\hat{r}_{\text{loo}}(X, y; \theta)\|^2
  \,,
$$
where the dependence on $\theta$ is through the sample kernel Gram matrix $K_{XX}$.
It is worth noting that the leave-one-out residuals are not affected by the scale
$\sigma^2$ of the kernel $\Kcal = \sigma^2(\lambda \delta_{x,x'} + K(x, x'))$.

The third option is to use a nonparametric method suggested in \cite{Nemirovsky1997}.

As a result, the model of the predictive variance is effectively fitted on data that has not been used to fit the model of the predictive mean, where in principle no over-fitting can have occurred and so the bias in the predictive variance is eliminated. 

Therefore any estimate of predictive variance based on leave-one-out cross-validation will also be greater than the estimate based on the output of a model trained on the entire dataset, thereby reducing, if not actually eliminating, the known conser- vative bias in the latter

The use of leave-one-out cross-validation is shown to
eliminate the bias inherent in estimates of the predictive variance.

% subsection kernel_parameter_estimation (end)

% section derivation (end)

\section{Numerical study} % (fold)
\label{sec:numerical_study}

Validity of conformal predictors in the online learning setting has been shown in
\cite{vovk2005}, however, no result of this kind is known in the batch learning setting.
Our experiments aim to evaluate the empirical performance of the conformal prediction
in this setting: with dedicated train and test datasets. In this section we conduct
a set of experiments to examine the validity of the confidence intervals produced
by the conformal Kernel Ridge Regression and compare its efficiency to the Bayesian
confidence intervals.

We primarily test the conformalized KRR predictions in cases when the input space
$\Xcal$ is a unit cube in either $\Real$ or $\Real^2$. The rationale for this is
that since conformal procedure is oblivious to the structure of the input dataset,
there is no reason for its validity to deteriorate with increasing dimensionality
of $\Xcal$. Indeed, in alg.~\ref{alg:conf_predictor} the input data from $\Xcal$
in the training and the test sets are fed into the NCM $A$, which, in general, can
be an arbitrary computable function, and never ``leak'' into the procedure itself.

The dimensionality of the input data impacts the performance of the procedure only
through the choice of $A$, which in our case is the absolute value of the Kernel
Ridge Regression residuals, and ultimately affects the efficiency (the width) of
the resulting confidence regions. In this study we focus exclusively on the kernel
ridge regression.

We consider the isotropic \textbf{R}adial \textbf{B}asis \textbf{F}unction kernel
for both the Conformal Kernel Ridge regression and the Gaussian Process regression:
\begin{equation*}
  K(x,x')
  = \mathop{\text{exp}}\bigl\{-\theta \|x - x'\|^2\bigr\}
  \,,
\end{equation*}
with the precision parameter $\theta>0$, and defined over $\Xcal \subseteq \Real^{d\times 1}$.
This kernel is widely used in practice, because it has very nice properties. In
particular if $\Xcal$ is compact its canonical RKHS is universal, $\Hcal_K$ is dense
in the set $C_0(\Xcal)$ of continuous bounded maps with respect to the uniform norm
$\|\cdot\|_\infty$, see.~\cite{ref:rbf_universal}.
% I. Steinwart. On the influence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67–93, 2001.

% Another example of a universal kernel is the Laplacian kernel 
% \begin{equation*}
%   K(x,x')
%   = \mathop{\text{exp}}\bigl\{-\theta \|x - x'\|_1\bigr\}
%   \,,
% \end{equation*}
% where $\|\cdot\|_1$ is the $L_1$ norm on $\Xcal$. In contrast to the RBF, the Laplacian
% kernel 
% Its spectral density is the Cauchy
% density, as opposed to the Gaussian density in the case of RBF. The

We study the effects of the kernel parameters and the choice of the conformal procedure
by varying the following hyper-parameters: \begin{enumerate}
  \item the precision of the RBF kernel $\theta$ is picked from $\{10^{-1}, 1, 10\}$,
  since the input domain is the unit cube, for which $\sup_{x,x'\in[0,1]^d}\|x-x'\|^2 = d$;
  \item the noise-to-signal ratio $\lambda \in\{10^{-k}\,:\,k=6, 4, 2, 0\}$, with
  smaller values corresponding to the interpolation task, and the larger -- to filtering;
  \item the size of the training sample $n$ is chosen from $25 2^k$, $k=0,\ldots, 8$ --
  to study the effects the train sample size on the validity and efficiency;
  \item the NCM ($A_{\text{in}}$ or $A_{\text{loo}}$).
\end{enumerate}

%% Mention that we are testing and working in the batch learning setting

\subsection{1D setting} % (fold)
\label{sub:1d_setting}

First, we assess the performance of the Bayesian and conformal confidence regions
in the case of recovering nonlinear functions on one-dimensional domain, $\Xcal =
[0, 1]$. The following functions are considered: a smooth function (``pressure2'',
fig.~\ref{fig:1dfunc_pressure2}), a piecewise smooth (``f6'', fig.~\ref{fig:1dfunc_f6})
and a step function (``heaviside'', fig.~\ref{fig:1dfunc_heaviside}).
\begin{figure}[t, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=0.9\linewidth]{1d_func_f6}
    % \caption{``f6''}
    \caption{} \label{fig:1dfunc_f6}
  \end{subfigure}~
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=0.9\linewidth]{1d_func_pressure2}
    % \caption{``pressure2''}
    \caption{} \label{fig:1dfunc_pressure2}
  \end{subfigure}~
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=0.9\linewidth]{1d_func_heaviside}
    % \caption{``heaviside''}
    \caption{} \label{fig:1dfunc_heaviside}
  \end{subfigure}
  \caption{Typical profiles of studied 1D functions.}\label{fig:animals}
\end{figure}
In each experiment, we construct the confidence regions performance for the unobserved
values on a $1$-d function on a common set of test inputs, $X^* \subset \Xcal$,
given by $\{k N^{-1}\,:\,k=0,\ldots, N+1\}$ for $N=10^3$ -- a regular gird in $[0, 1]$.
With $X^*$ fixed, we end up with the following experiment setup: for the given
parameters $\lambda, \theta, n$ and the NCM $A$
\begin{enumerate}
  \item perform $L$ independent replications of the following steps: for $l=1,\ldots, L$
  \begin{enumerate}
    \item generate a random train set $X = (x_i)_{i=1}^n\sim \mathcal{U}(\Xcal)$ of
    size $n$ and put $X_{\text{pool}} = X \cup X^*$;
    \item draw target values $y_x = f(x)$, $x\in X_{\text{pool}}$, from some
    function $f:\Xcal \mapsto \Ycal$;
    \item fix an RBF kernel $K$ with precision $\theta$ and fit a Gaussian Process
    Regression with $m(x) = 0$ to the train dataset $(y_x)_{x\in X}$ using kernel
    $\Kcal = \sigma^2(\lambda \delta_{x,x'} + K(x,x'))$, where the amplitude of
    the kernel $\sigma^2$ is also estimated;
    \item for each $x^* \in X^*$ construct both the Bayesian region, $\Bcal_{X, y}^\alpha(x^*)$,
    and the conformal $\epsilon$-confidence region, $\Gamma_{X, y}^\alpha(x^*)$ using
    the KRR NCS $A$ with kernel $\Kcal$;
    \item estimate the coverage rate $p_l(R) = |X^*|^{-1}\sum_{x\in X^*} 1_{y_x\in R_x}$
    and the width of the convex hull $w_l(R) = \inf\{b-a\,:\,R \subseteq [a, b]\}$
    for both confidence regions $R=\Bcal_{X, y}^\alpha$ and $R=\Gamma_{X, y}^\alpha$;
  \end{enumerate}
  \item assess the validity and the efficiency of the Bayesian and Conformal confidence
  regions by comparing the sample statistics of the coverage rate and width.
\end{enumerate}
It is worth emphasizing that in this experiment the input train sample is drawn from
the uniform distribution on $\Xcal$.

In the first experiment we compare the validity of the confidence regions. The fig.~
\ref{fig:1d_validity} depicts the dependence of the coverage rate upon the size of the
train sample.

In the second experiment we assess the efficiency of the Conformal Procedure by
comparing the widths of the constructed regions $\Bcal$ and $\Gamma$. In picture
\ref{fig:1d_efficiency}, 

We make the following empirical conclusions based on the conditional numerical study
in the $1$-d case: \begin{enumerate}
  \item the choice of the NCM does not appear to affect the validity of the confidence
  regions;
  \item Bayesian confidence regions, based on the Gaussian Process regression, seem
  to perform poorly relatively to the Conformal regions in non-gaussian cases;

\end{enumerate}

Gaussian Process interpretation of the KRR (\ref{sub:bayesian_krr_interval}) also
permits adaptive selection of the precision parameter $\theta$ of the RBF kernel
by maximizing the joint likelihood of the training data $(X, y)$. This options has
also been investigates, but yielded conclusions which are qualitatively identical
to the ones summarized above.

% subsection 1d_setting (end)

\subsection{2D setting} % (fold)
\label{sub:2d_setting}

% subsection 2d_setting (end)

% section numerical_study (end)

\section{Conclusion and further work} % (fold)
\label{sec:conclusion_and_further_work}

% section conclusion_and_further_work (end)

\end{document}

