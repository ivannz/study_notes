\documentclass{ITaSconf}
% \documentclass[a4paper]{article}
% \documentclass[a4paper,14pt]{extarticle}

\usepackage[utf8]{inputenc}

\usepackage{geometry}
\usepackage{fullpage}

\usepackage[mathcal]{euscript}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}
\usepackage{algorithm2e}
\usepackage{subcaption}
\usepackage{caption}

\newcommand{\ex}{\mathop{\mathbb{E}}\nolimits}
\newcommand{\pr}{\mathop{\mathbb{P}}\nolimits}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\nil}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\diag}{\mathop{\text{diag}}\nolimits}

\title{Conformalized Kernel Ridge Regression}
\author{Burnaev, E. V., Nazarov, I. N.}

\begin{document}
\maketitle
% \tableofcontents
% \clearpage


\section{Introduction} % (fold)
\label{sec:introduction}

Regression methods are concerned with studying pattern of dependence between the
target variable $y$ and the input variable $x$ based on a finite train sample of 
possibly noisy observations $(X, y)$ from $\Omega\times \Ycal \subseteq \Xcal\times\Real$.
These methods postulate that there exits a stable unobserved relationship $y_x = f(x)$
and observations of $y$ at any given $x\in\Omega$ are corrupted by additive noise:
\begin{equation} \label{eq:signal_model}
  y_x = f(x) + \xi_x \,,
\end{equation}
with $\ex \xi_x = 0$ for all $x\in \Omega$. The estimated relation $\hat{f}$ of $f$
is used to produce point predictions of $y$ at any $x$. However, without additional
distributional assumptions regression methods do note readily yield any measure of
uncertainty in the prediction.

\noindent \textbf{SOMETHING}

In this study, we demonstrate the applicability of conformal prediction in the case
of offline batch learning, and show that when model assumptions do hold, the conformal
confidence sets of the kernel ridge regression do not perform worse than the prediction
confidence intervals of the Gaussian Process Regression with the same kernel (Bayesian
interpretation of the Kernel ridge regression).

% section introduction (end)

\section{Kernel ridge regression} % (fold)
\label{sec:kernel_ridge_regression}

Suppose $\Xcal = \Real^{d\times 1}$. Ordinary least squares regression assumes that
there exists $\beta\in \Real^{d\times 1}$ such that $f(x) = x'\beta$. If the underlying
relationship is non-linear, the trained linear model will have high bias, i.e.
it will make statistically non-negligible systematic errors in its predictions.
However, the linear formulation does not limit the applicability of the ordinary
regression, since by choosing a richer input-feature mapping $\phi: \Omega \mapsto
\Real^{p\times 1}$ it is possible to cover the case of nonlinear target-observation
relationships. The map $\phi$ defines a basis with respect to which the unknown
underlying $f$ is approximated. For instance, the mapping $x\mapsto (\prod_{i=1}^d
x_i^{m_i})_{m\in M}$, with integers $M=(m_i)_{i=1}^d\geq 0$ and $\sum_{i=1}^d m_i \leq p$,
permits approximation of polynomial $f$.

In general, the larger the feature space (i.e. the more diverse ) the more expressive
the regression model is and thus the wider the class of problems it is applicable
to. However, more coefficients in the feature expansion of $f$ make the regression
problem overdetermined and, thus prone to over-fitting. This increases the variance
of the trained regression, thereby reducing it generalization ability. This can be
remedied by restricting the set of functions available for approximation by regularizing,
or shrinking the coefficients $\beta$ in the expansion.

Given train-set observations $(x_i, y_i)_{i=1}^n \in \Real^{d\times 1}\times \Real$,
with $x_i$ collected row-wise in a design matrix $X\in \Real^{n\times d}$, the target
vector -- $y=(y_i)_{i=1}^n \in \Real^{n\times 1}$, and a regularization parameter
$\lambda > 0$, the sample-space \textbf{R}idge \textbf{R}egression (RR) problem solves
this minimization problem
\begin{equation}
  \| y - X\beta - \one\beta_0 \|^2 + \lambda \|\beta\|^2
    \to \min_{\beta_0\in \Real, \beta \in \Real^{d\times 1}} \,,
\end{equation}
where $\one \in \Real^{n\times 1}$ is a vector of ones. Basically ridge regression
is linear least squares regression with $L_2$-norm regularization.

The intercept term is given by
\begin{equation}
  \hat{\beta}_0 = \bar{y} - \bar{X} \hat{\beta} \,,
\end{equation}
where $\bar{y}$ and $\bar{X}$ are the mean target and observation values respectively,
and $\hat{\beta}$ solves the intercept-free ridge regression problem run on centered
observations and targets. Therefore in the following we omit the intercept term and
assume that the relevant columns in the design matrix have been dealt with, and both
the design matrix and the targets have been centered.

Since this one is problem of convex optimization, there exists a unique solution
given by 
\begin{align*}
  \hat{\beta}
    &= (X'X + a I_p)^{-1} X' y \\
    &= X' (a I_n + XX')^{-1} y \,,
\end{align*}
in the direct and the equivalent dual forms, respectively. Predictions on the test
set $X^* = (x_i)_{i=n+1}^{n+m}\in \Real^{m\times d}$, given the train-set, are
\begin{align}
  \hat{y}^*_{\vert(X,y), X^*} = X^* \hat{\beta}
    &= X^* (X'X + a I_p)^{-1} X' y \nonumber \\ 
    &= X^* X' (a I_n + XX')^{-1} y \label{eq:rr_dual_sol}\,,
\end{align}
again in the direct and dual form, respectively.

The so called ``Kernel-trick'', \cite{kernel_trick}, generalizes any ML algorithm,
provided it could be reduced to a form, which depends on the training sample data
only thorough inner-products, or equivalents, between the objects (the sample Gram
matrix). The generalization is done by replacing the products with the products of
the feature maps $\langle\phi(x_i),\phi(x_j)\rangle$ in some RKHS $\Hcal$, or, equivalently,
by the gram matrix of some Mercer-type kernel (see sec.~\ref{sub:kernel_methods}).
It should be noted that care must be taken to ensure additional conditions on the
input domain imposed by this ``trick'' are met.

For instance, the \textbf{RR} prediction in its dual form, eq.~\ref{eq:rr_dual_sol},
depends only on the Gram matrix if inter-object inner-products $X X'$ in the input
domain, which is why it is possible to restate it as the Kernel ridge regression (KRR)
problem.

\subsection{Kernel methods} % (fold)
\label{sub:kernel_methods}

In this section we are going to give a brief recap of the foundations of kernel methods
and their applications.

A tuple $(\Hcal, \langle\cdot, \cdot\rangle)$ is an inner-product space over $\Real$
if $\Hcal$ is a vector space over $\Real$, and $\langle\cdot, \cdot\rangle : \Hcal
\times \Hcal \mapsto \Real$ is an inner product in $\Real$ -- a map linear in both
arguments, symmetric, and possessing the following properties:
\begin{enumerate}
  \item $\langle f, f\rangle\geq 0$ for any $f\in \Hcal$;
  \item $\langle f, f\rangle = 0$ if and only if $f = \nil_\Hcal$.
\end{enumerate}
Over the field of complex numbers $\Cplx$ the requirements of symmetry and bi-
linearity are swapped with being Hermitian, and linear with respect to the first
argument. In the following presentation all considered vector spaces are over the
field of reals.

A Hilbert space is an inner product space, which is complete with respect to the
natural metric induced by the norm $ \|f\| = \sqrt{\langle f, f\rangle} $. A \textbf{R}eproducing
\textbf{K}ernel \textbf{H}ilbert \textbf{S}pace is a complete inner-product vector
space of maps $\Xcal \mapsto \Real$ over the field $\Real$, in which the evaluation
functional defined by $\epsilon_x(f) = f(x)$ is $(\Hcal, \|\cdot\|) \mapsto (\Real, |\cdot|)$
bounded for any $x\in \Xcal$ (in normed spaces bounded linearity is equivalent to
continuity). Equivalently, an \textbf{RKHS} is a Hilbert space for which there exists
a map $\phi:\Xcal\to\Hcal$, and a \textbf{P}ositive \textbf{D}efinite kernel function
$K:\Xcal \times \Xcal \mapsto \Real$ such that \begin{enumerate}
  \item $K(x,y) = \langle \phi(x), \phi(y) \rangle$ for all $x, y\in \Xcal$;
  \item $g(x) = \langle g, K(x, \cdot)\rangle$ for all $x\in \Xcal$ and every
  $g\in \Hcal$.
\end{enumerate}
A function $K:\Xcal \times \Xcal \mapsto \Real$ is positive definite if for any
$n\geq1$, $(x_i)_{i=1}^n \in \Xcal$ and $\alpha \in \Real^{n\times 1}$
\begin{equation*}
  \alpha'K\alpha
    = \sum_{i=1}^n \sum_{j=1}^n \alpha_j \alpha_i K(x_i, x_j)
    \geq 0
    \,,
\end{equation*}
or, in other words the Gram matrix $K$ is positive semi-definite, where $K$ is an
$n \times n$ matrix of $K(x_i, x_j)$. Note that unlike linear algebra, ``positive
definite'' here permits such non-zero weights, for which the product is exactly
zero. Positive definite kernels are also known as Mercer kernels, or covariance
kernels.

Finally, any \textbf{PD} kernel $K$ gives rise to a so called \textbf{canonical RKHS}
associated with $K$ constructed by a standard metric-completion argument for a pre-Hilbert
space (a non-complete inner-product space)
\begin{equation*}
\Hcal_0
  = \bigl\{
    \sum_{i=1}^n \alpha_i K(x_i, \cdot)
    \,:\, n\geq1, (x_i)_{i=1}^n \in \Xcal, (\alpha_i)_{i=1}^n\in \Real
  \bigr\} \,,
\end{equation*}
with an inner-product 
\begin{equation*}
  \langle f, g \rangle = \sum_{i=1}^n \sum_{j=1}^m \alpha_i K(x_i, z_j) \beta_j \,,
\end{equation*}
where $f, g\in \Hcal_0$ with representations $f = \sum_{i=1}^n \alpha_i K(x_i, \cdot)$
and $g = \sum_{j=1}^m \beta_j K(z_j, \cdot)$. In the completed RKHS $\Hcal = \bigl[\Hcal_0\bigr]$
the canonical feature map $\phi$ is given by the canonical map $x\mapsto K(x, \cdot)$
and $K(\cdot, \cdot)$ is the reproducing kernel. The inner-product is extended to
$\Hcal$ by continuity as a step in completion of $\Hcal_0$. It is worth noting, that
the pre-Hilbert space $\Hcal_0$ is by construction dense everywhere in $\Hcal$ with
respect to the metric induced by the completed inner-product.

Given a train sample $(x_i, y_i)_{i=1}^n \in X\times \Real$ a general kernel-based
machine learning regularized loss minimization problem is to solve
\begin{equation}
  \Lcal\bigl((x_i, y_i)_{i=1}^n, f\bigr)
    + \lambda \Omega\bigl(\|f\|\bigr)
    \to \min_{f\in \Hcal} \,,
\end{equation}
where $\Hcal$ is a Hilbert Space with Reproducing Kernel $K$, $\Lcal$ is a loss
function which depends on $f$ through $(f(x_i))_{i=1}^n$, and $\Omega:\Real\mapsto\Real$
is a monotone increasing function used for regularization. In this setting the Representer
theorem, \cite{ref:rkhs_representer}, states that the optimal solution is to be sought
in a finite dimensional linear span of the canonical feature maps $\phi: x\mapsto K(x, \cdot)$
evaluated at the sample objects:
\begin{equation*}
  f^* \in \bigl\{ \beta'\Phi\,:\, \beta \in \Real^{n\times 1} \bigr\} \,,
\end{equation*}
where $\Phi = \bigl(\phi(x_i)\bigr)_{i=1}^n \in \Hcal^{n\times 1}$.

Kernel ridge regression (KRR) combines ridge regression with the kernel trick, and
learns a linear function in the feature space induced by the selected kernel and
sample data. Given a training sample $X = (x_i)_{i=1}^n$, $X\in \Xcal^{n\times 1}$
and $y \in \Real^{n\times 1}$, KRR solves the following problem
\begin{equation*}
  \|y - f\|^2 + \lambda \|f\|^2 \to \min_{f \in \Hcal} \,,
\end{equation*}
with $f \in \Real^{n\times 1}$ being a column-vector of $f$ evaluated at every input
in $X$. Here $\Hcal$ is the canonical RKHS associated with a Mercer-type kernel $K$.
Note that with slight abuse of notation the inner-products and norms over $\Hcal$
and $\Real^{n\times 1}$ are used without denoting which is which, since it is always
clear from both the context and the type of the argument of the norm as a function.

The Representer theorem states that the solution to this minimization is of the form
$f = \Phi'\beta$, for $\Phi = (\phi(x_i))_{i=1}^n \in \Hcal^{n\times 1}$. By definition
of the Gram matrix $K$ and due to its symmetry for any $j=1,\ldots,n$
\begin{equation*}
  f(x_j)
    = \Phi(x_j)' \beta
    = k_{x_j}' \beta
    = (K e_j)' \beta
    = e_j' K \beta
    \,,
\end{equation*}
where $k_x = \Phi(x) = (\phi(x_i)(y))_{i=1}^n \in \Real^{n\times 1}$ and $e_j$ is
the $j$-th unit vector in $\Real^{n\times 1}$. Furthermore
\begin{equation*}
  \| f \|^2
    = \sum_{i=1}^n\sum_{j=1}^n\beta_i\beta_j \langle\phi(x_i), \phi(x_j)\rangle
    % = \sum_{i=1}^n\sum_{j=1}^n\beta_i\beta_j K(x_i, x_j)
    = \beta' K \beta \,.
\end{equation*}
Thus the kernel ridge regression problem is reduced to the following finite-dimensional
convex minimization problem:
\begin{equation*}
  \|y - K \beta \|^2 + \lambda \beta' K \beta
    \to \min_{\beta\in \Real^{n\times 1}} \,.
\end{equation*}
The First order conditions imply that
\begin{equation*}
  \hat{\beta} = (aI_n + K)^{-1} y \,\text{ and }\, \hat{y}_x = k_x' \beta \,,
\end{equation*}
-- the estimated weight vector and an optimal prediction at $x\in \Xcal$, respectively.

Consider a problem of estimating the prediction accuracy of a test sample $(X^*, y^*)$
given a training sample $(X, y)$, where $X = (x_i)_{i=1}^n$, $y\in \Real^{n\times 1}$,
$X^* = (x^*_j)_{j=1}^m = (x_i)_{i=n+1}^{n+m}$ and $y^*\in \Real^{m\times 1}$. 
The out-of-train-sample prediction residuals are given by
\begin{equation*}
  y^* - \hat{y}^*_{|(X, y), X^*}
    = y^* - K_{XX^*}' (\lambda I_n + K_{XX})^{-1} y
    \,,
\end{equation*}
with $K_{XX^*} = (K_X(x^*_j))_{j=1}^m \in \Real{n\times m}$.
Now an in-sample prediction based on the optimal weights over the pooled sample
$(\tilde{X}, \tilde{y})$ given by $((X, X^*), (y, y^*))$. First, since the matrix
$\lambda I_n + K_{XX}$ is always invertible for $\lambda > 0$, block-matrix inversion
formula implies that $Q_{\tilde{X}} = (\lambda I_{n+m} + K_{\tilde{X}\tilde{X}})^{-1}$
is given by:
\begin{equation*}
  % & = \begin{pmatrix}
  %   \lambda I_n + K_{XX} & K_{XX^*} \\
  %   K_{X^*X} & \lambda I_m + K_{X^*X^*}
  % \end{pmatrix}^{-1} \\
  \begin{pmatrix}
    Q_X + Q_X K_{XX^*} M^{-1} K_{X^*X} Q_X & - Q_X K_{XX^*} M^{-1} \\
    - M^{-1} K_{X^*X} Q_X & M^{-1}
  \end{pmatrix}
  \,,
\end{equation*}
where $Q_X = \bigl( \lambda I_n + K_{XX} \bigr)^{-1}$ and
\begin{equation*}
  M = \lambda I_m + K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*} \,.
\end{equation*}
Since $\lambda I_{n+m} + K_{\tilde{X}\tilde{X}}$ is an invertible positive definite
matrix ($M\succeq 0$), its inverse is \textbf{PD} as well, whence $M$ is positive
definite and invertible as well. If $H_{\tilde{X}} = K_{\tilde{X}\tilde{X}} Q_{\tilde{X}}$,
then $I_{n+m} - H_{\tilde{X}} = \lambda Q_{\tilde{X}}$, whence
\begin{align}
  y^* - \hat{y}^*_{|(\tilde{X}, \tilde{y})}
    &= \lambda \begin{pmatrix} 0\\ I_m \end{pmatrix} Q_{\tilde{X}}
        \begin{pmatrix} y\\ y^* \end{pmatrix} \nonumber \\
    % &= \lambda \begin{pmatrix} - M^{-1} K_{X^*X} Q_X & M^{-1} \end{pmatrix}
    %   \begin{pmatrix} y\\ y^* \end{pmatrix} \\
    &= \lambda M^{-1} \bigl( y^* - K_{X^*X} Q_X y \bigr) %\nonumber \\
    % &= \lambda M^{-1} \bigl( y^* - \hat{y}^*_{|(X, y), X^*} \bigr)
    \label{eq:holdout_resid}
    \,.
\end{align}
It is possible to show that the matrix $K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*}$ is also
\textbf{PD}. Therefore, we have that $M \succeq \lambda I_m$ whence $\lambda M^{-1}
\preceq I_m$, or that the matrix $M$ defines an expanding linear transformation
$\Real^{n\times 1} \mapsto \Real^{n\times 1}$. This shows that the out-of-sample
residuals are generally larger than the corresponding in-sample ones.

In particular, eq.~\ref{eq:holdout_resid} enables fast computation of the leave-one-out
residuals of the KRR, without refitting all the way. The full sample residuals of a
fitted KRR are given by a vector $\hat{r}_{\text{in}}(X, y) = (I_n - K_{XX} Q_X) y$,
and the $i$-th, $i=1,\ldots, n$, leave-one-out residual the train sample is
\begin{equation}
  e_i' \hat{r}_{\text{loo}}(X, y)
  = y_i - K_{-i}(x_i)' Q_{-i} y_{-i} \,,
\end{equation}
where $K_{-i} = K_{X_{-i}}: \Xcal \mapsto \Real^{(n-1)\times1}$ is the canonical map
vector, and $Q_{-i}$ is given by $(\lambda I_{n-1} + K_{X_{-i}X_{-i}})^{-1}$.
Therefor, from eq.~\ref{eq:holdout_resid} it follows that
\begin{align}
  e_i' \hat{r}_{\text{in}}(X, y)
  % = \lambda e_i' P_{i:n} P_{i:n} Q_X P_{i:n} P_{i:n} y
  % = \lambda e_n'
  %   \begin{pmatrix}
  %     Q_{-i} + Q_{-i} K_{-i}(x_n) m_i^{-1} K_{-i}(x_n)' Q_{-i} & - Q_{-i} K_{-i}(x_n) m_i^{-1} \\
  %     - m_i^{-1} K_{-i}(x_n)' Q_{-i} & m_i^{-1} \\
  %   \end{pmatrix}
  %   \begin{pmatrix} y_{-i} \\ y_i \end{pmatrix} \\
  &= \lambda m_i^{-1} \bigl(y_i - K_{-i}(x_i)' Q_{-i} y_{-i} \bigr) \nonumber \\
  &= \lambda m_i^{-1} e_i' \hat{r}_{\text{loo}}(X, y) \label{eq:loo_resid} \,,
\end{align}
where $\lambda m_i^{-1}$ is the KRR analogue of the LS ``leverage'' score of the
$i$-th observation, with
\begin{equation*}
  m_i = \lambda + K(x_i, x_i) - K_{-i}(x_i)' Q_{-i} K_{-i}(x_i) \,.
\end{equation*}
The main diagonal of the KRR hat matrix $H_X$ is given by $(\lambda m_i^{-1})_{i=1}^n$,
as can be easily seen by applying the block-inversion formula to the matrix $Q_X$
with symmetrically permuted rows and columns. This ``leverage'' depends only on the
input part, $X$, of the dataset and is always within $[0, 1]$, since the kernel $K$
is \textbf{PD} (see argument after eq.~\ref{eq:holdout_resid}).

% subsection kernel_ridge_regression (end)

\subsection{Bayesian KRR confidence interval} % (fold)
\label{sub:bayesian_krr_confidence_interval}

It has been mentioned above that the Gaussian Process interpretation of the KRR
readily yields predictive confidence regions. In this subsection we are going to
derive the Bayesian confidence interval, against which the conformal confidence
regions will be compared in the numerical study.

%% Brief intro into Gaussian processes
A random process $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$, with mean $m : \Xcal
\mapsto \Real$ and \textbf{PD} function $\Kcal : \Xcal \times \Xcal \mapsto \Real$
is a Gaussian Process if all its finite dimensional distributions are multivariate
Gaussian. In particular, for any $n\geq1$ and any $X = (x_i)_{i=1}^n \in \Xcal$ the
vector $f_X = (f(x_i))_{i=1}^n \in \Real^{n\times 1}$ has Gaussian distribution with
mean $m_X = (m(x_i))_{i=1}^n$ and covariance matrix $K_{XX} = (\Kcal(x_i,x_j))_{ij}$ --
the $n\times n$ Gram matrix of $\Kcal(\cdot,\cdot)$ evaluated at points $(x_i)_{i=1}^n$:
\begin{equation*}
  f_X \sim \Ncal_n(m_X, K_{XX}) \,.
\end{equation*}

In this formulation the model \ref{eq:signal_model} represents the addition of an
independent white noise process $(\epsilon_x)_{x\in \Xcal}$ with variance $\lambda$
to $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$. Indeed, for any $n\geq1$ and
any $X = (x_i)_{i=1}^n \in \Xcal$ the finite dimensional distribution of $(y_x)_{x\in\Xcal}$
is $y_X\sim \Ncal_n(m_X, \lambda I_n + K_{XX})$, because
\begin{equation*}
  y_X
    = \begin{pmatrix} I_n & I_n \end{pmatrix}
    \begin{pmatrix} f_X \\ \epsilon_X \end{pmatrix}
    \,,
\end{equation*}
-- a linear combination of $f$ and $\epsilon$ with joint distribution
\begin{equation*}
  \begin{pmatrix} f \\ \epsilon \end{pmatrix}
    \sim \Ncal_{n+n}\begin{pmatrix}
        \begin{pmatrix} m \\ 0 \end{pmatrix},
        \begin{pmatrix}
          K_{XX} & 0 \\
          0 & \lambda I_n
        \end{pmatrix}
      \end{pmatrix}
    \,.
\end{equation*}
Therefore $(y_x)_{x\in\Xcal} \sim GP(m(x), \Kcal(x,x'))$ with $\Kcal$ given by a PD
kernel
\begin{equation*}
  \Kcal(x,x') = \lambda \delta_{x,x'} + K(x,x') \,,
\end{equation*}
where $\delta_{x,x'}$ is the delta-function on $\Xcal\times \Xcal$.

%% Conditional distribution
The conformal procedure constructs confidence intervals for test observations $y_x$,
contaminated by the additive noise $\epsilon_x$, as opposed to true values $f_x$ in
the model \ref{eq:signal_model}. In the Bayesian setting this means that it is necessary
to get a conditional distribution of a test sample $y_{X^*} = (y_{x^*_j})_{j=1}^l$,
with respect to the train sample $y_X = (y_{x_i})_{i=1}^n$. Gaussianity makes it
especially easy to derive conditional distributions. Indeed, if
\begin{equation*}
  \begin{pmatrix}z_1 \\ z_2\end{pmatrix}
    \sim \Ncal_{d_1+d_2}\Biggl(
      \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix},
      \begin{pmatrix}
        \Sigma_{11} & \Sigma_{12} \\
        \Sigma_{21} & \Sigma_{22}
      \end{pmatrix}
    \Biggr)
    \,,
\end{equation*}
then the conditional distribution of $z_1$ given $z_2$ is Gaussian with
\begin{equation*}
  {z_1}_{|z_2}
    \sim \Ncal_{d_1}\bigl(
      \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2),
      \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
    \bigr)
    \,,
\end{equation*}
provided the inverse of $\Sigma_{22}$ exits. Therefore the conditional distribution
is given by
\begin{equation} \label{eq:cond_distr}
  y_{X^*}\vert_{y_X}
    \sim \Ncal_l\bigl(
      m_{X^*} + K_{X^*X} Q_X (y_X - m_X),
      \Sigma_K(X^*)
    \bigr)
    \,,
\end{equation}
where $\Sigma_K(X^*) = K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*}$, and
$Q_X = \bigl(K_{XX}\bigr)^{-1}$, since $(y_x)_{x\in\Xcal}$ is a GP and
\begin{equation*}
  \begin{pmatrix} y_X \\ y_{X^*} \end{pmatrix}
    \sim \Ncal_{n+l}\begin{pmatrix}
      \begin{pmatrix} m_X \\ m_{X^*} \end{pmatrix},
      \begin{pmatrix}
        K_{XX} & K_{XX^*} \\
        K_{X^*X} & K_{X^*X^*}
      \end{pmatrix}
    \end{pmatrix}
    \,,
\end{equation*}
with $K_{XX^*} = (\Kcal(x_i, x^*_j))\in \Real^{n\times l}$.

The Bayesian prediction of $y_{X^*}$ conditional on observing $y_X$ is given by
the Maximum Posterior prediction (which in the Gaussian case coincides with the
true Bayesian prediction $\ex\bigl(y_{X^*}\,|\, y_X\bigr)$):
\begin{equation*}
  \hat{y}_{y_X}(X^*) = m_{X^*} + K_{X^*X} Q_X \bigl(y_X - m_X\bigr) \,.
\end{equation*}

The Gaussian Process Kernel Ridge Regression is usually formulated with $m=0$ and
$\Kcal(x,x') = \sigma^2(\lambda \delta_{x,x'} + K(x,x'))$, for some PD kernel $K$,
whence the distribution of an observation at a test object $x^*\in \Xcal$, conditional
on the train data $y_X$ is
\begin{equation} \label{eq:gp_cond_dist}
{y_{x^*}}_{|y_X}
  \sim \Ncal\bigl( \hat{y}_{y_X}(x^*), \sigma^2 \sigma_K^2(x^*) \bigr) \,,
\end{equation}
with $\hat{y}_{y_X}(x^*) = K_X(x^*)' Q_X y_X$, and
\begin{equation*}
  \sigma_K^2(x^*)
    = \lambda + K(x^*, x^*) - K_X(x^*)' Q_X K_X(x^*) \,,
\end{equation*}
where $Q_X = \bigl(\lambda I_n + K_{XX}\bigr)^{-1}$, $K_{XX} = (K(x_i,x_j))_{ij}$,
and $K_X = (K(x_i, \cdot))_{i=1}^n: \Xcal \mapsto \Real^{n\times1}$. The $1-\alpha$
confidence interval is thus given by
\begin{equation} \label{eq:gp_conf_int}
\Gamma^\alpha_{y_X}(x^*)
  = \hat{y}_{y_X}(x^*)
  + \sigma \sqrt{\sigma_K^2(x^*)}
  \times [z_{\frac{\alpha}{2}}, z_{1-\frac{\alpha}{2}}]
  \,,
\end{equation}
where $\hat{y}_{y_X}(x^*) = K_X(x^*)' Q_X y_X$, and $z_\alpha$ is the $\alpha$
quantile of $\Ncal(0, 1)$.

Gaussian interpretation of the Kernel Ridge Regression naturally enables adaptive
estimation of parameters $\theta$ of the kernel $K$ by maximizing the joint likelihood
of the train data $(X, y)$, given by the likelihood of the multivariate Gaussian
distribution with zero mean and covariance $\sigma^2 (\lambda I_n + K_{XX}))$.
\begin{equation} \label{eq:bkrr_likelihood}
%% Write out the likelihood of the train sample.
  \Lcal = \ldots
  \,.
\end{equation}
With the explicit solution to the problem of finding the ``best'' $\hat{\beta}$ for
some fixed kernel $K_\theta$, it is possible to gradually estimate the optimal parameters
$\theta$ using alternating optimization algorithm, by minimizing \ref{eq:bkrr_likelihood}.

Another alternative is to select the parameters based on the data that has not been
used in fitting the conditional mean of the kernel ridge regression. One could use
leave-one-out cross-validation and minimize the RMSE of the deleted residuals
(eq.~\ref{eq:loo_resid}):
\begin{equation}
  \hat{\theta}
    = \mathop{\text{argmin}}_{\theta \in\Theta}
      n^{-1} \| \hat{r}_{\text{loo}}(X, y; \theta) \|^2
    \,,
\end{equation}
where the dependence on $\theta$ is through the sample kernel Gram matrix $K_{XX}$.
It is worth noting that the leave-one-out residuals are not affected by the scale
$\sigma^2$ of the kernel $\Kcal = \sigma^2(\lambda \delta_{x,x'} + K(x, x'))$.
Similarly to the case of predictive variances in \cite{pcw20005a7}, using leave-one-out
cross-validation eliminates the bias inherent in estimates of the kernel parameters.

% subsection bayesian_krr_confidence_interval (end)

% section kernel_ridge_regression (end)

\section{Conformal prediction} % (fold)
\label{sec:conformal_prediction}

Conformal prediction is a distribution-free technique designed to yield a statistically
valid confidence sets for predictions made by machine learning algorithms. The key
advantage of the method is that it offers coverage probability guarantees under standard
IID assumptions, even in cases when assumptions of the underlying prediction algorithm
fail to be satisfied. The method was introduced in \cite{vovk2005} and is applicable
in both the supervised and unsupervised online learning settings. In the following
we consider supervised setting with $\Zcal$ denoting the object-target space $\Xcal \times \Ycal$.

The nature of the underlying ML predictive algorithm is irrelevant to conformal
prediction, as the core of the procedure is a measurable map $A: \Zcal^*\times \Zcal \mapsto \Real$,
a \textbf{N}on-\textbf{C}onformity \textbf{M}easure, \textbf{NCM}, which for a training
sample $Z_{:n}=(z_i)_{i=1}^n\in\Zcal$ and a test object $z_* \in \Zcal$ quantifies
how much different $z_*$ is relative to the sample $Z_{:n}$.

A conformal predictor over the NCM $A$ is a procedure, that for every sample $Z_{:n}$,
a test object $x_{n+1} \in \Xcal$, and a confidence level $\epsilon\in(0,1)$, computes
a confidence set $\Gamma_{Z+{:n}}^\epsilon(x^*)$ for the target value $y_{n+1}$ corresponding
to $x_{n+1}$:
\begin{equation} \label{eq:conf_pred_set}
  \Gamma_{Z_{:n}}^\epsilon(x_{n+1})
    = \bigl\{ y\in \Ycal \,:\, p_{Z_{:n}}(\tilde{z}^y_{n+1}) \geq \epsilon \bigr\} \,,
\end{equation}
where $\tilde{z}^y_{n+1} = (x_{n+1}, y)$ a synthetic test observation with target
label $y$. The function $p:\Zcal^*\times (\Xcal\times \Ycal)\mapsto [0,1]$ is given
by
\begin{equation} \label{eq:conf_p_value}
  p_{Z_{:n}}(\tilde{z})
    = (n+1)^{-1} \bigl\lvert\{ i \,:\,
      \eta_i^{\tilde{z}} \geq \eta_{n+1}^{\tilde{z}} \}\bigr\rvert \,,
\end{equation}
where $i=1,\ldots, n+1$, and $\eta_i^{\tilde{z}} = A(S^{\tilde{z}}_{-i}, S^{\tilde{z}}_i)$
-- the degree of non-conformity of the $i$-th observation with respect to the augmented
sample $S^{\tilde{z}} = (Z_{:n}, {\tilde{z}}^y_{n+1}) \in \Zcal^{n+1}$. For any $i$,
$S^{\tilde{z}}_i$ is the $i$-th element of the sample, and $S^{\tilde{z}}_{-i}$ is the
sample with the $i$-th observation omitted.

A distribution $P$ on $\Zcal^n$ is called \textbf{exchangeable} if for any permutation
$\sigma$ of ${1,\ldots,n}$ for any measurable $B\subseteq \Zcal^n$
\begin{multline} \label{eq:exchangeability}
  \pr\bigl(\{z\in\Zcal^n \,:\, (z_i)_{i=1}^n\in B\}\bigr)
  = \\ \pr\bigl(\{z\in\Zcal^n \,:\, (z_{\sigma(i)})_{i=1}^n \in B\} \bigr)\,.
\end{multline}
Any product-measure on $\Zcal^n$ is exchangeable, and therefore exchangeability
generalizes independence. A distribution $P$ on $\Zcal^\infty$ is exchangeable
is all its finite distributions are exchangeable.

In \cite{Vovketal2005}, chapter 2, is has been shown, that if a sequence of examples
$(z_n)_{n \geq1}$ is generated by an exchangeable distribution $P$ on $\Zcal^\infty$,
then the coverage probability of the prediction set $\Gamma^\epsilon$, yielded by
the procedure \ref{eq:conf_pred_set} is at least $1-\epsilon$ and successive errors
are independent in online learning and prediction setting. Thus the outlined procedure
guarantees unconditional validity of the confidence region (eq.~\ref{eq:conf_pred_set}):
for any $\epsilon \in (0,1)$, for any $n\geq1$ and any exchangeable distribution
$P_n$ on $\Zcal^n$ one has
\begin{equation} \label{eq:conservative_coverage}
  \pr_{Z_{:n}\sim P_n} \bigl(
    y_n \notin \Gamma^\epsilon_{Z_{:(n-1)}}(x_n)
  \bigr) \leq \epsilon \,,
\end{equation} 
where $(x_n, y_n) = z_n$. In a special case, when $z_n$ are independent the measure
$P_n$ is just the product measure $P_{z_1} \otimes \ldots \otimes P_{z_n}$.

Intuitively, the event $y_n \notin \Gamma^\epsilon_{Z_{:(n-1)}}(x_n)$ is equivalent
to $\eta_n$ being among the largest $\lfloor n\epsilon\rfloor$ values of $\eta_i = A(Z_{-i}, Z_i)$,
which is equal to $\frac{\lfloor n\epsilon\rfloor}{n}$, due to exchangeability of
$Z_{:n}$ (this heuristic argument assumes that all $\eta_i$ are distinct and that
probability under $P_{:n}$ of any $(\eta_i)_{i=1}^n$ is positive, for a rigorous
proof see \cite{vovk2005}, chapter 8).

In general, any real-valued jointly measurable \textbf{NCM} could be used, the only
difference will be in the size of the predicted confidence set (efficiency) and the
computational complexity of the conformal procedure. In the general case, despite
theoretical guarantees, computing eq.~\ref{eq:conf_pred_set} requires exhaustive
search through the target space $\Ycal$. This complexity issue is not acute in typical
classification settings, when $\Ycal$ is finite, but is infeasible in regression
settings when $\Ycal$ is $\Real$. Furthermore, the inner eq.~\ref{eq:conf_p_value}
requires looping over all observations in the augmented sample $S^{\tilde{z}}$.

In regression setting for specific non-conformity measures it is possible to come
up with efficient procedure for computing the confidence region (\ref{eq:conf_pred_set}),
as demonstrated in \cite{vovk2005}, chapter 2, and sec.~\ref{sub:conformalized_krr}.

% section conformal_prediction (end)

\subsection{Confromalized ridge regression} % (fold)
\label{sub:confromalized_ridge_regression}

The problem of confromalized ridge regression (CRR) has been studied in \cite{BurVovk2014},
where it has been shown that under assumptions of the bayesian ridge regression
the conformal confidence region, eq.~\ref{eq:conf_pred_set}, for an NCM based on
the ridge regression residuals is asymptotically efficient:

%% Say that RR can be made better by using the kernel trick
The major drawback of this statement of the \textbf{RR} problem is that it does not
provide any confidence intervals for these predictions. One possibility of remedying
this is to introduce some distributional assumptions and project the \textbf{RR} problem
into Bayesian framework.

\noindent \textbf{WRITE HERE SOMETHING FROM THE 2014 PAPER.}

% subsection confromalized_ridge_regression (end)

\subsection{Conformalized kernel ridge regression} % (fold)
\label{sub:conformalized_krr}

In this section we describe the construction of confidence regions of the conformal
procedure eq.~\ref{eq:conf_pred_set} for the case of the non-conformity measures
based on kernel ridge regression.

We consider two versions of the NCM proposed in \cite{vovk2005}, chapter 2,
for the regression setting: an in-sample and a \textbf{l}eave-\textbf{o}ne-\textbf{o}ut
version. To this end consider a sample $(X, y) = (x_i, y_{x_i})_{i=1}^n$, and for
any $i=1\ldots, n$ put $X = (X_{-i}, x_i)$, and $y = (y_{-i}, y_i)$.
Let $e_i\in \Real^{n\times 1}$ be the $i$-th unit vector.

\subsubsection{Ridge Regression Confidence Machine} % (fold)
\label{ssub:ridge_regression_confidence_machine}
Conformal Confidence Region (\ref{sub:conformal_prediction}) requires a non-conformity
measure.

The ``in-sample'' NCM $A_{\text{in}}$ measures the absolute value of the regression
residual and is given by:
\begin{align}
  A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    &= |y_i - \hat{y}_{|(X, y)}(x_i)| \nonumber\\
    &= |e_i' \hat{r}_{\text{in}}(X, y)| \label{eq:ins_ncm}
    \,,
\end{align}
where $\hat{y}_{|(X, y)}(x_i)$ is the residual of the regression fit on the complete
dataset $(X, y)$. The ``loo'' NCM is defined similarly, but uses loo-residuals for
the task:
\begin{align*}
  A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    &= |y_i - \hat{y}_{|(X_{-i}, y_{-i})}(x_i)| \\
    &= |e_i' \hat{r}_{\text{loo}}(X, y)|
    \,.
\end{align*}
Note that both are interrelated using the formula:
\begin{multline*}
  A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    \\ = \lambda^{-1} \diag(Q_X)^{-1}
    A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    \,,
\end{multline*}
based on eq.~\ref{eq:loo_resid}.

For the NCM $A$ the $1-\alpha$ conformal confidence interval for the $n$-th observation
is given by
\begin{equation} \label{eq:conf_ci}
\Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
  = \bigl\{ z\in \Real \,:\, p_n\bigl((X, \tilde{y}_n^z)\bigr) \geq \alpha \bigr\}
  \,,
\end{equation}
where $\tilde{y}_j^z = (y_{-j}, z)$ -- the augmented sample $y$, with the $j$-th
value replaced by $z$. The ``conformal likelihood'' of the $j$-th observation in
some sample $(X, y)$ is given by
\begin{equation*}
  p_j\bigl((X, y)\bigr)
    = n^{-1} \bigl\lvert \bigl\{
        i = 1,\ldots, n \, : \,
        \eta_i \geq \eta_j
    \bigr\} \bigr\rvert
    \,,
\end{equation*}
for $\eta_i = A\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)$.
Thus it is necessary to obtain a formula, describing dependency of the residuals on
the augmentation of the $n$-th observation. The in-sample KRR residuals of the augmented
dataset $(X, \tilde{y}_n^z)$ are given by
\begin{align}
    \hat{r}_{\text{in}}(X, \tilde{y}_n^z)
    % &= \lambda Q_X \tilde{y}_n^z \nonumber \\
    % &= \lambda
    %   \begin{pmatrix}
    %     Q_{-n} + Q_{-n} K_{-n}(x_n) m_n^{-1} K_{-n}(x_n)' Q_{-n} & - Q_{-n} K_{-n}(x_n) m_n^{-1} \\
    %     - m_n^{-1} K_{-n}(x_n)' Q_{-n} & m_n^{-1} \\
    %   \end{pmatrix}
    %   \begin{pmatrix} y_{-n} \\ z \end{pmatrix} \nonumber \\
    % &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix}
    %  - \lambda \begin{pmatrix} Q_{-n} K_{-n}(x_n) \\ -1 \end{pmatrix}
    %    m_n^{-1} \bigl(z - K_{-n}(x_n)' Q_{-n} y_{-n} \bigr) \nonumber \\
    &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix} \nonumber \\
    &- \lambda B_{-n}(x_n) K_{-n}(x_n)' Q_{-n} y_{-n} \nonumber \\
    &+ \lambda B_{-n}(x_n) z \label{eq:krr_in_resid} \,,
\end{align}
Where the vector $B_{-n}(x_n)\in\Real^{n\times 1}$ is given by
\begin{equation*}
  B_{-n}(x_n)
    = \begin{pmatrix} - Q_{-n} K_{-n}(x_n) \\ 1 \end{pmatrix} m_n^{-1}
    \,.
\end{equation*}

In general, the construction of the confidence region (\ref{eq:conf_ci}) requires
swiping through all $y\in \Ycal$ that satisfy the significance level requirement,
which is infeasible unless $\Ycal$ is finite. The construction for the NCM (\ref{eq:ins_ncm})
for in-sample residuals in this particular problem relies on the representation
of each residual as
\begin{equation*}
  \hat{r}_i^z
    = e_i' \hat{r}_{\text{in}}(X, \tilde{y}_n^z)
    = \lambda a_i + \lambda b_i z
    \,,
\end{equation*}
with $c_i = e_i' C_{-n}\bigl((X, y), x_n\bigr)$ and
\begin{align*}
  C_{-n}\bigl((X, y), x_n\bigr)
    &= \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix} \\
    &- B_{-n}(x_n) K_{-n}(x_n)' Q_{-n} y_{-n}
    \,.
\end{align*}

Since absolute values of the residuals are compared, it is possible to consistently
manipulate the signs of each entry in $C$ and $B$ to ensure that $e_i'B\geq 0$ for
all $i$. The regions $S_i = \{z\in\Real\,:\, |\hat{r}_i^z| \geq |\hat{r}_n^z|\}$, for
$i=1,\ldots, n$, are either closed intervals, complements of open intervals,
one-side closed half-rays in $\Real$, depending on the values of $C$ and $B$. In
particular, with $p_i$ and $q_i$ denoting the values $-\frac{c_i+c_n}{b_i+b_n}$ and
$\frac{c_i-c_n}{b_n-b_i}$, respectively (whenever each is defined), each region
$S_i$ has one of the representations:
\begin{enumerate}
%% a picture of shifted and scaled x->|x| helps in derivation of this.
  \item $b_i=b_n=0$: $S_i = \Real$ if $|c_i| \geq |c_n|$, or $S_i = \emptyset$
  otherwise;
  \item $b_n = b_i > 0$: $S_i$ is either $(-\infty, p_i]$ if $c_i < c_n$, $[p_i, +\infty)$ if
  $c_i > c_n$, or $\Real$ otherwise;
  \item $b_n > b_i \geq 0$: $S_i$ is either $[p_i, q_i]$ if $c_i b_n \geq c_n b_i$,
  or $[q_i, p_i]$ otherwise;
  \item $b_i > b_n \geq 0$: $S_i$ is $\Real\setminus (q_i, p_i)$ when $c_i b_n \geq c_n b_i$,
  or $\Real\setminus (p_i, q_i)$ otherwise.
\end{enumerate}
Let $P$ and $Q$ be the sets of all well-defined $p_i$ and $q_i$ respectively, and let
$(g_i)_{j=1}^{J+1}$ be an enumeration of $\{\pm\infty\}\cup P \cup Q$ in ascending
order. Then the conformal confidence region $\Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)$ is
constructed from intervals $G_j = [g_j, g_{j+1}] \cap \Real$:
\begin{equation*} \label{eq:rrcm_conf_ci}
  \Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
    = \bigcup_{j\,:\, N_j \geq \alpha n} G_j
    \,,
\end{equation*}
where $N_j = |\{i\,:\,G_j \subseteq S_i\}|$ is the number of times $G_j$ is covered by
any region $S_i$.

For a single test point $x_n$ the worst-case complexity of this construction is
$\mathcal{O}(n \log n)$: it is necessary to sort at most $2n$ distinct endpoints
of $G_j$, then locate the values $p_i$ and $q_i$ associated with each region $S_i$
($\mathcal{O}(n\log n)$), and finally in at most $\mathcal{O}(2n)$ time compute
the coverage numbers $N_j$. The memory footprint is $\mathcal{O}(n)$.

% subsubsection ridge_regression_confidence_machine (end)

\subsubsection{Two sided} % (fold)
\label{ssub:two_sided}
Another possibility is to use the NCM proposed in \cite{BurVovk2014}

% subsubsection two_sided (end)

% \subsection{Kernel parameter estimation} % (fold)
% \label{sub:kernel_parameter_estimation}

% The third option is to use a nonparametric method suggested in \cite{Nemirovsky1997}.

% % subsection kernel_parameter_estimation (end)

% subsection conformalized_krr (end)

\section{Numerical study} % (fold)
\label{sec:numerical_study}

Validity of conformal predictors in the online learning setting has been shown in
\cite{vovk2005}, however, no result of this kind is known in the batch learning setting.
Our experiments aim to evaluate the empirical performance of the conformal prediction
in this setting: with dedicated train and test datasets. In this section we conduct
a set of experiments to examine the validity of the confidence intervals produced
by the conformal Kernel Ridge Regression and compare its efficiency to the Bayesian
confidence intervals.

We primarily test the conformalized KRR predictions in cases when the input space
$\Xcal$ is a unit cube in either $\Real$ or $\Real^2$. The rationale for this is
that since conformal procedure is oblivious to the structure of the input dataset,
there is no reason for its validity to deteriorate with increasing dimensionality
of $\Xcal$. Indeed, in alg.~\ref{alg:conf_predictor} the input data from $\Xcal$
in the training and the test sets are fed into the NCM $A$, which, in general, can
be an arbitrary computable function, and never ``leak'' into the procedure itself.

The dimensionality of the input data impacts the performance of the procedure only
through the choice of $A$, which in our case is the absolute value of the Kernel
Ridge Regression residuals, and ultimately affects the efficiency (the width) of
the resulting confidence regions. In this study we focus exclusively on the kernel
ridge regression.

We consider the isotropic \textbf{R}adial \textbf{B}asis \textbf{F}unction kernel
for both the Conformal Kernel Ridge regression and the Gaussian Process regression:
\begin{equation*}
  K(x,x')
  = \mathop{\text{exp}}\bigl\{-\theta \|x - x'\|^2\bigr\}
  \,,
\end{equation*}
with the precision parameter $\theta>0$, and defined over $\Xcal \subseteq \Real^{d\times 1}$.
This kernel is widely used in practice, because it has very nice properties. In
particular if $\Xcal$ is compact its canonical RKHS is universal, $\Hcal_K$ is dense
in the set $C_0(\Xcal)$ of continuous bounded maps with respect to the uniform norm
$\|\cdot\|_\infty$, see.~\cite{ref:rbf_universal}.
% I. Steinwart. On the influence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67â€“93, 2001.

% Another example of a universal kernel is the Laplacian kernel 
% \begin{equation*}
%   K(x,x')
%   = \mathop{\text{exp}}\bigl\{-\theta \|x - x'\|_1\bigr\}
%   \,,
% \end{equation*}
% where $\|\cdot\|_1$ is the $L_1$ norm on $\Xcal$. In contrast to the RBF, the Laplacian
% kernel 
% Its spectral density is the Cauchy
% density, as opposed to the Gaussian density in the case of RBF. The

We study the effects of the kernel parameters and the choice of the conformal procedure
by varying the following hyper-parameters: \begin{enumerate}
  \item the precision of the RBF kernel $\theta$ is picked from $\{10^{-1}, 1, 10\}$,
  since the input domain is the unit cube, for which $\sup_{x,x'\in[0,1]^d}\|x-x'\|^2 = d$;
  \item the noise-to-signal ratio $\lambda \in\{10^{-k}\,:\,k=6, 4, 2, 0\}$, with
  smaller values corresponding to the interpolation task, and the larger -- to filtering;
  \item the size of the training sample $n$ is chosen from $25 2^k$, $k=0,\ldots, 8$ --
  to study the effects the train sample size on the validity and efficiency;
  \item the NCM ($A_{\text{in}}$ or $A_{\text{loo}}$).
\end{enumerate}

%% Mention that we are testing and working in the batch learning setting

\subsection{1D setting} % (fold)
\label{sub:1d_setting}

First, we assess the performance of the Bayesian and conformal confidence regions
in the case of recovering nonlinear functions on one-dimensional domain, $\Xcal =
[0, 1]$. The following functions are considered: a smooth function (``pressure2'',
fig.~\ref{fig:1dfunc_pressure2}), a piecewise smooth (``f6'', fig.~\ref{fig:1dfunc_f6})
and a step function (``heaviside'', fig.~\ref{fig:1dfunc_heaviside}).
\begin{figure}[t, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=0.9\linewidth]{1d_func_f6}
    % \caption{``f6''}
    \caption{} \label{fig:1dfunc_f6}
  \end{subfigure}~
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=0.9\linewidth]{1d_func_pressure2}
    % \caption{``pressure2''}
    \caption{} \label{fig:1dfunc_pressure2}
  \end{subfigure}~
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=0.9\linewidth]{1d_func_heaviside}
    % \caption{``heaviside''}
    \caption{} \label{fig:1dfunc_heaviside}
  \end{subfigure}
  \caption{Typical profiles of studied 1D functions.} \label{fig:1dfunc}
\end{figure}
In each experiment, we construct the confidence regions performance for the unobserved
values on a $1$-d function on a common set of test inputs, $X^* \subset \Xcal$,
given by $\{k N^{-1}\,:\,k=0,\ldots, N+1\}$ for $N=10^3$ -- a regular gird in $[0, 1]$.
With $X^*$ fixed, we end up with the following experiment setup: for the given
parameters $\lambda, \theta, n$ and the NCM $A$
\begin{enumerate}
  \item perform $L$ independent replications of the following steps: for $l=1,\ldots, L$
  \begin{enumerate}
    \item generate a random train set $X = (x_i)_{i=1}^n\sim \mathcal{U}(\Xcal)$ of
    size $n$ and put $X_{\text{pool}} = X \cup X^*$;
    \item draw target values $y_x = f(x)$, $x\in X_{\text{pool}}$ from some function
    $f:\Xcal \mapsto \Ycal$;
    \item fix an RBF kernel $K$ with precision $\theta$ and fit a Gaussian Process
    Regression with $m(x) = 0$ to the train dataset $(y_x)_{x\in X}$ using kernel
    $\Kcal = \sigma^2(\lambda \delta_{x,x'} + K(x,x'))$, where the amplitude of
    the kernel $\sigma^2$ is also estimated;
    \item for each $x^* \in X^*$ construct both the Bayesian KRR region, $\Bcal_{X, y}^\alpha(x^*)$,
    and the conformal $\epsilon$-confidence region, $\Gamma_{X, y}^\alpha(x^*)$ using
    the KRR NCM $A$ with kernel $\Kcal$;
    \item estimate the coverage rate $p_l(R) = |X^*|^{-1}\sum_{x\in X^*} 1_{y_x\in R_x}$
    and the width of the convex hull $w_l(R) = \inf\{b-a\,:\,R \subseteq [a, b]\}$
    for both confidence regions $R=\Bcal_{X, y}^\alpha$ and $R=\Gamma_{X, y}^\alpha$;
  \end{enumerate}
  \item assess the validity and the efficiency of the Bayesian and Conformal confidence
  regions by comparing the sample statistics of the coverage rate and width.
\end{enumerate}
It is worth emphasizing that in this experiment the input train sample is drawn from
the uniform distribution on $\Xcal$.

In the first experiment we compare the validity of the confidence regions on the
test functions in fig.~\ref{fig:1dfunc}, but we present results for the function
on fig.~\ref{fig:1dfunc_f6} only, because the conclusions are similar.

The empirical study showed that the confidence region $\Bcal_{X, y}^\alpha$ is not
valid in the non gaussian case of are not.

in the noiseless case the choice of kernel parameters does not seem to 

be wider than
First it is wirth 


The fig.~
\ref{fig:1d_validity} depicts the dependence of the coverage rate upon the size of the
train sample.

In the second experiment we assess the efficiency of the Conformal Procedure by
comparing the widths of the constructed regions $\Bcal$ and $\Gamma$. In picture
\ref{fig:1d_efficiency}, 

We make the following empirical conclusions based on the conditional numerical study
in the $1$-d case: \begin{enumerate}
  \item the choice of the NCM does not appear to affect the validity of the confidence
  regions;
  \item Bayesian confidence regions, based on the Gaussian Process regression, seem
  to perform poorly relatively to the Conformal regions in non-gaussian cases;

\end{enumerate}

Gaussian Process interpretation of the KRR (\ref{sub:bayesian_krr_confidence_interval}) also
permits adaptive selection of the precision parameter $\theta$ of the RBF kernel
by maximizing the joint likelihood of the training data $(X, y)$. This options has
also been investigates, but yielded conclusions which are qualitatively identical
to the ones summarized above.

% subsection 1d_setting (end)

\subsection{2D setting} % (fold)
\label{sub:2d_setting}

% subsection 2d_setting (end)

% section numerical_study (end)

\section{Conclusion and further work} % (fold)
\label{sec:conclusion_and_further_work}

% section conclusion_and_further_work (end)

\end{document}

