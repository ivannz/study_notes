\documentclass[a4paper,14pt]{article}
% \documentclass[a4paper,14pt]{extarticle}
\usepackage[utf8]{inputenc}

\usepackage{geometry}
\usepackage{fullpage}

\usepackage[mathcal]{euscript}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}
\usepackage{algorithm2e}

\newcommand{\ex}{\mathop{\mathbb{E}}\nolimits}
\newcommand{\pr}{\mathop{\mathbb{P}}\nolimits}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\diag}{\mathop{\text{diag}}\nolimits}

\title{Conformalized Kernel Ridge Regression}
\author{Burnaev, E. V., Nazarov, I. N.}

\begin{document}
\maketitle
\tableofcontents
\clearpage

\section{Introduction} % (fold)
\label{sec:introduction}

\subsection{Problem statement} % (fold)
\label{sub:problem_statement}

The Kernel Ridge Regression is formulated as the following problem:
$$ $$

Mention the kernel trick

The Kernel Ridge Regression for a given PD kernel $K:\Xcal\times \Xcal \to \Real$
solves the following problem:
$$ \|y - K_{XX}\beta\|^2 + \lambda \beta' K_{XX} \beta
  \to \min_{\beta\in \Real^{n\times 1}}
  \,, $$
with $\|\cdot\|$ -- the euclidean norm, and $K_{XX}$ defined as in \ref{eq:gp_cond_dist}.
Assuming $K_{XX}$ is invertible, elementary matrix algebra yields the following
equation for the minimizer $\beta$:
$$ (\lambda I_n + K_{XX}) \beta - y = 0 \,. $$

% subsection problem_statement (end)

\subsection{Conformal prediction} % (fold)
\label{sub:conformal_prediction}

Conformal prediction is a technique designed to yield a statistically valid measure
of confidence for individual predictions made by a machine learning algorithm and
to be applicable in both the supervised and unsupervised online learning settings.
In the supervised case, let $\Zcal$ denote the object-target space $\Xcal \times \Ycal$.
The core of this technique is a measurable map $A: \Zcal^+\times \Zcal \mapsto \Real$
-- a \textbf{N}on-\textbf{C}onformity \textbf{M}easure, which for a training sample
$Z = (z_i)_{i=1}^n$ and a test object $z_*\in \Zcal$ returns a value $A(Z, z_*)$,
which quantifies how much different $z_*$ is relative to a sample $Z$. A conformal
predictor is a procedure, that takes in a sample $Z_{:n}=(z_i)_{i=1}^n\in\Zcal$,
a test object $x_*\in\Xcal$, and a confidence level $\epsilon\in(0,1)$, and outputs
a confidence set $\Gamma^\epsilon(Z, x_*) \subseteq Y$ for the corresponding target
value $y_*$ (see algorithm~\ref{conf_predictor}). The central idea is to compute
an empirical estimate the p-value of a test object-target pair $(x_*, y_*)$ using
some convenient \textbf{NCM} with respect to the reference sample $Z_{:n}$.

\begin{algorithm}[H]
  \caption{Conformal predictor}\label{conf_predictor}
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{
    $A$ -- \textbf{NCM}, $\epsilon \in (0,1)$ -- significance level, training sample
    $Z_{:n}=(z_i)_{i=1}^n \in \Zcal^+$ and a test object $x_*\in \Xcal$.}
  \Output{Confidence set $\Gamma^\epsilon$ for the test target $y_*$.}
  \BlankLine
  $\Gamma^\epsilon \leftarrow \emptyset$\;
  \For{$y \in Y$}{
    $z_{n+1} \leftarrow (x_*, y)$\;
    \For{$i = 1,\ldots, n, n+1$}{
      $Z_{-i} \leftarrow \bigl(z_j\bigr)_{j=1, j\neq i}^{n+1}$\;
      $\alpha_i \leftarrow A(Z_{-i}, z_i)$\;
    }
    $ p^y \leftarrow (n+1)^{-1} \bigl\lvert \{
        i=1,\ldots, n, n+1 \,:\, \alpha_i \geq \alpha_{n+1}
      \} \bigr\rvert $\;
    \If{$p^y > \epsilon$}{
      $\Gamma^\epsilon \leftarrow \Gamma^\epsilon \cup\{y\}$\;
    }
  }
  Return $\Gamma^\epsilon$\;
\end{algorithm}

In \cite{Vovketal2005} is has been shown, that if a sequence $(z_n)_n{\geq1}$ is
generated by an exchangeable distribution $P$, then the coverage probability of
the prediction set $\Gamma^\epsilon$, yielded by procedure \ref{conf_predictor},
is at least $1-\epsilon$ and successive errors are independent in online learning
and prediction setting. Thus the outlined procedure guarantees that unconditionally
$$ \pr\bigl( y_* \notin \Gamma^\epsilon(Z_{:n}, x_*)\bigr) \leq \epsilon \,, $$
where $\pr = P_{Z_{:n}, {z_*}}$, $(x_*, y_*)=z_*$, and $Z_{:n}$ denotes a sample of
size $n$. In a special case, when $z_n$ are iid the measure is just the product measure
$P_{z_1} \otimes \ldots \otimes P_{z_n} \otimes P_{z_*}$.

In general, any real-valued jointly measurable \textbf{NCM} could be used, the only
difference will be in the size of the predicted confidence set (efficiency) and its
informativeness. Despite attractive theoretical guarantees, the procedure suffers
from a significant drawback: in the general case it is very computationally inefficient,
since it exhaustively searches over $\Ycal$ (generally infeasible in regression
settings) and loops over all observations in the extended sample $Z_{:n}\cup\{z_*\}$
to get the calibration set of $(\alpha_i)_{i=1}^{n+1}$.

% subsection conformal_prediction (end)

\subsection{Goals} % (fold)
\label{sub:goals}

The goal of this paper is study to perform numerical a study of the validity of
the conformal confidence intervals in the batch learning setting with Kernel Ridge
Regression.

% subsection goals (end)

% section introduction (end)

\section{Derivation} % (fold)
\label{sec:derivation}

\subsection{Bayesian KRR interval} % (fold)
\label{sub:bayesian_krr_interval}

Confidence interval in the Bayesian setting can easily be obtained if the KRR
problem is examined from a Gaussian Process point of view.

%% Brief intro into Gaussian processes
A random process $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$, with mean $m : \Xcal
\mapsto \Real$ and \textbf{PD} function $\Kcal : \Xcal \times \Xcal \mapsto \Real$
is a Gaussian Process if all its finite dimensional distributions are multivariate
Gaussian. In particular, for any $n\geq1$ and any $X = (x_i)_{i=1}^n \in \Xcal$ the
vector $f_X = (f(x_i))_{i=1}^n \in \Real^{n\times 1}$ has Gaussian distribution with
mean $m_X = (m(x_i))_{i=1}^n$ and covariance matrix $K_{XX} = (\Kcal(x_i,x_j))_{ij}$ --
the $n\times n$ Gram matrix of $\Kcal(\cdot,\cdot)$ evaluated at points $(x_i)_{i=1}^n$:
$$ f_X \sim \Ncal_n(m_X, K_{XX}) \,. $$

In this formulation the model \ref{eq:signal_model} represents the addition of an
independent white noise process $(\epsilon_x)_{x\in \Xcal}$ with variance $\lambda$
to $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$. Indeed, for any $n\geq1$ and
any $X = (x_i)_{i=1}^n \in \Xcal$ the finite dimensional distribution of $(y_x)_{x\in\Xcal}$
is $y_X\sim \Ncal_n(m_X, \lambda I_n + K_{XX})$, because
$$ y_X
    = \begin{pmatrix} I_n & I_n \end{pmatrix}
    \begin{pmatrix} f_X \\ \epsilon_X \end{pmatrix}
    \,, $$
-- a linear combination of $f$ and $\epsilon$ with joint distribution
$$ \begin{pmatrix} f \\ \epsilon \end{pmatrix}
  \sim \Ncal_{n+n}\begin{pmatrix}
      \begin{pmatrix} m \\ 0 \end{pmatrix},
      \begin{pmatrix}
        K_{XX} & 0 \\
        0 & \lambda I_n
      \end{pmatrix}
    \end{pmatrix}
  \,. $$
Therefore $(y_x)_{x\in\Xcal} \sim GP(m(x), \Kcal(x,x'))$ with $\Kcal$ given by a PD
kernel
$$ \Kcal(x,x') = \lambda \delta_{x,x'} + K(x,x') \,, $$
where $\delta_{x,x'}$ is the delta-function on $\Xcal\times \Xcal$.

%% Conditional distribution
The conformal procedure constructs confidence intervals for test observations $y_x$,
contaminated by the additive noise $\epsilon_x$, as opposed to true values $f_x$ in
the model \ref{eq:signal_model}. In the Bayesian setting this means that it is necessary
to get a conditional distribution of a test sample $y_{X^*} = (y_{x^*_j})_{j=1}^l$,
with respect to the train sample $y_X = (y_{x_i})_{i=1}^n$. Gaussianity makes it
especially easy to derive conditional distributions. Indeed, if
$$ \begin{pmatrix}z_1 \\ z_2\end{pmatrix}
  \sim \Ncal_{d_1+d_2}\Biggl(
    \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix},
    \begin{pmatrix}
      \Sigma_{11} & \Sigma_{12} \\
      \Sigma_{21} & \Sigma_{22}
    \end{pmatrix}
  \Biggr)
  \,,$$
then the conditional distribution of $z_1$ given $z_2$ is Gaussian with 
$$ {z_1}_{|z_2}
  \sim \Ncal_{d_1}\bigl(
    \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2),
    \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
  \bigr) \,,$$
provided the inverse of $\Sigma_{22}$ exits. Therefore the conditional distribution
is given by
$$ {y_{X^*}}_{y_X}
  \sim \Ncal_l\Bigl(
    m_{X^*} + K_{X^*X} Q_X \bigl(y_X - m_X\bigr),
    \lambda K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*}
  \Bigr) \,, $$
where $Q_X = \bigl(K_{XX}\bigr)^{-1}$, since $(y_x)_{x\in\Xcal}$ is a GP and
$$ \begin{pmatrix} y_X \\ y_{X^*} \end{pmatrix}
  \sim \Ncal_{n+l}\begin{pmatrix}
    \begin{pmatrix} m_X \\ m_{X^*} \end{pmatrix},
    \begin{pmatrix}
      K_{XX} & K_{XX^*} \\
      K_{X^*X} & K_{X^*X^*}
    \end{pmatrix}
  \end{pmatrix}
  \,, $$
with $K_{XX^*} = (\Kcal(x_i, x^*_j))\in \Real^{n\times l}$.

The Bayesian prediction of $y_{X^*}$ conditional on observing $y_X$ is given by
the Maximum Posterior prediction (which in the Gaussian case coincides with the
true Bayesian prediction $\ex\bigl(y_{X^*}\,|\, y_X\bigr)$):
$$ \hat{y}_{y_X}(X^*) = m_{X^*} + K_{X^*X} Q_X \bigl(y_X - m_X\bigr) \,. $$

The Gaussian Process Kernel Ridge Regression is usually formulated with $m=0$ and
$\Kcal(x,x') = \sigma^2(\lambda \delta_{x,x'} + K(x,x'))$, for some PD kernel $K$,
whence the distribution of an observation at a test object $x^*\in \Xcal$, conditional
on the train data $y_X$ is
\begin{equation} \label{eq:gp_cond_dist}
{y_{x^*}}_{|y_X}
  \sim \Ncal\bigl(
    K_X(x^*)' Q_X y_X,
    \sigma^2( \lambda + K(x^*, x^*) - K_X(x^*)' Q_X K_X(x^*) )
  \bigr) \,,
\end{equation}
with $Q_X = \bigl(\lambda I_n + K_{XX}\bigr)^{-1}$, $K_{XX} = (K(x_i,x_j))_{ij}$ and
$K_X = (K(x_i, \cdot))_{i=1}^n: \Xcal \mapsto \Real^{n\times1}$. The $1-\alpha$
confidence interval is thus given by
\begin{equation} \label{eq:gp_conf_int}
\Gamma^\alpha_{y_X}(x^*)
  = \hat{y}_{y_X}(x^*)
  + \sigma \sqrt{\lambda + K(x^*, x^*) - K_X(x^*)' Q_X K_X(x^*)}
  \times [z_{\frac{\alpha}{2}}, z_{1-\frac{\alpha}{2}}]
  \,,
\end{equation}
where $\hat{y}_{y_X}(x^*) = K_X(x^*)' Q_X y_X$ and $z_\alpha$ is the $\alpha$ quantile
of $\Ncal(0, 1)$.

% subsection bayesian_krr_interval (end)

\subsection{Conformal KRR interval} % (fold)
\label{sub:conformal_krr_interval}

Conformal Confidence Region (\ref{sec:conformal_prediction}) requires a non-conformity
measure. We consider two versions of the NCF suggested in \cite{vovk2005} for the
regression setting: an in-sample and a \textbf{l}eave-\textbf{o}ne-\textbf{o}ut version.
To this end consider a sample $(X, y) = (x_i, y_{x_i})_{i=1}^n$, and for any $i=1\ldots, n$
put $X = (X_{-i}, x_i)$, and $y = (y_{-i}, y_i)$. Let $e_i\in \Real^{n\times 1}$ be
the $i$-th unit vector.

The ``in-sample'' NCF $A_{\text{in}}$ measures the absolute value of the regression
residual and is given by:
$$ A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
  = |y_i - \hat{y}_{|(X, y)}(x_i)| = |e_i' \hat{r}_{\text{in}}(X, y)|
  \,, $$
where $\hat{y}_{|(X, y)}(x_i)$ is the residual of the regression fit on the complete
dataset $(X, y)$. The ``loo'' NCF is defined similarly, but uses loo-residuals for
the task:
$$ A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
  = |y_i - \hat{y}_{|(X_{-i}, y_{-i})}(x_i)| = |e_i' \hat{r}_{\text{loo}}(X, y)|
  \,. $$
Note that both are interrelated using the formula:
$$ A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
  = \lambda^{-1} \diag(Q_X)^{-1}
  A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
  \,. $$
Indeed, for all $i=1,\ldots,n$, the full sample residual of a fitted KRR is given
by $\hat{r}_{\text{in}}(X, y) = (I_n - K_{XX} Q_X) y$. Since $Q_X$ is invertible,
block matrix inversion yields the following expression: for all $i=1,\ldots, n$
\begin{equation} \label{eq:loo_resid}
  e_i' \hat{r}_{\text{in}}(X, y)
  % = \lambda e_i' P_{i:n} P_{i:n} Q_X P_{i:n} P_{i:n} y
  % = \lambda e_n'
  %   \begin{pmatrix}
  %     Q_{-i} + Q_{-i} K_{-i}(x_n) m_i^{-1} K_{-i}(x_n)' Q_{-i} & - Q_{-i} K_{-i}(x_n) m_i^{-1} \\
  %     - m_i^{-1} K_{-i}(x_n)' Q_{-i} & m_i^{-1} \\
  %   \end{pmatrix}
  %   \begin{pmatrix} y_{-i} \\ y_i \end{pmatrix} \\
  = \lambda m_i^{-1} \bigl(y_i - K_{-i}(x_i)' Q_{-i} y_{-i} \bigr)
  = \lambda m_i^{-1} e_i' \hat{r}_{\text{loo}}(X, y) \,,
\end{equation}
where $K_{-i} = K_{X_{-i}}: \Xcal \mapsto \Real^{(n-1)\times1}$ (see eqn.~\ref{eq:gp_cond_dist})
is the canonical map vector, $Q_{-i}$ is given by $(\lambda I_{n-1} + K_{X_{-i}X_{-i}})^{-1}$,
and $\lambda m_i^{-1}$ is the KRR leverage of the $i$-th observation, with
$$ m_i = \lambda + K(x_i, x_i) - K_{-i}(x_i)' Q_{-i} K_{-i}(x_i) \,. $$
This ``leverage'' is always within $[0,1]$, since $K$ is PD, and depends only on
the input part, $X$, of the dataset. Furthermore, it can be shown that the main
diagonal of $Q_X$ is given by $(m_i^{-1})_{i=1}^n$. Indeed, to see this apply the
block-inversion formula to the matrix $Q_X$ with symmetrically permuted rows and
columns.

For the NCM $A$ the $1-\alpha$ conformal confidence interval for the $n$-th observation
is given by
$$ \Gamma_{X_{-n}, y_{-n}}^\alpha (x_n)
  = \bigl\{ z\in \Real \,:\, p_n\bigl((X, \tilde{y}_n^z)\bigr) \geq \alpha \bigr\}
  \,, $$
where $\tilde{y}_j^z = (y_{-j}, z)$ -- the augmented sample $y$, with the $j$-th
value replaced by $z$. The ``conformal likelihood'' of the $j$-th observation in
some sample $(X, y)$ is given by
$$ p_j\bigl((X, y)\bigr)
  = n^{-1} \bigl\lvert \bigl\{ i = 1,\ldots, n \, : \,
      A\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
      \geq A\bigl((X_{-j}, y_{-j}), (x_j, y_j)\bigr)
    \bigr\} \bigr\rvert
  \,. $$
Thus it is necessary to obtain a formula, describing dependency of the residuals on
the augmentation of the $n$-th observation. The in-sample KRR residuals of the augmented
dataset $(X, \tilde{y}_n^z)$ are given by
\begin{align}
    \hat{r}_{\text{in}}(X, \tilde{y}_n^z)
    % &= \lambda Q_X \tilde{y}_n^z \nonumber \\
    % &= \lambda
    %   \begin{pmatrix}
    %     Q_{-n} + Q_{-n} K_{-n}(x_n) m_n^{-1} K_{-n}(x_n)' Q_{-n} & - Q_{-n} K_{-n}(x_n) m_n^{-1} \\
    %     - m_n^{-1} K_{-n}(x_n)' Q_{-n} & m_n^{-1} \\
    %   \end{pmatrix}
    %   \begin{pmatrix} y_{-n} \\ z \end{pmatrix} \nonumber \\
    &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix}
     - \lambda \begin{pmatrix} Q_{-n} K_{-n}(x_n) \\ -1 \end{pmatrix}
       m_n^{-1} \bigl(z - K_{-n}(x_n)' Q_{-n} y_{-n} \bigr) \label{eq:krr_in_resid} \\
    &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix}
     - \lambda B_{-n}(x_n) K_{-n}(x_n)' Q_{-n} y_{-n}
     + \lambda B_{-n}(x_n) z\,,
\end{align}
Denoting by $B_{-n}(x_n)\in\Real^{n\times 1}$ the vector
$$ B_{-n}(x_n) = \begin{pmatrix} - Q_{-n} K_{-n}(x_n) \\ 1 \end{pmatrix} m_n^{-1} \,, $$
and by $A_{-n}(x_n)\in\Real^{n\times 1}$ the matrix
$$ \begin{pmatrix} - Q_{-n} K_{-n}(x_n) \\ 1 \end{pmatrix} m_n^{-1} \,, $$

This gives an expression ready for
and the loo-residuals are given by
\begin{align}
  \hat{r}_{\text{loo}}(X, \tilde{y}_n^z)
  &= \lambda^{-1} \diag(Q_X)^{-1} \hat{r}_{\text{in}}(X, \tilde{y}_n^z) \label{eq:krr_loo_resid} \,.
\end{align}


\subsubsection{Regularity lemma} % (fold)
\label{ssub:regularity_lemma}

% subsubsection regularity_lemma (end)

% subsection conformal_krr_interval (end)

% section derivation (end)

\section{Numerical study} % (fold)
\label{sec:numerical_study}

%% Mention that we are testing and working in the batch learning setting
\subsection{1D setting} % (fold)
\label{sub:1d_setting}

% subsection 1d_setting (end)

\subsection{2D setting} % (fold)
\label{sub:2d_setting}
%% Include 1-2 1D plots and 1 2D plot: whichever are prettier

% subsection 2d_setting (end)

% section numerical_study (end)

\section{Conclusion and further work} % (fold)
\label{sec:conclusion_and_further_work}

% section conclusion_and_further_work (end)

\end{document}

