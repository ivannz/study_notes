% \documentclass{ITaSconf}
% \documentclass[a4paper]{article}
\documentclass[a4paper,14pt]{extarticle}
\usepackage[utf8]{inputenc}

\usepackage{geometry}
\usepackage{fullpage}

\usepackage[mathcal]{euscript}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}
\usepackage{algorithm2e}
\usepackage{subcaption}
\usepackage{caption}

\newcommand{\ex}{\mathop{\mathbb{E}}\nolimits}
\newcommand{\pr}{\mathop{\mathbb{P}}\nolimits}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\nil}{\mathbf{0}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\diag}{\mathop{\text{diag}}\nolimits}

\title{Conformalized Kernel Ridge Regression}
\author{Burnaev, E. V., Nazarov, I. N.}

\begin{document}
\maketitle
% \tableofcontents
% \clearpage

\section{Introduction} % (fold)
\label{sec:introduction}

\subsection{Problem statement} % (fold)
\label{sub:problem_statement}

The Kernel Ridge Regression is formulated as the following problem:
% \begin{equation*}
% \end{equation*}

Mention the kernel trick.
Briefly say something about RKHS.

The Kernel Ridge Regression for a given PD kernel $K:\Xcal\times \Xcal \to \Real$
solves the following problem:
\begin{equation*}
  \|y - K_{XX}\beta\|^2 + \lambda \beta' K_{XX} \beta
    \to \min_{\beta\in \Real^{n\times 1}}
    \,,
\end{equation*}
with $\|\cdot\|$ -- the euclidean norm, and $K_{XX}$ defined as in \ref{eq:gp_cond_dist}.
Assuming $K_{XX}$ is invertible, elementary matrix algebra yields the following
equation for the minimizer $\beta$:
\begin{equation*}
  (\lambda I_n + K_{XX}) \beta - y = 0 \,.
\end{equation*}

% subsection problem_statement (end)

\subsection{Conformal prediction} % (fold)
\label{sub:conformal_prediction}

Conformal prediction is a technique designed to yield a statistically valid measure
of confidence for individual predictions made by a machine learning algorithm and
to be applicable in both the supervised and unsupervised online learning settings.
In the supervised case, let $\Zcal$ denote the object-target space $\Xcal \times \Ycal$.
The core of this technique is a measurable map $A: \Zcal^+\times \Zcal \mapsto \Real$
-- a \textbf{N}on-\textbf{C}onformity \textbf{M}easure, which for a training sample
$Z = (z_i)_{i=1}^n$ and a test object $z_*\in \Zcal$ returns a value $A(Z, z_*)$,
which quantifies how much different $z_*$ is relative to a sample $Z$. A conformal
predictor is a procedure, that takes in a sample $Z_{:n}=(z_i)_{i=1}^n\in\Zcal$,
a test object $x_*\in\Xcal$, and a confidence level $\epsilon\in(0,1)$, and outputs
a confidence set $\Gamma^\epsilon(Z, x_*) \subseteq Y$ for the corresponding target
value $y_*$ (see algorithm~\ref{alg:conf_predictor}). The central idea is to compute
an empirical estimate the p-value of a test object-target pair $(x_*, y_*)$ using
some convenient \textbf{NCM} with respect to the reference sample $Z_{:n}$.

\begin{algorithm}%[H]
  \caption{Conformal predictor} \label{alg:conf_predictor}
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{
    $A$ -- \textbf{NCM}, $\epsilon \in (0,1)$ -- significance level, training sample
    $Z_{:n}=(z_i)_{i=1}^n \in \Zcal^+$ and a test object $x_*\in \Xcal$.}
  \Output{Confidence set $\Gamma^\epsilon$ for the test target $y_*$.}
  \BlankLine
  $\Gamma^\epsilon \leftarrow \emptyset$\;
  \For{$y \in Y$}{
    $z_{n+1} \leftarrow (x_*, y)$\;
    \For{$i = 1,\ldots, n, n+1$}{
      $Z_{-i} \leftarrow \bigl(z_j\bigr)_{j=1, j\neq i}^{n+1}$\;
      $\eta_i \leftarrow A(Z_{-i}, z_i)$\;
    }
    $ p^y \leftarrow (n+1)^{-1} \bigl\lvert \{
        i \,:\, \eta_i \geq \eta_{n+1}
      \} \bigr\rvert $\;
    \If{$p^y > \epsilon$}{
      $\Gamma^\epsilon \leftarrow \Gamma^\epsilon \cup\{y\}$\;
    }
  }
  Return $\Gamma^\epsilon$\;
\end{algorithm}

In \cite{Vovketal2005} is has been shown, that if a sequence $(z_n)_n{\geq1}$ is
generated by an exchangeable distribution $P$, then the coverage probability of
the prediction set $\Gamma^\epsilon$, yielded by procedure \ref{alg:conf_predictor},
is at least $1-\epsilon$ and successive errors are independent in online learning
and prediction setting. Thus the outlined procedure guarantees that unconditionally
$$ \pr\bigl( y_* \notin \Gamma^\epsilon(Z_{:n}, x_*)\bigr) \leq \epsilon \,, $$
where $\pr = P_{Z_{:n}, {z_*}}$, $(x_*, y_*)=z_*$, and $Z_{:n}$ denotes a sample of
size $n$. In a special case, when $z_n$ are iid the measure is just the product measure
$P_{z_1} \otimes \ldots \otimes P_{z_n} \otimes P_{z_*}$.

In general, any real-valued jointly measurable \textbf{NCM} could be used, the only
difference will be in the size of the predicted confidence set (efficiency) and its
informativeness. Despite attractive theoretical guarantees, the procedure suffers
from a significant drawback: in the general case it is very computationally inefficient,
since it exhaustively searches over $\Ycal$ (generally infeasible in regression
settings) and loops over all observations in the extended sample $Z_{:n}\cup\{z_*\}$
to get the calibration set of $(\eta_i)_{i=1}^{n+1}$.

% subsection conformal_prediction (end)

\subsection{Goals} % (fold)
\label{sub:goals}

The goal of this paper is study to perform numerical a study of the validity of
the conformal confidence intervals in the batch learning setting with Kernel Ridge
Regression.

% subsection goals (end)

% section introduction (end)

\subsection{Kernel Ridge Regression} % (fold)
\label{sub:kernel_ridge_regression}

As stated in \cite{BurVovk2014} it would be interesting to generalize this result
to the case of Kernel Ridge regression. However, first it a proper recap of what 
kind of mathematical object an RKHS is seems appropriated.

\subsubsection{RKHS} % (fold)
\label{ssub:rkhs}

A tuple $(\Hcal, \langle\cdot, \cdot\rangle)$ is an inner-product space over $\Real$
if $\Hcal$ is a vector space over $\Real$, and $\langle\cdot, \cdot\rangle : \Hcal
\times \Hcal \mapsto \Real$ is an inner product in $\Real$ -- a map linear in both
arguments, symmetric, and possessing the following properties:
\begin{enumerate}
  \item $\langle f, f\rangle\geq 0$ for any $f\in \Hcal$;
  \item $\langle f, f\rangle = 0$ if and only if $f = \nil_\Hcal$.
\end{enumerate}
Over the field of complex numbers $\Cplx$ the requirements of symmetry and bi-
linearity are swapped with being Hermitian, and linear with respect to the first
argument. In the following presentation all considered vector spaces are over the
field of reals.

A Hilbert space is an inner product space, which is complete with respect to the
natural metric induced by the norm $ \|f\| = \sqrt{\langle f, f\rangle} $. A \textbf{R}eproducing
\textbf{K}ernel \textbf{H}ilbert \textbf{S}pace is a complete inner-product vector
space of maps $\Xcal \mapsto \Real$ over the field $\Real$, in which the evaluation
functional defined by $\epsilon_x(f) = f(x)$ is $(\Hcal, \|\cdot\|) \mapsto (\Real, |\cdot|)$
bounded for any $x\in \Xcal$ (in normed spaces bounded linearity is equivalent to
continuity). Equivalently, an \textbf{RKHS} is a Hilbert space for which there exists
a map $\phi:\Xcal\to\Hcal$, and a \textbf{P}ositive \textbf{D}efinite kernel function
$K:\Xcal \times \Xcal \mapsto \Real$ such that \begin{enumerate}
  \item $K(x,y) = \langle \phi(x), \phi(y) \rangle$ for all $x, y\in \Xcal$;
  \item $g(x) = \langle g, K(x, \cdot)\rangle$ for all $x\in \Xcal$ and every
  $g\in \Hcal$.
\end{enumerate}
A function $K:\Xcal \times \Xcal \mapsto \Real$ is positive definite if for any
$n\geq1$, $(x_i)_{i=1}^n \in \Xcal$ and $\alpha \in \Real^{n\times 1}$
$$ \alpha'K\alpha
  = \sum_{i=1}^n \sum_{j=1}^n \alpha_j \alpha_i K(x_i, x_j)
  \geq 0
  \,, $$
or, in other words the Gram matrix $K$ is positive semi-definite, where $K$ is an
$n \times n$ matrix of $K(x_i, x_j)$. Note that unlike linear algebra, ``positive
definite'' here permits such non-zero weights, for which the product is exactly zero.
Positive definite kernels are also known as Mercer kernels, or covariance kernels.

Finally, any \textbf{PD} kernel $K$ gives rise to a so called \textbf{canonical RKHS}
associated with $K$ constructed by a standard metric-completion argument for a pre-Hilbert
space (a non-complete inner-product space)
$$ \Hcal_0
  = \bigl\{
    \sum_{i=1}^n \alpha_i K(x_i, \cdot)
    \,:\, n\geq1, (x_i)_{i=1}^n \in \Xcal, (\alpha_i)_{i=1}^n\in \Real
  \bigr\}
  \,, $$
with an inner-product $ \langle f, g \rangle = \sum_{i=1}^n \sum_{j=1}^m \alpha_i K(x_i, z_j) \beta_j $,
where $f, g\in \Hcal_0$ with representations $f = \sum_{i=1}^n \alpha_i K(x_i, \cdot)$
and $g = \sum_{j=1}^m \beta_j K(z_j, \cdot)$. In the completed RKHS $\Hcal = \bigl[\Hcal_0\bigr]$
the map $\phi$ is given by the canonical map $x\mapsto K(x, \cdot)$ and $K$ is the
reproducing kernel. The inner-prodcut is extended to $\Hcal$ by contonuity as a step
in completion of $\Hcal_0$. It is worth noting, that the pre-Hilbert space $\Hcal_0$
is by construction dense everywhere in $\Hcal$ with respect to the norm-mertic
induced by the completed inner-product.

% subsubsection rkhs (end)

\subsubsection{Kernel methods} % (fold)
\label{ssub:kernel_methods}

Given a train sample $(x_i, y_i)_{i=1}^n \in X\times \Real$ a general kernel-based
machine learning regularized loss minimization problem is to solve
$$ \Lcal\bigl((x_i, y_i)_{i=1}^n, f\bigr)
  + \lambda \Omega\bigl(\|f\|\bigr)
  \to \min_{f\in \Hcal}\,, $$
where $\Hcal$ is a Hilbert Space with Reproducing Kernel $K$, $\Lcal$ is a loss
function which depends on $f$ through $(f(x_i))_{i=1}^n$, and $\Omega:\Real\mapsto\Real$
is a monotone increasing function used for regularization. In this setting the Representer
theorem states that the optimal solution is to be sought in a finite dimensional
linear span of the canonical feature maps $\phi: x\mapsto K(x, \cdot)$ evaluated at
the sample objects:
$$ f^* \in \bigl\{ \beta'\Phi\,:\, \beta \in \Real^{n\times 1} \bigr\} \,, $$
where $\Phi = \bigl(\phi(x_i)\bigr)_{i=1}^n \in \Hcal^{n\times 1}$.

Another principle, the so called ``Kernel-trick'' states that whenever an ML algorithm
could be reduced to a form, which depends on the training sample data only thorough
inner-products between the objects (a or Gram matrix), then it is possible to construct
another ML algorithm by simply replacing the products with an arbitrary \textbf{PD}
kernel. It should be noted that care must be taken to ensure additional conditions
on the input domain imposed by this ``trick'' are met.

In particular, the \textbf{RR} solution in its dual form depends only on the Gram
matrix if inter-object inner-products $X X'$ in the input domain, which is why it is
possible to restate it as the Kernel \textbf{RR} problem: given a training sample
$X = (x_i)_{i=1}^n\in\Xcal^{n\times 1}$ and $y\in\Real^{n\times 1}$
$$ \|y - f\|^2 + a \|f\|^2 \to \min_{f \in \Hcal} \,, $$
with $f \in \Real^{n\times 1}$ being a column-vector of $f$ evaluated at every input
in $X$. Here $\Hcal$ is the canonical RKHS associated with a Mercer kernel $K$. Note
that with slight abuse of notation the inner-products and norms over $\Hcal$ and
$\Real^{n\times 1}$ are used without denoting which is which, since it is always
clear from both the context and the type of the argument of the norm as a function.

The Representer theorem states that the solution to this minimization is of the form
$f = \Phi'\beta$, for $\Phi = (\phi(x_i))_{i=1}^n \in \Hcal^{n\times 1}$. By definition
of the Gram matrix $K$ and due to its symmetry for any $j=1,\ldots,n$
$$ f(x_j)
  = \Phi(x_j)' \beta
  = k_{x_j}' \beta
  = (K e_j)' \beta
  = e_j' K \beta
  \,, $$
where $k_x = \Phi(x) = (\phi(x_i)(y))_{i=1}^n \in \Real^{n\times 1}$ and $e_j$ is
the $j$-th unit vector in $\Real^{n\times 1}$. Furthermore
$$ \| f \|^2
  = \sum_{i=1}^n\sum_{j=1}^n\beta_i\beta_j \langle\phi(x_i), \phi(x_j)\rangle
  = \sum_{i=1}^n\sum_{j=1}^n\beta_i\beta_j K(x_i, x_j)
  = \beta' K \beta \,. $$
Thus the kernel Ridge Regression problem is reduced to the following finite-dimensional
convex minimization problem:
$$ \|y - K \beta \|^2 + \lambda \beta' K \beta \to \min_{\beta\in \Real^{n\times 1}} \,. $$
The First order conditions imply that
$$ \hat{\beta} = (aI_n + K)^{-1} y \,\text{ and }\, \hat{y}_x = k_x' \beta \,, $$
-- the estimated weight vector and an optimal prediction at $x\in \Xcal$, respectively.

% subsubsection kernel_methods (end)


\section{Derivation} % (fold)
\label{sec:derivation}

\subsection{Bayesian KRR interval} % (fold)
\label{sub:bayesian_krr_interval}

Confidence interval in the Bayesian setting can easily be obtained if the KRR
problem is examined from a Gaussian Process point of view.

%% Brief intro into Gaussian processes
A random process $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$, with mean $m : \Xcal
\mapsto \Real$ and \textbf{PD} function $\Kcal : \Xcal \times \Xcal \mapsto \Real$
is a Gaussian Process if all its finite dimensional distributions are multivariate
Gaussian. In particular, for any $n\geq1$ and any $X = (x_i)_{i=1}^n \in \Xcal$ the
vector $f_X = (f(x_i))_{i=1}^n \in \Real^{n\times 1}$ has Gaussian distribution with
mean $m_X = (m(x_i))_{i=1}^n$ and covariance matrix $K_{XX} = (\Kcal(x_i,x_j))_{ij}$ --
the $n\times n$ Gram matrix of $\Kcal(\cdot,\cdot)$ evaluated at points $(x_i)_{i=1}^n$:
\begin{equation*}
  f_X \sim \Ncal_n(m_X, K_{XX}) \,.
\end{equation*}

In this formulation the model \ref{eq:signal_model} represents the addition of an
independent white noise process $(\epsilon_x)_{x\in \Xcal}$ with variance $\lambda$
to $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$. Indeed, for any $n\geq1$ and
any $X = (x_i)_{i=1}^n \in \Xcal$ the finite dimensional distribution of $(y_x)_{x\in\Xcal}$
is $y_X\sim \Ncal_n(m_X, \lambda I_n + K_{XX})$, because
\begin{equation*}
  y_X
    = \begin{pmatrix} I_n & I_n \end{pmatrix}
    \begin{pmatrix} f_X \\ \epsilon_X \end{pmatrix}
    \,,
\end{equation*}
-- a linear combination of $f$ and $\epsilon$ with joint distribution
\begin{equation*}
  \begin{pmatrix} f \\ \epsilon \end{pmatrix}
    \sim \Ncal_{n+n}\begin{pmatrix}
        \begin{pmatrix} m \\ 0 \end{pmatrix},
        \begin{pmatrix}
          K_{XX} & 0 \\
          0 & \lambda I_n
        \end{pmatrix}
      \end{pmatrix}
    \,.
\end{equation*}
Therefore $(y_x)_{x\in\Xcal} \sim GP(m(x), \Kcal(x,x'))$ with $\Kcal$ given by a PD
kernel
\begin{equation*}
  \Kcal(x,x') = \lambda \delta_{x,x'} + K(x,x') \,,
\end{equation*}
where $\delta_{x,x'}$ is the delta-function on $\Xcal\times \Xcal$.

%% Conditional distribution
The conformal procedure constructs confidence intervals for test observations $y_x$,
contaminated by the additive noise $\epsilon_x$, as opposed to true values $f_x$ in
the model \ref{eq:signal_model}. In the Bayesian setting this means that it is necessary
to get a conditional distribution of a test sample $y_{X^*} = (y_{x^*_j})_{j=1}^l$,
with respect to the train sample $y_X = (y_{x_i})_{i=1}^n$. Gaussianity makes it
especially easy to derive conditional distributions. Indeed, if
\begin{equation*}
  \begin{pmatrix}z_1 \\ z_2\end{pmatrix}
    \sim \Ncal_{d_1+d_2}\Biggl(
      \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix},
      \begin{pmatrix}
        \Sigma_{11} & \Sigma_{12} \\
        \Sigma_{21} & \Sigma_{22}
      \end{pmatrix}
    \Biggr)
    \,,
\end{equation*}
then the conditional distribution of $z_1$ given $z_2$ is Gaussian with
\begin{equation*}
  {z_1}_{|z_2}
    \sim \Ncal_{d_1}\bigl(
      \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2),
      \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
    \bigr)
    \,,
\end{equation*}
provided the inverse of $\Sigma_{22}$ exits. Therefore the conditional distribution
is given by
\begin{equation} \label{eq:cond_distr}
  y_{X^*}\vert_{y_X}
    \sim \Ncal_l\bigl(
      m_{X^*} + K_{X^*X} Q_X (y_X - m_X),
      \Sigma_K(X^*)
    \bigr)
    \,,
\end{equation}
where $\Sigma_K(X^*) = K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*}$, and
$Q_X = \bigl(K_{XX}\bigr)^{-1}$, since $(y_x)_{x\in\Xcal}$ is a GP and
\begin{equation*}
  \begin{pmatrix} y_X \\ y_{X^*} \end{pmatrix}
    \sim \Ncal_{n+l}\begin{pmatrix}
      \begin{pmatrix} m_X \\ m_{X^*} \end{pmatrix},
      \begin{pmatrix}
        K_{XX} & K_{XX^*} \\
        K_{X^*X} & K_{X^*X^*}
      \end{pmatrix}
    \end{pmatrix}
    \,,
\end{equation*}
with $K_{XX^*} = (\Kcal(x_i, x^*_j))\in \Real^{n\times l}$.

The Bayesian prediction of $y_{X^*}$ conditional on observing $y_X$ is given by
the Maximum Posterior prediction (which in the Gaussian case coincides with the
true Bayesian prediction $\ex\bigl(y_{X^*}\,|\, y_X\bigr)$):
\begin{equation*}
  \hat{y}_{y_X}(X^*) = m_{X^*} + K_{X^*X} Q_X \bigl(y_X - m_X\bigr) \,.
\end{equation*}

The Gaussian Process Kernel Ridge Regression is usually formulated with $m=0$ and
$\Kcal(x,x') = \sigma^2(\lambda \delta_{x,x'} + K(x,x'))$, for some PD kernel $K$,
whence the distribution of an observation at a test object $x^*\in \Xcal$, conditional
on the train data $y_X$ is
\begin{equation} \label{eq:gp_cond_dist}
{y_{x^*}}_{|y_X}
  \sim \Ncal\bigl( \hat{y}_{y_X}(x^*), \sigma^2 \sigma_K^2(x^*) \bigr) \,,
\end{equation}
with $\hat{y}_{y_X}(x^*) = K_X(x^*)' Q_X y_X$, and
\begin{equation*}
  \sigma_K^2(x^*)
    = \lambda + K(x^*, x^*) - K_X(x^*)' Q_X K_X(x^*) \,,
\end{equation*}
where $Q_X = \bigl(\lambda I_n + K_{XX}\bigr)^{-1}$, $K_{XX} = (K(x_i,x_j))_{ij}$,
and $K_X = (K(x_i, \cdot))_{i=1}^n: \Xcal \mapsto \Real^{n\times1}$. The $1-\alpha$
confidence interval is thus given by
\begin{equation} \label{eq:gp_conf_int}
\Gamma^\alpha_{y_X}(x^*)
  = \hat{y}_{y_X}(x^*)
  + \sigma \sqrt{\sigma_K^2(x^*)}
  \times [z_{\frac{\alpha}{2}}, z_{1-\frac{\alpha}{2}}]
  \,,
\end{equation}
where $\hat{y}_{y_X}(x^*) = K_X(x^*)' Q_X y_X$, and $z_\alpha$ is the $\alpha$
quantile of $\Ncal(0, 1)$.

% subsection bayesian_krr_interval (end)

\subsection{Conformal KRR interval} % (fold)
\label{sub:conformal_krr_interval}

Conformal Confidence Region (\ref{sec:conformal_prediction}) requires a non-conformity
measure. We consider two versions of the NCM suggested in \cite{vovk2005} for the
regression setting: an in-sample and a \textbf{l}eave-\textbf{o}ne-\textbf{o}ut version.
To this end consider a sample $(X, y) = (x_i, y_{x_i})_{i=1}^n$, and for any $i=1\ldots, n$
put $X = (X_{-i}, x_i)$, and $y = (y_{-i}, y_i)$. Let $e_i\in \Real^{n\times 1}$ be
the $i$-th unit vector.

The ``in-sample'' NCM $A_{\text{in}}$ measures the absolute value of the regression
residual and is given by:
\begin{align}
  A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    &= |y_i - \hat{y}_{|(X, y)}(x_i)| \nonumber\\
    &= |e_i' \hat{r}_{\text{in}}(X, y)| \label{eq:ins_ncm}
    \,,
\end{align}
where $\hat{y}_{|(X, y)}(x_i)$ is the residual of the regression fit on the complete
dataset $(X, y)$. The ``loo'' NCM is defined similarly, but uses loo-residuals for
the task:
\begin{align*}
  A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    &= |y_i - \hat{y}_{|(X_{-i}, y_{-i})}(x_i)| \\
    &= |e_i' \hat{r}_{\text{loo}}(X, y)|
    \,.
\end{align*}
Note that both are interrelated using the formula:
\begin{multline*}
  A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    = \\ \lambda^{-1} \diag(Q_X)^{-1}
    A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    \,.
\end{multline*}
Indeed, for all $i=1,\ldots,n$, the full sample residual of a fitted KRR is given
by $\hat{r}_{\text{in}}(X, y) = (I_n - K_{XX} Q_X) y$. Since $Q_X$ is invertible,
block matrix inversion yields the following expression: for all $i=1,\ldots, n$
\begin{align}
  e_i' \hat{r}_{\text{in}}(X, y)
  % = \lambda e_i' P_{i:n} P_{i:n} Q_X P_{i:n} P_{i:n} y
  % = \lambda e_n'
  %   \begin{pmatrix}
  %     Q_{-i} + Q_{-i} K_{-i}(x_n) m_i^{-1} K_{-i}(x_n)' Q_{-i} & - Q_{-i} K_{-i}(x_n) m_i^{-1} \\
  %     - m_i^{-1} K_{-i}(x_n)' Q_{-i} & m_i^{-1} \\
  %   \end{pmatrix}
  %   \begin{pmatrix} y_{-i} \\ y_i \end{pmatrix} \\
  &= \lambda m_i^{-1} \bigl(y_i - K_{-i}(x_i)' Q_{-i} y_{-i} \bigr) \nonumber \\
  &= \lambda m_i^{-1} e_i' \hat{r}_{\text{loo}}(X, y) \label{eq:loo_resid} \,,
\end{align}
where $K_{-i} = K_{X_{-i}}: \Xcal \mapsto \Real^{(n-1)\times1}$ (see eqn.~\ref{eq:gp_cond_dist})
is the canonical map vector, $Q_{-i}$ is given by $(\lambda I_{n-1} + K_{X_{-i}X_{-i}})^{-1}$,
and $\lambda m_i^{-1}$ is the KRR leverage of the $i$-th observation, with
\begin{equation*}
  m_i = \lambda + K(x_i, x_i) - K_{-i}(x_i)' Q_{-i} K_{-i}(x_i) \,.
\end{equation*}
This ``leverage'' is always within $[0,1]$, since $K$ is PD, and depends only on
the input part, $X$, of the dataset. Furthermore, it can be shown that the main
diagonal of $Q_X$ is given by $(m_i^{-1})_{i=1}^n$. Indeed, to see this apply the
block-inversion formula to the matrix $Q_X$ with symmetrically permuted rows and
columns.

For the NCM $A$ the $1-\alpha$ conformal confidence interval for the $n$-th observation
is given by
\begin{equation} \label{eq:conf_ci}
\Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
  = \bigl\{ z\in \Real \,:\, p_n\bigl((X, \tilde{y}_n^z)\bigr) \geq \alpha \bigr\}
  \,,
\end{equation}
where $\tilde{y}_j^z = (y_{-j}, z)$ -- the augmented sample $y$, with the $j$-th
value replaced by $z$. The ``conformal likelihood'' of the $j$-th observation in
some sample $(X, y)$ is given by
\begin{equation*}
  p_j\bigl((X, y)\bigr)
    = n^{-1} \bigl\lvert \bigl\{
        i = 1,\ldots, n \, : \,
        \eta_i \geq \eta_j
    \bigr\} \bigr\rvert
    \,,
\end{equation*}
for $\eta_i = A\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)$.
Thus it is necessary to obtain a formula, describing dependency of the residuals on
the augmentation of the $n$-th observation. The in-sample KRR residuals of the augmented
dataset $(X, \tilde{y}_n^z)$ are given by
\begin{align}
    \hat{r}_{\text{in}}(X, \tilde{y}_n^z)
    % &= \lambda Q_X \tilde{y}_n^z \nonumber \\
    % &= \lambda
    %   \begin{pmatrix}
    %     Q_{-n} + Q_{-n} K_{-n}(x_n) m_n^{-1} K_{-n}(x_n)' Q_{-n} & - Q_{-n} K_{-n}(x_n) m_n^{-1} \\
    %     - m_n^{-1} K_{-n}(x_n)' Q_{-n} & m_n^{-1} \\
    %   \end{pmatrix}
    %   \begin{pmatrix} y_{-n} \\ z \end{pmatrix} \nonumber \\
    % &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix}
    %  - \lambda \begin{pmatrix} Q_{-n} K_{-n}(x_n) \\ -1 \end{pmatrix}
    %    m_n^{-1} \bigl(z - K_{-n}(x_n)' Q_{-n} y_{-n} \bigr) \nonumber \\
    &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix} \nonumber \\
    &- \lambda B_{-n}(x_n) K_{-n}(x_n)' Q_{-n} y_{-n} \nonumber \\
    &+ \lambda B_{-n}(x_n) z \label{eq:krr_in_resid} \,,
\end{align}
Where the vector $B_{-n}(x_n)\in\Real^{n\times 1}$ is given by
\begin{equation*}
  B_{-n}(x_n)
    = \begin{pmatrix} - Q_{-n} K_{-n}(x_n) \\ 1 \end{pmatrix} m_n^{-1}
    \,.
\end{equation*}

In general, the construction of the confidence region (\ref{eq:conf_ci}) requires
swiping through all $y\in \Ycal$ that satisfy the significance level requirement,
which is infeasible unless $\Ycal$ is finite. The construction for the NCM (\ref{eq:ins_ncm})
for in-sample residuals in this particular problem relies on the representation
of each residual as
\begin{equation*}
  \hat{r}_i^z
    = e_i' \hat{r}_{\text{in}}(X, \tilde{y}_n^z)
    = \lambda a_i + \lambda b_i z
    \,,
\end{equation*}
with $c_i = e_i' C_{-n}\bigl((X, y), x_n\bigr)$ and
\begin{align*}
  C_{-n}\bigl((X, y), x_n\bigr)
    &= \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix} \\
    &- B_{-n}(x_n) K_{-n}(x_n)' Q_{-n} y_{-n}
    \,.
\end{align*}

Since absolute values of the residuals are compared, it is possible to consistently
manipulate the signs of each entry in $C$ and $B$ to ensure that $e_i'B\geq 0$ for
all $i$. The regions $S_i = \{z\in\Real\,:\, |\hat{r}_i^z| \geq |\hat{r}_n^z|\}$, for
$i=1,\ldots, n$, are either closed intervals, complements of open intervals,
one-side closed half-rays in $\Real$, depending on the values of $C$ and $B$. In
particular, with $p_i$ and $q_i$ denoting the values $-\frac{c_i+c_n}{b_i+b_n}$ and
$\frac{c_i-c_n}{b_n-b_i}$, respectively (whenever each is defined), each region
$S_i$ has one of the representations:
\begin{enumerate}
%% a picture of shifted and scaled x->|x| helps in derivation of this.
  \item $b_i=b_n=0$: $S_i = \Real$ if $|c_i| \geq |c_n|$, or $S_i = \emptyset$
  otherwise;
  \item $b_n = b_i > 0$: $S_i$ is either $(-\infty, p_i]$ if $c_i < c_n$, $[p_i, +\infty)$ if
  $c_i > c_n$, or $\Real$ otherwise;
  \item $b_n > b_i \geq 0$: $S_i$ is either $[p_i, q_i]$ if $c_i b_n \geq c_n b_i$,
  or $[q_i, p_i]$ otherwise;
  \item $b_i > b_n \geq 0$: $S_i$ is $\Real\setminus (q_i, p_i)$ when $c_i b_n \geq c_n b_i$,
  or $\Real\setminus (p_i, q_i)$ otherwise.
\end{enumerate}
Let $P$ and $Q$ be the sets of all well-defined $p_i$ and $q_i$ respectively, and let
$(g_i)_{j=1}^{J+1}$ be an enumeration of $\{\pm\infty\}\cup P \cup Q$ in ascending
order. Then the conformal confidence region $\Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)$ is
constructed from intervals $G_j = [g_j, g_{j+1}] \cap \Real$:
\begin{equation*}\label{eq:rrcm_conf_ci}
  \Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
    = \bigcup_{j\,:\, N_j \geq \alpha n} G_j
    \,,
\end{equation*}
where $N_j = |\{i\,:\,G_j \subseteq S_i\}|$ is the number of times $G_j$ is covered by
any region $S_i$.

For a single test point $x_n$ the worst-case complexity of this construction is
$\mathcal{O}(n \log n)$: it is necessary to sort at most $2n$ distinct endpoints
of $G_j$, then locate the values $p_i$ and $q_i$ associated with each region $S_i$
($\mathcal{O}(n\log n)$), and finally in at most $\mathcal{O}(2n)$ time compute
the coverage numbers $N_j$. The memory footprint is $\mathcal{O}(n)$.

% subsection conformal_krr_interval (end)

% section derivation (end)

\section{Numerical study} % (fold)
\label{sec:numerical_study}

Validity of conformal predictors in the online learning setting has been shown in
\cite{vovk2005}, however, no result of this kind is known in the batch learning setting.
Our experiments aim to evaluate the empirical performance of the conformal prediction
in this setting: with dedicated train and test datasets. In this section we conduct
a set of experiments to examine the validity of the confidence intervals produced
by the conformal Kernel Ridge Regression and compare its efficiency to the Bayesian
confidence intervals.

We primarily test the conformalized KRR predictions in cases when the input space
$\Xcal$ is a unit cube in either $\Real$ or $\Real^2$. The rationale for this is
that since conformal procedure is oblivious to the structure of the input dataset,
there is no reason for its validity to deteriorate with increasing dimensionality
of $\Xcal$. Indeed, in alg.~\ref{alg:conf_predictor} the input data from $\Xcal$
in the training and the test sets are fed into the NCM $A$, which, in general, can
be an arbitrary computable function, and never ``leak'' into the procedure itself.

The dimensionality of the input data impacts the performance of the procedure only
through the choice of $A$, which in our case is the absolute value of the Kernel
Ridge Regression residuals, and ultimately affects the efficiency (the width) of
the resulting confidence regions. In this study we focus exclusively on the kernel
ridge regression.

We consider the isotropic \textbf{R}adial \textbf{B}asis \textbf{F}unction kernel
for both the Conformal Kernel Ridge regression and the Gaussian Process regression:
\begin{equation*}
  K(x,x')
  = \mathop{\text{exp}}\bigl\{-\theta \|x-x' \|^2\bigr\}
  \,,
\end{equation*}
with the precision parameter $\theta>0$, and defined over $\Xcal \subseteq \Real^{d\times 1}$.
This kernel is widely used in practice, because it has very nice properties. In
particular if $\Xcal$ is compact its canonical RKHS is universal, $\Hcal_K$ is dense
in the set $C_0(\Xcal)$ of continuous bounded maps with respect to the uniform norm
$\|\cdot\|_\infty$, see.~\cite{ref:rbf_universal}.
% I. Steinwart. On the influence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67–93, 2001.

We study the effects of the kernel parameters and the choice of the conformal procedure
by varying the following hyper-parameters: \begin{enumerate}
  \item the precision of the RBF kernel $\theta$ is picked from $\{10^{-1}, 1, 10\}$,
  since the input domain is the unit cube, for which $\sup_{x,x'\in[0,1]^d}\|x-x'\|^2 = d$;
  \item the noise-to-signal ratio $\lambda \in\{10^{-k}\,:\,k=6, 4, 2, 0\}$, with
  smaller values corresponding to the interpolation task, and the larger -- to filtering;
  \item the size of the training sample $n$ is chosen from $25 2^k$, $k=0,\ldots, 8$ --
  to study the effects the train sample size on the validity and efficiency;
  \item the NCM ($A_{\text{in}}$ or $A_{\text{loo}}$).
\end{enumerate}

%% Mention that we are testing and working in the batch learning setting

\subsection{1D setting} % (fold)
\label{sub:1d_setting}

First, we assess the performance of the Bayesian and conformal confidence regions
in the case of recovering nonlinear functions on one-dimensional domain, $\Xcal =
[0, 1]$. The following functions are considered: a smooth function (``pressure2'',
fig.~\ref{fig:1dfunc_pressure2}), a piecewise smooth (``f6'', fig.~\ref{fig:1dfunc_f6})
and a step function (``heaviside'', fig.~\ref{fig:1dfunc_heaviside}).
\begin{figure}[t, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=0.9\linewidth]{1d_func_f6}
    % \caption{``f6''}
    \caption{} \label{fig:1dfunc_f6}
  \end{subfigure}~
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=0.9\linewidth]{1d_func_pressure2}
    % \caption{``pressure2''}
    \caption{} \label{fig:1dfunc_pressure2}
  \end{subfigure}~
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=0.9\linewidth]{1d_func_heaviside}
    % \caption{``heaviside''}
    \caption{} \label{fig:1dfunc_heaviside}
  \end{subfigure}
  \caption{Typical profiles of studied 1D functions.}\label{fig:animals}
\end{figure}
In each experiment, we construct the confidence regions performance for the unobserved
values on a $1$-d function on a common set of test inputs, $X^* \subset \Xcal$,
given by $\{k N^{-1}\,:\,k=0,\ldots, N+1\}$ for $N=10^3$ -- a regular gird in $[0, 1]$.
With $X^*$ fixed, we end up with the following experiment setup: for the given
parameters $\lambda, \theta, n$ and the NCM $A$
\begin{enumerate}
  \item perform $L$ independent replications of the following steps: for $l=1,\ldots, L$
  \begin{enumerate}
    \item generate a random train set $X = (x_i)_{i=1}^n\sim \mathcal{U}(\Xcal)$ of
    size $n$ and put $X_{\text{pool}} = X \cup X^*$;
    \item draw target values $y_x = f(x)$, $x\in X_{\text{pool}}$, from some
    function $f:\Xcal \mapsto \Ycal$;
    \item fix an RBF kernel $K$ with precision $\theta$ and fit a Gaussian Process
    Regression with $m(x) = 0$ to the train dataset $(y_x)_{x\in X}$ using kernel
    $\Kcal = \sigma^2(\lambda \delta_{x,x'} + K(x,x'))$, where the amplitude of
    the kernel $\sigma^2$ is also estimated;
    \item for each $x^* \in X^*$ construct both the Bayesian region, $\Bcal_{X, y}^\alpha(x^*)$,
    and the conformal $\epsilon$-confidence region, $\Gamma_{X, y}^\alpha(x^*)$ using
    the KRR NCS $A$ with kernel $\Kcal$;
    \item estimate the coverage rate $p_l(R) = |X^*|^{-1}\sum_{x\in X^*} 1_{y_x\in R_x}$
    and the width of the convex hull $w_l(R) = \inf\{b-a\,:\,R \subseteq [a, b]\}$
    for both confidence regions $R=\Bcal_{X, y}^\alpha$ and $R=\Gamma_{X, y}^\alpha$;
  \end{enumerate}
  \item assess the validity and the efficiency of the Bayesian and Conformal confidence
  regions by comparing the sample statistics of the coverage rate and width.
\end{enumerate}
It is worth emphasizing that in this experiment the input train sample is drawn from
the uniform distribution on $\Xcal$.

In the first experiment we compare the validity of the confidence regions. The fig.~
\ref{fig:1d_validity} depicts the dependence of the coverage rate upon the size of the
train sample.

In the second experiment we assess the efficiency of the Conformal Procedure by
comparing the widths of the constructed regions $\Bcal$ and $\Gamma$. In picture
\ref{fig:1d_efficiency}, 

We make the following empirical conclusions based on the conditional numerical study
in the $1$-d case: \begin{enumerate}
  \item the choice of the NCM does not appear to affect the validity of the confidence
  regions;
  \item Bayesian confidence regions, based on the Gaussian Process regression, seem
  to perform poorly relatively to the Conformal regions in non-gaussian cases;
  
\end{enumerate}
% subsection 1d_setting (end)

\subsection{2D setting} % (fold)
\label{sub:2d_setting}

% subsection 2d_setting (end)

% section numerical_study (end)

\section{Conclusion and further work} % (fold)
\label{sec:conclusion_and_further_work}

% section conclusion_and_further_work (end)

\end{document}

