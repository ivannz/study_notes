% \documentclass{ITaSconf}
% \documentclass[a4paper]{article}
\documentclass[a4paper,14pt]{extarticle}

%% >
\usepackage{extsizes}
\linespread{1.3}
\usepackage[T2A]{fontenc}
%% <

\usepackage[utf8]{inputenc}

%% >
\usepackage{indentfirst}
%% <

\usepackage{geometry}
\geometry{top=2cm} % отступ сверху
\geometry{bottom=2cm} % отступ снизу
\geometry{left=3.5cm}
\geometry{right=2cm} % отступ справа

% \usepackage{fullpage}
\usepackage[square,numbers,sort&compress]{natbib}

\usepackage[mathcal]{euscript}
\usepackage{booktabs}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathptmx}
\usepackage{algorithm2e}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{float}

\newcommand{\ex}{\mathop{\mathbb{E}}\nolimits}
\newcommand{\pr}{\mathop{\mathbb{P}}\nolimits}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\BigO}{\mathcal{O}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\nil}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\diag}{\mathop{\text{diag}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}


\def\bbljan{Jan.}
\def\bblfeb{Feb.}
\def\bblmar{Mar.}
\def\bblapr{Apr.}
\def\bblmay{May}
\def\bbljun{Jun.}
\def\bbljul{Jul.}
\def\bblaug{Aug.}
\def\bblsep{Sep.}
\def\bbloct{Oct.}
\def\bblnov{Nov.}
\def\bbldec{Dec.}

% \title{Conformalized Kernel Ridge Regression}
% \author{Nazarov, I. N., Burnaev, E. V.}

\begin{document}

\begin{titlepage}
    \selectlanguage{russian}
    \pagenumbering{gobble}
    \vbox to \textheight {
        \renewcommand{\baselinestretch}{1}\selectfont
        \begin{center}
            \textbf{%\Large
            Правительство Российской Федерации\\[0.2cm]
            Федеральное государственное автономное образовательное учреждение 
            высшего профессионального образования\\%[0.5cm]
            Национальный Исследовательский Университет\\%[0.5cm]
            <<Высшая Школа Экономики>>}\\[0.7cm]

	        \textbf{%\Large
	        	Факультет Компьютерных Наук\\[0.2cm]
	            Магистерская программа <<Науки о Данных>>\\[0.2cm]
	            кафедра <<Технологии Моделирования Сложных Систем>>\\[2.0cm]}

            {\large \bfseries ВЫПУСКНАЯ КВАЛИФИКАЦИОННАЯ РАБОТА}\\[0.5cm]
            на тему\\[1.0cm]
            {\large \bfseries <<Конформные методы в многомерных линейных моделях и детектировании аномалий>>}\\[0.5cm]
            % {\large \bfseries <<Применение конформных процедур в многомерных линейных моделях и детектировании аномалий>>}\\[0.5cm]
        \end{center}
        %\end{center}

        \vspace{3.0cm}

        \begin{flushright}
            Студент группы м14НоД ИССА\\
            Назаров Иван Николаевич\\[2.0cm]
            Научный руководитель\\
            к.ф-м.н., доцент\\
			Бурнаев Евгений Владимирович\\[3cm]
        \end{flushright}

        \vspace{2.0cm}

        \vfill
        \begin{center}
            Москва, 2016\\[3.0cm]
            % \includegraphics{hsecmyk}\\[1cm]
        \end{center}
    }
\end{titlepage}
\clearpage

\selectlanguage{english}
\begin{titlepage}
    \selectlanguage{english}
    \pagenumbering{gobble}
    \vbox to \textheight {
        \renewcommand{\baselinestretch}{1}\selectfont
        \begin{center}
            {%\Large
            Government of Russian Federation\\[0.2cm]
            Federal State Autonomous Educational Institution of Higher Professional Education\\
            National Research University\\
            ``Higher School of Economics''}\\[0.7cm]

            {%\Large
            Faculty of Computer Science\\[0.2cm] 
            Master's programme in Data Science\\[0.2cm]
            Department of ``Technologies of Modeling of Complex Systems''\\[2.0cm]}
            
            \textbf{\large \bfseries MASTER THESIS}\\[0.5cm]
            on\\[1.0cm]
            % {\large \bfseries ``Conformal procedures in multivariate linear models and anomaly detection'' }\\[0.5cm]
            {\large \bfseries ``Conformalized multidimensional linear modelling and anomaly detection'' }\\[0.5cm]
        \end{center}

        \vspace{3.0cm}

        \begin{flushright}
        	by Nazarov Ivan \\student, group \rus{м14НоД ИССА}\\[2.0cm]
            Scientific Advisor:\\
            Evgeniy Brunaev, PhD\\
            Associate Professor\\[3cm]
        \end{flushright}

        \vspace{2.0cm}

        \vfill
        \begin{center}
            Moscow, 2016\\[3.0cm]
            % \includegraphics{hsecmyk}\\[1cm]
        \end{center}
    }
\end{titlepage}

\clearpage
\pagenumbering{gobble}
\selectlanguage{russian}
\section*{Аннотация}

Обнаружение аномалий является важной важной прикладной задачей. Одним из методов
обнаружения является построение предиктивных моделей, на основе прогнозов которых
выносится суждение об аномальности. Однако без дополнительных вероятностных предположений
подобные методы не способны указать степень уверенности в собственном предсказании.
В настоящей работе рассматривается конформный метод построения доверительных множеств
в задаче ядерной регрессии, позволяющий получить вероятностные гарантии относительно
точности прогноза, что очевидным образом даёт возможность определят аномалии как
события выхода за пределы построенных доверительных интервалов. В данной работе
приводится описание и построение эффективной процедуры для построения конформных
интервалов, а также проводится сравнительное эмпирическое исследование конформной
ядерной гребневой регрессии с Байесовскими доверительными интервалами, полученными
из регрессии на Гауссвоских процессах.
Основная цель работы состоит в проведении эмпирического исследования, результаты
которого призваны эмпирически обосновать применимость конформной процедуры в оффлайн
режиме к проблеме обнаружения аномалий. Методология исследования заключается в проведении
численных экспериментов для сравнения конформных доверительных интервалов с Байесовскими
в рамках ядерной гребневой регрессией с Гауссовским ядром на синтетических данных,
порождённых специально подобранными тестовыми функциями как в $1$-d, так и в $2$-d
случаях, как с наличием шума, так и без него. Отдельно исследовались данные, порождённые
гауссовским процессом, для валидации результатов экспериментов в условиях выполнения
предпосылок, необходимых для регрессии на Гауссвоских процессах.
Основные результаты данной работы заключаются в получении эмпирических свидетельств
в пользу того, что конформные процедуры в могут быть использованы для построения
предиктивных моделей для детектирования аномалий.

\hfill\\\hfill\\
\noindent\textbf{Ключевые слова:} конформные процедуры, ядерные методы, гребневая регрессия,
доверительныне множества, консервативная валидность;

\clearpage
\pagenumbering{gobble}
\selectlanguage{english}
\section*{Abstract}

The problem of anomaly detection is an important applied task, among the most common
approaches to which is anomaly detection based on predictive modelling. The predictions 
of such models are compared to the actually observed new data in order to decide
whether there is an anomaly or not. However, without additional distributional assumptions
such models cannot provide an probabilistic measure of the degree of their own confidence
in the prediction. This report considers conformal methods for constructing non-
parametric confidence regions in the kernel ridge regression modelling framework,
that offer certain probabilistic guarantees regarding their validity. With a ready
confidence region a new observation is considered anomalous if it is not covered
by the constructed confidence set.
In this report we provide a detailed description of a computationally efficient conformal
procedure, derive the key distributions and formulae, and conduct a comparative numerical
study to see how well the conformal confidence regions perform against the confidence
sets, produced by the Gaussian Process Regression (Kriging) in the fully Gaussian
setting. The main goal of the report is to investigate empirically the question if
it is possible to use conformal procedures in offline setting in anomaly detection
tasks.
The main research methodology is numerical experimentation, which tested the validity
of the confidence intervals in different settings: $1$-d or $2$-d, noisy or noiseless
observations, -- for a certain set of non-smooth test functions. The case of data,
generated by a Gaussian Process was considered separately in order to verify if the
conformal confidence regions are remain valid and asymptotically efficient in the 
setting where Kriging (Gaussian Process Regression) should perform well.
The experimental results allowed us to conclude that conformal procedures are indeed
useful in constructing anomaly detection systems based on predictive models.

\hfill\\\hfill\\
\noindent\textbf{Keywords:} conformal prediction, kernel methods, ridge regression,
confidence sets, conservative validity;

\clearpage
\pagenumbering{gobble}
\selectlanguage{english}
\tableofcontents

\clearpage
\pagenumbering{arabic}
\selectlanguage{english}

\section{Introduction} % (fold)
\label{sec:introduction}

Anomaly detection is concerned with identifying observations in new incoming test
data, which diverge in a some sense from the previously seen data. The central idea,
common to all methods in this field, is to infer from the provided sample data some
notion of ``normal'' data, and based on that, derive a notion of ``anomalous'' data.
By definition, the volume of positive, ``normal'', observations overwhelms the amount
of negative, ``anomaluos'', examples.

Methods of anomaly detection are widespread in applied fields, which deal with complex
multi-component systems, that generate enormous volumes of telemetry and other data.
For example, breast cancer detection based on mammography screening data \cite{tarassenko1995}
and other fields of medical diagnostics \cite{quinn2007,clifton2011}, detection of
failures and breakdown in complex industrial systems \cite{tarassenko2009}, on-site
structure monitoring \cite{surace2010}, intrusion detection and anti-fraud solutions
in bank and mobile network operators \cite{patcha2007,jyothsna2011}, et c. The complexity
of modern multi-component systems stems from practical difficulty of recovering the
complete picture of interactions between the constituent parts. As a result, there
is a large number of ``anomalous'' regimes of operation, some of which may not be
known in advance, and that render standard multi-class classification solutions inapplicable.
One way of resolving this issue of incomplete information is to employ anomaly detection
methods, that ``learns'' what the ``normal'' mode is by using the overwhelming volume
of positive examples. Such methods check the patterns in the test observations against
the ``learnt'' notion of ``normality'' by estimating their degree of abnormality.
Such measure, which may have probabilistic interpretation, is compared to a decision
threshold, and the test examples are classified as anomalous if the threshold is
exceeded.

In many applied situations, like anomaly detection in telemetry of some equipment,
online filtering and monitoring of potentially interesting events, or power grid
load balancing, it is necessary not only to make optimal predictions with respect
to some loss, but also to be able to quantify the degree of confidence in the obtained
forecasts. At the same time it is necessary to take into consideration exogenous
variables, that in certain ways affect the object of study. Anomaly detection techniques
based on predictive modelling are concerned with applying regression and classification
methods to model the data, and based on the learnt conditional model, decide if the
new data is normal or not.

To recap, standard regression methods are concerned with studying pattern of dependence
between the target variable $y$ and the input variable $x$ based on a finite train
sample of  possibly noisy observations $(X, y)$ from $\Omega\times \Ycal \subseteq
\Xcal\times\Real$. These methods postulate that there exits a stable unobserved relationship
$y_x = f(x)$ and observations of $y$ at any given $x\in\Omega$ are corrupted by
additive noise:
\begin{equation} \label{eq:signal_model}
  y_x = f(x) + \xi_x \,,
\end{equation}
with $\ex \xi_x = 0$ for all $x\in \Omega$. Regression methods allow one to construct
an estimate $\hat{f}$ of the true relation $f$, which can be used to produce point
predictions at any $x$.

These methods, however, do not readily provide the user with any sort of prediction
confidence measure, and in order to acquire such measure additional probabilistic
assumptions regarding the observation model (eq.~\ref{eq:signal_model}) are required.
With their help, it is usually quite straightforward to get a posterior probability
measure for unobserved data given the training data using standard Bayesian derivation
methods.

For example, in the ordinary least squares regression with fixed design the assumption
that the noise observations is Gaussian readily yields the posterior distribution of
the target variable at new test points: if $X\in \Real^{n\times d}$, and $y\in\Real^{n\times1}$
is the observed training sample, then at a new $x_0\Real^{d\times 1}$
\begin{equation}
  y_{x_0} - x_0'\hat{\beta} \sim \Ncal(0, \sigma^2(1 + x_0'(X'X)^{-1} x_0)) \,,
\end{equation}
for the OLS estimate $\hat{\beta} = (X'X)^{-1} X'y$ and assuming known variance $\sigma^2$.
If the variance is is estimated form the training data and the final distribution,
which can be used to measure confidence in unobserved target at $x_0$ is
\begin{equation*}
  \frac{y_{x_0} - x_0'\hat{\beta}}{\sqrt{\hat{s}^2 (1 + x_0'(X'X)^{-1} x_0)}}
    \sim t_{n-d} \,,
\end{equation*}
where $t_{n-d}$ is the Student-t distribution with $n-d$ degrees of freedom, and
$\hat{s}^2 = (n-d)^{-1}\|y-X\hat{\beta}\|^2$ is the estimate of the noise variance.

The distributional assumptions used to derive this posterior distribution are too
restrictive: even though the additive effect of many uncontrollable factors is usually
quite faithfully modelled by a Gaussian distribution (according to the CLT), still
it is desirable to have distribution-free method that measures confidence of predictions
of a machine learning algorithm, and works well in practice, yet does not loose too
much performance when Gaussianity assumptions are satisfied. Such a non-parametric
method coupled with such a powerful curve fitting and interpolation technique, as
the kernel ridge regression (KRR) could be beneficial in anomaly detection: it models
the dependence of a target variable on the input features, and provides the user with
a model-free data-driven measure of predictive confidence at each new input.

One such method is ``conformal prediction'' -- a model-free approach developed in
\cite{vovk2005} for online learning setting, that under the standard independence
assumption constructs a set in the space of targets, that contains yet unobserved
data with a pre-specified confidence level. Conformal prediction can be built on
top of any other ML algorithm: regression (ridge, $k$-NN et c., \cite{vovk2005},
chapter 2) or classification, -- any tool, that is able to make predictions about
yet unobserved target variables. The chief purpose of ``conformalizing'' is to make
predictive models, especially Bayesian models, valid under less restrictive distributional
assumptions, and yet offer not much worse performance, when the assumptions of the
original method are valid. ``Conformalization'' of ML algorithms is in spirit similar
to designing non-parametric, robust analogues to classical parametric tests, which
rely on Gaussianity.

Conformal prediction essentially does the following: for every possible value $z$
of an object $Z_{n+1}$ the procedure tests $H_0: Z_{n+1}=z$, and then inverts the
test to get a confidence region. The hypothesis tests are based on the observed sample
$(Z_i)_{i=1}^n$ and hypothesized $z$, and are designed to have a pre-specified type-I
error rate $\alpha$. In classification, the class label hypotheses for new $x_{n+1}$
are tested, whereas in regression the test are conducted for target levels.

In this study, we provide empirical evidence supporting the applicability of conformal
prediction in the offline batch learning setting. In particular, we report evidence
showing that when model assumptions do hold, the conformal confidence sets, constructed
over the Kernel Ridge Regression with isotropic Gaussian kernel do not perform worse
than the prediction confidence intervals of a Bayesian version of the KRR -- the
Gaussian Process Regression (GPR) with the same kernel. The obtained empirical results
demonstrate applicability of conformal methods to anomaly detection in supervised
offline learning setting. The chief contribution of the conducted research is in
the field of applicability of comformalized models, specifically the kernel ridge
regression methods, anomaly and novelty detection in complex multi-component systems
with multi-dimensional datasets.

The report is structured as follows: in sec.~\ref{sec:survey} we review common state
of the art approaches to anomaly detection in multivariate data, in sec.~\ref{sec:kernel_ridge_regression}
we provide an brief overview of the machinery behind the kernel ridge regression, in
section~\ref{sub:bayesian_interpretation_of_krr} we restate the KRR in the Gaussian
setting, and in section~\ref{sec:conformal_prediction} a concise overview of what
conformal prediction is and what is required to construct such kind of confidence
predictor is given. Section~\ref{sec:conformalized_krr} describes the particular
steps needed to build a conformal predictor atop the kernel ridge regression.
The main empirical study is reported in section~\ref{sec:numerical_study}, where
we study the properties of the predictor in a batch learning setting for a KRR
with specific kernel (isotropic Gaussian). We conclude this study with discussion
and prospects of future research on this topic (sec.~\ref{sec:conclusion_and_further_work}).

% section introduction (end)

\section{Survey of anomaly detection methods} % (fold)
\label{sec:survey}

Practical importance and difficulty of anomaly detection in general spurred a great
deal of research, which resulted in a large volume of heterogeneous approaches and
methods to its solution. In this section we are going to review some main approaches
for anomaly detection in multidimensional i.i.d. observations.

\paragraph{Anomaly detection based on probabilistic models} % (fold)
\label{par:anomaly_detection_based_on_probabilistic_models}

Probabilistic approaches to anomaly detection rely on approximating the generative
distribution of the observed data. Later, the learnt distribution can be truncated
in order to determine a ``normal'' region in the space of data, and be used to tell
if test data is generated by this distribution, or another one. It is assumed that
the train dataset is indeed generated by a process with a probabilistic nature, and
that its estimation is feasible. The resulting distribution is usually considered
as the ``normal'' mode. Furthermore, the abnormality decision threshold can be endowed
with a probabilistic interpretation, if one is required.

The tools employed in such tasks vary in their complexity. The simplest approaches
are based on the machinery of statistical hypothesis testing, which are usually equivalent
to statistical outlier tests \cite{vic1994}. These procedures are able to determine
whether an example comes from the same probability distribution as the normal data.
The majority of these tests are based on Gaussian assumption (c.f.~\cite{grubbs1969}),
and work only for univariate continuous data, however there are versions adapted
to multivariate case (c.f.~\cite{aggarwal2008,vic1994}). We are not going to consider
classical univariate outlier test any further, and instead will examine more modern
methods, that can handle complex multivariate distributions.

Estimation of the distribution, which generated the multivariate observations, has
been extensively studied (c.f.~\cite{chow1970,scott2008}). The relevant methods can
be either parametric or non-parametric. Parametric methods rely on a rather restrictive
assumption, that the data distribution belongs to a certain class of models, where
each model is identified by a finite set of parameters. Such restriction usually
lead to significant bias, in particular if the real model is not covered by the assumed
parametric class. Non-parametric models are more flexible in general, yet require
much larger datasets for estimation. One of the most popular parametric models is
the Gaussian Mixture model \cite{markou2003,chandola2009,miljkovic2010}, and one
of the most frequently used non-parametric models is Kernel Density estimation
\cite{duda2012,markou2003,chandola2009}.

% paragraph anomaly_detection_based_on_probabilistic_models (end)

\paragraph{Anomaly detection based on distance similarity} % (fold)
\label{par:anomaly_detection_based_on_distance_similarity}

Metric based anomaly detection includes clustering and nearest-neighbour (NN) methods
and constitute an alternative approach, which solves a problem, equivalent to generative
distribution estimation. These methods are based on special metrics, that quantify
the similarity or the degree of closeness between observations. NN methods are very
rarely overlooked. The $k$-NN approach is founded on the principle that neighbours
of normal observations belong to the ``normal'' sample, whereas abnormal observations
are located far away \cite{hautamaki2004}: an example is considered an outlier, or
an anomaly, if an aggregated distance to its neighbours is significant. Usually,
Euclidean metric is used, less frequently -- the Mahalanobis metric, which take
into account the shape of the sample data as measured by the sample covariance matrix.

In practice the following methods are usually employed in measuring the similarity
between observations \cite{duda2012}: distance-based, such as the distance to the
$k$-th nearest neighbour \cite{zhang2006}, methods based on local density, which
measure the average distance in the local neighbourhood of size $k$ around an example
\cite{hautamaki2004}.

The key principle of the anomaly detection based on clustering is that the ``normal''
class can be described by a relatively small number of prototypes. Therefore, the
degree of abnormality is derived from the minimal distance from a test observation
to the closest prototype.  One of the most popular algorithms is $k$-means clustering,
see \cite{srivastava2005,srivastava2006}.

Density-based anomaly detection was suggested in \cite{breunig2000} and is based
on a so called Local Outlier Factor (LOF) score which is computed for each observation.
from on its local topology within the train dataset. The local topology is defined
in terms of neighbourhood containing at least as many points as specified by the
user: the $k$-NN neighbourhood $N_k(x)$ based on $d$ excluding the $x$ itself. In
this setting a natural the notion of local density arises: the ``local reachability
density'' (as defined in \cite{breunig2000}) is given by
\begin{equation*}
    \rho_x = \frac{|N_k(x)|}{\sum_{y\in N_k(x)} \max\{d_y, d(x,y)\}} \,,
\end{equation*}
with $d_x = \max_{y\in N_k(x)} d(x, y)$ the characteristic distance of a point's
local topology. The LOF score, in essence, is the ratio of the local density $\rho_x$
of a point $x$ to averaged local density of each of its local neighbours. It was
shown in \cite{breunig2000}, that whenever a point lies deep within a cluster (well
embedded) then its ``LOF'' is approximately equal to $1$, and conversely that if
$l_x < 1$ then the average local density to the neighbours is greater than the
local density within the neighbourhood, which means that $x$ is actually well embedded
within a cluster. It is worth noting, however, that LOF ranks the observation by
their abnormality score, but still can miss potential outliers, the local density
of which is sufficiently large, yet close to the neighbourhood's average.

In \cite{kriegel2009} the ``LoOP'' (local outlier probability) method was introduced,
which is similar in spirit to ``LOF'', but addresses its lack of natural abnormality
degree scale, with which to assign probabilities. This method posits that the key
local characteristic of a point is its local context $S_x$, which, in practice, is
taken to be the $k$-NN neighbourhood of the train sample with respect to dissimilarity,
or metric $d$. The local context of $x$ gives rise to the so called $\phi$-distance,
defined as such a value $\rho_x\geq 0$, that $P_{z\sim S_x}(d(x,z)\leq \rho_x)\geq \phi$
for some fixed probability threshold $\phi\in(0,1)$. Authors make a simplifying assumption
that $d(x,z)$ for $z$ within $S_x$ is distributed like $a\sim \Ncal(0, \sigma^2)$
given $a\geq0$, that enables them to re-parametrize the $\phi$-distance in terms of
the number of root mean squared distance in $S_x$, which yields a simpler formula
for $\rho_x$:
\begin{equation*}
    \rho_x
    = \bigl(|S_x|^{-1} \sum_{y \in S_x} d^2(x, y)\bigr)^\frac{1}{2}
    \,.
\end{equation*}
The scores are given by 
$$ l_x = \frac{|S_x|\rho_x}{\sum_{y\in S_x} \rho_y}-1\,, $$
and the local anomaly probability is given by
$$ p_x = \max\bigl\{0, \text{erf}\bigl(\frac{l_x}{N_T\sqrt{2}}\bigr)\Bigr\}\,, $$
for $\text{erf}(c) = \frac{2}{\sqrt{\pi}} \int_0^c e^{-x^2} dx$ and $N_T$ is the
normalization constant $\mu$ times the root means squared $l_x$ score on the train
dataset.

Other possible approaches to endowing LOF with probabilistic scores include using
the empirical distribution of the scores over the train dataset, or applying conformal
anomaly detection procedures to assign calibrated probabilities, \cite{laxhammar2014}.

% paragraph anomaly_detection_based_on_distance_similarity (end)

\paragraph{Anomaly detection based on predictive modelling} % (fold)
\label{par:anomaly_detection_based_on_predictive_modelling}

Anomaly detection techniques based on prediction employ regression and classification
methods, which model the observed dataset. For each test observation a forecast error
is computed, defined as the discrepancy between the observed and the predicted vectors,
and then used to derive the abnormality score. For example, neural networks (multilayer
perceptrons) are used to this end in \cite{augusteijn2002}. An approach to multivariate
anomaly detection based on autoencoders, or replicating neural networks, is considered
in \cite{hawkins2002,williams2002}. Autoencoders are multilayer neural networks of
special structure, which are trained to replicate as close as possible any input
observation vector. Autoencoders are structured in such a way as to force dimensionality
reduction, and thus generalization and information compression, within the network.
Finally, an observation is considered anomalous is it suffers large reconstruction
error.

Another type of algorithms for anomaly detection, also known as spectral methods
\cite{chandola2009}, consists of constructing combinations of input features, that
most comprehensively describe the variability of the input data. It is assumed that
projecting the data onto a lower dimensional space would make it easier to distinguish
between the anomalous and normal observations. Principal Component Analysis is a
standard technique for dimensionality reduction that yields such a set of orthonormal
directions along which the variability of the data is maximal. This method can be
used to construct an approximation of the train data distribution in the transformed
space \cite{jolliffe2014}. PCA based methods for anomaly detection are reviewed in
\cite{dutta2007,shyu2003}. In \cite{hoffmann2007,scholkopf1998} the PCA is generalized
to non-linear case with a non-linear data transformation using a special kernel mapping
(Kernel PCA).

% paragraph anomaly_detection_based_on_predictive_modelling (end)

\paragraph{Domain-based anomaly detection} % (fold)
\label{par:domain_based_anomaly_detection}

Yet another class of anomaly detection methods relies on describing the decision
boundary of the ``normal'' data. This sort of methods is unaffected by the density
of the normal class, since only the boundary of the relevant region is modelled, not
its interior. Whether an observation belongs to a certain class is decided on the
proximity to the decision boundary. As in the case of two-class SVM, SVM for anomaly
detection (one-class SVM) defines the normal-abnormal separating hypersurface using
the supporting vectors close to it, while the remaining observations are not used
at all, see \cite{scholkopf1999,tax1999,manevitz2002}. Decision rule is built from
a solution to an optimization problem, which constructs a hyperplane in the feature
space (induced by some kernel) that maximizes the margin of separation of the transformed
data from the origin.

% paragraph domain_based_anomaly_detection (end)

% section survey (end)

\section{Kernel ridge regression} % (fold)
\label{sec:kernel_ridge_regression}

Suppose $\Xcal = \Real^{d\times 1}$. Ordinary least squares regression assumes that
there exists $\beta\in \Real^{d\times 1}$ such that $f(x) = x'\beta$. If the underlying
relationship is non-linear, the trained linear model will have high bias, i.e.
it will make statistically non-negligible systematic errors in its predictions.
However, the linear formulation does not limit the applicability of the ordinary
regression, since by choosing a richer input-feature mapping $\phi: \Omega \mapsto
\Real^{p\times 1}$ it is possible to cover the case of non-linear target-observation
relationships. The map $\phi$ defines a basis with respect to which the unknown
underlying $f$ is approximated. For instance, the mapping $x\mapsto (\prod_{i=1}^d
x_i^{m_i})_{m\in M}$, with integers $M=(m_i)_{i=1}^d\geq 0$ and $\sum_{i=1}^d m_i \leq p$,
permits approximation of polynomial $f$.

In general, the larger the feature space (i.e. the more diverse ) the more expressive
the regression model is and thus the wider the class of problems it is applicable
to. However, more coefficients in the feature expansion of $f$ make the regression
problem overdetermined and, thus prone to over-fitting. This increases the variance
of the trained regression, thereby reducing it generalization ability. This can be
remedied by restricting the set of functions available for approximation by regularizing,
or shrinking the coefficients $\beta$ in the expansion.

Given train-set observations $(x_i, y_i)_{i=1}^n \in \Real^{d\times 1}\times \Real$,
with $x_i$ collected row-wise in a design matrix $X\in \Real^{n\times d}$, the target
vector -- $y=(y_i)_{i=1}^n \in \Real^{n\times 1}$, and a regularization parameter
$\lambda > 0$, the sample-space Ridge Regression (RR) problem solves this minimization
problem
\begin{equation}
  \| y - X\beta - \one\beta_0 \|^2 + \lambda \|\beta\|^2
    \to \min_{\beta_0\in \Real, \beta \in \Real^{d\times 1}} \,,
\end{equation}
where $\one \in \Real^{n\times 1}$ is a vector of ones. Basically ridge regression
is linear least squares regression with $L_2$-norm regularization.

The intercept term is given by
\begin{equation}
  \hat{\beta}_0 = \bar{y} - \bar{X} \hat{\beta} \,,
\end{equation}
where $\bar{y}$ and $\bar{X}$ are the mean target and observation values respectively,
and $\hat{\beta}$ solves the intercept-free ridge regression problem run on centred
observations and targets. Therefore in the remainder of this section we omit the intercept
term and assume that the relevant columns in the design matrix have been dealt with,
and both the design matrix and the targets have been centred.

Since this one is problem of convex optimization, there exists a unique solution
given by 
\begin{align*}
  \hat{\beta}
    &= (X'X + \lambda I_p)^{-1} X' y \\
    &= X' (\lambda I_n + XX')^{-1} y \,,
\end{align*}
in the direct and the equivalent dual forms, respectively. Predictions on the test
set $X^* = (x_i)_{i=n+1}^{n+m}\in \Real^{m\times d}$, given the train-set, are
\begin{align}
  \hat{y}^*_{\vert(X,y), X^*} = X^* \hat{\beta}
    &= X^* (X'X + \lambda I_p)^{-1} X' y \nonumber \\ 
    &= X^* X' (\lambda I_n + XX')^{-1} y \label{eq:rr_dual_sol}\,,
\end{align}
again in the direct and dual form, respectively.

The so called ``Kernel-trick'', \cite{scholkopf2002}, generalizes any ML algorithm,
provided it could be reduced to a form, which depends on the training sample data
only thorough inner-products, or equivalents, between the objects (the sample Gram
matrix). The generalization is done by replacing the products with the products of
the feature maps $\langle\phi(x_i),\phi(x_j)\rangle$ in some RKHS $\Hcal$, or, equivalently,
by the gram matrix of some Mercer-type kernel (see sec.~\ref{sub:kernel_methods}).
It should be noted that care must be taken to ensure additional conditions on the
input domain imposed by this ``trick'' are met.

For instance, the RR prediction in its dual form, eq.~\ref{eq:rr_dual_sol}, depends
only on the Gram matrix if inter-object inner-products $X X'$ in the input domain,
which is why it is possible to restate it as the kernel ridge regression problem.

\subsection{Kernel methods} % (fold)
\label{sub:kernel_methods}

In this section we are going to give a brief recap of the foundations of kernel methods
and their applications (for a more thorough overview see \cite{hofmann2008}).

A tuple $(\Hcal, \langle\cdot, \cdot\rangle)$ is an inner-product space over $\Real$
if $\Hcal$ is a vector space over $\Real$, and $\langle\cdot, \cdot\rangle : \Hcal
\times \Hcal \mapsto \Real$ is an inner product in $\Real$ -- a map linear in both
arguments, symmetric, and possessing the following properties:
\begin{enumerate}
  \item $\langle f, f\rangle\geq 0$ for any $f\in \Hcal$;
  \item $\langle f, f\rangle = 0$ if and only if $f = \nil_\Hcal$.
\end{enumerate}
Over the field of complex numbers $\Cplx$ the requirements of symmetry and bi-
linearity are swapped with being Hermitian, and linear with respect to the first
argument. In the following presentation all considered vector spaces are over the
field of reals.

A Hilbert space is an inner product space, which is complete with respect to the
natural metric induced by the norm $ \|f\| = \sqrt{\langle f, f\rangle} $. A Reproducing
Kernel Hilbert Space (RKHS) is a complete inner-product vector space of maps $\Xcal
\mapsto \Real$ over the field $\Real$, in which the evaluation functional defined
by $\epsilon_x(f) = f(x)$ is $(\Hcal, \|\cdot\|) \mapsto (\Real, |\cdot|)$ bounded
for any $x\in \Xcal$ (in normed spaces bounded linearity is equivalent to continuity).
Equivalently, an \textbf{RKHS} is a Hilbert space for which there exists a map
$\phi:\Xcal\to\Hcal$, and a Positive Definite (PD) kernel function
$K:\Xcal \times \Xcal \mapsto \Real$ such that \begin{enumerate}
  \item $K(x,y) = \langle \phi(x), \phi(y) \rangle$ for all $x, y\in \Xcal$;
  \item $g(x) = \langle g, K(x, \cdot)\rangle$ for all $x\in \Xcal$ and every
  $g\in \Hcal$.
\end{enumerate}
A function $K:\Xcal \times \Xcal \mapsto \Real$ is positive definite if for any
$n\geq1$, $(x_i)_{i=1}^n \in \Xcal$ and $\alpha \in \Real^{n\times 1}$
\begin{equation*}
  \alpha'K\alpha
    = \sum_{i=1}^n \sum_{j=1}^n \alpha_j \alpha_i K(x_i, x_j)
    \geq 0
    \,,
\end{equation*}
or, in other words the Gram matrix $K$ is positive semi-definite, where $K$ is an
$n \times n$ matrix of $K(x_i, x_j)$. Note that unlike linear algebra, ``positive
definite'' here permits such non-zero weights, for which the product is exactly
zero. Positive definite kernels are also known as Mercer kernels, or covariance
kernels.

Finally, any PD kernel $K$ gives rise to a so called ``canonical RKHS'' associated
with $K$ constructed by a standard metric-completion argument for a pre-Hilbert
space (a non-complete inner-product space)
\begin{equation*}
\Hcal_0
  = \bigl\{
    \sum_{i=1}^n \alpha_i K(x_i, \cdot)
    \,:\, n\geq1, (x_i)_{i=1}^n \in \Xcal, (\alpha_i)_{i=1}^n\in \Real
  \bigr\} \,,
\end{equation*}
with an inner-product 
\begin{equation*}
  \langle f, g \rangle = \sum_{i=1}^n \sum_{j=1}^m \alpha_i K(x_i, z_j) \beta_j \,,
\end{equation*}
where $f, g\in \Hcal_0$ with representations $f = \sum_{i=1}^n \alpha_i K(x_i, \cdot)$
and $g = \sum_{j=1}^m \beta_j K(z_j, \cdot)$. In the completed RKHS, $\Hcal = \bigl[\Hcal_0\bigr]$,
the canonical feature map $\phi$ is given by the canonical map $x\mapsto K(x, \cdot)$
and $K(\cdot, \cdot)$ is the reproducing kernel. The inner-product is extended to
$\Hcal$ by continuity as a step in completion of $\Hcal_0$. It is worth noting, that
the pre-Hilbert space $\Hcal_0$ is by construction dense everywhere in $\Hcal$ with
respect to the metric induced by the completed inner-product.

Given a train sample $(x_i, y_i)_{i=1}^n \in X\times \Real$ a general kernel-based
machine learning regularized loss minimization problem is to solve
\begin{equation}
  \Lcal\bigl((x_i, y_i)_{i=1}^n, f\bigr)
    + \lambda \Omega\bigl(\|f\|\bigr)
    \to \min_{f\in \Hcal} \,,
\end{equation}
where $\Hcal$ is a Hilbert Space with Reproducing Kernel $K$, $\Lcal$ is a loss
function which depends on $f$ through $(f(x_i))_{i=1}^n$, and $\Omega:\Real\mapsto\Real$
is a non-decreasing function used for regularization. In this setting the Representer
theorem, \cite{scholkopf2002}, states that the optimal solution is to be sought in a
finite dimensional linear span of the canonical feature maps $\phi: x\mapsto K(x, \cdot)$
evaluated at the sample objects:
\begin{equation*}
  f^* \in \bigl\{ \beta'\Phi\,:\, \beta \in \Real^{n\times 1} \bigr\} \,,
\end{equation*}
where $\Phi = \bigl(\phi(x_i)\bigr)_{i=1}^n \in \Hcal^{n\times 1}$.

Kernel ridge regression combines ridge regression with the kernel trick, and learns
a linear function in the feature space induced by the selected kernel and sample
data. Given a training sample $X = (x_i)_{i=1}^n$, $X\in \Xcal^{n\times 1}$ and
$y \in \Real^{n\times 1}$, KRR solves the following problem
\begin{equation*}
  \|y - f\|^2 + \lambda \|f\|^2 \to \min_{f \in \Hcal} \,,
\end{equation*}
with $f \in \Real^{n\times 1}$ being a column-vector of $f$ evaluated at every input
in $X$. Here $\Hcal$ is the canonical RKHS associated with a Mercer-type kernel $K$.
Note that with slight abuse of notation the inner-products and norms over $\Hcal$
and $\Real^{n\times 1}$ are used without denoting which is which, since it is always
clear from both the context and the type of the argument of the norm as a function.

The Representer theorem states that the solution to this minimization is of the form
$f = \Phi'\beta$, for $\Phi = (\phi(x_i))_{i=1}^n \in \Hcal^{n\times 1}$. By definition
of the Gram matrix $K$ and due to its symmetry for any $j=1,\ldots,n$
\begin{equation*}
  f(x_j)
    = \Phi(x_j)' \beta
    = k_{x_j}' \beta
    = (K e_j)' \beta
    = e_j' K \beta
    \,,
\end{equation*}
where $k_x = \Phi(x) = (\phi(x_i)(x))_{i=1}^n \in \Real^{n\times 1}$ and $e_j$ is
the $j$-th unit vector in $\Real^{n\times 1}$. Furthermore
\begin{equation*}
  \| f \|^2
    = \sum_{i=1}^n\sum_{j=1}^n\beta_i\beta_j \langle\phi(x_i), \phi(x_j)\rangle
    % = \sum_{i=1}^n\sum_{j=1}^n\beta_i\beta_j K(x_i, x_j)
    = \beta' K \beta \,.
\end{equation*}
Thus the kernel ridge regression problem is reduced to the following finite-dimensional
convex minimization problem:
\begin{equation*}
  \|y - K \beta \|^2 + \lambda \beta' K \beta
    \to \min_{\beta\in \Real^{n\times 1}} \,.
\end{equation*}
The first order conditions imply that
\begin{equation} \label{eq:krr_approx}
  \hat{\beta} = (\lambda I_n + K)^{-1} y \,\text{ and }\, \hat{y}_x = k_x' \beta \,,
\end{equation}
-- the estimated weight vector and an optimal prediction at $x\in \Xcal$, respectively.

Consider a problem of estimating the prediction accuracy of a test sample $(X^*, y^*)$
given a training sample $(X, y)$, where $X = (x_i)_{i=1}^n$, $y\in \Real^{n\times 1}$,
$X^* = (x^*_j)_{j=1}^m = (x_i)_{i=n+1}^{n+m}$ and $y^*\in \Real^{m\times 1}$. 
The out-of-train-sample prediction residuals are given by
\begin{equation*}
  y^* - \hat{y}^*_{|(X, y), X^*}
    = y^* - K_{XX^*}' (\lambda I_n + K_{XX})^{-1} y
    \,,
\end{equation*}
with $K_{XX^*} = (K(x_i, x^*_j))_{i,j} \in \Real^{n\times m}$ and $K_{XX}$ defined as
$(K(x_i, x_j))_{i,j} \in \Real^{n\times n}$. Now an in-sample prediction based on the
optimal weights over the pooled sample $(\tilde{X}, \tilde{y})$ given by $((X, X^*),
(y, y^*))$. First, since the matrix $\lambda I_n + K_{XX}$ is always invertible for
$\lambda > 0$, block-matrix inversion formula implies that $Q_{\tilde{X}} =
(\lambda I_{n+m} + K_{\tilde{X}\tilde{X}})^{-1}$ is given by:
\begin{equation*}
  % \begin{pmatrix}
  %   \lambda I_n + K_{XX} & K_{XX^*} \\
  %   K_{X^*X} & \lambda I_m + K_{X^*X^*}
  % \end{pmatrix}^{-1} =
  \begin{pmatrix}
    Q_X + Q_X K_{XX^*} M^{-1} K_{X^*X} Q_X & - Q_X K_{XX^*} M^{-1} \\
    - M^{-1} K_{X^*X} Q_X & M^{-1}
  \end{pmatrix}
  \,,
\end{equation*}
where $Q_X = \bigl( \lambda I_n + K_{XX} \bigr)^{-1}$ and $M = \lambda I_m + K_{X^*X^*}
- K_{X^*X} Q_X K_{XX^*}$. Since $\lambda I_{n+m} + K_{\tilde{X}\tilde{X}}$ is an
invertible positive definite matrix, its inverse must be positive definite, and thus
$M$ must be positive definite and invertible as well.
If $H_{\tilde{X}} = K_{\tilde{X}\tilde{X}} Q_{\tilde{X}}$,
then $I_{n+m} - H_{\tilde{X}} = \lambda Q_{\tilde{X}}$, whence
\begin{align}
  y^* - \hat{y}^*_{|(\tilde{X}, \tilde{y})}
    &= \lambda \begin{pmatrix} 0\\ I_m \end{pmatrix} Q_{\tilde{X}}
        \begin{pmatrix} y\\ y^* \end{pmatrix} \nonumber \\
    % &= \lambda \begin{pmatrix} - M^{-1} K_{X^*X} Q_X & M^{-1} \end{pmatrix}
    %   \begin{pmatrix} y\\ y^* \end{pmatrix} \\
    &= \lambda M^{-1} \bigl( y^* - K_{X^*X} Q_X y \bigr) %\nonumber \\
    % &= \lambda M^{-1} \bigl( y^* - \hat{y}^*_{|(X, y), X^*} \bigr)
    \label{eq:holdout_resid}
    \,.
\end{align}

It is possible to show that the matrix $K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*}$ is also
PD. Indeed, one can take a sum of two zero-mean independent Gaussian Processes $f_x$
and $\epsilon_x$ with kernels $K(x,x')$ and $\lambda \delta_{x,x'}$, respectively
(see sec.~\ref{sub:bayesian_interpretation_of_krr}), which are equivalent in finite-
dimensional distributions to a zero-mean GP $y_x=f_x+\epsilon_x$ with combined kernel
$K(x,x') + \lambda \delta_{x,x'}$, and then derive a distribution of $(f_x)_{x\in X^*}$
conditional on the observed sample $(y_x)_{x\in X}$. This distribution is Gaussian
with mean $K_{X^*X}Q_X y_X$ and variance $K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*}$.

Therefore, we have that $M \succeq \lambda I_m$, whence $\lambda M^{-1} \preceq I_m$,
which means that the matrix $M^{-1}$ defines a non-expanding (in terms of the Euclidean
norm) linear transformation $\Real^{n\times 1} \mapsto \Real^{n\times 1}$. This shows
that the out-of-sample residuals are generally larger than the corresponding in-sample
ones.

In particular, eq.~\ref{eq:holdout_resid} enables fast computation of the leave-one-out
residuals of the KRR, without refitting all the way. The full sample residuals of a
fitted KRR are given by a vector $\hat{r}_{\text{in}}(X, y) = (I_n - K_{XX} Q_X) y$,
and the $i$-th, $i=1,\ldots, n$, leave-one-out residual the train sample is
\begin{equation}
  e_i' \hat{r}_{\text{loo}}(X, y)
  = y_i - K_{-i}(x_i)' Q_{-i} y_{-i} \,,
\end{equation}
where $K_{-i} = K_{X_{-i}}: \Xcal \mapsto \Real^{(n-1)\times1}$ is the canonical map
vector, and $Q_{-i}$ is given by $(\lambda I_{n-1} + K_{X_{-i}X_{-i}})^{-1}$.
Therefore, from eq.~\ref{eq:holdout_resid} it follows that
\begin{align}
  e_i' \hat{r}_{\text{in}}(X, y)
  % = \lambda e_i' P_{i:n} P_{i:n} Q_X P_{i:n} P_{i:n} y
  % = \lambda e_n'
  %   \begin{pmatrix}
  %     Q_{-i} + Q_{-i} K_{-i}(x_n) m_i^{-1} K_{-i}(x_n)' Q_{-i} & - Q_{-i} K_{-i}(x_n) m_i^{-1} \\
  %     - m_i^{-1} K_{-i}(x_n)' Q_{-i} & m_i^{-1} \\
  %   \end{pmatrix}
  %   \begin{pmatrix} y_{-i} \\ y_i \end{pmatrix} \\
  &= \lambda m_i^{-1} \bigl(y_i - K_{-i}(x_i)' Q_{-i} y_{-i} \bigr) \nonumber \\
  &= \lambda m_i^{-1} e_i' \hat{r}_{\text{loo}}(X, y) \label{eq:loo_resid} \,,
\end{align}
where $\lambda m_i^{-1}$ is the basis of the KRR analogue of the ``leverage'' score
of the $i$-th observation in least squares regression, with
\begin{equation} \label{eq:krr_leverage}
  m_i = \lambda + K(x_i, x_i) - K_{-i}(x_i)' Q_{-i} K_{-i}(x_i) \,.
\end{equation}
The main diagonal of the KRR hat matrix $H_X$ is given by $(1-\lambda m_i^{-1})_{i=1}^n$,
as can be easily seen by applying the block-inversion formula to the matrix $Q_X$
with symmetrically permuted rows and columns. This ``leverage'' depends only on the
input part, $X$, of the dataset and is always within $[0, 1]$, since the kernel $K$
is \textbf{PD} (see argument after eq.~\ref{eq:holdout_resid}).

% subsection kernel_ridge_regression (end)

\subsection{Bayesian interpretation} % (fold)
\label{sub:bayesian_interpretation_of_krr}

Kernel ridge regression as a problem of fitting some scattered data can be viewed
from the stochastic Gaussian process, or Kriging, perspective, \cite{rasmussen2006}.
As we will see in this subsection, this approach to KRR readily yields predictive
Bayesian confidence intervals, to which in the numerical study (sec.~\ref{sec:numerical_study})
the conformal confidence regions (sec.~\ref{sec:conformalized_krr}) will be compared
to.

A random process $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$, with mean $m : \Xcal
\mapsto \Real$ and \textbf{PD} function $\Kcal : \Xcal \times \Xcal \mapsto \Real$
is a Gaussian Process if all its finite dimensional distributions are multivariate
Gaussian. In particular, for any $n\geq1$ and any $X = (x_i)_{i=1}^n \in \Xcal$ the
vector $f_X = (f(x_i))_{i=1}^n \in \Real^{n\times 1}$ has Gaussian distribution with
mean $m_X = (m(x_i))_{i=1}^n$ and covariance matrix $K_{XX} = (\Kcal(x_i,x_j))_{ij}$ --
the $n\times n$ Gram matrix of $\Kcal(\cdot,\cdot)$ evaluated at points $(x_i)_{i=1}^n$:
\begin{equation*}
  f_X \sim \Ncal_n(m_X, K_{XX}) \,.
\end{equation*}

In this formulation the model \ref{eq:signal_model} represents the addition of an
independent white noise process $(\epsilon_x)_{x\in \Xcal}$ with variance $\lambda$
to $(f_x)_{x\in \Xcal} \sim GP(m(x), \Kcal(x,x'))$. Indeed, for any $n\geq1$ and
any $X = (x_i)_{i=1}^n \in \Xcal$ the finite dimensional distribution of $(y_x)_{x\in\Xcal}$
is $y_X\sim \Ncal_n(m_X, \lambda I_n + K_{XX})$, because
\begin{equation*}
  y_X
    = \begin{pmatrix} I_n & I_n \end{pmatrix}
    \begin{pmatrix} f_X \\ \epsilon_X \end{pmatrix}
    \,,
\end{equation*}
-- a linear combination of $f_X$ and $\epsilon_X$ with joint distribution
\begin{equation*}
  \begin{pmatrix} f \\ \epsilon \end{pmatrix}
    \sim \Ncal_{n+n}\begin{pmatrix}
        \begin{pmatrix} m \\ 0 \end{pmatrix},
        \begin{pmatrix}
          K_{XX} & 0 \\
          0 & \lambda I_n
        \end{pmatrix}
      \end{pmatrix}
    \,.
\end{equation*}
Therefore $(y_x)_{x\in\Xcal} \sim GP(m(x), \Kcal(x,x'))$ with $\Kcal$ given by a PD
kernel
\begin{equation*}
  \Kcal(x,x') = \lambda \delta_{x,x'} + K(x,x') \,,
\end{equation*}
where $\delta_{x,x'}$ is the delta-function on $\Xcal\times \Xcal$.

The conformal procedure constructs confidence intervals for test observations $y_x$,
contaminated by the additive noise $\epsilon_x$, as opposed to true values $f_x$ in
the model \ref{eq:signal_model} (see sec.~\ref{sec:conformal_prediction}). In the
Bayesian setting this means that it is necessary to get a conditional distribution
of observations in a test sample $y_{X^*} = (y_{x^*_j})_{j=1}^l$, with respect to
the train sample $y_X = (y_{x_i})_{i=1}^n$. Gaussianity makes it especially easy
to derive conditional distributions. Indeed, if
\begin{equation*}
  \begin{pmatrix}z_1 \\ z_2\end{pmatrix}
    \sim \Ncal_{d_1+d_2}\Biggl(
      \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix},
      \begin{pmatrix}
        \Sigma_{11} & \Sigma_{12} \\
        \Sigma_{21} & \Sigma_{22}
      \end{pmatrix}
    \Biggr)
    \,,
\end{equation*}
then the conditional distribution of $z_1$ given $z_2$ is Gaussian with
\begin{equation*}
  {z_1}_{|z_2}
    \sim \Ncal_{d_1}\bigl(
      \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2),
      \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
    \bigr)
    \,,
\end{equation*}
provided the inverse of $\Sigma_{22}$ exits. Therefore the conditional distribution
is given by
\begin{equation} \label{eq:cond_distr}
  y_{X^*}\vert_{y_X}
    \sim \Ncal_l\bigl(
      m_{X^*} + K_{X^*X} Q_X (y_X - m_X),
      \Sigma_K(X^*)
    \bigr)
    \,,
\end{equation}
where $\Sigma_K(X^*) = K_{X^*X^*} - K_{X^*X} Q_X K_{XX^*}$, and
$Q_X = \bigl(K_{XX}\bigr)^{-1}$, since $(y_x)_{x\in\Xcal}$ is a GP and
\begin{equation*}
  \begin{pmatrix} y_X \\ y_{X^*} \end{pmatrix}
    \sim \Ncal_{n+l}\begin{pmatrix}
      \begin{pmatrix} m_X \\ m_{X^*} \end{pmatrix},
      \begin{pmatrix}
        K_{XX} & K_{XX^*} \\
        K_{X^*X} & K_{X^*X^*}
      \end{pmatrix}
    \end{pmatrix}
    \,,
\end{equation*}
with $K_{XX^*} = (\Kcal(x_i, x^*_j))\in \Real^{n\times l}$.

The Bayesian prediction of $y_{X^*}$ conditional on observing $y_X$ is given by
the Maximum Posterior prediction (which in the Gaussian case coincides with the
true Bayesian prediction $\ex\bigl(y_{X^*}\,|\, y_X\bigr)$):
\begin{equation*}
  \hat{y}_{y_X}(X^*) = m_{X^*} + K_{X^*X} Q_X \bigl(y_X - m_X\bigr) \,.
\end{equation*}

Gaussian process regression (GPR), also known as ``Kriging'', posits that the observed
data is drawn from $GP(m(x), \Kcal(x, x'))$ with
\begin{equation} \label{eq:krig_kernel}
  \Kcal(x,x') = \sigma^2( \lambda \delta_{x, x'} + K(x, x') ) \,,
\end{equation}
for a PD kernel $K$, and $m(x) = \Phi(x)'\beta$, where $\Phi: \Xcal\mapsto \Real^{d\times 1}$
is the regression feature map. This model naturally enables adaptive estimation of
parameters of the kernel $K$ by maximizing the joint likelihood of the train data
$(X, y)$:
\begin{align} \label{eq:bkrr_likelihood}
  \Lcal
    &= -\frac{n}{2} \log 2\pi
    - \frac{n}{2}\log \sigma^2
    - \frac{1}{2}\log \lvert R_X \rvert \nonumber\\
    &- \frac{1}{2\sigma^2} (y - F\beta)' R_X^{-1} (y - F\beta)
    \,,
\end{align}
where $R_X = \lambda I_n + K_{XX}$, and $F \in\Real^{n\times d}$ is the design matrix
of $X$ under $\Phi$. The optimal $\beta$ is given by the generalized least squares
solution
\begin{equation*}
  \hat{\beta} = (F'R_X^{-1}F)^{-1} F'R_X^{-1} y \,.
\end{equation*}
Minimization of \ref{eq:bkrr_likelihood} by alternating between kernel parameter
and $\beta$ steps, makes it possible to gradually estimate the optimal parameters.

Another alternative is to select the parameters based on the data that has not been
used in fitting the conditional mean of the kernel ridge regression. One could use
leave-one-out cross-validation and minimize the RMSE of the deleted residuals
(eq.~\ref{eq:loo_resid}):
\begin{equation}
  \hat{\theta}
    = \mathop{\text{argmin}}_{\theta \in\Theta}
      n^{-1} \| \hat{r}_{\text{loo}}(X, y; \theta) \|^2
    \,,
\end{equation}
where the dependence on $\theta$ is through the sample kernel Gram matrix $K_{XX}$.
It is worth noting that the leave-one-out residuals are not affected by the scale
$\sigma^2$ of the kernel $\Kcal$ (eq.~\ref{eq:krig_kernel}). Similarly to the case
of predictive variances in \cite{pcw20005a7}, using leave-one-out cross-validation
eliminates the bias inherent in estimates of the kernel parameters.

Bayesian kernel ridge regression confidence interval for an observed values $y_{x^*}$
at $x^*\in \Xcal$ can be obtained from eq.~\ref{eq:cond_distr} by fixing $\beta = 0$.
Indeed, the distribution of $y_{x^*}$, conditional on the train data $y_X$, is
\begin{equation} \label{eq:gp_cond_dist}
{y_{x^*}}_{|y_X}
  \sim \Ncal\bigl( \hat{y}_{y_X}(x^*), \sigma^2 \sigma_K^2(x^*) \bigr) \,,
\end{equation}
with $\hat{y}_{y_X}(x^*) = K_X(x^*)' Q_X y_X$, and
\begin{equation*}
  \sigma_K^2(x^*)
    = \lambda + K(x^*, x^*) - K_X(x^*)' Q_X K_X(x^*) \,,
\end{equation*}
where $Q_X = \bigl(\lambda I_n + K_{XX}\bigr)^{-1}$, $K_{XX} = (K(x_i,x_j))_{ij}$,
and $K_X = (K(x_i, \cdot))_{i=1}^n: \Xcal \mapsto \Real^{n\times1}$. Thus, the $1 - \alpha$
confidence interval is thus given by
\begin{equation} \label{eq:gp_conf_int}
\Gamma^\alpha_{y_X}(x^*)
  = \hat{y}_{y_X}(x^*)
  + \sigma \sqrt{\sigma_K^2(x^*)}
  \times [z_{\frac{\alpha}{2}}, z_{1-\frac{\alpha}{2}}]
  \,,
\end{equation}
where $z_\gamma$ is the $\gamma$ quantile of $\Ncal(0, 1)$.

% subsection bayesian_interpretation_of_krr (end)

% section kernel_ridge_regression (end)

\section{Conformal prediction} % (fold)
\label{sec:conformal_prediction}

Conformal prediction is a distribution-free technique designed to yield a statistically
valid confidence sets for predictions made by machine learning algorithms. The key
advantage of the method is that it offers coverage probability guarantees under standard
IID assumptions, even in cases when assumptions of the underlying prediction algorithm
fail to be satisfied. The method was introduced in \cite{vovk2005} and is applicable
in both the supervised and unsupervised online learning settings. In the following
we consider supervised setting with $\Zcal$ denoting the object-target space $\Xcal \times \Ycal$.

The nature of the underlying ML predictive algorithm is irrelevant to conformal
prediction, as the core of the procedure is a measurable map $A: \Zcal^*\times \Zcal \mapsto \Real$,
a Non-Conformity Measure, NCM, which for a training sample $Z_{:n}=(z_i)_{i=1}^n\in\Zcal$
and a test object $z_* \in \Zcal$ quantifies how much different $z_*$ is relative
to the sample $Z_{:n}$.

A conformal predictor over the NCM $A$ is a procedure, that for every sample $Z_{:n}$,
a test object $x_{n+1} \in \Xcal$, and a confidence level $\alpha\in(0,1)$, computes
a confidence set $\Gamma_{Z+{:n}}^\alpha(x^*)$ for the target value $y_{n+1}$ corresponding
to $x_{n+1}$:
\begin{equation} \label{eq:conf_pred_set}
  \Gamma_{Z_{:n}}^\alpha(x_{n+1})
    = \bigl\{ y\in \Ycal \,:\, p_{Z_{:n}}(\tilde{z}^y_{n+1}) \geq \alpha \bigr\} \,,
\end{equation}
where $\tilde{z}^y_{n+1} = (x_{n+1}, y)$ a synthetic test observation with target
label $y$. The function $p:\Zcal^*\times (\Xcal\times \Ycal)\mapsto [0,1]$ is given
by
\begin{equation} \label{eq:conf_p_value}
  p_{Z_{:n}}(\tilde{z})
    = (n+1)^{-1} \bigl\lvert\{ i \,:\,
      \eta_i^{\tilde{z}} \geq \eta_{n+1}^{\tilde{z}} \}\bigr\rvert \,,
\end{equation}
where $i=1,\ldots, n+1$, and $\eta_i^{\tilde{z}} = A(S^{\tilde{z}}_{-i}, S^{\tilde{z}}_i)$
-- the degree of non-conformity of the $i$-th observation with respect to the augmented
sample $S^{\tilde{z}} = (Z_{:n}, {\tilde{z}}^y_{n+1}) \in \Zcal^{n+1}$. For any $i$,
$S^{\tilde{z}}_i$ is the $i$-th element of the sample, and $S^{\tilde{z}}_{-i}$ is
the sample with the $i$-th observation omitted. Intuitively, the p-value (eq.~\ref{eq:conf_p_value})
measures the likelihood of $\tilde{z}$ based on its non-conformity, or with $Z_{:n}$.

A distribution $P$ on $\Zcal^n$ is called \textbf{exchangeable} if for any permutation
$\sigma$ of ${1,\ldots,n}$ for any measurable $B\subseteq \Zcal^n$
\begin{equation} \label{eq:exchangeability}
  \pr\bigl(\{z\in\Zcal^n \,:\, (z_i)_{i=1}^n\in B\}\bigr)
  = \pr\bigl(\{z\in\Zcal^n \,:\, (z_{\sigma(i)})_{i=1}^n \in B\} \bigr)\,.
\end{equation}
Any product-measure on $\Zcal^n$ is exchangeable, and therefore exchangeability
generalizes independence. A distribution $P$ on $\Zcal^\infty$ is exchangeable
is all its finite distributions are exchangeable.

In \cite{vovk2005}, chapter 2, is has been shown, that if a sequence of examples
$(z_n)_{n \geq1}$ is generated by an exchangeable distribution $P$ on $\Zcal^\infty$,
then the coverage probability of the prediction set $\Gamma^\alpha$, yielded by
the procedure \ref{eq:conf_pred_set} is at least $1-\alpha$ and successive errors
are independent in online learning and prediction setting. Thus the outlined procedure
guarantees unconditional validity of the confidence region (eq.~\ref{eq:conf_pred_set}):
for any $\alpha \in (0,1)$, for any $n\geq1$ and any exchangeable distribution
$P_n$ on $\Zcal^n$ one has
\begin{equation} \label{eq:conservative_coverage}
  \pr_{Z_{:n}\sim P_n} \bigl(
    y_n \notin \Gamma^\alpha_{Z_{:(n-1)}}(x_n)
  \bigr) \leq \alpha \,,
\end{equation} 
where $(x_n, y_n) = z_n$. In a special case, when $z_n$ are independent the measure
$P_n$ is just the product measure $P_{z_1} \otimes \ldots \otimes P_{z_n}$.

Intuitively, the event $y_n \notin \Gamma^\alpha_{Z_{:(n-1)}}(x_n)$ is equivalent
to $\eta_n$ being among the largest $\lfloor n\alpha\rfloor$ values of $\eta_i = A(Z_{-i}, Z_i)$,
which is equal to $\frac{\lfloor n\alpha\rfloor}{n}$, due to exchangeability of
$Z_{:n}$ (this heuristic argument assumes that all $\eta_i$ are distinct and that
probability under $P_{:n}$ of any $(\eta_i)_{i=1}^n$ is positive, for a rigorous
proof see \cite{vovk2005}, chapter 8).

In general, any real-valued jointly measurable \textbf{NCM} could be used, the only
difference will be in the size of the predicted confidence set (efficiency) and the
computational complexity of the conformal procedure. In the general case, despite
theoretical guarantees, computing eq.~\ref{eq:conf_pred_set} requires exhaustive
search through the target space $\Ycal$. This complexity issue is not acute in typical
classification settings, when $\Ycal$ is finite, but is infeasible in regression
settings when $\Ycal$ is $\Real$. Furthermore, the inner eq.~\ref{eq:conf_p_value}
requires looping over all observations in the augmented sample $S^{\tilde{z}}$.
In regression setting for specific non-conformity measures it is possible to come
up with an efficient procedure for computing the confidence region (\ref{eq:conf_pred_set}),
as demonstrated in \cite{vovk2005}, chapter 2, and sec.~\ref{sec:conformalized_krr}.

It should be note, that a conformal prediction procedure can also be constructed
based on ``conformity measures'', as opposed to ``non-conformity measures''. As
the name suggests conformity measures quantify the similarity of a test object to
the train sample. Both approaches yield equivalent guarantees and are almost identically
constructed: the ``$\geq$'' sign must be switched to ``$\leq$'' in eq.~\ref{eq:conf_p_value}
if a conformity measure is used.

% section conformal_prediction (end)

\section{Conformalized kernel ridge regression} % (fold)
\label{sec:conformalized_krr}

In this section we describe the construction of confidence regions of the conformal
procedure eq.~\ref{eq:conf_pred_set} for the case of the non-conformity measures
based on kernel ridge regression. We begin by describing the result in \cite{burnaevV14}
concerning the asymptotic efficiency of conformal prediction confidence regions
for the ordinary ridge regression.

\subsection{Confromalized ridge regression} % (fold)
\label{sub:conformalized_ridge_regression}

Consider a sample $Z = (x_i, y_i)_{i=1}^n \in \Zcal$, with $\Xcal = \Real^{d\times 1}$
and $\Ycal=\Real$. Let $X\in\Real^{n\times d}$ be the overall design matrix of $Z$
and $y\in\Real^{n\times1}$ -- the target vector of the sample. Bayesian Ridge Regression
(BRR) is a particular case of the Gaussian Process Regression with the kernel being
the inner product in $\Real^{d\times1}$:
\begin{equation*}
  \Kcal(x,x') = \sigma^2(\lambda \delta_{x,x'} + \langle x,x'\rangle) \,.
\end{equation*}
Thus BRR assumes that $y = x'\beta + \epsilon_x$ for any $x\in\Xcal$ as well as
the following:
\begin{enumerate}
  \item $\beta\sim\Ncal_d(0, \sigma^2 \lambda^{-1} I_p)$;
  \item $\epsilon_x\sim \Ncal(0, \sigma^2)$ and iid for all $x\in \Real^{d\times 1}$;
  \item $\beta$ and $\epsilon_x$ are independent.
\end{enumerate}
Assuming a known $\sigma^2$, equation~\ref{eq:gp_cond_dist} implies that the distribution
of $y^*$ at $x^*$ conditional on the observed sample is
\begin{align} \label{eq:brr_cond_dist}
  {y_{x^*}}_{|y_X}
    \sim \Ncal\bigl(&{x^*}'(\lambda I_p + X'X)^{-1}X'y, \nonumber\\
      &\sigma^2 (\lambda + {x^*}' (\lambda I_p + X'X)^{-1} x^* ) \bigr)
    \,.
\end{align}

The properties of ``Conformalized Ridge Regression'' (CRR) have been studied in \cite{burnaevV14},
where it has been shown that under more general assumptions (the Gaussianity of $\beta$
is relaxed), the conformal confidence region, eq.~\ref{eq:conf_pred_set}, constructed
with an NCM based on the ridge regression residuals is asymptotically efficient.

The conformal procedure in \cite{burnaevV14} is constructed from a conformity measure
\begin{align} \label{eq:crr_ncm}
  \eta_i &= A(Z_{-i}, Z_i) \nonumber \\
    &= \bigl\lvert\{j\,:\, \hat{r}_j \geq \hat{r}_i \} \bigr\rvert \wedge
       \bigl\lvert\{j\,:\, \hat{r}_j \leq \hat{r}_i \} \bigr\rvert \,,
\end{align}
where $\hat{r} = \hat{r}_Z$ are the in-sample ridge regression residuals given by
\begin{equation} \label{eq:rr_insample_resid}
  \hat{r}_Z = (I_n - X (\lambda I_p + X'X)^{-1} X') y \,,
\end{equation}

In the paper it was also shown that for any $\alpha\in(0,1)$ the confidence
region $\Gamma^\alpha$ produced by CRR procedure for the conformity measure in
eq.~\ref{eq:crr_ncm} is equivalent to the intersection of confidence sets yielded
by conformal procedures with non-conformity measures given by $\eta_i = \hat{r}_i$
and $\eta_i = -\hat{r}_i$ at significance levels $\frac{\alpha}{2}$. Individually,
these NCMs define a \textbf{upper} and \textbf{lower} CRR sets respectively, and
together constitute a ``two-sided'' conformal procedure.

The main result of \cite{burnaevV14} states that under relaxed BRR assumptions if
a sequence $(x_n)_{n\geq1}\in\Xcal$ is i.i.d. with an existing and non-singular second
moment matrix $\ex x_1x_1' \succeq 0$, then for all sufficiently large $n$ the CRR
produces confidence regions that are intervals, and the difference between the upper
and lower ends of the Bayesian confidence interval and the conformal confidence
intervals is asymptotically Gaussian with zero mean. For the ridge regression this
means that when Gaussianity is valid little efficiency is lost if conformal confidence
intervals are used instead of Bayesian: the upper endpoint of the prediction interval
deviates asymptotically as $\mathcal{O}_p\bigl(n^{-\frac{1}{2}}\bigr)$ (the discrepancy
times $\sqrt{n}$ is stochastically bounded), where $n$ is the number of observations
in the train sample.

Since ridge regression has a dual form solution, it is possible to apply the kernel
trick to it to get a more powerful ML algorithm. The posterior distribution for the
GPR confidence intervals has been derived in sec.~\ref{sub:bayesian_interpretation_of_krr},
so in the next section the construction of conformal prediction procedures will be
discussed. Two methods in particular: in sec.~\ref{sub:ridge_regression_confidence_machine}
we consider the original ridge regression approach from \cite{vovk2005}, and in
sec.~\ref{sub:kernel_crr} a ``two-sided'' version, which is more amenable to theoretical
analysis, \cite{burnaevV14}.

% subsection conformalized_ridge_regression (end)

\subsection{Ridge Regression Confidence Machine} % (fold)
\label{sub:ridge_regression_confidence_machine}

In this and the following sections we derive the formulae required for conformal
confidence prediction based on the KRR non-conformity measure, and provide details
of a computationally efficient procedure, that actually builds confidence regions
for applications.

We consider two versions of the NCM for the Ridge Regression Confidence Machine
(RRCM),proposed in \cite{vovk2005}, chapter 2, for the regression setting: an in-sample
and a \textbf{l}eave-\textbf{o}ne-\textbf{o}ut version. For this purpose consider
a sample $(X, y) = (x_i, y_{x_i})_{i=1}^n$, and for any $i=1\ldots, n$ put $X =
(X_{-i}, x_i)$, and $y = (y_{-i}, y_i)$. Let $e_i\in \Real^{n\times 1}$ be the $i$-th
unit vector.

The ``in-sample'' NCM $A_{\text{in}}$ measures the absolute value of the regression
residual and is given by:
\begin{align}
  A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    &= |y_i - \hat{y}_{|(X, y)}(x_i)| \nonumber\\
    &= |e_i' \hat{r}_{\text{in}}(X, y)| \label{eq:ins_ncm}
    \,,
\end{align}
where $\hat{y}_{|(X, y)}(x_i)$ is the residual of the regression fit on the complete
dataset $(X, y)$. The ``loo'' NCM is defined similarly, but uses ``loo''-residuals
for the task:
\begin{align*}
  A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    &= |y_i - \hat{y}_{|(X_{-i}, y_{-i})}(x_i)| \\
    &= |e_i' \hat{r}_{\text{loo}}(X, y)|
    \,.
\end{align*}
Note that both are interrelated using the formula:
\begin{multline*}
  A_{\text{loo}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    \\ = \lambda^{-1} \diag(Q_X)^{-1}
    A_{\text{in}}\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)
    \,,
\end{multline*}
based on eq.~\ref{eq:loo_resid}.

For the NCM $A$ the $1-\alpha$ conformal confidence interval for the $n$-th observation
is given by
\begin{equation} \label{eq:conf_ci}
\Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
  = \bigl\{ z\in \Real \,:\, p_n\bigl((X, \tilde{y}_n^z)\bigr) \geq \alpha \bigr\}
  \,,
\end{equation}
where $\tilde{y}_j^z = (y_{-j}, z)$ -- the augmented sample $y$, with the $j$-th
value replaced by $z$. The ``conformal likelihood'' of the $j$-th observation in
some sample $(X, y)$ is given by
\begin{equation*}
  p_j\bigl((X, y)\bigr)
    = n^{-1} \bigl\lvert \bigl\{
        i = 1,\ldots, n \, : \,
        \eta_i \geq \eta_j
    \bigr\} \bigr\rvert
    \,,
\end{equation*}
for $\eta_i = A\bigl((X_{-i}, y_{-i}), (x_i, y_i)\bigr)$.
Thus it is necessary to obtain a formula, describing dependency of the residuals on
the augmentation of the $n$-th observation. The in-sample KRR residuals of the augmented
dataset $(X, \tilde{y}_n^z)$ are given by
\begin{align}
    \hat{r}_{\text{in}}(X, \tilde{y}_n^z)
    % &= \lambda Q_X \tilde{y}_n^z \nonumber \\
    % &= \lambda
    %   \begin{pmatrix}
    %     Q_{-n} + Q_{-n} K_{-n}(x_n) m_n^{-1} K_{-n}(x_n)' Q_{-n} & - Q_{-n} K_{-n}(x_n) m_n^{-1} \\
    %     - m_n^{-1} K_{-n}(x_n)' Q_{-n} & m_n^{-1} \\
    %   \end{pmatrix}
    %   \begin{pmatrix} y_{-n} \\ z \end{pmatrix} \nonumber \\
    % &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix}
    %  - \lambda \begin{pmatrix} Q_{-n} K_{-n}(x_n) \\ -1 \end{pmatrix}
    %    m_n^{-1} \bigl(z - K_{-n}(x_n)' Q_{-n} y_{-n} \bigr) \nonumber \\
    &= \lambda \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix} \nonumber \\
    &- \lambda B_{-n}(x_n) K_{-n}(x_n)' Q_{-n} y_{-n} \nonumber \\
    &+ \lambda B_{-n}(x_n) z \label{eq:krr_in_resid} \,,
\end{align}
Where the vector $B_{-n}(x_n)\in\Real^{n\times 1}$ is given by
\begin{equation} \label{eq:krr_in_resid_B}
  B_{-n}(x_n)
    = \begin{pmatrix} - Q_{-n} K_{-n}(x_n) \\ 1 \end{pmatrix} m_n^{-1}
    \,.
\end{equation}

In general, the construction of the confidence region (\ref{eq:conf_ci}) requires
searching through all $y\in \Ycal$ that satisfy the significance level requirement,
which is infeasible unless $\Ycal$ is finite. The construction for the NCM (\ref{eq:ins_ncm})
for in-sample residuals in this particular problem relies on the representation
of each residual as
\begin{equation*}
  \hat{r}_i^z
    = e_i' \hat{r}_{\text{in}}(X, \tilde{y}_n^z)
    = \lambda c_i + \lambda b_i z
    \,,
\end{equation*}
with $c_i = e_i' C_{-n}\bigl((X, y), x_n\bigr)$ and
\begin{align*}
  C_{-n}\bigl((X, y), x_n\bigr)
    &= \begin{pmatrix} Q_{-n} y_{-n} \\ 0 \end{pmatrix} \\
    &- B_{-n}(x_n) K_{-n}(x_n)' Q_{-n} y_{-n}
    \,.
\end{align*}

Since absolute values of the residuals are compared, it is possible to consistently
manipulate the signs of each entry in $C$ and $B$ to ensure that $e_i'B\geq 0$ for
all $i$. The regions $S_i = \{z\in\Real\,:\, |\hat{r}_i^z| \geq |\hat{r}_n^z|\}$, for
$i=1,\ldots, n$, are either closed intervals, complements of open intervals,
one-side closed half-rays in $\Real$, depending on the values of $C$ and $B$. In
particular, with $p_i$ and $q_i$ denoting the values $-\frac{c_i+c_n}{b_i+b_n}$ and
$\frac{c_i-c_n}{b_n-b_i}$, respectively (whenever each is defined), each region
$S_i$ has one of the representations:
\begin{enumerate}
%% a picture of shifted and scaled x->|x| helps in derivation of this.
  \item $b_i=b_n=0$: $S_i = \Real$ if $|c_i| \geq |c_n|$, or $S_i = \emptyset$
  otherwise;
  \item $b_n = b_i > 0$: $S_i$ is either $(-\infty, p_i]$ if $c_i < c_n$, $[p_i, +\infty)$ if
  $c_i > c_n$, or $\Real$ otherwise;
  \item $b_n > b_i \geq 0$: $S_i$ is either $[p_i, q_i]$ if $c_i b_n \geq c_n b_i$,
  or $[q_i, p_i]$ otherwise;
  \item $b_i > b_n \geq 0$: $S_i$ is $\Real\setminus (q_i, p_i)$ when $c_i b_n \geq c_n b_i$,
  or $\Real\setminus (p_i, q_i)$ otherwise.
\end{enumerate}
Let $P$ and $Q$ be the sets of all well-defined $p_i$ and $q_i$ respectively, and
let $(g_j)_{j=0}^{J+1}$ enumerate distinct values of $\{\pm\infty\} \cup P \cup Q$,
so that $g_j < g_{j+1}$ for all $j$. Then the confidence region is a closed subset
of $\Real$ constructed from sets $G^m_j = [g_j, g_{j+m}]\cap \Real$ for $m=0, 1$:
\begin{equation} \label{eq:rrcm_conf_ci}
  \Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
    = \bigcup_{m\in\{0,1\}} \bigcup_{j\,:\, N^m_j \geq n \alpha} G^m_j
    \,,
\end{equation}
where $N^m_j = |\{i \,:\, G^m_j \subseteq S_i\}|$ is the coverage frequency of $G^m_j$.
In general, the resulting confidence set might contain isolated singletons $G^0_j$.

This set, can be constructed efficiently in $\BigO(n \log{} n)$ time with $\BigO(n)$
memory footprint. Indeed, it is necessary to sort at most $J\leq 2n$ distinct endpoints
of $G_j$, then locate the values $p_i$ and $q_i$ associated with each region $S_i$
($\BigO(n \log{} n)$). Then, since the building blocks $G^m_j$ of $\Gamma^\alpha$
are either singletons ($m=0$), or intervals made up from adjacent singletons ($m=1$),
coverage numbers $N^m_j$ can be computed in at most $\BigO(n)$ time.

% subsection ridge_regression_confidence_machine (end)

\subsection{Kernel two-sided confidence predictor} % (fold)
\label{sub:kernel_crr}
Another possibility is to use the two-sided procedure proposed in \cite{burnaevV14}
and mentioned in sec.~\ref{sub:conformalized_ridge_regression}. Recall that, in order
to construct a CRR confidence region for significance level $\alpha$ (eq.~\ref{eq:crr_ncm}),
it is necessary to intersect confidence sets yielded by conformal procedures with NCM
scores given by $\hat{r}_i^z$ (upper) and $-\hat{r}_i^z$ (lower) at significance level
$\frac{\alpha}{2}$. Confidence regions based on CRR, much like RRCM, can use any kind
of residual: leave-one-out, or in-sample. In this section we present an efficient
algorithm for constructing this confidence region.

For the upper CRR consider the regions $U_i = \{z\in\Real\,:\, \hat{r}_i^z \geq \hat{r}_n^z\}$,
$i=1,\ldots, n$ are either empty, full $\Real$ or one-side closed half-rays. Since
$\hat{r}_i^z = \lambda c_i + \lambda b_i z$, $U_i$ takes one of the following forms:
\begin{enumerate}
  \item $b_i=b_n$: $U_i = \Real$ if $c_i\geq c_n$, and $\emptyset$ otherwise;
  \item $b_i\neq b_n$: $U_i = [q_i, +\infty)$ if $b_i>b_n$, or
  $U_i = (-\infty, q_i]$ otherwise;
\end{enumerate}
with $q_i = \frac{c_i-c_n}{b_n-b_i}$. The forms of regions $L_i$ for the lower CRR
are computed similarly, but with the signs of $c_i$ and $b_i$ flipped for each $i=1, \ldots, n$.

Both upper and lower confidence regions are built similarly to the kernel RRCM region
eq.~\ref{eq:rrcm_conf_ci} in sec.~\ref{sub:ridge_regression_confidence_machine}. The
final the Kernel CRR confidence set is:
\begin{equation} \label{eq:crr_conf_ci}
  \Gamma_{X_{-n}, y_{-n}}^\alpha(x_n)
    = \Gamma_{X_{-n}, y_{-n}}^{\alpha,\text{u}}(x_n)
    \cap \Gamma_{X_{-n}, y_{-n}}^{\alpha,\text{l}}(x_n)
    \,.
\end{equation}
This intersection can be computed efficiently in $\BigO(n \log{} n)$, since the regions
are built form sets anchored at a finite set $Q$ with at most $n+2$ values. Therefore,
the CRR confidence set for a fixed significance level $\alpha$ has $\BigO(n\log{} n)$
complexity.

% subsection kernel_crr (end)

% section conformalized_krr (end)

\section{Numerical study} % (fold)
\label{sec:numerical_study}

Validity of conformal predictors in the online learning setting has been shown in
\cite{vovk2005}, chapter 2, however, no result of this kind is known in the batch
learning setting. Our experiments aim to evaluate the empirical performance of the
conformal prediction in this setting: with dedicated train and test datasets. In
this section we conduct a set of experiments to examine the validity of the regions,
produced by the conformal Kernel Ridge Regression and compare its efficiency to
the Bayesian confidence intervals.

We primarily test the conformalized KRR predictions in cases when the input space
$\Xcal$ is a compact set in $\Real^{d\times 1}$ for either $d=1$ or $d=2$. The rationale
for this is that since conformal procedure is oblivious to the structure of the
input dataset, there is no reason for its validity to deteriorate with increasing
dimensionality of $\Xcal$. Indeed, the conformal confidence region eq.~\ref{eq:conf_pred_set}
the input data from $\Xcal$ in the training and the test sets are fed into the NCM
$A$, which, in general, can be an arbitrary computable function, and never ``leak''
into the procedure itself. The dimensionality of the input data, however, may impact
the efficiency (the width) of the resulting confidence region. And our experiments,
to a certain extent, assess their effect.

We consider the isotropic Gaussian kernel for both the Conformal Kernel ridge regression
and the Gaussian Process Regression:
\begin{equation} \label{eq:gauss_kenrel}
  K(x,x')
  = \mathop{\text{exp}}\bigl\{-\theta \|x - x'\|^2\bigr\}
  \,,
\end{equation}
where $\theta>0$ is the precision parameter. This kernel is widely used in practice,
because it has very nice properties. In particular, if $\Xcal$ is compact its canonical
RKHS is universal: $\Hcal_K$ is dense in the set $C_0(\Xcal)$ of continuous bounded
maps with respect to the uniform norm $\|\cdot\|_\infty$, see.~\cite{steinwart2002}.

% Another example of a universal kernel is the Laplacian kernel 
% \begin{equation*}
%   K(x,x')
%   = \mathop{\text{exp}}\bigl\{-\theta \|x - x'\|_1\bigr\}
%   \,,
% \end{equation*}
% where $\|\cdot\|_1$ is the $L_1$ norm on $\Xcal$. In contrast to the Gaussian, the Laplacian
% kernel 
% Its spectral density is the Cauchy
% density, as opposed to the Gaussian density in the case of Gaussian kernel. The

\subsection{Setup} % (fold)
\label{sub:setup}

The off-line validity and efficiency is studied in two settings: the fully Gaussian
case, and the non-Gaussian cases. In the first case, we study the validity and
efficiency of GPR and conformal confidence regions on a sample path of a Gaussian
process on $\Xcal$. We focus consider $4$ alternatives, depending on whether the
parameter $\theta$ or $\lambda$ of the KRR are equal, or not, to their counterparts
in the kernel of the synthesized Gaussian process.

In the second setting, we concentrate on deliberately non-Gaussian test functions,
and examine the effects of moderate Gaussian additive noise on the performance of
both Bayesian and Conformal confidence regions. In this setting, the assumptions
of the confidence regions of Gaussian Process Regression are valid to a certain
extent: a deterministic function is contaminated by moderate Gaussian noise. We
explicitly examine two cases: whether the regularization in the kernel ridge regression
is equal to the theoretical level of the noise in the data, or not.

Below is a list of working conjectures we aim to find empirical evidence for: \begin{itemize}
  \item conformal confidence regions for different NCMs and residuals are at least
  asymptotically equivalent tin terms of coverage and efficiency;
  \item the Bayesian confidence regions perform gradually worse the further the data
  is from the GPR assumptions;
  \item in the fully Gaussian case both Bayesian and conformal confidence intervals
  possess the asymptotic validity guarantees, and the conformal procedure yields
  asymptotically efficient (close to the Bayesian) regions;
  \item the asymptotic validity (at least conservative) of the conformal confidence
  set is expected to hold in all cases.
\end{itemize}

We study the effects of the kernel parameters and the choice of the conformal procedure
on the validity and efficiency of the confidence regions by varying the following
hyper-parameters: \begin{enumerate}
  \item the magnitude of the Gaussian noise-to-signal ratio in the data: negligible
  $\gamma = 10^{-6}$ and moderate $\gamma = 10^{-1}$;
  \item the size $n$ of the train sample: small ($n \in \{ 25, 50, 100\}$),
  moderate, and large ($n \in \{200 k\,:\, k=1, \ldots, 8 \}$);
  \item the NCM $A$ for the confidence region: either the RRCM, or the CRR;
  \item the type of residual $\hat{r}$: either $\hat{r}_{\text{in}}$, or $\hat{r}_{\text{loo}}$;
  \item the noise-to-signal (regularization) ratio $\lambda$ in the kernel ridge
  regression is chosen from two regimes: high $\lambda = 10^{-1}$ (smoothing), and
  low $\lambda=10^{-6}$ (interpolation);
  \item the precision of the Gaussian kernel $\theta$ is picked from
  $\{10, 10^2, 10^3\}$, since the input domain is the unit cube, for which
  $\|x-x'\| \leq \sqrt{d}$ for all $x,x'\in[0,1]^d$;
  \item the fixed $\theta$ or its MLE estimate ($\theta$ that minimizes \ref{eq:bkrr_likelihood});
\end{enumerate}
For a given test function $f:\Xcal \mapsto \Real$ on some compact domain $\Xcal$
each experiment for $n, \theta, \lambda, A$ and $\hat{r}$ consists of the following
steps:
\begin{enumerate}
  \item The test inputs, $X^*$, are given by a grid with constant spacing in $\Xcal$;
  \item Train inputs, $X$, are sampled from a uniform distribution over $\Xcal$;
  \item For all $x\in X_{\text{pool}} = X \cup X^*$, draw target values $y_x = f(x)$
  from the data generation process;
  \item let $\tilde{T} = (x, y_x)_{x\in X}$;
  \item perform $L$ independent replications of the following steps: for $l=1,\ldots, L$
  \begin{enumerate}
    \item draw an independent random sample of size $n$ without replacement from $\tilde{T}$;
    \item fit a Gaussian Process regression with $\beta = 0$ and $\Kcal$ given by
    eq.~\ref{eq:krig_kernel} for the Gaussian kernel $K$ with the specified precision
    $\theta$ with fixed $\lambda > 0$;
    \item for each $x^* \in X^*$ construct, the Bayesian KRR region, $\Bcal_l^\alpha(x^*)$,
    and the conformal confidence regions, $\Gamma_l^\alpha(x^*; A, \hat{r})$ using
    the specified NCM $A$, the residuals $\hat{r}$, and the kernel $\Kcal$ (with
    estimated $\sigma^2$);
    \item estimate the coverage rate and the width of the convex hull of the region
    over the test sample $X^*$: $p_l(R) = |X^*|^{-1}\sum_{x\in X^*} 1_{y_x\in R_x}$,
    and $w_l(R) = \inf\{b-a\,:\,R \subseteq [a, b]\}$, where $R$ is a confidence
    region;
  \end{enumerate}
\end{enumerate}
For the $1$-D case the domain $\Xcal$ is $[0,1]$ and the test input set is given by
$\{k N^{-1}\,:\,k=0,\ldots, N+1\}$ for $N=1000$. In $2$-D case we consider functions
defined on $\Xcal=[-1,1]^2$, and use $X^* = \{k N^{-1}\,:\,k=0,\ldots, N+1\}^2$
for $N=50$ as the test set.

To measure the quality of fit of the kernel ridge regression for each KRR set of
hyper-parameters, we compute the ratio of the root mean squared error to the standard
derivation on the test set (``RMSE-std'' on the plot the results section). This
reflects how well the KRR predictions fares against the constant prediction.

%% What is the estimate of \sigma^2

% subsection setup (end)

\subsection{Results: $1$-d} % (fold)
\label{sub:results_1_d}

We begin with the examination of the fully-Gaussian setup with $\Xcal = [0, 1]$.
To illustrate the constructed confidence regions, we generated a sample path of
the $1$-d Gaussian process with $y_x\sim GP(0, \gamma \delta_{x,x'} + K_{x,x'})$
for the Gaussian kernel (eq.~\ref{eq:gauss_kenrel}) on a regular grid $G=\{500^{-1} k \,:\, k=0,
\ldots, 500\}$, and then use its sub-grid of size $51$ with stepping $0.9\cdot 20^{-1}$
in $[0.05, 0.95]$ for training the Kernel Ridge regression and constructing the
confidence regions. The training sample was deliberately chosen to highlight the
blowup of the size of the confidence bands near the boundaries of the train sample.
Figure~\ref{fig:gauss_1d_prof_gpr} illustrates one realization of $y_x$ and the
constructed Bayesian confidence regions (eq.~\ref{eq:gp_conf_int}). Here the abbreviation
``GPR-p'' stands for a confidence region for the true value $f_x$ in eq.~\ref{eq:signal_model},
and ``GPR-f'' -- for the observed value. Recall that the non-parametric vanilla
conformal procedures yield predictions for the observed values $y_x$ only. That
is why in the following we are going to focus on the ``GPR-f'' confidence intervals,
and the ``GPR-p'' band is presented for illustrative purposes only.

On the provided illustration (fig.~\ref{fig:gauss_1d_prof_gpr}) the confidence bands
on are too wide in the negligible noise case, whereas the bands in seem to have better
coverage rate due to higher noise. Figure~\ref{fig:gauss_1d_prof_conf} shows sample
conformal confidence bands in the same setting. Near the endpoints of the $[0,1]$,
the confidence regions dramatically increase their size, reflecting increasing uncertainty.

In can be argued theoretically, that confidence regions for any observation $x_n$
sufficiently far away from the bulk of the training dataset have constant size,
determined only by the train sample (fig.~\ref{fig:limit_1d_ci_size}). Indeed, as
$\|x_n\|^2\to \infty$ ($n$-fixed) the vector $B_{-n}$ in (eq.~\ref{eq:krr_in_resid_B})
approaches the $n$-th unit vector $e_n$, since for the Gaussian kernel the vector
$\|K_{-n}(x_n)\|^2\to 0$. Since the kernel is bounded, the value $m_n$ (eq.~\ref{eq:krr_leverage})
is a bounded function of $x_n$, which, in turn, implies that eventually all RRCM
regions $S_i$ assume the form of closed intervals $[-|q_i|, |q_i|]$, where
$q_i = m_n (e_i'Q_{-n}y_{-n}) + o(\|x_n\|^2)$, $i \neq n$. The same is true for
the CRR procedure. Therefore, the conformal procedure essentially reverts to a
constant-size confidence region, determined by the  $n^{-1}\lfloor n(1-\alpha)\rfloor$-th
order statistic of $(|q_i|)_{i=1}^n$. Analogous effects can be observed for the
Gaussian Process confidence interval (eq.~\ref{eq:gp_conf_int}).
% Is this true for any stationary kernel on $\Real^{d\times 1})$? Bochner's theorem, maybe?
\begin{figure}%[t, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/0.1_0.1/50/profile_gaussian_0,1_0,1_100_5p-GPR_50.pdf}
  \end{subfigure}%~
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/1e-06_0.1/50/profile_gaussian_1e-06_0,1_100_5p-GPR_50.pdf}
  \end{subfigure}%
  \caption{A sample path of a $1$-d Gaussian Process on $\Xcal$ ($y_x$,  $\hat{y}_x$),
  and the forecast and prediction confidence bands (GPR-f and GPR-p, respectively).
  \textit{Left:} a sample path with $\gamma=10^{-1}$; \textit{right:} $\gamma=10^{-6}$.}
  \label{fig:gauss_1d_prof_gpr}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/0.1_0.1/50/profile_gaussian_0,1_0,1_100_5p-RRCM_50.pdf}
  \end{subfigure}%~
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/1e-06_0.1/50/profile_gaussian_1e-06_0,1_100_5p-RRCM_50.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/0.1_1e-06/200/profile_gaussian_0,1_1e-06_100_5p-RRCM_200.pdf}
  \end{subfigure}%~
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussian/1e-06_1e-06/200/profile_gaussian_1e-06_1e-06_100_5p-RRCM_200.pdf}
  \end{subfigure}%
  \caption{Sample RRCM confidence bands (eq.~\ref{eq:rrcm_conf_ci}).}
  \label{fig:gauss_1d_prof_conf}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussiantest/1e-06_0.1/200/profile_gaussian_1e-06_0,1_100_1p-GPR_200.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussiantest/1e-06_0.1/200/profile_gaussian_1e-06_0,1_100_1p-RRCM_200.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussiantest/0.1_0.1/200/profile_gaussian_0,1_0,1_100_1p-GPR_200.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/gaussiantest/0.1_0.1/200/profile_gaussian_0,1_0,1_100_1p-RRCM_200.pdf}
  \end{subfigure}
  \caption{Limiting behaviour of GPR (\textit{left}) and RRCM (\textit{right})
  confidence regions for a sample path of a Gaussian process with negligible noise
  (\textit{top}, $\gamma=10^{-6}$) and high noise-to-signal level (\textit{bottom},
  $\gamma=10^{-1}$).}
  \label{fig:limit_1d_ci_size}
\end{figure}

Now, let's consider the experimental setting with negligible noise in the observed
processes, disregarding for a while the fact that in the noiseless case the necessity
of confidence measures of predictions seems questionable.

The first important observation is that conformal procedures (eq.\ref{eq:conf_pred_set})
can allow for some uncertainty by construction. Indeed, for a sample $(X, y)$, the
NCMs based on KRR (eq.~\ref{eq:krr_in_resid} and eq.~\ref{eq:loo_resid}) and continuous
functions of $y_n$. Therefore the vector of non-conformity scores $(\eta^z_i)_{i=1}^n$
also depends continuously on $y_n$, which mean that for small perturbations of $y_n$
the relative ordering of $\eta^z_i$ remains the same. Hence, in the case of perfectly
noiseless observations, the confidence regions have positive width, thereby having
necessarily higher coverage rate. The GPR based confidence regions are subject to
the same effect in the noiseless case but for another reason: the predictive variance
includes the noise-to-signal ratio.

This conservativeness is indeed confirmed by the experimental results (see fig.~
\ref{fig:gaussian_1d_low_noise}). Within each column the confidence regions were
computed under exactly identical conditions: for each hyper-parameter setting (excluding
$\gamma$) we draw $L$ random samples from some fixed realization of a Gaussian process
(with kernel precision $\theta_0$), compute the ML estimate of $\theta$, if necessary,
construct the confidence intervals, and then average the coverage rate for each test
input, to average out the effects of random choice of the input data.

Judging by the coverage rate, conformal confidence regions are much more narrower
than the Bayesian intervals. This suggests, that the procedure (eq.~\ref{eq:conf_pred_set})
adapts to the noise level not through the regularization $\lambda$, but through
the sample distribution on the non-conformity scores (fig.~\ref{fig:gaussian_1d_low_noise_c2}
and \ref{fig:gaussian_1d_low_noise_c2}). Indeed, fig.~\ref{fig:gaussian_1d_low_noise_width}
clearly show that GPR intervals tend to be wider.

With lower kernel precision parameter ($\theta$) the KRR estimate of the underlying
$f$ becomes less variable, thus enabling better coverage by the GPR and conformal
regions (fig.~\ref{fig:gaussian_1d_low_noise_arb} columns \subref{fig:gaussian_1d_low_noise_arb_c1}
and \subref{fig:gaussian_1d_low_noise_arb_c2}).

It is noteworthy, the conformal procedure yields correct coverage despite relatively
poor fit (the last row in fig.~\ref{fig:gaussian_1d_low_noise_arb}). This suggests,
a certain flexibility of a KRR-based conformal prediction: it is able to maintain
the required coverage rate regardless of the quality of prediction within the NCM.
The region sizes at fig.~\ref{fig:gaussian_1d_low_noise_width_arb_c2} implies that
the GPR regions kept being symmetric around a less accurate prediction, which hints
at the reason why their coverage rate suffered. At the same time the coverage rate
of the conformal regions, symmetry of which is not required, demonstrated no significant
deviations from specified confidence levels. It should be noted, however, that all
intervals in col.~\subref{fig:gaussian_1d_low_noise_width_arb_c2} are wider than
those of other columns in fig.~\ref{fig:gaussian_1d_low_noise_width_arb}.

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/GPR-f/coverage_gaussian_1e-06_1e-06_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/GPR-f/coverage_gaussian_1e-06_0,1_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/GPR-f/coverage_gaussian_1e-06_1e-06_auto_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/GPR-f/coverage_gaussian_1e-06_0,1_auto_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/RRCM/coverage_gaussian_1e-06_1e-06_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/RRCM/coverage_gaussian_1e-06_0,1_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/RRCM/coverage_gaussian_1e-06_1e-06_auto_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/RRCM/coverage_gaussian_1e-06_0,1_auto_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/RRCM-loo/coverage_gaussian_1e-06_1e-06_100_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/RRCM-loo/coverage_gaussian_1e-06_0,1_100_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/RRCM-loo/coverage_gaussian_1e-06_1e-06_auto_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/RRCM-loo/coverage_gaussian_1e-06_0,1_auto_RRCM-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/CRR/coverage_gaussian_1e-06_1e-06_100_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/CRR/coverage_gaussian_1e-06_0,1_100_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/CRR/coverage_gaussian_1e-06_1e-06_auto_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/CRR/coverage_gaussian_1e-06_0,1_auto_CRR.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/CRR-loo/coverage_gaussian_1e-06_1e-06_100_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/CRR-loo/coverage_gaussian_1e-06_0,1_100_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/CRR-loo/coverage_gaussian_1e-06_1e-06_auto_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/CRR-loo/coverage_gaussian_1e-06_0,1_auto_CRR-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/gaussian_1e-06_1e-06_100_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/gaussian_1e-06_0,1_100_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/gaussian_1e-06_1e-06_auto_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/gaussian_1e-06_0,1_auto_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_c4}
  \end{subfigure}%
  \caption{Coverage rate dynamics in the fully Gaussian low-noise case $\gamma=10^{-6}$
  for different $(\theta, \lambda)$:
  \subref{fig:gaussian_1d_low_noise_c1}~--~$(\theta_0, 10^{-6})$,
  \subref{fig:gaussian_1d_low_noise_c2}~--~$(\theta_0, 10^{-1})$,
  \subref{fig:gaussian_1d_low_noise_c3}~--~$(\hat{\theta}_\text{ML}, 10^{-6})$,
  \subref{fig:gaussian_1d_low_noise_c4}~--~$(\hat{\theta}_\text{ML}, 10^{-1})$.
  Rows from \textit{top} to \textit{bottom}: ``GPR-f'', ``RRCM'', ``RRCM-loo'',
  ``CRR'', ``CRR-loo'', and the last depicting ``RMSE-std'' ratio.}
  \label{fig:gaussian_1d_low_noise}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/GPR-f/width_gaussian_1e-06_1e-06_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/GPR-f/width_gaussian_1e-06_0,1_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/GPR-f/width_gaussian_1e-06_1e-06_auto_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/GPR-f/width_gaussian_1e-06_0,1_auto_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/RRCM/width_gaussian_1e-06_1e-06_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/RRCM/width_gaussian_1e-06_0,1_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/RRCM/width_gaussian_1e-06_1e-06_auto_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/RRCM/width_gaussian_1e-06_0,1_auto_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/RRCM-loo/width_gaussian_1e-06_1e-06_100_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/RRCM-loo/width_gaussian_1e-06_0,1_100_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/RRCM-loo/width_gaussian_1e-06_1e-06_auto_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/RRCM-loo/width_gaussian_1e-06_0,1_auto_RRCM-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/CRR/width_gaussian_1e-06_1e-06_100_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/CRR/width_gaussian_1e-06_0,1_100_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/CRR/width_gaussian_1e-06_1e-06_auto_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/CRR/width_gaussian_1e-06_0,1_auto_CRR.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/CRR-loo/width_gaussian_1e-06_1e-06_100_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_width_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/CRR-loo/width_gaussian_1e-06_0,1_100_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_width_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/CRR-loo/width_gaussian_1e-06_1e-06_auto_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_width_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/CRR-loo/width_gaussian_1e-06_0,1_auto_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_width_c4}
  \end{subfigure}%
  \caption{Asymptotic width of the confidence regions in the fully Gaussian low-noise case
  $\gamma=10^{-6}$ for different $(\theta, \lambda)$. Upward triangles indicate the
  $5\%$ sample quantile across the whole test sample of confidence regions' widths,
  whereas downward triangles indicate the maximal width. The median width is drawn
  with a slightly thicker line. The colouring matches the colours of respective confidence
  levels as in fig.~\ref{fig:gaussian_1d_low_noise}.}
  \label{fig:gaussian_1d_low_noise_width}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/GPR-f/coverage_gaussian_1e-06_1e-06_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/GPR-f/coverage_gaussian_1e-06_0,1_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/GPR-f/coverage_gaussian_1e-06_1e-06_1000_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/GPR-f/coverage_gaussian_1e-06_0,1_1000_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/RRCM/coverage_gaussian_1e-06_1e-06_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/RRCM/coverage_gaussian_1e-06_0,1_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/RRCM/coverage_gaussian_1e-06_1e-06_1000_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/RRCM/coverage_gaussian_1e-06_0,1_1000_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/RRCM-loo/coverage_gaussian_1e-06_1e-06_10_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/RRCM-loo/coverage_gaussian_1e-06_0,1_10_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/RRCM-loo/coverage_gaussian_1e-06_1e-06_1000_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/RRCM-loo/coverage_gaussian_1e-06_0,1_1000_RRCM-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/CRR/coverage_gaussian_1e-06_1e-06_10_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/CRR/coverage_gaussian_1e-06_0,1_10_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/CRR/coverage_gaussian_1e-06_1e-06_1000_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/CRR/coverage_gaussian_1e-06_0,1_1000_CRR.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/CRR-loo/coverage_gaussian_1e-06_1e-06_10_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/CRR-loo/coverage_gaussian_1e-06_0,1_10_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/coverage/CRR-loo/coverage_gaussian_1e-06_1e-06_1000_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/coverage/CRR-loo/coverage_gaussian_1e-06_0,1_1000_CRR-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/gaussian_1e-06_1e-06_10_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_arb_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/gaussian_1e-06_0,1_10_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_arb_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/gaussian_1e-06_1e-06_1000_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_arb_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/gaussian_1e-06_0,1_1000_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_arb_c4}
  \end{subfigure}%
  \caption{The coverage rate dependence on $n$ in the fully Gaussian low-noise case $\gamma=10^{-6}$
  for arbitrary $\theta$:
  (\subref{fig:gaussian_1d_low_noise_arb_c1}, \subref{fig:gaussian_1d_low_noise_arb_c2})~--~$\theta=10$,
  and (\subref{fig:gaussian_1d_low_noise_arb_c3}, \subref{fig:gaussian_1d_low_noise_arb_c4})~--~$\theta=1000$.
  For description refer to fig.~\ref{fig:gaussian_1d_low_noise}.}
  \label{fig:gaussian_1d_low_noise_arb}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/GPR-f/width_gaussian_1e-06_1e-06_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/GPR-f/width_gaussian_1e-06_0,1_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/GPR-f/width_gaussian_1e-06_1e-06_1000_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/GPR-f/width_gaussian_1e-06_0,1_1000_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/RRCM/width_gaussian_1e-06_1e-06_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/RRCM/width_gaussian_1e-06_0,1_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/RRCM/width_gaussian_1e-06_1e-06_1000_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/RRCM/width_gaussian_1e-06_0,1_1000_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/RRCM-loo/width_gaussian_1e-06_1e-06_10_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/RRCM-loo/width_gaussian_1e-06_0,1_10_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/RRCM-loo/width_gaussian_1e-06_1e-06_1000_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/RRCM-loo/width_gaussian_1e-06_0,1_1000_RRCM-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/CRR/width_gaussian_1e-06_1e-06_10_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/CRR/width_gaussian_1e-06_0,1_10_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/CRR/width_gaussian_1e-06_1e-06_1000_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/CRR/width_gaussian_1e-06_0,1_1000_CRR.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/CRR-loo/width_gaussian_1e-06_1e-06_10_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_width_arb_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/CRR-loo/width_gaussian_1e-06_0,1_10_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_width_arb_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_1e-06/width/CRR-loo/width_gaussian_1e-06_1e-06_1000_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_width_arb_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/1e-06_0.1/width/CRR-loo/width_gaussian_1e-06_0,1_1000_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_low_noise_width_arb_c4}
  \end{subfigure}%
  \caption{Confidence region size dynamics for arbitrary $\theta$ in the fully Gaussian
  case (see description in fig.~\ref{fig:gaussian_1d_low_noise_arb}).}
  \label{fig:gaussian_1d_low_noise_width_arb}
\end{figure}

We now proceed to the non-Gaussian noiseless experiments. The results for the conformal
confidence intervals are not qualitatively different in the non-Gaussian setting:
coverage rate maintains its convergence to the specified confidence levels and all
conformal procedures demonstrate very similar asymptotic validity. For the Heaviside
step function typical confidence bands are shown in fig.~\ref{fig:nongauss_1d_heaviside},
and the asymptotic coverage rate of various confidence bands are presented at fig.~
\ref{fig:heaviside_1d_low_noise}, and~\ref{fig:heaviside_1d_low_noise_arb}. For another
non-Gaussian function (``f6'', fig.~\ref{fig:nongauss_1d_f6}) see fig.~\ref{fig:f6_1d_low_noise},
and~\ref{fig:f6_1d_low_noise_arb}.
\begin{figure}%[t, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.33\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/heaviside/1e-06_1e-06/50/profile_heaviside_1e-06_1e-06_100_10p-GPR_50.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.33\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/heaviside/1e-06_1e-06/50/profile_heaviside_1e-06_1e-06_100_10p-CRR_50.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.33\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/heaviside/1e-06_1e-06/50/profile_heaviside_1e-06_1e-06_100_10p-RRCM_50.pdf}
  \end{subfigure}
  \caption{Sample Bayesian and conformal confidence bands for the ``Heaviside'' step
  function (train sample size $n=50$): \textit{left} -- GPR, \textit{middle} -- CRR,
  and \textit{right} -- RRCM.}
  \label{fig:nongauss_1d_heaviside}
\end{figure}
\begin{figure}%[t, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.33\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/f6/1e-06_1e-06/50/profile_f6_1e-06_1e-06_100_10p-GPR_50.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.33\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/f6/1e-06_1e-06/50/profile_f6_1e-06_1e-06_100_10p-CRR_50.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.33\linewidth}
    \includegraphics[width=0.9\linewidth]{images/output_pdf/profile/f6/1e-06_1e-06/50/profile_f6_1e-06_1e-06_100_10p-RRCM_50.pdf}
  \end{subfigure}
  \caption{Sample Bayesian and conformal confidence bands for the ``f6'' function
  (train sample size $n=50$): \textit{left} -- GPR, \textit{middle} -- CRR, and
  \textit{right} -- RRCM.}
  \label{fig:nongauss_1d_f6}
\end{figure}

In contrast to the fully Gaussian setting, the GPR confidence intervals are not
consistently conservative. As is evident from coverage rate dynamics for both functions,
for relatively high regularization $\lambda=10^{-1}$ the GPR region fails to show
conservative validity in a consistent manner, despite quite faithful approximation
by the KRR as demonstrated by the ``RMSE-std'' ratio. At the same time conformal
procedures, e.g. RRCM, show no significant departures from claimed validity.

The main conclusion we can draw from the case on negligible noise is that at least
for the Gaussian kernel the conformal confidence intervals seem to perform reasonably
well both in terms of validity in a non-Gaussian setting and efficiency in Gaussian
setting.

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/GPR-f/coverage_heaviside_1e-06_1e-06_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/GPR-f/coverage_heaviside_1e-06_0,1_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/GPR-f/coverage_heaviside_1e-06_1e-06_auto_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/GPR-f/coverage_heaviside_1e-06_0,1_auto_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/RRCM/coverage_heaviside_1e-06_1e-06_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/RRCM/coverage_heaviside_1e-06_0,1_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/RRCM/coverage_heaviside_1e-06_1e-06_auto_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/RRCM/coverage_heaviside_1e-06_0,1_auto_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/heaviside_1e-06_1e-06_100_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_low_noise_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/heaviside_1e-06_0,1_100_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_low_noise_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/heaviside_1e-06_1e-06_auto_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_low_noise_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/heaviside_1e-06_0,1_auto_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_low_noise_c4}
  \end{subfigure}%
  \caption{Coverage dynamics for the the ``Heaviside'' step function.}
  \label{fig:heaviside_1d_low_noise}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/GPR-f/coverage_heaviside_1e-06_1e-06_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/GPR-f/coverage_heaviside_1e-06_0,1_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/GPR-f/coverage_heaviside_1e-06_1e-06_1000_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/GPR-f/coverage_heaviside_1e-06_0,1_1000_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/RRCM/coverage_heaviside_1e-06_1e-06_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/RRCM/coverage_heaviside_1e-06_0,1_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/RRCM/coverage_heaviside_1e-06_1e-06_1000_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/RRCM/coverage_heaviside_1e-06_0,1_1000_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/heaviside_1e-06_1e-06_10_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_low_noise_arb_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/heaviside_1e-06_0,1_10_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_low_noise_arb_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_1e-06/heaviside_1e-06_1e-06_1000_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_low_noise_arb_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/1e-06_0.1/heaviside_1e-06_0,1_1000_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_low_noise_arb_c4}
  \end{subfigure}%
  \caption{Coverage dynamics for the the ``Heaviside'' step function(c.f. fig.~\ref{fig:heaviside_1d_low_noise}).}
  \label{fig:heaviside_1d_low_noise_arb}
\end{figure}

% \begin{figure}%[b, width=0.5\textwidth]
%   \centering
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/coverage/GPR-f/coverage_pressure2_1e-06_1e-06_100_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/coverage/GPR-f/coverage_pressure2_1e-06_0,1_100_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/coverage/GPR-f/coverage_pressure2_1e-06_1e-06_auto_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/coverage/GPR-f/coverage_pressure2_1e-06_0,1_auto_GPR-f.pdf}
%   \end{subfigure}\\
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/coverage/RRCM/coverage_pressure2_1e-06_1e-06_100_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/coverage/RRCM/coverage_pressure2_1e-06_0,1_100_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/coverage/RRCM/coverage_pressure2_1e-06_1e-06_auto_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/coverage/RRCM/coverage_pressure2_1e-06_0,1_auto_RRCM.pdf}
%   \end{subfigure}\\
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/pressure2_1e-06_1e-06_100_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_low_noise_c1}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/pressure2_1e-06_0,1_100_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_low_noise_c2}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/pressure2_1e-06_1e-06_auto_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_low_noise_c3}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/pressure2_1e-06_0,1_auto_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_low_noise_c4}
%   \end{subfigure}%
%   \caption{Coverage dynamics for the the ``pressure2'' function.}
%   \label{fig:pressure2_1d_low_noise}
% \end{figure}

% \begin{figure}%[b, width=0.5\textwidth]
%   \centering
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/coverage/GPR-f/coverage_pressure2_1e-06_1e-06_10_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/coverage/GPR-f/coverage_pressure2_1e-06_0,1_10_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/coverage/GPR-f/coverage_pressure2_1e-06_1e-06_1000_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/coverage/GPR-f/coverage_pressure2_1e-06_0,1_1000_GPR-f.pdf}
%   \end{subfigure}\\
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/coverage/RRCM/coverage_pressure2_1e-06_1e-06_10_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/coverage/RRCM/coverage_pressure2_1e-06_0,1_10_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/coverage/RRCM/coverage_pressure2_1e-06_1e-06_1000_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/coverage/RRCM/coverage_pressure2_1e-06_0,1_1000_RRCM.pdf}
%   \end{subfigure}\\
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/pressure2_1e-06_1e-06_10_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_low_noise_arb_c1}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/pressure2_1e-06_0,1_10_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_low_noise_arb_c2}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_1e-06/pressure2_1e-06_1e-06_1000_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_low_noise_arb_c3}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/1e-06_0.1/pressure2_1e-06_0,1_1000_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_low_noise_arb_c4}
%   \end{subfigure}%
%   \caption{Coverage dynamics for the the ``pressure2'' function(c.f. fig.~\ref{fig:pressure2_1d_low_noise}).}
%   \label{fig:pressure2_1d_low_noise_arb}
% \end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/coverage/GPR-f/coverage_f6_1e-06_1e-06_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/coverage/GPR-f/coverage_f6_1e-06_0,1_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/coverage/GPR-f/coverage_f6_1e-06_1e-06_auto_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/coverage/GPR-f/coverage_f6_1e-06_0,1_auto_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/coverage/RRCM/coverage_f6_1e-06_1e-06_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/coverage/RRCM/coverage_f6_1e-06_0,1_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/coverage/RRCM/coverage_f6_1e-06_1e-06_auto_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/coverage/RRCM/coverage_f6_1e-06_0,1_auto_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/f6_1e-06_1e-06_100_ratio.pdf}
    \caption{} \label{fig:f6_1d_low_noise_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/f6_1e-06_0,1_100_ratio.pdf}
    \caption{} \label{fig:f6_1d_low_noise_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/f6_1e-06_1e-06_auto_ratio.pdf}
    \caption{} \label{fig:f6_1d_low_noise_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/f6_1e-06_0,1_auto_ratio.pdf}
    \caption{} \label{fig:f6_1d_low_noise_c4}
  \end{subfigure}%
  \caption{Coverage dynamics and ``RMSE-std'' ratio for the ``f6'' function.}
  \label{fig:f6_1d_low_noise}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/coverage/GPR-f/coverage_f6_1e-06_1e-06_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/coverage/GPR-f/coverage_f6_1e-06_0,1_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/coverage/GPR-f/coverage_f6_1e-06_1e-06_1000_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/coverage/GPR-f/coverage_f6_1e-06_0,1_1000_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/coverage/RRCM/coverage_f6_1e-06_1e-06_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/coverage/RRCM/coverage_f6_1e-06_0,1_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/coverage/RRCM/coverage_f6_1e-06_1e-06_1000_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/coverage/RRCM/coverage_f6_1e-06_0,1_1000_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/f6_1e-06_1e-06_10_ratio.pdf}
    \caption{} \label{fig:f6_1d_low_noise_arb_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/f6_1e-06_0,1_10_ratio.pdf}
    \caption{} \label{fig:f6_1d_low_noise_arb_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_1e-06/f6_1e-06_1e-06_1000_ratio.pdf}
    \caption{} \label{fig:f6_1d_low_noise_arb_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/1e-06_0.1/f6_1e-06_0,1_1000_ratio.pdf}
    \caption{} \label{fig:f6_1d_low_noise_arb_c4}
  \end{subfigure}%
  \caption{Coverage dynamics and ``RMSE-std'' ratio for the ``f6'' function (see
  the description of fig.~\ref{fig:f6_1d_low_noise}).}
  \label{fig:f6_1d_low_noise_arb}
\end{figure}

Now let's assess the performance of the confidence regions in noisy setting. We
study the effects of a moderate noise-to-signal $\gamma=10^{-1}$ ratio, relative
to the variability of the studied test functions.

Firstly, we consider the case when the $\theta$ hyper-parameter is exactly equal
to $\theta_0$. In this setting the sample GPR and KRR RRCM confidence bands are
depicted in fig.~\ref{fig:gauss_1d_prof_conf}, and fig.~\ref{fig:gauss_1d_prof_gpr}.
Qualitatively there is no difference to the negligible nose case, except in the current
setting the band are wider do to higher observation noise. Despite the overall worse
fit, however, both conformal and Bayesian confidence regions show approximately correct
asymptotic coverage rates (fig.~\ref{fig:gaussian_1d_high_noise_c1} and~\ref{fig:gaussian_1d_high_noise_c2}).
Regarding the efficiency, GPR confidence intervals are narrower that the conformal
regions, but for sufficiently large train samples $n$ the width of the latter gradually
shrinks to that of the Bayesian interval (fig.~\ref{fig:gaussian_1d_high_noise_width_c1}
and~\ref{fig:gaussian_1d_high_noise_width_c2}. Thus, in this setting, as in the
previous one, we found evidence supporting the conjectured properties of conformal
procedures over KRR with the Gaussian kernel.

In the setting, when the kernel precision $\theta$ no equal to the theoretical $\theta_0$,
but is either estimated with the MLE $\hat{\theta}_\text{MLE}$, provided by the GPR
likelihood, eq.~\ref{eq:bkrr_likelihood}, (fig.~\ref{fig:gaussian_1d_high_noise_c3}
and~\ref{fig:gaussian_1d_high_noise_c4}), or too low $\theta=10$ (fig.~\ref{fig:gaussian_1d_high_noise_arb_c1}
and~\ref{fig:gaussian_1d_high_noise_arb_c1}), or too high $\theta=10^3$ (fig.~\ref{fig:gaussian_1d_high_noise_arb_c3}
and~\ref{fig:gaussian_1d_high_noise_arb_c4}) we arrive at similar conclusions: confidence
intervals constructed with an NCM based on KRR with kernel eq.~\ref{eq:gauss_kenrel}
provide at least conservatively the required level of validity regardless of the
parameters ($\theta$, $\lambda$) at the core of the non-conformity measure.

From this particular experiment in this setting, can be concluded that there is evidence
in favour of the conjectured asymptotic conservative validity of the conformal confidence
regions for all considered NCMs and residual types as well as their asymptotic efficiency.
As expected, the GPR confidence predictions uphold their theoretical guarantees in
this case.

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/GPR-f/coverage_gaussian_0,1_1e-06_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/GPR-f/coverage_gaussian_0,1_0,1_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/GPR-f/coverage_gaussian_0,1_1e-06_auto_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/GPR-f/coverage_gaussian_0,1_0,1_auto_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM/coverage_gaussian_0,1_1e-06_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM/coverage_gaussian_0,1_0,1_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM/coverage_gaussian_0,1_1e-06_auto_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM/coverage_gaussian_0,1_0,1_auto_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM-loo/coverage_gaussian_0,1_1e-06_100_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM-loo/coverage_gaussian_0,1_0,1_100_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM-loo/coverage_gaussian_0,1_1e-06_auto_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM-loo/coverage_gaussian_0,1_0,1_auto_RRCM-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/CRR/coverage_gaussian_0,1_1e-06_100_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR/coverage_gaussian_0,1_0,1_100_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/CRR/coverage_gaussian_0,1_1e-06_auto_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR/coverage_gaussian_0,1_0,1_auto_CRR.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/CRR-loo/coverage_gaussian_0,1_1e-06_100_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR-loo/coverage_gaussian_0,1_0,1_100_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/CRR-loo/coverage_gaussian_0,1_1e-06_auto_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR-loo/coverage_gaussian_0,1_0,1_auto_CRR-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/gaussian_0,1_1e-06_100_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/gaussian_0,1_0,1_100_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/gaussian_0,1_1e-06_auto_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/gaussian_0,1_0,1_auto_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_c4}
  \end{subfigure}%
  \caption{Coverage rate dynamics in the noisy fully Gaussian case $\gamma=10^{-1}$
  for different $(\theta, \lambda)$:
  \subref{fig:gaussian_1d_high_noise_c1}~--~$(\theta_0, 10^{-6})$,
  \subref{fig:gaussian_1d_high_noise_c2}~--~$(\theta_0, 10^{-1})$,
  \subref{fig:gaussian_1d_high_noise_c3}~--~$(\hat{\theta}_\text{ML}, 10^{-6})$,
  \subref{fig:gaussian_1d_high_noise_c4}~--~$(\hat{\theta}_\text{ML}, 10^{-1})$.
  Rows from \textit{top} to \textit{bottom}: ``GPR-f'', ``RRCM'', ``RRCM-loo'',
  ``CRR'', ``CRR-loo'', and the last depicting ``RMSE-std'' ratio.}
  \label{fig:gaussian_1d_high_noise}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/GPR-f/width_gaussian_0,1_1e-06_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/GPR-f/width_gaussian_0,1_0,1_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/GPR-f/width_gaussian_0,1_1e-06_auto_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/GPR-f/width_gaussian_0,1_0,1_auto_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/RRCM/width_gaussian_0,1_1e-06_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM/width_gaussian_0,1_0,1_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/RRCM/width_gaussian_0,1_1e-06_auto_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM/width_gaussian_0,1_0,1_auto_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/RRCM-loo/width_gaussian_0,1_1e-06_100_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM-loo/width_gaussian_0,1_0,1_100_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/RRCM-loo/width_gaussian_0,1_1e-06_auto_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM-loo/width_gaussian_0,1_0,1_auto_RRCM-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/CRR/width_gaussian_0,1_1e-06_100_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/CRR/width_gaussian_0,1_0,1_100_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/CRR/width_gaussian_0,1_1e-06_auto_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/CRR/width_gaussian_0,1_0,1_auto_CRR.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/CRR-loo/width_gaussian_0,1_1e-06_100_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_width_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/CRR-loo/width_gaussian_0,1_0,1_100_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_width_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/CRR-loo/width_gaussian_0,1_1e-06_auto_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_width_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/CRR-loo/width_gaussian_0,1_0,1_auto_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_width_c4}
  \end{subfigure}
  \caption{Confidence region width as a function of the train sample size $n$ for
  different $(\theta, \lambda)$ (see fig.~\ref{fig:gaussian_1d_high_noise}).}
  \label{fig:gaussian_1d_high_noise_width}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/GPR-f/coverage_gaussian_0,1_1e-06_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/GPR-f/coverage_gaussian_0,1_0,1_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/GPR-f/coverage_gaussian_0,1_1e-06_1000_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/GPR-f/coverage_gaussian_0,1_0,1_1000_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM/coverage_gaussian_0,1_1e-06_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM/coverage_gaussian_0,1_0,1_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM/coverage_gaussian_0,1_1e-06_1000_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM/coverage_gaussian_0,1_0,1_1000_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM-loo/coverage_gaussian_0,1_1e-06_10_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM-loo/coverage_gaussian_0,1_0,1_10_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM-loo/coverage_gaussian_0,1_1e-06_1000_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM-loo/coverage_gaussian_0,1_0,1_1000_RRCM-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/CRR/coverage_gaussian_0,1_1e-06_10_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR/coverage_gaussian_0,1_0,1_10_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/CRR/coverage_gaussian_0,1_1e-06_1000_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR/coverage_gaussian_0,1_0,1_1000_CRR.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/CRR-loo/coverage_gaussian_0,1_1e-06_10_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR-loo/coverage_gaussian_0,1_0,1_10_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/CRR-loo/coverage_gaussian_0,1_1e-06_1000_CRR-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/CRR-loo/coverage_gaussian_0,1_0,1_1000_CRR-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/gaussian_0,1_1e-06_10_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_arb_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/gaussian_0,1_0,1_10_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_arb_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/gaussian_0,1_1e-06_1000_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_arb_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/gaussian_0,1_0,1_1000_ratio.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_arb_c4}
  \end{subfigure}%
  \caption{Coverage rate dynamics in the noisy fully Gaussian case $\gamma=10^{-1}$
  for different $(\theta, \lambda)$ (see fig.~\ref{fig:gaussian_1d_high_noise}).}
  \label{fig:gaussian_1d_high_noise_arb}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/GPR-f/width_gaussian_0,1_1e-06_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/GPR-f/width_gaussian_0,1_0,1_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/GPR-f/width_gaussian_0,1_1e-06_1000_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/GPR-f/width_gaussian_0,1_0,1_1000_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/RRCM/width_gaussian_0,1_1e-06_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM/width_gaussian_0,1_0,1_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/RRCM/width_gaussian_0,1_1e-06_1000_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM/width_gaussian_0,1_0,1_1000_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/RRCM-loo/width_gaussian_0,1_1e-06_10_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM-loo/width_gaussian_0,1_0,1_10_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/RRCM-loo/width_gaussian_0,1_1e-06_1000_RRCM-loo.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM-loo/width_gaussian_0,1_0,1_1000_RRCM-loo.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/CRR/width_gaussian_0,1_1e-06_10_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/CRR/width_gaussian_0,1_0,1_10_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/CRR/width_gaussian_0,1_1e-06_1000_CRR.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/CRR/width_gaussian_0,1_0,1_1000_CRR.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/CRR-loo/width_gaussian_0,1_1e-06_10_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_arb_width_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/CRR-loo/width_gaussian_0,1_0,1_10_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_arb_width_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/CRR-loo/width_gaussian_0,1_1e-06_1000_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_arb_width_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/gaussian/0.1_0.1/width/CRR-loo/width_gaussian_0,1_0,1_1000_CRR-loo.pdf}
    \caption{} \label{fig:gaussian_1d_high_noise_arb_width_c4}
  \end{subfigure}
  \caption{Confidence region width as a function of the train sample size $n$ for
  different $(\theta, \lambda)$ (see fig.~\ref{fig:gaussian_1d_high_noise_arb}).}
  \label{fig:gaussian_1d_high_noise_arb_width}
\end{figure}

In the non-Gaussian setting with noise-to-signal ratio $\gamma=10^{-1}$, all experiments
yielded results similar to the non-Gaussian setting with negligible noise: all conformal
procedures produced asymptotically valid confidence sets. As for the Gaussian Process
Regression confidence intervals, the results were similar but were less extreme.
In the case of the ``Heaviside'' step test function, despite the fact that KRR achieved
adequate fit to the test data on average, GPR confidence intervals do not provide
conservative validity on all significance levels consistently for kernel precision
parameters $\theta \leq 10^3$, with the ML estimate being $\hat{\theta}\approx 2000$
for $n=1600$ (fig.~\ref{fig:heaviside_1d_high_noise} and~\ref{fig:heaviside_1d_high_noise_arb}).
This is due to the fact that the test function is discontinuous, and of relative
low precision setting, the induced basis functions are more spread out, which results
in worse fit in the vicinity of the discontinuity.

In contrast, GPR confidence intervals exhibit more adequate validity in the case
of ``f6'' test function: which is a pair of polynomials continuously pasted at $0.5$.
That is why the performance of the GPR confidence intervals in noticeably better
(fig.~\ref{fig:f6_1d_high_noise} and.~\ref{fig:f6_1d_high_noise_arb}). The widths
of the Bayesian and conformal intervals converge as the train sample grows larger
(fig.~\ref{fig:f6_1d_high_noise_width}).

The conducted experiments in the $1$-d setting yielded empirical support for the
following conclusions:
\begin{itemize}
  \item the kernel ridge regression conformal procedure seems to be asymptotically
  conservatively valid in all setups, despite being applied in the off-line learning
  setting;
  \item  the confidence intervals of the GPR fails in the non-Gaussian settings,
  especially with low noise-to-signal ratio;
  \item in those experiments, where the GPR intervals were approximately valid, the
  size of conformal confidence regions converged to the width of the Bayesian confidence
  set.
\end{itemize}

%%
\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/GPR-f/coverage_heaviside_0,1_1e-06_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/GPR-f/coverage_heaviside_0,1_0,1_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/GPR-f/coverage_heaviside_0,1_1e-06_auto_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/GPR-f/coverage_heaviside_0,1_0,1_auto_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/RRCM/coverage_heaviside_0,1_1e-06_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/RRCM/coverage_heaviside_0,1_0,1_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/RRCM/coverage_heaviside_0,1_1e-06_auto_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/RRCM/coverage_heaviside_0,1_0,1_auto_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/heaviside_0,1_1e-06_100_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_high_noise_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/heaviside_0,1_0,1_100_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_high_noise_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/heaviside_0,1_1e-06_auto_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_high_noise_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/heaviside_0,1_0,1_auto_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_high_noise_c4}
  \end{subfigure}%
  \caption{Coverage dynamics for the the ``Heaviside'' step function.}
  \label{fig:heaviside_1d_high_noise}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/GPR-f/coverage_heaviside_0,1_1e-06_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/GPR-f/coverage_heaviside_0,1_0,1_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/GPR-f/coverage_heaviside_0,1_1e-06_1000_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/GPR-f/coverage_heaviside_0,1_0,1_1000_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/RRCM/coverage_heaviside_0,1_1e-06_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/RRCM/coverage_heaviside_0,1_0,1_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/RRCM/coverage_heaviside_0,1_1e-06_1000_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/RRCM/coverage_heaviside_0,1_0,1_1000_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/heaviside_0,1_1e-06_10_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_high_noise_arb_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/heaviside_0,1_0,1_10_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_high_noise_arb_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_1e-06/heaviside_0,1_1e-06_1000_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_high_noise_arb_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/heaviside/0.1_0.1/heaviside_0,1_0,1_1000_ratio.pdf}
    \caption{} \label{fig:heaviside_1d_high_noise_arb_c4}
  \end{subfigure}%
  \caption{Coverage dynamics for the the ``Heaviside'' step function (c.f. fig.~\ref{fig:heaviside_1d_high_noise}).}
  \label{fig:heaviside_1d_high_noise_arb}
\end{figure}

% \begin{figure}%[b, width=0.5\textwidth]
%   \centering
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/coverage/GPR-f/coverage_pressure2_0,1_1e-06_100_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/coverage/GPR-f/coverage_pressure2_0,1_0,1_100_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/coverage/GPR-f/coverage_pressure2_0,1_1e-06_auto_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/coverage/GPR-f/coverage_pressure2_0,1_0,1_auto_GPR-f.pdf}
%   \end{subfigure}\\
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/coverage/RRCM/coverage_pressure2_0,1_1e-06_100_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/coverage/RRCM/coverage_pressure2_0,1_0,1_100_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/coverage/RRCM/coverage_pressure2_0,1_1e-06_auto_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/coverage/RRCM/coverage_pressure2_0,1_0,1_auto_RRCM.pdf}
%   \end{subfigure}\\
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/pressure2_0,1_1e-06_100_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_high_noise_c1}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/pressure2_0,1_0,1_100_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_high_noise_c2}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/pressure2_0,1_1e-06_auto_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_high_noise_c3}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/pressure2_0,1_0,1_auto_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_high_noise_c4}
%   \end{subfigure}%
%   \caption{Coverage dynamics for the the ``pressure2'' function.}
%   \label{fig:pressure2_1d_high_noise}
% \end{figure}

% \begin{figure}%[b, width=0.5\textwidth]
%   \centering
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/coverage/GPR-f/coverage_pressure2_0,1_1e-06_10_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/coverage/GPR-f/coverage_pressure2_0,1_0,1_10_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/coverage/GPR-f/coverage_pressure2_0,1_1e-06_1000_GPR-f.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/coverage/GPR-f/coverage_pressure2_0,1_0,1_1000_GPR-f.pdf}
%   \end{subfigure}\\
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/coverage/RRCM/coverage_pressure2_0,1_1e-06_10_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/coverage/RRCM/coverage_pressure2_0,1_0,1_10_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/coverage/RRCM/coverage_pressure2_0,1_1e-06_1000_RRCM.pdf}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/coverage/RRCM/coverage_pressure2_0,1_0,1_1000_RRCM.pdf}
%   \end{subfigure}\\
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/pressure2_0,1_1e-06_10_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_high_noise_arb_c1}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/pressure2_0,1_0,1_10_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_high_noise_arb_c2}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_1e-06/pressure2_0,1_1e-06_1000_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_high_noise_arb_c3}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.25\linewidth}
%     \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/pressure2/0.1_0.1/pressure2_0,1_0,1_1000_ratio.pdf}
%     \caption{} \label{fig:pressure2_1d_high_noise_arb_c4}
%   \end{subfigure}%
%   \caption{Coverage dynamics for the the ``pressure2'' function(c.f. fig.~\ref{fig:pressure2_1d_high_noise}).}
%   \label{fig:pressure2_1d_high_noise_arb}
% \end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/coverage/GPR-f/coverage_f6_0,1_1e-06_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/coverage/GPR-f/coverage_f6_0,1_0,1_100_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/coverage/GPR-f/coverage_f6_0,1_1e-06_auto_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/coverage/GPR-f/coverage_f6_0,1_0,1_auto_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/coverage/RRCM/coverage_f6_0,1_1e-06_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/coverage/RRCM/coverage_f6_0,1_0,1_100_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/coverage/RRCM/coverage_f6_0,1_1e-06_auto_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/coverage/RRCM/coverage_f6_0,1_0,1_auto_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/f6_0,1_1e-06_100_ratio.pdf}
    \caption{} \label{fig:f6_1d_high_noise_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/f6_0,1_0,1_100_ratio.pdf}
    \caption{} \label{fig:f6_1d_high_noise_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/f6_0,1_1e-06_auto_ratio.pdf}
    \caption{} \label{fig:f6_1d_high_noise_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/f6_0,1_0,1_auto_ratio.pdf}
    \caption{} \label{fig:f6_1d_high_noise_c4}
  \end{subfigure}%
  \caption{Coverage dynamics and ``RMSE-std'' ratio for the ``f6'' function.}
  \label{fig:f6_1d_high_noise}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/coverage/GPR-f/coverage_f6_0,1_1e-06_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/coverage/GPR-f/coverage_f6_0,1_0,1_10_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/coverage/GPR-f/coverage_f6_0,1_1e-06_1000_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/coverage/GPR-f/coverage_f6_0,1_0,1_1000_GPR-f.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/coverage/RRCM/coverage_f6_0,1_1e-06_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/coverage/RRCM/coverage_f6_0,1_0,1_10_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/coverage/RRCM/coverage_f6_0,1_1e-06_1000_RRCM.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/coverage/RRCM/coverage_f6_0,1_0,1_1000_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/f6_0,1_1e-06_10_ratio.pdf}
    \caption{} \label{fig:f6_1d_high_noise_arb_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/f6_0,1_0,1_10_ratio.pdf}
    \caption{} \label{fig:f6_1d_high_noise_arb_c2}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/f6_0,1_1e-06_1000_ratio.pdf}
    \caption{} \label{fig:f6_1d_high_noise_arb_c3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/f6_0,1_0,1_1000_ratio.pdf}
    \caption{} \label{fig:f6_1d_high_noise_arb_c4}
  \end{subfigure}%
  \caption{Coverage dynamics and ``RMSE-std'' ratio for the ``f6'' function (see
  the description of fig.~\ref{fig:f6_1d_high_noise}).}
  \label{fig:f6_1d_high_noise_arb}
\end{figure}

\begin{figure}%[b, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/width/GPR-f/width_f6_0,1_1e-06_auto_GPR-f.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_1e-06/width/RRCM/width_f6_0,1_1e-06_auto_RRCM.pdf}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/width/GPR-f/width_f6_0,1_0,1_auto_GPR-f.pdf}
    \caption{} \label{fig:f6_1d_high_noise_width_c1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\linewidth}
    \includegraphics[width=0.95\linewidth]{images/output_pdf/exp_1d/f6/0.1_0.1/width/RRCM/width_f6_0,1_0,1_auto_RRCM.pdf}
    \caption{} \label{fig:f6_1d_high_noise_width_c2}
  \end{subfigure}
  \caption{Width dynamics of the KRR and GPR confidence sets for the ML estimate
  of $\theta$ for the values of the ``f6'' test function, distorted by Gaussian noise
  with $\gamma=10^{-1}$.}
  \label{fig:f6_1d_high_noise_width}
\end{figure}

% subsection results_1_d (end)

\subsection{Results: $2$-d} % (fold)
\label{sub:results_2_d}

In this section we study the performance in the $2$-d setting. We begin by testing
the conformal procedure in a Gaussian process setting, where the Bayesian GPR intervals
are provably correct: we compare the validity and assess the efficiency. We test
the effects of using ML estimate of the kernel precision parameter $\theta$ and
fixing it to deliberately large or small values. Finally, we use specific test functions,
fig.~\ref{fig:nongauss_2d_profile}, observations of which are contaminated by moderate
Gaussian noise.

\begin{figure}%[t, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=\linewidth]{images/output_pdf/exp_2d/f2/0.1_1e-06/1500/profile/f2_0,1_1e-06_1000_test.pdf}
  \end{subfigure}%~
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=\linewidth]{images/output_pdf/exp_2d/f5/0.1_1e-06/1500/profile/f5_0,1_1e-06_1000_test.pdf}
  \end{subfigure}
  \caption{A sample path of a $2$-d non-Gaussian function, contaminated by moderate
  independent $\Ncal(0, \gamma)$ noise. Function: \textit{left:} ``f2''; \textit{right:}
  ``f5''.}
  \label{fig:nongauss_2d_profile}
\end{figure}

For the first part we generated realizations of a $GP(0, K(x,x'))$ process on $\Xcal=[0,1]^2$
with kernel defined in eq.~\ref{eq:gauss_kenrel} and precision $\theta_0 = 10^2$.
The typical sample path is depicted in fig.~\ref{fig:gauss_2d_profile}. As in the
$1$-d experiments we synthesized one sample path of $GP$ and then for each set of
hyper-parameters, excluding $\gamma$, we drew $L=25$ random samples from $\Xcal$
to average the effects of random sampling on the KRR and the derived confidence regions.
The goodness-of-fit ratios are presented in table~\ref{tab:gaussian_2d_rmse_std}.
For the true precision $\theta=\theta_0$ and small sample $n=150$ we see overfitting
in the case of high noise in the observations ($\gamma$) and low regularization
parameter ($\lambda$).

\begin{figure}%[t, width=0.5\textwidth]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=\linewidth]{images/output_pdf/exp_2d/gaussian/1e-06_1e-06/1500/profile/gaussian_1e-06_1e-06_1000_test.pdf}
  \end{subfigure}%~
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=\linewidth]{images/output_pdf/exp_2d/gaussian/0.1_1e-06/1500/profile/gaussian_0,1_1e-06_1000_test.pdf}
  \end{subfigure}
  \caption{A sample path of a $2$-d Gaussian process with kernel $\gamma \delta_{x,x'} + K(x,x')$:
  \textit{left:} $\gamma=10^{-6}$; \textit{right:} $\gamma=10^{-1}$.}
  \label{fig:gauss_2d_profile}
\end{figure}

Table~\ref{tab:gaussian_2d_cov_gpr} shows the error rates ($y^*\notin\Bcal(x^*)$) of
the GPR confidence intervals on a fixed test sample, the inputs $X^*\subseteq \Xcal$
of which are given by
\begin{equation*}
  X^* = \bigl\{(N^{-1} i, N^{-1} j)\,:\,i,j=0\ldots, N\bigr\} \,,
\end{equation*}
for $N=50$. Columns 1 and 4 show that the regions are approximately valid in the
cases when the KRR parameters are set to the true values used to generate the data
and with larger train sample the coverage improves. As in the $1$-d case the intervals
are more conservative for the case of low noise in the data $\gamma=10^{-6}$ and
high regularization $\lambda=10^{-1}$. Table~\ref{tab:gaussian_2d_cov_gpr_neq} show
that in current setting, the validity of the GPR confidence intervals is sensitive
to misspecification of the kernel precision.

\begin{table}
  \centering
  \caption{Goodness of fit of the KRR of simulated $2$-d Gaussian process.}
  \label{tab:gaussian_2d_rmse_std}
  \begin{tabular}{ll||rr|rr}
  \toprule
       & $\gamma$ & $10^{-6}$ &          & $10^{-1}$ &          \\\cline{2-2}
       & $\lambda$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ \\\cline{2-2}
  $n$ & $\theta$ &          &          &          &          \\
  \midrule
  $150$  & $10^2$ &    \textbf{0.404} &    0.465 &    1.874 &    \textbf{0.503} \\
       & $\hat{\theta}_\text{ML}$ &    0.404 &    0.469 &    0.707 &    0.499 \\\cline{2-6}
       & $10^1$ &    1.606 &    0.795 &    2.010 &    0.774 \\
       & $10^3$ &    0.783 &    0.795 &    0.799 &    0.798 \\%\cline{2-6}
  \midrule
  $1500$ & $10^2$ &    \textbf{0.004} &    0.081 &    0.677 &    \textbf{0.291} \\
       & $\hat{\theta}_\text{ML}$ &    0.005 &    0.056 &    0.706 &    0.291 \\\cline{2-6}
       & $10^1$ &    0.342 &    0.640 &    0.373 &    0.606 \\
       & $10^3$ &    0.144 &    0.188 &    2.239 &    0.377 \\%\cline{2-6}
  \bottomrule
  \end{tabular}
\end{table}
\begin{table}
\centering
  \caption{The empirical error rate ($\%$) of the GPR confidence interval for simulated
  $2$-d Gaussian process.}
  \label{tab:gaussian_2d_cov_gpr}
  \begin{tabular}{lll||rr|rr}
  \toprule
       &      & $\gamma$ & $10^{-6}$ &          & $10^{-1}$ &          \\\cline{3-3}
       &      & $\lambda$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ \\\cline{3-3}
  $n$ & $\theta$ & $\alpha(\%)$ &          &          &          &          \\
  \midrule
  $150$  & $10^2$ & $1$ &     1.3 &     0.7 &    14.6 &     0.8 \\
       &      & $5$ &     6.7 &     3.2 &    20.1 &     4.2 \\
       &      & $10$ &    12.8 &     6.1 &    24.3 &     8.9 \\
       &      & $25$ &    28.9 &    15.9 &    33.5 &    23.3 \\\cline{2-7}
       & $\hat{\theta}_\text{ML}$ & $1$ &     0.8 &     0.7 &     3.2 &     0.7 \\
       &      & $5$ &     4.8 &     3.1 &     6.5 &     4.0 \\
       &      & $10$ &    10.0 &     6.3 &    10.2 &     8.6 \\
       &      & $25$ &    25.1 &    16.5 &    22.2 &    23.2 \\
  \midrule
  $1500$ & $10^2$ & $1$ &     0.9 &     0.1 &     4.1 &     0.7 \\
       &      & $5$ &     4.5 &     0.5 &    12.0 &     4.4 \\
       &      & $10$ &     9.2 &     1.0 &    19.2 &     9.3 \\
       &      & $25$ &    23.8 &     3.3 &    36.1 &    24.5 \\\cline{2-7}
       & $\hat{\theta}_\text{ML}$ & $1$ &     0.8 &     0.1 &     2.3 &     0.8 \\
       &      & $5$ &     4.4 &     0.5 &     4.9 &     4.5 \\
       &      & $10$ &     9.0 &     0.9 &     8.0 &     9.4 \\
       &      & $25$ &    23.6 &     2.3 &    18.9 &    24.7 \\
  \bottomrule
  \end{tabular}
\end{table}
\begin{table}
  \centering
  \caption{The empirical error rate ($\%$) of the GPR confidence interval for simulated
  $2$-d Gaussian process for misspecified $\theta$.}
  \label{tab:gaussian_2d_cov_gpr_neq}
  \begin{tabular}{lll||rr|rr}
  \toprule
       &      & $\gamma$ & $10^{-6}$ &          & $10^{-1}$ &          \\\cline{3-3}
       &      & $\lambda$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ \\\cline{3-3}
  $n$ & $\theta$ & $\alpha(\%)$ &          &          &          &          \\
  \midrule
  $150$  & $10^1$ & $1$ &      7.2 &      1.9 &      6.6 &      1.7 \\
       &      & $5$ &     15.1 &      6.2 &     15.6 &      6.2 \\
       &      & $10$ &     21.6 &     10.7 &     23.1 &     11.6 \\
       &      & $25$ &     36.7 &     24.3 &     39.8 &     26.2 \\\cline{2-7}
       & $10^3$ & $1$ &      1.4 &      1.4 &      1.5 &      0.7 \\
       &      & $5$ &      5.4 &      5.3 &      4.5 &      3.7 \\
       &      & $10$ &      8.9 &      8.8 &      8.1 &      7.4 \\
       &      & $25$ &     18.6 &     18.6 &     20.8 &     20.6 \\
  \midrule
  $1500$ & $10^1$ & $1$ &      1.6 &      0.6 &      1.0 &      0.6 \\
       &      & $5$ &      5.3 &      4.2 &      5.2 &      3.7 \\
       &      & $10$ &      9.6 &      8.6 &     10.4 &      8.2 \\
       &      & $25$ &     22.5 &     23.4 &     26.3 &     22.6 \\\cline{2-7}
       & $10^3$ & $1$ &      0.4 &      0.4 &      9.1 &      1.1 \\
       &      & $5$ &      1.2 &      1.2 &     13.1 &      4.9 \\
       &      & $10$ &      2.0 &      2.0 &     16.1 &      9.6 \\
       &      & $25$ &      4.7 &      4.5 &     24.0 &     24.0 \\
  \bottomrule
  \end{tabular}
\end{table}

In contrast to the GPR confidence intervals, the conformal region show remarkable
insensitivity to both the goodness of fit and the parameter misspecification
as demonstrated in table.~\ref{tab:gaussian_2d_cov_conf}, where we show the maximal
absolute deviation of the interval error rate from the stated rate $\alpha$
across all studied significance levels (eq.~\ref{eq:mad_alpha}).
\begin{equation} \label{eq:mad_alpha}
  \mathtt{MAD}(\Gamma, A; \Theta)
  = \max_{\alpha\in A}\Bigl\lvert
    m^{-1}\#\{j\,:\, y^*_j\notin \Gamma^\alpha_n(X^*_j; \Theta)\} - \alpha
  \Bigr\rvert
    \,,
\end{equation}
where $\Theta$ is the vector of hyper-parameters of the experiment (excluding $\gamma$),
$A = \{1\%, 5\%, 10\%, 25\%\}$, $(X^*_j, y^*_j)_{j=1}^{|X^*|}$ -- the test sample, and
$\Gamma$ is the conformal confidence region.

Conclusions regarding the conformal procedure in the $2$-d Gaussian setting are
similar to the respective $1$-d setting. In the case of low observation noise (interpolation
case) the intervals have more coverage for the same reasons: the procedure itself
allows for some uncertainty due to flexibility of the KRR with the Gaussian kernel.

\begin{table}
  \centering
  \caption{The maximal absolute deviation $\mathtt{MAD}(\Gamma, A; \Theta)$ ($\%$)
  of the empirical error rate from the theoretical significance level of conformal
  confidence regions for simulated $2$-d Gaussian process.}
  \label{tab:gaussian_2d_cov_conf}
  \begin{tabular}{ll||rrrr|rrrr}
  \toprule
       & $n$ &    $150$  &          &          &          &     $1500$ &          &          &          \\\cline{2-2}
       & $\gamma$ & $10^{-6}$ &          & $10^{-1}$ &          & $10^{-6}$ &          & $10^{-1}$ &          \\\cline{2-2}
       & $\lambda$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ \\\cline{2-2}
  type & $\theta$ &          &          &          &          &          &          &          &          \\
  \midrule
  RRCM & $10^1$ &      1.5 &      0.7 &      1.6 &      1.1 &      0.3 &      0.2 &      0.8 &      0.4 \\
       & $10^2$ &      1.3 &      1.0 &      1.9 &      0.5 &      1.7 &      1.3 &      1.1 &      0.7 \\
       & $10^3$ &      1.4 &      0.5 &      1.5 &      0.9 &      1.1 &      2.6 &      1.2 &      0.5 \\
       & $\hat{\theta}_\text{ML}$ &      0.8 &      0.9 &      0.6 &      1.5 &      1.7 &      2.2 &      0.1 &      0.5 \\
  \midrule
  RRCM-loo & $10^1$ &      1.6 &      0.7 &      2.1 &      1.4 &      1.3 &      0.3 &      2.0 &      0.4 \\
       & $10^2$ &      0.9 &      0.3 &      1.1 &      1.2 &      2.4 &      1.7 &      2.0 &      0.4 \\
       & $10^3$ &      1.1 &      0.5 &      0.7 &      1.1 &      2.9 &      2.6 &      0.1 &      0.6 \\
       & $\hat{\theta}_\text{ML}$ &      1.1 &      0.7 &      1.2 &      0.5 &      2.6 &      2.6 &      0.8 &      0.6 \\
  \midrule
  CRR & $10^1$ &      2.4 &      1.2 &      2.3 &      1.0 &      0.3 &      0.1 &      0.8 &      0.3 \\
       & $10^2$ &      1.9 &      1.4 &      2.5 &      1.1 &      1.6 &      1.2 &      1.2 &      0.8 \\
       & $10^3$ &      1.0 &      1.2 &      2.0 &      2.1 &      0.8 &      2.2 &      1.3 &      0.5 \\
       & $\hat{\theta}_\text{ML}$ &      1.3 &      1.0 &      1.3 &      1.9 &      1.8 &      2.1 &      0.1 &      0.4 \\
  \midrule
  CRR-loo & $10^1$ &      1.0 &      1.0 &      1.4 &      1.0 &      1.2 &      0.3 &      2.0 &      0.2 \\
       & $10^2$ &      1.0 &      1.2 &      1.0 &      1.0 &      2.4 &      1.7 &      2.1 &      0.5 \\
       & $10^3$ &      1.1 &      1.0 &      1.0 &      1.6 &      2.6 &      2.4 &      0.3 &      0.5 \\
       & $\hat{\theta}_\text{ML}$ &      1.0 &      1.0 &      1.7 &      1.4 &      2.6 &      2.4 &      0.8 &      0.6 \\
  \bottomrule
  \end{tabular}
\end{table}

We proceed to the non-Gaussian case, with sample paths of the test functions with
domain $\Xcal=[-1,1]^2$ and plotted fig.~\ref{fig:nongauss_2d_profile} (p.~\pageref{fig:nongauss_2d_profile}).
Table~\ref{tab:nongaussian_2d_gpr_fit} shows the goodness-of-fit on the test sample
of the KRR for various hyper-parameters. Except for obvious overfitting in the case
of $\theta=10^3$ for $n=150$ and occasionally for low regularization $\lambda=10^{-6}$,
the overall fit is adequate. The input of the test sample in question is $X^*\subseteq \Xcal$
constructed as
\begin{equation*}
  X^* = \bigl\{(\frac{2}{N} i, \frac{2}{N} j)\,:\,i,j = -\frac{N}{2}, \ldots, \frac{N}{2}\bigr\} \,,
\end{equation*}
for $N=50$.
\begin{table}
  \centering
  \caption{``RMSE-std'' ratio averaged across $L=25$ replications of the train sample choice.}
  \label{tab:nongaussian_2d_gpr_fit}
  \begin{tabular}{ll||rrrr|rrrr}
  \toprule
       & $f(\cdot)$ &       ``f2'' &          &          &          &       ``f5'' &          &          &          \\\cline{2-2}
       & $\gamma$ & $10^{-6}$ &          & $10^{-1}$ &          & $10^{-6}$ &          & $10^{-1}$ &          \\\cline{2-2}
       & $\lambda$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ \\\cline{2-2}
  $n$ & $\theta$ &          &          &          &          &          &          &          &          \\
  \midrule
  $150$  & $10^1$ &     0.23 &     0.18 &     2.81 &     0.34 &     4.09 &     0.47 &     3.47 &     0.50 \\
       & $10^2$ &     0.55 &     0.57 &     0.68 &     0.62 &     0.75 &     0.76 &     0.80 &     0.76 \\
       & $10^3$ &     1.00 &     1.01 &     1.00 &     1.01 &     1.18 &     1.19 &     1.18 &     1.19 \\
       & $\hat{\theta}_\text{ML}$ &     0.14 &     0.20 &     0.66 &     0.38 &     0.65 &     0.48 &     0.73 &     0.50 \\
  \midrule
  $1500$ & $10^1$ &     0.02 &     0.07 &     0.31 &     0.26 &     0.31 &     0.34 &     0.36 &     0.37 \\
       & $10^2$ &     0.02 &     0.05 &     3.11 &     0.29 &     1.67 &     0.27 &     2.54 &     0.31 \\
       & $10^3$ &     0.53 &     0.56 &     0.71 &     0.60 &     0.66 &     0.70 &     0.72 &     0.70 \\
       & $\hat{\theta}_\text{ML}$ &     0.01 &     0.03 &     0.72 &     0.26 &     0.46 &     0.27 &     0.67 &     0.31 \\
  \bottomrule
  \end{tabular}
\end{table}

Tables~\ref{tab:nongaussian_f2_2d_cov_gpr} and~\ref{tab:nongaussian_f5_2d_cov_gpr}
show the empirical error rate of the GPR Bayesian confidence interval on a fixed
test sample. For large train size $n$ the ML estimate of the kernel precision parameter
$\theta$ yields conservatively valid GPR prediction confidence regions for both
``f2'' and ``f5'' test functions. Note that for ``f2'' test function with high noise
$\gamma=10^{-1}$ the empirical error rate of GPR confidence intervals for the ML
estimate of $\theta$ becomes closer to the requires significance level. This is
due to the smoothness of the used function. In contrast, the ``f5'' function is
discontinuous, and thus cannot be approximated well (as seen in tab.~\ref{tab:nongaussian_2d_gpr_fit}).
Therefore, the GPR confidence intervals are more conservative for large $n$.
\begin{table}
  \centering
  \caption{The empirical error rate ($\%$) of the GPR confidence interval for the
  ``f2'' test function.}
  \label{tab:nongaussian_f2_2d_cov_gpr}
  \begin{tabular}{ll||rrrr|rrrr}
  \toprule
       & $n$ & $150$ &          &           &          & $1500$ &          &           &          \\\cline{2-2}
       & $\gamma$ & $10^{-6}$ &          & $10^{-1}$ &          & $10^{-6}$ &          & $10^{-1}$ &          \\\cline{2-2}
       & $\lambda$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ \\\cline{2-2}
  $\theta$ & $\alpha(\%)$ &          &          &          &          &          &          &          &          \\
  \midrule
   $10^1$ & $1\%$ &     12.2 &      1.9 &     20.8 &      1.6 &      2.3 &      1.9 &      2.2 &      1.1 \\
        & $5\%$ &     16.9 &      3.0 &     29.7 &      4.8 &      3.2 &      3.0 &      7.9 &      4.9 \\
        & $10\%$ &     20.6 &      3.8 &     35.9 &      8.9 &      4.0 &      3.7 &     13.8 &      9.7 \\
        & $25\%$ &     29.6 &      5.5 &     48.5 &     22.4 &      5.8 &      5.6 &     29.9 &     24.3 \\
  \midrule
   $10^2$ & $1\%$ &      0.3 &      0.2 &      2.8 &      0.2 &      0.3 &      0.0 &     19.1 &      1.3 \\
        & $5\%$ &      2.3 &      2.2 &      5.0 &      2.2 &      0.6 &      0.1 &     28.6 &      5.9 \\
        & $10\%$ &      4.6 &      4.8 &      7.9 &      5.2 &      0.9 &      0.2 &     35.0 &     11.1 \\
        & $25\%$ &     14.1 &     13.7 &     17.6 &     16.7 &      2.6 &      1.2 &     48.3 &     26.5 \\
  \midrule
   $10^3$ & $1\%$ &      0.0 &      0.0 &      0.2 &      0.1 &      0.1 &      0.1 &      2.4 &      0.1 \\
        & $5\%$ &      4.1 &      4.4 &      4.1 &      3.8 &      1.9 &      2.1 &      4.0 &      1.9 \\
        & $10\%$ &     10.3 &     10.7 &     10.1 &      9.8 &      4.1 &      4.5 &      6.0 &      5.0 \\
        & $25\%$ &     26.1 &     26.5 &     25.9 &     25.5 &     12.9 &     13.4 &     13.9 &     16.4 \\
  \midrule
   $\hat{\theta}_\text{ML}$ & $1\%$ &      5.1 &      1.9 &      4.1 &      2.3 &      3.4 &      0.6 &      2.0 &      1.1 \\
        & $5\%$ &      7.1 &      3.0 &      7.1 &      5.3 &      4.4 &      1.1 &      3.9 &      5.1 \\
        & $10\%$ &      8.7 &      3.9 &     10.5 &      9.1 &      5.2 &      1.4 &      6.9 &     10.0 \\
        & $25\%$ &     13.1 &      6.2 &     21.6 &     21.4 &      7.1 &      2.4 &     18.1 &     24.9 \\
  \bottomrule
  \end{tabular}
\end{table}
\begin{table}
  \centering
  \caption{The empirical error rate ($\%$) of the GPR confidence interval for the
  ``f5'' test function.}
  \label{tab:nongaussian_f5_2d_cov_gpr}
  \begin{tabular}{ll||rrrr|rrrr}
  \toprule
       & $n$ & $150$ &          &        &          & $1500$ &          &        &          \\\cline{2-2}
       & $\gamma$ & $10^{-6}$ &          & $10^{-1}$ &          & $10^{-6}$ &          & $10^{-1}$ &          \\\cline{2-2}
       & $\lambda$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ \\\cline{2-2}
  $\theta$ & $\alpha(\%)$ &          &          &          &          &          &          &          &          \\
  \midrule
  $10^1$ & $1\%$ &     19.2 &      4.2 &     23.8 &      4.1 &      4.5 &      4.1 &      3.8 &      3.7 \\
       & $5\%$ &     25.7 &      7.2 &     32.6 &      7.5 &      6.9 &      6.9 &      6.3 &      6.5 \\
       & $10\%$ &     30.4 &      9.7 &     38.8 &     10.1 &      8.7 &      9.2 &      8.4 &      8.7 \\
       & $25\%$ &     40.9 &     15.4 &     51.4 &     17.3 &     12.9 &     13.9 &     15.4 &     14.4 \\
  \midrule
  $10^2$ & $1\%$ &      1.6 &      0.8 &      2.4 &      0.8 &     10.2 &      3.2 &     15.7 &      2.7 \\
       & $5\%$ &      3.9 &      2.9 &      5.1 &      2.9 &     13.8 &      4.7 &     23.9 &      4.3 \\
       & $10\%$ &      6.4 &      5.9 &      7.9 &      5.5 &     16.4 &      5.9 &     30.0 &      5.9 \\
       & $25\%$ &     15.4 &     15.8 &     17.8 &     15.7 &     22.2 &      8.7 &     42.7 &     12.3 \\
  \midrule
  $10^3$ & $1\%$ &      0.0 &      0.0 &      0.1 &      0.0 &      0.6 &      0.4 &      1.6 &      0.4 \\
       & $5\%$ &      2.2 &      2.0 &      2.3 &      2.6 &      2.0 &      2.3 &      3.3 &      2.0 \\
       & $10\%$ &      7.7 &      7.4 &      7.6 &      8.2 &      4.4 &      4.8 &      5.7 &      4.6 \\
       & $25\%$ &     26.3 &     25.7 &     26.3 &     27.2 &     13.3 &     13.9 &     14.9 &     14.0 \\
  \midrule
  $\hat{\theta}_\text{ML}$ & $1\%$ &      5.1 &      4.5 &      5.0 &      3.8 &      3.2 &      3.7 &      2.5 &      3.3 \\
       & $5\%$ &      8.0 &      7.7 &      8.5 &      7.2 &      4.6 &      5.7 &      4.4 &      5.4 \\
       & $10\%$ &     10.7 &     10.3 &     12.0 &     10.0 &      5.9 &      7.1 &      6.6 &      7.0 \\
       & $25\%$ &     18.4 &     16.6 &     22.6 &     17.0 &      9.7 &     10.2 &     15.3 &     12.6 \\
  \bottomrule
  \end{tabular}
\end{table}

The performance of the conformal confidence predictions are summarized in tables~\ref{tab:nongaussian_f2_2d_cov_conf}
and~\ref{tab:nongaussian_f5_2d_cov_conf} using the MAD metric introduced earlier.
Overall the error rates do not sway too far from the stated levels, and the conformal
procedure yields intervals less sensitive to the hyper-parameters related to the KRR,
which is expected, since the conservative validity of the conformal prediction should
not be affected by the internals of the used non-conformity measure.

\begin{table}
  \centering
  \caption{The maximal absolute deviation $\mathtt{MAD}(\Gamma, A; \Theta)$ ($\%$)
  of the empirical error rate from the theoretical significance level of conformal
  confidence regions for the ``f2'' test function.}
  \label{tab:nongaussian_f2_2d_cov_conf}
  \begin{tabular}{ll||rrrr|rrrr}
  \toprule
       & $n$ &     $150$  &          &          &          &     $1500$ &          &          &          \\\cline{2-2}
       & $\gamma$ & $10^{-6}$ &          & $10^{-1}$ &          & $10^{-6}$ &          & $10^{-1}$ &          \\\cline{2-2}
       & $\lambda$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ \\\cline{2-2}
  type & $\theta$ &          &          &          &          &          &          &          &          \\
  \midrule
  CRR & $10^1$ &      1.9 &      1.3 &      2.0 &      1.6 &      1.0 &      1.3 &      1.2 &      0.2 \\
       & $10^2$ &      1.1 &      1.0 &      1.9 &      2.9 &      0.7 &      2.7 &      1.4 &      0.6 \\
       & $10^3$ &      1.4 &      1.2 &      1.1 &      1.0 &      0.3 &      1.1 &      1.0 &      0.1 \\
       & $\hat{\theta}_\text{ML}$ &      2.4 &      1.0 &      3.4 &      1.0 &      1.4 &      2.2 &      0.6 &      0.5 \\
  \midrule
  CRR-loo & $10^1$ &      1.0 &      1.4 &      1.0 &      1.4 &      0.8 &      1.4 &      1.3 &      0.2 \\
       & $10^2$ &      1.0 &      1.0 &      1.4 &      1.0 &      3.0 &      3.2 &      1.9 &      0.5 \\
       & $10^3$ &      1.5 &      1.3 &      1.2 &      1.1 &      1.0 &      1.0 &      0.2 &      0.2 \\
       & $\hat{\theta}_\text{ML}$ &      1.4 &      1.0 &      1.2 &      1.5 &      2.6 &      2.6 &      0.5 &      0.2 \\
  \midrule
  RRCM & $10^1$ &      1.2 &      0.9 &      1.3 &      1.0 &      0.9 &      1.4 &      1.2 &      0.2 \\
       & $10^2$ &      1.3 &      1.5 &      1.5 &      2.4 &      0.7 &      2.6 &      1.3 &      0.6 \\
       & $10^3$ &      1.9 &      0.9 &      0.4 &      0.6 &      0.4 &      0.5 &      0.9 &      0.3 \\
       & $\hat{\theta}_\text{ML}$ &      1.7 &      0.8 &      2.9 &      1.2 &      1.3 &      2.3 &      0.8 &      0.4 \\
  \midrule
  RRCM-loo & $10^1$ &      1.5 &      0.5 &      0.4 &      1.0 &      0.8 &      1.6 &      1.2 &      0.3 \\
       & $10^2$ &      0.8 &      0.5 &      0.7 &      0.5 &      3.0 &      3.2 &      2.0 &      0.6 \\
       & $10^3$ &      1.9 &      0.8 &      0.7 &      0.9 &      0.9 &      0.6 &      0.2 &      0.2 \\
       & $\hat{\theta}_\text{ML}$ &      0.6 &      1.4 &      1.0 &      2.1 &      2.6 &      2.6 &      0.7 &      0.1 \\
  \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{The maximal absolute deviation $\mathtt{MAD}(\Gamma, A; \Theta)$ ($\%$)
  of the empirical error rate from the theoretical significance level of conformal
  confidence regions for the ``f5'' test function.}
  \label{tab:nongaussian_f5_2d_cov_conf}
  \begin{tabular}{ll||rrrr|rrrr}
  \toprule
       & $n$ &     $150$  &          &          &          &     $1500$ &          &          &          \\\cline{2-2}
       & $\gamma$ & $10^{-6}$ &          & $10^{-1}$ &          & $10^{-6}$ &          & $10^{-1}$ &          \\\cline{2-2}
       & $\lambda$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ & $10^{-6}$ & $10^{-1}$ \\\cline{2-2}
  type & $\theta$ &          &          &          &          &          &          &          &          \\
  \midrule
  CRR & $10^1$ &      2.3 &      2.5 &      1.0 &      1.6 &      1.2 &      0.6 &      1.3 &      0.8 \\
       & $10^2$ &      1.0 &      1.0 &      1.0 &      1.0 &      1.1 &      1.3 &      1.2 &      0.4 \\
       & $10^3$ &      1.0 &      1.1 &      1.3 &      1.1 &      0.2 &      0.8 &      0.6 &      0.3 \\
       & $\hat{\theta}_\text{ML}$ &      2.4 &      1.0 &      1.1 &      1.4 &      0.3 &      0.3 &      1.0 &      0.8 \\
  \midrule
  CRR-loo & $10^1$ &      2.2 &      1.4 &      1.0 &      1.0 &      0.1 &      0.5 &      0.6 &      0.5 \\
       & $10^2$ &      1.0 &      1.0 &      1.0 &      1.2 &      0.3 &      1.8 &      1.7 &      1.2 \\
       & $10^3$ &      1.0 &      1.2 &      1.4 &      1.0 &      0.4 &      0.8 &      0.3 &      0.5 \\
       & $\hat{\theta}_\text{ML}$ &      1.0 &      1.0 &      1.0 &      1.0 &      1.0 &      0.9 &      0.1 &      0.3 \\
  \midrule
  RRCM & $10^1$ &      1.8 &      1.7 &      0.3 &      1.2 &      1.2 &      0.6 &      1.2 &      0.7 \\
       & $10^2$ &      0.8 &      0.6 &      1.4 &      0.6 &      1.0 &      1.6 &      1.1 &      0.3 \\
       & $10^3$ &      0.5 &      1.1 &      0.3 &      0.3 &      0.2 &      1.0 &      0.5 &      0.5 \\
       & $\hat{\theta}_\text{ML}$ &      1.6 &      0.5 &      0.5 &      0.8 &      0.4 &      0.5 &      0.9 &      0.8 \\
  \midrule
  RRCM-loo & $10^1$ &      1.5 &      0.5 &      0.6 &      0.3 &      0.2 &      0.5 &      0.6 &      0.5 \\
       & $10^2$ &      1.4 &      1.1 &      1.3 &      0.6 &      0.3 &      2.1 &      1.6 &      1.1 \\
       & $10^3$ &      0.2 &      1.1 &      0.5 &      1.1 &      0.8 &      1.3 &      0.4 &      0.6 \\
       & $\hat{\theta}_\text{ML}$ &      1.5 &      0.5 &      2.0 &      0.4 &      0.9 &      1.0 &      0.3 &      0.2 \\
  \bottomrule
  \end{tabular}
\end{table}

Based on the empirical evidence from the $2$-d experiments we arrive at similar
conclusions: \begin{itemize}
  \item the Gaussian Process Regression offers conservatively valid confidence regions
  in the Gaussian case, provided the hyper-parameters are not severely misspecified;
  \item in the non-Gaussian case experimental results regarding the validity of the
  GPR confidence sets are less favourable;
  \item there is empirical evidence in favour of conformal predictions based on
  the kernel ridge regression with Gaussian kernel being asymptotically valid in
  the offline setting;
  \item numerical experiments support the  conjecture that conformal procedures
  are insensitive to the choice of the core NCM: the hyper-parameters of the KRR
  weakly affected the validity of the regions (as measure by MAD metric) in contrast
  to the GPR confidence prediction.
\end{itemize}

% subsection results_2_d (end)

% section numerical_study (end)

\section{Conclusion and further work} % (fold)
\label{sec:conclusion_and_further_work}

In conclusion we summarize the results obtained in section~\ref{sec:numerical_study},
and outline further research on the applicability of the conformal procedures based
on the kernel ridge regression in batch learning setting.

In the non-Gaussian experiments we found empirical evidence suggesting that the GPR
confidence regions are not consistently valid if the Gaussianity assumptions fail
to hold, or if the KRR hyper-parameters are misspecified. Identical experiments on
the conformal procedures demonstrated that indeed, there is empirical evidence in
support of the conjectured conservative validity in the batch setting. At the same
time, confidence intervals of the conformalized kernel ridge regression seem to
perform not worse than the confidence regions of the Gaussian Process regression,
when the Gaussianity assumptions hold.

Further work on the validity of conformal procedures in batch setting, with applications
to anomaly detection, shall include: \begin{itemize}
  \item establishing theoretical foundations for the proposed conjectures for the
  kernel ridge regression with Gaussian kernel, or isolating special cases when it
  holds, and studying the cases when it fails;
  \item obtaining a generalization to the asymptotic efficiency result in \cite{burnaevV14},
  in for a kernel ridge regression with general kernels;
\end{itemize}

However, future research should not be limited to the items, listed above. Below we
briefly outline possible novel application of conformal prediction, that might be
no less interesting than the conformalized kernel ridge regression.

It would be interesting in general to investigate possible benefits conformalization
could provide for such unsupervised machine learning algorithms, as the LOF anomaly
detection method (see sec.~\ref{par:anomaly_detection_based_on_distance_similarity}
of this report, and \cite{breunig2000}), Bayesian PCA \cite{tippnigbishop1999} and
mixtures thereof, and the kernel PCA \cite{hoffmann2007}.

Finally, we would like to outline two potentially interesting research topics: in
sec.~\ref{sub:conformalized_kernel_embeddings} we state the possible use of conformal
prediction in constructing more direct unsupervised anomaly detection method, and
in sec.~\ref{sub:conformal_kernel_precision_selection} we propose a method for selection
of kernel precision based on conformal confidence sets.

\subsection{Conformalized kernel embeddings} % (fold)
\label{sub:conformalized_kernel_embeddings}

It might be fruitful to research conformal procedures the field of kernel embedding
of probability distributions, \cite{smola2007}. Basically in a suitable RKHS many non-
pathological distributions on metric spaces can be identified with a unique element
of said RKHS. In particular, if $K$ is a kernel that induces a universal RKHS $\Hcal$
of functions $\Xcal\mapsto\Real$, then for any probability distribution $P$ on $\Xcal$
there is a unique $\mu_P\in \Hcal$ whenever $\ex_{x\sim P} \sqrt{K(x,x)}$ is finite.
For instance, the Gaussian kernel in eq.~\ref{eq:gauss_kenrel} satisfies this property
and its induced canonical RKHS is universal. Now, the boundedness of $K$ in expectation
implies that $f\mapsto \ex_P f$ is a bounded linear functional in $\Hcal$, whence by
Riesz representation theorem there exists a unique $\mu_P\in \Hcal$ with $\ex_P f
= \langle \mu_P, f\rangle$ for all $f\in \Hcal$. In particular, for $f = k(x, \cdot)$
the reproducing property of $K$ implies that
\begin{equation*}
  \ex_{y\sim P} K(x, y) = \langle \mu_P, K(x, \cdot) \rangle = \mu_P(x) \,,
\end{equation*}
which provides a closed formula for the embedding of $P$.

In \cite{gretton2012}, a powerful two-sample distribution test was constructed, based
on this elegant idea. Essentially, for a compact metric space $\Xcal$ the distributions
$P$ and $Q$ coincide if and only if $\ex_P f = \ex_Q f$ for all continuous and bounded
$f:\Xcal\mapsto \Real$. However, because the RKHS is universal it is enough to check
the equality only in the class of functions from the Hilbert space $\Hcal$. This
reduces the problem of testing to studying the norm between the embeddings $\mu_P$
and $\mu_Q$ in $\Hcal$.

The proposed test statistic, \cite{gretton2012}, the \textit{maximum mean discrepancy}
(MDD), measures the RKHS norm of the deviation of the embeddings of the empirical
distributions. This MDD measure can readily be used as a non-conformity measure
for the following anomaly detection problem: given a sample $Z_{:n-1} = (z_i)_{i=1}^{n-1}$
decide if a new example $z_n$ is anomalous or not by measuring how well it conforms
to $Z$. If the full sample is denoted by $Z_{:n} = (z_i)_{i=1}^n$, then the proposed
conformal procedure would be based on the following NCM:
\begin{equation*}
  A(Z_{-i:n}, z_i) = \| \mu_{\hat{P}_{-i:n}} - \mu_{\hat{P}_{:n}} \|_{\Hcal} \,,
\end{equation*}
where $\hat{P}_{-i:n}$ and $\hat{P}_{:n}$ are empirical distributions over $Z_{-i:n}$
and $Z_{:n}$, respectively, with $Z_{:n-1}$ being the sample $Z_{:n}$ without the
$i$-th observation. Note, that it would be necessary to consider only vector-matrix
multiplications, since for any sub-sample $S$ of $Z_{:n}$ the empirical distribution
embedding is given by
\begin{equation*}
  \mu_{\hat{P}_S}(\cdot) = (\one_m'\one_m)^{-1} \one_m' k_S(\cdot) \,,
\end{equation*}
where $m = |S|$, $k_S(\cdot) = (K(z_i, \cdot))_{i\in S} \in \Hcal^{m\times 1}$ is
the sample evaluation vector of the kernel $K$, and $\one_m \in \Real^{m\times 1}$
is the vector of ones. Since the empirical embeddings are elements of the data-driven
pre-Hilbert space, their inner products can be computed from the sample Gram matrix
of $K$. Indeed, using the ordinary formula for sequential update of the sample average
we get
\begin{align*}
  % (n-1)\bigl(\mu_{\hat{P}_{-i:n}} - \mu_{\hat{P}_{:n}}\bigr)
  %   % &= (n-1)^{-1}\sum_{j\neq i} K(z_j, \cdot) - n^{-1}\sum_j K(z_j, \cdot) \\
  %   &= n^{-1} \sum_j K(z_j, \cdot) - K(z_i, \cdot) \,,
  (n-1)^2 \bigl\|\mu_{\hat{P}_{-i:n}} - \mu_{\hat{P}_{:n}} \bigr\|^2_\Hcal
    &= \bigl\| n^{-1} \sum_j K(z_j, \cdot) - K(z_i, \cdot) \bigr\|^2_\Hcal \\
    % &= \bigl\| e_i' (I_n - \one(\one'\one)^{-1}\one') k_Z(\cdot) \bigr\|^2_\Hcal \\
    &= e_i'K_{ZZ} e_i + \frac{1}{n^2} \one' K_{ZZ} \one - \frac{2}{n} \one' K_{ZZ} e_i \,,
    % &= e_i'(I_n - \one(\one'\one)^{-1}\one')K_{ZZ}(I_n - \one(\one'\one)^{-1}\one') e_i \,,
\end{align*}
where $\one$ is the vector of $1$'s and $e_i$ is the unit vector both of appropriate
dimensions. This is basically the distance of the $i$-th canonical feature map from
the centre of dataset in $\Hcal$.

Conformalization of this procedure would most likely proceed in the direction of
using eq.~\ref{eq:conf_p_value} as a measure of the degree of abnormality of new
examples. Furthermore, it seems that in low dimensional cases there might exist an
efficient approximation procedure that could yield multivariate quantiles or even
confidence sets based on inverting hypothesis test in the conformal prediction.

% subsection conformalized_kernel_embeddings (end)

\subsection{Conformal kernel precision selection} % (fold)
\label{sub:conformal_kernel_precision_selection}

Yet another research direction, which is intimately connected to both the conformalized
ridge regression and kernel embeddings, is concerned with designing a non-parametric
procedure, that could yield estimates of the kernel precision parameter based on
conformal confidence intervals, for example, similar in spirit to the central idea
of \cite{goldenshluger1997}. The authors consider the problem of restoring $f:[0,1]
\mapsto\Real$ in eq.~\ref{eq:signal_model} from observations contaminated by an i.i.d
sub-Gaussian noise, and propose a method to select a locally optimal kernel bandwidth
(the reciprocal of the precision $\theta$) based on approximate minimization of the
size of the confidence region for a true value of $f$ with fixed coverage rate. They
propose to approximate the optimal bandwidth with the largest bandwidth, for which
all confidence regions corresponding to lower bandwidths overlap.

In particular they show that in their setting the confidence regions for $f(x_0)$
have error rate at most $e^{- c t^2}$ for $c = \mathcal{O}(n)$, $n$ is the train
sample size, and are of the form
\begin{equation*}
  \bigl| f(x_0) - \hat{f}_\delta(x_0) \bigr|
    \leq \omega_f(x_0, \delta) + \frac{t}{\sqrt{\delta}} \,,
\end{equation*}
where $\omega_f(x_0, \delta) = \sup_{x\in[0,1]} |f(x)-f(x_0)|$ is the ``deterministic
dynamic error'', $\frac{t}{\sqrt{\delta}}$ is an upper bound for the ``stochastic error'',
and $\delta$ is the bandwidth of the kernel estimator $\hat{f}_\delta(\cdot)$. If
$f$ were known, the optimal $\delta^*$ would have been given by
\begin{equation*}
  \delta^*
    = \sup\bigl\{ \delta > 0 \, : \,
        \omega_f(x_0, \delta) \leq \frac{t}{\sqrt{\delta}} \bigr\}\,,
\end{equation*}
but, since $f$ is unknown, authors propose the ``intersection'' estimate:
\begin{equation*}
  \delta^+
    = \sup\{ \eta > 0 \, : \, \emptyset \neq 
        \cap_{\delta\leq \eta} [\hat{f}_\delta(x_0) - 2 \frac{t}{\sqrt{\delta}},
                                \hat{f}_\delta(x_0) + 2 \frac{t}{\sqrt{\delta}}] \} \,,
\end{equation*}
which they show makes the approximation error lie within acceptable bounds with high
probability: at least $ 1 - n e^{-c(n) t^2}$.

The main obstacle in their method is that the deterministic dynamic error depends
on the unknown $f$, which is why direct minimization of the width of the confidence
interval was not feasible. In contrast to their situation, conformal confidence
intervals have a clear and known width and fixed coverage guarantees, which means
that we can directly minimize the width to select the optimal bandwidth. Possible
challenge in this research might be selecting and justifying the aggregation method,
used to move from local bandwidth estimates to global ones.

% subsection conformal_kernel_precision_selection (end)

% section conclusion_and_further_work (end)

% \bibliographystyle{amsplain}
\clearpage
\bibliographystyle{ugost2008ls}
\bibliography{references}

\end{document}

