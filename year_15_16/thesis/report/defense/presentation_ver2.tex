% Этот шаблон документа разработан в 2014 году
% Данилом Фёдоровых (danil@fedorovykh.ru) 
% для использования в курсе 
% <<Документы и презентации в \LaTeX>>, записанном НИУ ВШЭ
% для Coursera.org: http://coursera.org/course/latex .
% Исходная версия шаблона --- 
% https://www.writelatex.com/coursera/latex/5.1

\documentclass[t]{beamer}  % [t], [c], или [b] --- вертикальное выравнивание на слайдах (верх, центр, низ)
%\documentclass[handout]{beamer} % Раздаточный материал (на слайдах всё сразу)
%\documentclass[aspectratio=169]{beamer} % Соотношение сторон

%\usetheme{Berkeley} % Тема оформления
%\usetheme{Bergen}
%\usetheme{Szeged}

%\usecolortheme{beaver} % Цветовая схема
%\useinnertheme{circles}
\useinnertheme{rectangles}

% \usetheme{HSE}
\usepackage{HSE-theme/beamerthemeHSE-en}

\usepackage{cmap}         % поиск в PDF
\usepackage{mathtext}         % русские буквы в формулах
\usepackage[T2A]{fontenc}     % кодировка
\usepackage[utf8]{inputenc}     % кодировка исходного текста
\usepackage[english]{babel} % локализация и переносы

% \newtheorem{rtheorem}{Теорема}
% \newtheorem{rproof}{Доказательство}
% \newtheorem{rexample}{Пример}

\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
% \usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление
\usepackage{mathptmx}
\usepackage{algorithm2e}

%% Номера формул
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
%\usepackage{leqno} % Нумерация формул слева

%% Свои команды
% \DeclareMathOperator{\sgn}{\mathop{sgn}}

%% Перенос знаков в формулах (по Львовскому)
% \newcommand*{\hm}[1]{#1\nobreak\discretionary{}
% {\hbox{$\mathsurround=0pt #1$}}{}}

\usepackage{graphicx}  % Для вставки рисунков
% \graphicspath{{images/}{images2/}}  % папки с картинками
% \setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
% \setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
\usepackage{wrapfig} % Обтекание рисунков текстом
\usepackage{subcaption}
\usepackage{caption}

\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
% \usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

% \usepackage{etoolbox} % логические операторы

\usepackage{lastpage} % Узнать, сколько всего страниц в документе.
\usepackage{soul} % Модификаторы начертания
\usepackage{csquotes} % Еще инструменты для ссылок
\usepackage[style=alphabetic,backend=biber,sorting=nty]{biblatex}

\bibliography{../references}

% \usepackage[style=authoryear,maxcitenames=2,backend=bibtex,sorting=nty]{biblatex}
\usepackage{multicol} % Несколько колонок

% \usepackage{tikz} % Работа с графикой
% \usepackage{pgfplots}
% \usepackage{pgfplotstable}

% \usepackage{fontspec}
% \defaultfontfeatures{Ligatures={TeX},Renderer=Basic}
% \setmainfont[Ligatures={TeX,Historic}]{Myriad Pro} % install Myriad Pro or replace with Arial
% \setsansfont{Myriad Pro}  % install Myriad Pro or replace with Arial
% \setmonofont{Courier New}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\ex}{\mathop{\mathbb{E}}\nolimits}
\newcommand{\pr}{\mathop{\mathbb{P}}\nolimits}

\title{Conformal methods in multidimensional linear models and anomaly detection}
% \title[Short title]{Presentation Title} 
% \subtitle{Presentation Subtitle or Conference Title}
\author[Nazarov Ivan]{Nazarov Ivan}
% \author[Author's name]{Author's name \\ \smallskip \scriptsize \url{author@hse.ru}\\\url{http://hse.ru/en/staff/author/}}
\date{\today}
\institute[Higher School of Economics]{National Research University \\ Higher School of Economics}


\begin{document}
\selectlanguage{english}
\frame[plain]{\titlepage} % Титульный слайд

\section{Introduction} % (fold)
\label{sec:introduction}

\begin{frame}[c]\frametitle{\insertsection}
  \begin{block}{Function approximation}
    Recover a faithful approximation $\hat{f}_{:n}$ of $f:\Xcal \mapsto \Real$, $\Xcal\subset \Real^{p\times 1}$,
    from a sample of noisy observations $Z_{:n} = (x_i, y_i)_{i=1}^n \sim \Dcal$ with
    $$ y_i = f(x_i) + \epsilon_i \,, $$
    and $\ex(\epsilon_i|x_i) = 0$.
  \end{block}

  \begin{block}{Confidence prediction}
    concerned with learning a ``well behaved'' point-set map $x \mapsto \hat{C}_{:n}^\alpha(x)$
    such that
    $$ \pr_\Dcal\bigl(y_x \notin \hat{C}_{:n}^\alpha(x)\bigr) \approx \alpha \,. $$
  \end{block}
\end{frame}

\subsection{Importance of confidence prediction} % (fold)
\label{sub:importance_of_cp}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{itemize}
    \item anomaly detection (batch setting): learn a confidence region for observations,
    to discriminate ``inliners'' from ``outliers'';
    \vspace{\baselineskip}
    \item forecasting (online setting): based on the dynamic model of a time series
    yield a predictive region with specified confidence level;
    \vspace{\baselineskip}
    \item function approximation (batch setting): assess the bounds for the approximation
    error of the unknown function $f$;
  \end{itemize}
  % All these models also usually rely on some predictive model;
\end{frame}

% subsection importance_of_cp (end)

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Typical approach}
  Linear regression or Kernel Ridge Regression (KRR):
  $$ \|y - K\beta \|^2 + \lambda \beta' K \beta \to \min_\beta \,, $$
  $y\in \Real^{n\times1}$, $\lambda > 0$, $K$ -- sample Gram matrix of kernel
  $K(\cdot,\cdot)$ on inputs $x_i$.
  \vspace{\baselineskip}
  \begin{description}
    \item[prediction] $\hat{y}(x) = \kappa_x' \hat{\beta} = \kappa_x' Q y$;
    \item[loo-cv] $\hat{r}^{\text{loo}}_i = \frac{h(x_i)}{\lambda} e_i'(y - K Q y)$;
  \end{description}
  with $h(x) = \lambda + K(x,x) - \kappa_x' Q \kappa_x$, $Q = (\lambda I_n + K)^{-1}$ and
  $\kappa_x$ -- $n\times 1$ vector of canonical feature maps.
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Bayesian KRR}
  \begin{block}{Gaussian Process Regression}  
    Observations are drawn from a Gaussian process with mean $0$ and covariance
    $$ \sigma^2(\lambda\delta_{x,x'} + K(x,x')) \,,$$
    $\sigma^2$ -- variance, $\lambda$ -- noise-to-signal ratio.
  \end{block}
  \vspace{\baselineskip}
  \begin{block}{Forecast distribution}
  Conditional on the observed data $(X, y)$ at a new $x$:
  $$ y_x \sim \Ncal(\kappa_x' Q y, \sigma^2 h(x)) \,. $$
  \end{block}
\end{frame}

% section introduction (end)

\section{Conformal prediction} % (fold)
\label{sec:conformal_prediction}

\begin{frame}[c, shrink=10]
  \frametitle{\insertsection}
  \begin{algorithm}[H]
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{Non-conformity measure $A$, significance level $\alpha \in (0,1)$,
    training sample $Z_{:n} = (x_i, y_i)_{i=1}^n$, a test object $x_{n+1}\in \Xcal$.}
  \Output{Confidence set $\Gamma_{:n}^\alpha(x_{n+1})$ for $y_{n+1}\in \Ycal \subseteq \Real$.}
  \BlankLine
  $\Gamma_{:n}^\alpha \leftarrow \emptyset$\;
  \For{$y \in \Ycal$}{
    $z_{n+1} \leftarrow (x_{n+1}, y)$\;
    \For{$i = 1,\ldots, n, n+1$}{
      $Z_{-i} \leftarrow \bigl(z_j\bigr)_{j=1, j\neq i}^{n+1}$\;
      $\eta_i \leftarrow A(Z_{-i}, z_i)$\;
    }
    $p_{:n+1} \leftarrow (n+1)^{-1} \bigl\lvert \{
        i \,:\, \eta_i \geq \eta_{n+1} \} \bigr\rvert $\;
    \If{$p_{:n+1} > \alpha$}{
      $\Gamma_{:n}^\alpha \leftarrow \Gamma_{:n}^\alpha \cup\{ y\}$\;
    }
  }
  \Return{$\Gamma_{:n}^\alpha$}\;
  \end{algorithm}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  A distribution-free method to construct statistically valid confidence sets for
  predictions made by Machine Learning algorithms.
  \vspace{\baselineskip}
  \begin{itemize}
    \item can be constructed atop \textbf{any} ML algorithm by incorporating it into
    $A(Z_{:n}, z)$ -- a score reflecting non-conformity of $z$ with respect to sample
    $Z_{:n}$;
    \vspace{\baselineskip}
    \item Asymptotic validity guarantees for i.i.d. observations in online setting
    established in \cite{vovk2005}:
    $$ \pr\bigl( y_{n+1} \notin \hat{\Gamma}_{:n}^\alpha(x_{n+1}) \bigr) \leq \alpha \,. $$
  \end{itemize}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \begin{itemize}
    \item Efficiency (e.g. width of prediction confidence intervals) depends on the
    non-conformity measure $A$ and model assumptions;
    \vspace{\baselineskip}
    \item Asymptotic efficiency in the batch setting for linear ridge regression shown
    in \cite{burnaevV14};
    \vspace{\baselineskip}
    \item Little is known about efficiency in the case of nonlinear models;
  \end{itemize}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Non-conformity measures}
  \begin{block}{KRR residuals}  
    The residual of the $i$-th observation in $Z_{:n}\cup\{(x_{n+1}, y_{n+1})\}$
    is a linear function of $n+1$-st target value, $\hat{r}_i(y_{n+1}; Z_{:n}, x_{n+1})$.
  \end{block}

  \begin{description}
    \item[Ridge Regression Confidence Machine] $A(Z_{-i}, z_i) = |\hat{r}_i(y_{n+1}; Z_{:n}, x_{n+1})|$;
    \vspace{\baselineskip}
    \item[Two-sided Conformal Region] \hfill\\
      \textit{upper} $A(Z_{-i}, z_i) = \hat{r}_i(y_{n+1}; Z_{:n}, x_{n+1})$;\hfill\\
      \textit{lower} $A(Z_{-i}, z_i) = -\hat{r}_i(y_{n+1}; Z_{:n}, x_{n+1})$;
  \end{description}

  \begin{block}{Leave-one-out versions}
    $$ \hat{r}^{\text{loo}}_i(y_{n+1}; Z_{:n}, x_{n+1})
      = \frac{h(x_i)}{\lambda} \hat{r}_i(y_{n+1}; Z_{:n}, x_{n+1}) \,. $$
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \begin{block}{Conformal p-value}
    $$ p_{Z_{:n}}(y_{n+1})
          = (n+1)^{-1} \bigl\lvert\{ i \,:\, \eta_i(y_{n+1}) \geq \eta_{n+1}(y_{n+1}) \}\bigr\rvert
      \,, $$
    for $\eta_i(y_{n+1}) = A(Z_{-i}, z_i)$.
  \end{block}
  \vspace{\baselineskip}
  \vspace{\baselineskip}
  \begin{block}{Conformal confidence set}
    $$ \hat{\Gamma}_{Z_{:n}}^\alpha(x_{n+1})
        = \bigl\{ y_{n+1}\in \Ycal \,:\, p_{Z_{:n}}(y_{n+1}) \geq \alpha \bigr\}
      \,. $$
  \end{block}
\end{frame}

\subsection{Ridge Regression Confidence Machine} % (fold)
\label{sub:ridge_regression_confidence_machine}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{Exhaustive search over $y\in \Real$ is not necessary}
    Regression residuals are special: each non-empty regions
    $S_i = \{y_{n+1}\in \Ycal \,:\, \eta_i(y_{n+1}) \geq \eta_i(y_{n+1})\}$
    is either \begin{itemize}
      \item $(-\infty, a_i]$ or $[b_i, +\infty)$, or union;
      \item $[a_i, b_i]$;
      \item $\Real$, or $\{a_i\}$;
    \end{itemize}
    \vspace{\baselineskip}
    The set $\hat{\Gamma}_{Z_{:n}}^\alpha(x_{n+1})$ is a union of closed intervals
    $[a, b]\cup\Real$ covered by at least $\alpha n$ regions $S_i$, with $a\geq b$
    in the set of endpoints of every $S_i$;
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
\framesubtitle{\insertsubsection}
  Fixed requirements per sample $Z_{:n}$: \begin{itemize}
    \item Initialization: $\Ocal(n^3)$ for inversion of the regularized Gram matrix;
    \item Memory complexity: $\Ocal(n^2)$ to store the inverse;
  \end{itemize}
  \vspace{\baselineskip}
  Requirements for each $x_{n+1}$:
  \begin{itemize}
    \item Memory complexity: $\Ocal(n)$ to store endpoints;
    \item Runtime complexity: \begin{itemize}
      \item $\Ocal(n^2)$ for kernel and matrix multiplication;
      \item $\Ocal(n \log n)$ for interval construction;
      \item $\Ocal(n)$ for pruning;
    \end{itemize}
  \end{itemize}
\end{frame}

% subsection ridge_regression_confidence_machine (end)

\subsection{Two-sided conformal region} % (fold)
\label{sub:two_sided_conformal_region}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  Same construction, less endpoints: each non-empty $S_i$ is \begin{itemize}
    \item $(-\infty, a_i]$ or $[a_i, +\infty)$;
    \item $\Real$.
  \end{itemize}
  \vspace{\baselineskip}
  \begin{block}{Specifics}
    must intersect the \textit{upper} interval $\hat{\Gamma}_{Z_{:n}}^{\frac{\alpha}{2}, U}(x_{n+1})$
    and the \textit{lower} interval $\hat{\Gamma}_{Z_{:n}}^{\frac{\alpha}{2}, L}(x_{n+1})$, which
    takes $\Ocal(1)$ time;
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  General $\hat{\Gamma}_{Z_{:n}}^\alpha(x_{n+1})$ is not necessarily connected;
  \vspace{\baselineskip}
  \begin{block}{Conjecture \# 1}
    For any sequence $(x_i)_{i=1}^n\in \Xcal$ and kernel $K(\cdot, \cdot)$, such
    that for any $f$ in the canonical RKHS of $K$
    $$ f(x_{n+1})^2 \leq \sum_{i=1}^n f(x_n)^2 + \lambda \|f\|^2 \,, $$
    it is true, that all $S_i$ for upper and lower NCM are of the form $(-\infty, a_i]$
    and $[a_i, +\infty)$ respectively.s
  \end{block}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{Conjecture \# 2}
    Condition of conjecture \#1 is satisfied when
    $$ K(x_{n+1}, x_{n+1}) - \kappa_{:n}(x_{n+1})' K^{-1}_{:n} \kappa_{:n}(x_{n+1})
      < \lambda \,, $$
    for all sufficiently large $n\geq 1$.
  \end{block}
\end{frame}

% subsection two_sided_conformal_region (end)

\subsection{Preliminary results} % (fold)
\label{sub:preliminary_results}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{So far:}
  \begin{itemize}
    \item an efficient polynomial-time algorithm for construction of conformal
    confidence regions;
    \item a sufficient condition for regularity of two-sided intervals for KRR
    (extends \cite{burnaevV14});
  \end{itemize}
  \end{block}
  \begin{block}{Next step}
    compare the conformal confidence region to the Bayesian confidence interval.
  \end{block}
\end{frame}

% subsection preliminary_results (end)

% section conformal_prediction (end)

\section{Numerical study} % (fold)
\label{sec:Numerical_study}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Bayesian interval}
  Gaussian Process Regression readily yields confidence intervals.
  \vspace{\baselineskip}
  \begin{block}{Bayesian predictions}
    $$ \hat{y}_{:n}(x) = \kappa_x' (\lambda I_n + K)^{-1} y_{:n} \,, $$
    where $\kappa_x = (K(x_i, x))_{i=1}^n$.
  \end{block}

  \begin{block}{Prediction confidence interval}
    $$ \frac{y_{n+1} - \hat{y}_{:n}(x_{n+1})}{\sigma \sqrt{h(x_{n+1})}} \in
      \bigl[- z_{1-\frac{\alpha}{2}}, z_{1-\frac{\alpha}{2}} \bigr] \,, $$
    where $ h(x) = \lambda + K(x,x) - \kappa_x' (\lambda I_n + K)^{-1} \kappa_x$,
    and $z_\alpha$ is the $\alpha$ quantile of the standard Normal distribution.
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  In the study we consider kernel ridge regression with the Gaussian kernel:
  $$ K(x,x') = \text{exp}\bigl\{ -\theta \|x-x'\|^2 \bigr\} \,. $$
  \begin{block}{Objectives}
    \begin{itemize}
      \item verify validity guaranties of conformal confidence regions for KRR in
      the batch learning setting;
      \item measure efficiency of conformal confidence regions under Gaussian
      assumptions.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \begin{block}{Experimental setup}
  \end{block}
  \begin{itemize}
    \item Test functions on compact $1$-d and $2$-d domains; \begin{itemize}
      \item a piecewise continuous;
      \item a piecewise smooth;
      \item a sample path of a Gaussian process;
    \end{itemize}
    \vspace{\baselineskip}
    \item Low or high Gaussian noise;
    \item KRR hyper-parameters: regularization $\lambda$, kernel precision $\theta$;
    \item Train sample size $n$;
  \end{itemize}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Experimental setup}
  \begin{enumerate}
    \item Draw a large random uniform sample from $\Xcal$ to serve as a train set;
      Use a regular grid as a test sample;
    \item Generate a large sample path over the pooled sample;
    % \vspace{\baselineskip}
    \item For each setting of hyper-parameters make $M$ replications: \begin{enumerate}
      \item pick a random sub-sample of specified size from the train sample;
      \item construct confidence regions (conformal, Bayesian) for the test sample;
      \item compute the coverage rate across the test sample and region size for each test object;
    \end{enumerate}
  \end{enumerate}
\end{frame}

\subsection{Results} % (fold)
\label{sub:results}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Example Bayesian and conformal (RRCM) confidence regions}
  \begin{figure}%[t, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.9\linewidth]{../images/output_pdf/profile/gaussian/0.1_0.1/50/profile_gaussian_0,1_0,1_100_5p-GPR_50.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.9\linewidth]{../images/output_pdf/profile/gaussian/0.1_0.1/50/profile_gaussian_0,1_0,1_100_5p-RRCM_50.pdf}
    \end{subfigure}
    \caption{Example Bayesian and conformal (RRCM) confidence regions.
      \textit{Upward} triangles indicate the $5\%$ sample quantile;
      \textit{downward} triangles indicate the maximal width.}
    \label{fig:gauss_1d_prof_gpr}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Asymptotic coverage rate in the Gaussian case with high noise}
  \begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/GPR-f/coverage_gaussian_0,1_1e-06_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM/coverage_gaussian_0,1_1e-06_auto_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/GPR-f/coverage_gaussian_0,1_0,1_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM/coverage_gaussian_0,1_0,1_auto_RRCM.pdf}
    \end{subfigure}\\
    % \label{fig:gaussian_1d_high_noise}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Typical confidence region widths in the Gaussian case with high noise}
  \begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/GPR-f/width_gaussian_0,1_1e-06_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/RRCM/width_gaussian_0,1_1e-06_auto_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/width/GPR-f/width_gaussian_0,1_0,1_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM/width_gaussian_0,1_0,1_auto_RRCM.pdf}
    \end{subfigure}\\
    % \label{fig:gaussian_1d_high_noise_width}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Typical coverage rate in non-Gaussian case with low noise}
  \begin{figure}%[b, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/GPR-f/coverage_heaviside_1e-06_1e-06_100_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/RRCM/coverage_heaviside_1e-06_1e-06_100_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/GPR-f/coverage_heaviside_1e-06_0,1_100_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/RRCM/coverage_heaviside_1e-06_0,1_100_RRCM.pdf}
    \end{subfigure}\\
    % \label{fig:heaviside_1d_low_noise}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Typical coverage rate in non-Gaussian case with high noise}
  \begin{figure}%[b, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/GPR-f/coverage_heaviside_0,1_1e-06_10_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/RRCM/coverage_heaviside_0,1_1e-06_10_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/GPR-f/coverage_heaviside_0,1_0,1_10_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/RRCM/coverage_heaviside_0,1_0,1_10_RRCM.pdf}
    \end{subfigure}\\
    % \label{fig:heaviside_1d_high_noise_arb}
  \end{figure}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{Results}
    \begin{itemize}
      \item in all settings conformal confidence regions are asymptotically valid,
      regardless of KRR hyper-parameters, quality of fit or noise-to-signal ratio;
      %% page 34, columns a,c; page 36, columns a,c
      %% non-Gaussian: figures 10-12
      \vspace{\baselineskip}
      \item Gaussian setting with high noise-to-signal: Bayesian intervals are valid,
      conformal regions are asymptotically efficient;
      %% fig 14-17, pp. 42-45;
      %% fig 18-19, p. 46 (double "the", different hyper-parameters)
      %% ``f6'' function is much Gaussian-like; thus the comparison on fig. 22
      \vspace{\baselineskip}
      \item Gaussian setting with low noise-to-signal: Bayesian intervals are
      conservatively valid and wider than conformal intervals;
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{Results}
    \begin{itemize}
      %% fig. 6, 16; fig. 10-13
      \item non-Gaussian settings with low noise: Bayesian intervals fail to have
      correct coverage rate;
      \item Bayesian confidence regions are sensitive to KRR hyper-parameters;
    \end{itemize}
  \end{block}
\end{frame}

% subsection results (end)

\subsection{Implications} % (fold)
\label{sub:implications}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  By design, conformal prediction procedure is oblivious to the nature of
  the non-conformity measure.

  \vspace{\baselineskip}
  Gathered empirical evidence is sufficient to \begin{itemize}
    % for validity and efficiency of the KRR conformal confidence regions in batch setting
    \item warrant further theoretical research in asymptotic properties of conformal
    kernel ridge regression predictions;
    \item recommend the use of conformal confidence regions in applied prediction
    tasks when the assumption of i.i.d. observations is reasonable;
  \end{itemize}
\end{frame}

% subsection implications (end)

% section Numerical_study (end)

\section{Further development beyond kernel ridge regression} % (fold)
\label{sec:further_development_beyond_krr}

\subsection{Kernel bandwidth selection} % (fold)
\label{sub:kernel_bandwidth_selection}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{Possible approaches}
  \begin{itemize}
    \item full Bayesian assumptions: use MLE of $\theta$;
    \item sub-Gaussian noise: use local spatially adaptive window estimate of
      \cite{goldenshluger1997} based on approximate confidence interval;
    \item model-free: use conformal confidence regions;
  \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{Conformal Bandwidth Selection}
    Conformal confidence sets have coverage guarantees;
    \begin{itemize}
      \item define the reasonable interval of kernel widths (reciprocal of $\theta$);
      \item construct confidence region for $f(x_0)$ for each width;
      \item use the minimizer the size of the confidence region as a locally optimal
      bandwidth;
    \end{itemize}
  \end{block}

  \vspace{\baselineskip}
  \begin{block}{Challenge}
    Move from local to global bandwidth estimate: \begin{itemize}
      \item Proposition: take the median of the locally optimal precision
      $\theta$ (similarly, bandwidth).
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Selected precision $\hat{\theta}$ for a Gaussian process}
  \begin{figure}%[b, width=0.45\textwidth]
    \centering
    \includegraphics[width=0.95\linewidth]{cbs_images/gaussian_01_100_0,5.pdf}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Selected precision $\hat{\theta}$ for a Gaussian process}
  \begin{figure}%[b, width=0.45\textwidth]
    \centering
    \includegraphics[width=0.95\linewidth]{cbs_images/gaussian_01_100_0,05.pdf}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Selected precision for a noisy ``triangle'' signal}
  \begin{figure}%[b, width=0.45\textwidth]
    \centering
    \includegraphics[width=0.95\linewidth]{cbs_images/triangle_00_0,5.pdf}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Selected precision for a noisy ``triangle'' signal}
  \begin{figure}%[b, width=0.45\textwidth]
    \centering
    \includegraphics[width=0.95\linewidth]{cbs_images/triangle_00_0,1.pdf}
  \end{figure}
\end{frame}

% subsection kernel_bandwidth_selection (end)

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Anomaly detection}
  \begin{block}{Supervised setting}
    Use KRR prediction conformal confidence region to discriminate between
    ``normal'' and ``anomalous'' observations.
  \end{block}

  \begin{block}{Unsupervised setting}
    Given a sample $(x_i)_{i=1}^n\sim \Dcal$, decide if $x_{n+1}$ was generated
    by $\Dcal$ as well. Possible approaches:
    \begin{itemize}
      \item one-class SVM: a kernel method for novelty detection;
      \item kernel distribution embedding with conformal abnormal probability (C-KDE);
    \end{itemize}
  \end{block}
\end{frame}

\subsection{Kernel embeddings of distributions} % (fold)
\label{sub:kernel_embeddings_of_distributions}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  A distribution $P$ on $\Xcal$ admits an embedding $\mu_P$ in an RKHS $\Hcal_K$
  induced by $K$, if $\ex_{x\sim P} \sqrt{K(x,x)} < +\infty$.
  
  \vspace{\baselineskip}
  In this case $\ex_{x\sim P} f(x) = \langle f, \mu_P \rangle_\Hcal$ for any
  $f\in \Hcal_K$, where $\mu_P\in \Hcal_K$ is unique and
  $$ \mu_P: x \mapsto \ex_{x'\sim P} K(x', x) \,. $$
  
  \vspace{\baselineskip}
  If the RKHS $\Hcal_K$ is universal and on a compact metric domain, then the
  distribution embedding $\mu_P$ is injective, \cite{gretton2012};
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{A toy example}
  \begin{figure}%[t, width=0.45\textwidth]
    \centering
    \includegraphics[width=0.5\linewidth]{ocSVM.pdf}
    \caption{one-class SVM with Gaussian kernel; normalized exp-slacks ($e^{\xi_i}$)
    are used as proxies for abnormality probability.}
    \label{fig:ocsvm}
  \end{figure}
  % one-class SVM work poorly when anomalies are relatively frequent, or form
  % a cluster.
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{C-KDE}
    Non-conformity measure
    $$ A(X_{-i}, x_i) = n^2 \| \mu_{\hat{P}_{-i}} - \mu_{\hat{P}} \|^2 \,. $$

    Define the abnormality probability as the conformal p-value of with respect
    to $A$.

    \begin{itemize}
      \item Analogous to jackknife bootstrapping;
      \item In special cases might permit computation of quantile regions;
      \item Extension: weighted observations in embedding;
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{A toy example}
  \begin{figure}%[t, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.9\linewidth]{ckde.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.9\linewidth]{ckde-lap.pdf}
    \end{subfigure}%
    \caption{The proposed abnormality p-values for empirical kernel embedding
    with Gaussian kernel (\textit{left}) and Laplacian kernel,
    $\text{exp}(-\theta\|x-x'\|)$, (\textit{right}).}
    \label{fig:ckde}
  \end{figure}
  % C-KDE is very sensitive to kernel bandwidth $\theta$: coalesces clusters for
  % moderately large bandwidth.
\end{frame}

% subsection kernel_embeddings_of_distributions (end)

% section further_development_beyond_krr (end)

\section{Summary} % (fold)
\label{sec:summary}

\begin{frame}[t]\frametitle{\insertsection}
  \begin{itemize}
    \item An efficient algorithm for valid non-parametric confidence regions for
    kernel ridge regression;

    \vspace{\baselineskip}
    \item Experimental verification of validity guaranties of conformal confidence
    regions over KRR with the Gaussian kernel in the batch learning setting;

    \vspace{\baselineskip}
    \item Extensive comparison of conformal confidence region to Bayesian confidence
    interval and investigation of its efficiency;

    \vspace{\baselineskip}
    \item Outlined promising research directions with preliminary results:
    \begin{itemize}
      \item kernel bandwidth selection;
      \item conformalized kernel distribution embeddings;
    \end{itemize}
  \end{itemize}
\end{frame}

% section summary (end)

\section{References} % (fold)
\label{sec:references}

\begin{frame}[t, shrink=25]\frametitle{\insertsection}
  \printbibliography
\end{frame}

% section references (end)

\end{document}
