% Этот шаблон документа разработан в 2014 году
% Данилом Фёдоровых (danil@fedorovykh.ru) 
% для использования в курсе 
% <<Документы и презентации в \LaTeX>>, записанном НИУ ВШЭ
% для Coursera.org: http://coursera.org/course/latex .
% Исходная версия шаблона --- 
% https://www.writelatex.com/coursera/latex/5.1

\documentclass[t]{beamer}  % [t], [c], или [b] --- вертикальное выравнивание на слайдах (верх, центр, низ)
%\documentclass[handout]{beamer} % Раздаточный материал (на слайдах всё сразу)
%\documentclass[aspectratio=169]{beamer} % Соотношение сторон

%\usetheme{Berkeley} % Тема оформления
%\usetheme{Bergen}
%\usetheme{Szeged}

%\usecolortheme{beaver} % Цветовая схема
%\useinnertheme{circles}
\useinnertheme{rectangles}

% \usetheme{HSE}
\usepackage{HSE-theme/beamerthemeHSE-en}

\usepackage{cmap}         % поиск в PDF
\usepackage{mathtext}         % русские буквы в формулах
\usepackage[T2A]{fontenc}     % кодировка
\usepackage[utf8]{inputenc}     % кодировка исходного текста
\usepackage[english]{babel} % локализация и переносы

% \newtheorem{rtheorem}{Теорема}
% \newtheorem{rproof}{Доказательство}
% \newtheorem{rexample}{Пример}

\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
% \usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление
\usepackage{mathptmx}
\usepackage{algorithm2e}

%% Номера формул
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
%\usepackage{leqno} % Нумерация формул слева

%% Свои команды
% \DeclareMathOperator{\sgn}{\mathop{sgn}}

%% Перенос знаков в формулах (по Львовскому)
% \newcommand*{\hm}[1]{#1\nobreak\discretionary{}
% {\hbox{$\mathsurround=0pt #1$}}{}}

\usepackage{graphicx}  % Для вставки рисунков
% \graphicspath{{images/}{images2/}}  % папки с картинками
% \setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
% \setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
\usepackage{wrapfig} % Обтекание рисунков текстом
\usepackage{subcaption}
\usepackage{caption}

\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
% \usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

% \usepackage{etoolbox} % логические операторы

\usepackage{lastpage} % Узнать, сколько всего страниц в документе.
\usepackage{soul} % Модификаторы начертания
\usepackage{csquotes} % Еще инструменты для ссылок
\usepackage[style=alphabetic,backend=biber,sorting=nty]{biblatex}

\bibliography{../references}

% \usepackage[style=authoryear,maxcitenames=2,backend=bibtex,sorting=nty]{biblatex}
\usepackage{multicol} % Несколько колонок

% \usepackage{tikz} % Работа с графикой
% \usepackage{pgfplots}
% \usepackage{pgfplotstable}

% \usepackage{fontspec}
% \defaultfontfeatures{Ligatures={TeX},Renderer=Basic}
% \setmainfont[Ligatures={TeX,Historic}]{Myriad Pro} % install Myriad Pro or replace with Arial
% \setsansfont{Myriad Pro}  % install Myriad Pro or replace with Arial
% \setmonofont{Courier New}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\ex}{\mathop{\mathbb{E}}\nolimits}
\newcommand{\pr}{\mathop{\mathbb{P}}\nolimits}

\title{Conformal methods in multidimensional linear models and anomaly detection}
% \title[Short title]{Presentation Title} 
% \subtitle{Presentation Subtitle or Conference Title}
\author[Nazarov Ivan]{Nazarov Ivan}
% \author[Author's name]{Author's name \\ \smallskip \scriptsize \url{author@hse.ru}\\\url{http://hse.ru/en/staff/author/}}
\date{\today}
\institute[Higher School of Economics]{National Research University \\ Higher School of Economics}


\begin{document}
\selectlanguage{english}
\frame[plain]{\titlepage} % Титульный слайд

\section{Introduction} % (fold)
\label{sec:introduction}

\begin{frame}[c]\frametitle{\insertsection}
  \begin{block}{Function approximation}
    Recover a faithful approximation $\hat{f}_{:n}$ of $f:\Xcal \mapsto \Real$, $\Xcal\subset \Real^{p\times 1}$,
    from a sample of noisy observations $Z_{:n} = (x_i, y_i)_{i=1}^n \sim \Dcal$ with
    $$ y_i = f(x_i) + \epsilon_i \,, $$
    and $\ex(\epsilon_i|x_i) = 0$.
  \end{block}

  \begin{block}{Confidence prediction}
    concerned with learning a ``well behaved'' point-set map $x \mapsto \hat{C}_{:n}^\alpha(x)$
    such that
    $$ \pr_\Dcal\bigl(y_x \notin \hat{C}_{:n}^\alpha(x)\bigr) \approx \alpha \,. $$
  \end{block}
\end{frame}

\subsection{Importance of confidence prediction} % (fold)
\label{sub:importance_of_cp}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{itemize}
    \item anomaly detection (batch setting): learn a confidence region for observatons,
    to discriminate ``inliners'' from ``outliers'';
    \item forecasting (online setting): based on the dynamic model of a time series
    yield a predictive region with specified confidence level;
    \item function approximation (batch setting): assess the bounds for the approximaton
    error of the unknown function $f$;
  \end{itemize}
  % All these models also usually rely on some predictive model;
\end{frame}

% subsection importance_of_cp (end)

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Typical approach}
  Linear regression or Kernel Ridge Regression (KRR):
  $$ \|y - K\beta \|^2 + \lambda \beta' K \beta \to \min_\beta \,, $$
  $y\in \Real^{n\times1}$, $\lambda > 0$, $K$ -- sample Gram matrix of kernel
  $K(\cdot,\cdot)$ on inputs $x_i$.
  \begin{description}
    \item[prediction] $\hat{y}(x) = \kappa_x' \hat{\beta} = \kappa_x' Q y$;
    \item[loo-cv] $\hat{r}^{\text{loo}}_i = \frac{h(x_i)}{\lambda} e_i'(y - K Q y)$;
  \end{description}
  with $h(x) = \lambda + K(x,x) - \kappa_x' Q \kappa_x$, $Q = (\lambda I_n + K)^{-1}$ and
  $\kappa_x$ -- $n\times 1$ vector of canonical feature maps.
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Bayesian KRR}
  Gaussian Process Regression: observations are drawn from a Gaussian process with
  mean $0$ and covariance $\sigma^2(\lambda\delta_{x,x'} + K(x,x'))$, $\sigma^2$
  -- variance, $\lambda$ -- noise-to-signal ratio.
  \begin{block}{Forecast distribution}
  Conditional on the observed data $(X, y)$ at a new $x$:
  $$ y_x \sim \Ncal(\kappa_x' Q y, \sigma^2 h(x)) \,. $$
  \end{block}
\end{frame}

% section introduction (end)

\section{Conformal prediction} % (fold)
\label{sec:conformal_prediction}

\begin{frame}[c, shrink=10]
  \frametitle{\insertsection}
  \begin{algorithm}[H]
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{Non-conformity measure $A$, significance level $\alpha \in (0,1)$,
    training sample $Z_{:n} = (x_i, y_i)_{i=1}^n$, a test object $x_{n+1}\in \Xcal$.}
  \Output{Confidence set $\Gamma_{:n}^\alpha(x_{n+1})$ for $y_{n+1}\in \Ycal \subseteq \Real$.}
  \BlankLine
  $\Gamma_{:n}^\alpha \leftarrow \emptyset$\;
  \For{$y \in \Ycal$}{
    $z_{n+1} \leftarrow (x_{n+1}, y)$\;
    \For{$i = 1,\ldots, n, n+1$}{
      $Z_{-i} \leftarrow \bigl(z_j\bigr)_{j=1, j\neq i}^{n+1}$\;
      $\eta_i \leftarrow A(Z_{-i}, z_i)$\;
    }
    $p_{:n+1} \leftarrow (n+1)^{-1} \bigl\lvert \{
        i \,:\, \eta_i \geq \eta_{n+1} \} \bigr\rvert $\;
    \If{$p_{:n+1} > \alpha$}{
      $\Gamma_{:n}^\alpha \leftarrow \Gamma_{:n}^\alpha \cup\{ y\}$\;
    }
  }
  \Return{$\Gamma_{:n}^\alpha$}\;
  \end{algorithm}
\end{frame}

\begin{frame}
  \frametitle{\insertsection}
  % A distribution-free technique designed to yield a statistically valid confidence
  % sets for predictions made by Machine Learning algorithms.
  \begin{itemize}
    \item can be constructed atop \textbf{any} ML algorithm by incorporating it into
    $A(Z_{:n}, z)$ -- a score reflecting non-conformity of $z$ with respect to sample
    $Z_{:n}$;
    \item Asymptotic validity guarantees for i.i.d. observations in online setting
    established in \cite{vovk2005}:
    $$ \pr\bigl( y_{n+1} \notin \hat{\Gamma}_{:n}^\alpha(x_{n+1}) \bigr) \leq \alpha \,. $$
    \item Efficiency (e.g. width of prediction confidence intervals) depends on the
    non-conformity measure $A$ and model assumptions; \begin{itemize}
      \item Asymptotic efficiency in the batch setting for linear ridge regression shown
      in \cite{burnaevV14};
      \item Nothing is known about efficiency in the case of nonlinear models;
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Non-conformity measures}
  The in-sample residual of the $i$-th observation of $Z_{:n}\cup\{(x_{n+1}, y_{n+1})\}$,
  $\hat{r}_i(y_{n+1}; Z_{:n}, x_{n+1})$, is a linear function of $n+1$-st target value.

  \begin{description}
    \item[Ridge Regression Confidence Machine] $A(Z_{-i}, z_i) = |\hat{r}_i(y_{n+1}; Z_{:n}, x_{n+1})|$;
    \item[Two-sided Conformal Region]
      \textit{upper} $A(Z_{-i}, z_i) = \hat{r}_i(y_{n+1}; Z_{:n}, x_{n+1})$ and
      \textit{lower} $A(Z_{-i}, z_i) = -\hat{r}_i(y_{n+1}; Z_{:n}, x_{n+1})$;
  \end{description}

  Leave-one-out versions use
  $$ \hat{r}^{\text{loo}}_i(y_{n+1}; Z_{:n}, x_{n+1})
    = \frac{h(x_i)}{\lambda} \hat{r}_i(y_{n+1}; Z_{:n}, x_{n+1}) \,. $$
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \begin{block}{Conformal p-value}
    Let $\eta_i(y_{n+1}) = A(Z_{-i}, z_i)$.
    $$ p_{Z_{:n}}(y_{n+1})
          = (n+1)^{-1} \bigl\lvert\{ i \,:\, \eta_i(y_{n+1}) \geq \eta_{n+1}(y_{n+1}) \}\bigr\rvert
      \,. $$
  \end{block}

  \begin{block}{Conformal confidence set}
    $$ \hat{\Gamma}_{Z_{:n}}^\alpha(x_{n+1})
        = \bigl\{ y_{n+1}\in \Ycal \,:\, p_{Z_{:n}}(y_{n+1}) \geq \alpha \bigr\}
      \,. $$
  \end{block}
\end{frame}

\subsection{Ridge Regression Confidence Machine} % (fold)
\label{sub:ridge_regression_confidence_machine}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{Exhaustive search over $y\in \Real$ is not necessary.}
    \begin{itemize}
      \item regression residuals are special: each non-empty regions
      $S_i = \{y_{n+1}\in \Ycal \,:\, \eta_i(y_{n+1}) \geq \eta_i(y_{n+1})\}$
      is either $(-\infty, a_i]$, $[b_i, +\infty)$, $[a_i, b_i]$, $\Real$, or
      $\Real\setminus (a_i, b_i)$;
      \item $\hat{\Gamma}_{Z_{:n}}^\alpha(x_{n+1})$ is a union of closed intervals
        $[a, b]\cup\Real$ covered by at least $\alpha n$ regions $S_i$, with
        $a\geq b$ in the set of endpoints of every $S_i$;
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
\framesubtitle{\insertsubsection}
  Fixed requirements per sample $Z_{:n}$: \begin{itemize}
    \item Initialization: inversion of the regularized Gram matrix $\Ocal(n^3)$;
    \item Memory complexity: $\Ocal(n^2)$ to store the inverse;
  \end{itemize}

  Requirements for each $(x_{n+1})$:
  \begin{itemize}
    \item Memory complexity: $\Ocal(n)$ to store endpoints;
    \item Runtime complexity: $\Ocal(n^2)$ for kernel and matrix multiplication,
    and $\Ocal(n \log n)$ for interval construction, $\Ocal(n)$ for pruning;
  \end{itemize}
\end{frame}

% subsection ridge_regression_confidence_machine (end)

\subsection{Two-sided conformal region} % (fold)
\label{sub:two_sided_conformal_region}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  Same constructon, less endpoints: each non-empty $S_i$ is $(-\infty, a_i]$,
  $[a_i, +\infty)$ or $\Real$.

  \begin{block}{Specifics}
    must intersect the \textit{upper} interval $\hat{\Gamma}_{Z_{:n}}^{\alpha, U}(x_{n+1})$
    and the \textit{lower} interval $\hat{\Gamma}_{Z_{:n}}^{\alpha, L}(x_{n+1})$, which
    takes $\Ocal(1)$ time;
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  General $\hat{\Gamma}_{Z_{:n}}^\alpha(x_{n+1})$ is not necessarily connected;
  \begin{block}{Conjecture \# 1}
    For any sequence $(x_i)_{i=1}^n\in \Xcal$ and kernel $K(\cdot, \cdot)$, such
    that for any $f$ in the canonical RKHS of $K$ one has
    $$ f(x_{n+1})^2 \leq \sum_{i=1}^n f(x_n)^2 + \lambda \|f\|^2 \,, $$
    then all $S_i$ for upper NCM are of the form $(-\infty, a_i]$.
  \end{block}
  \begin{block}{Conjecture \# 2}
    Condition of conjecture \#1 is satisfied when for sufficiently large $n\geq 1$
    $$ K(x_{n+1}, x_{n+1}) - \kappa_{:n}(x_{n+1})' K^{-1}_{:n} \kappa_{:n}(x_{n+1})
      < \lambda \,. $$
  \end{block}
\end{frame}

% subsection two_sided_conformal_region (end)

\subsection{Preliminary results} % (fold)
\label{sub:preliminary_results}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{itemize}
    \item an effcient ploynomial algorithm for construction of conformal
    confidence regions;
    \item have a sufficient condition for regularity of two-sided intervals
    for KRR (extends \cite{burnaevV14});
  \end{itemize}

  Next goal: compare the conformal confidence region to the Bayesian confidence
  interval.
\end{frame}

% subsection preliminary_results (end)

% section conformal_prediction (end)

\section{Numerical study} % (fold)
\label{sec:Numerical_study}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Bayesian interval}
  Gaussian Process Regression readily yields confidence prediction
  \begin{block}{Maximum aposteriori prediction}
    $$ \hat{y}_{:n}(x) = \kappa_x' (\lambda I_n + K)^{-1} y_{:n} \,, $$
    where $\kappa_x = (K(x_i, x))_{i=1}^n$.
  \end{block}

  \begin{block}{Prediction confidence interval}
    $$ y_{n+1} \in
      \bigl[\hat{y}_{:n}(x_{n+1}) - \sigma \sqrt{h(x_{n+1})},
            \hat{y}_{:n}(x_{n+1}) + \sigma \sqrt{h(x_{n+1})} \bigr] \,, $$
    where $ h(x) = \lambda + K(x,x) - \kappa_x' (\lambda I_n + K)^{-1} \kappa_x$.
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  we consider kernel ridge regression with Gaussian kernel:
  $$ K(x,x') = \text{exp}\bigl\{ -\theta \|x-x'\|^2 \bigr\} \,. $$
  \begin{block}{Objectives}
    \begin{itemize}
      \item verify validity guaranties of conformal confidence regions for KRR in
      the batch learning setting;
      \item measure efficiency of conformal confidence regions under gaussainity
      assumptions.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \begin{itemize}
    \item Test functions on compact $1-d$ and $2-d$ domains; \begin{itemize}
      \item a piecewise continuous;
      \item a piecewise smooth;
      \item a sample path of a Gaussain process;
    \end{itemize}
    \item Low or high Gaussian noise;
    \item KRR hyper-parameters: regularization $\lambda$, kernel precision $\theta$;
    \item Train sample size $n$;
  \end{itemize}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Experimental setup}
  \begin{enumerate}
    \item Draw a large random uniform sample from $\Xcal$ to serve as a train set;
      Use a regular grid as a test sample;
    \item Generate a large sample path over the pooled sample;
    \item For each setting of hyper-parameters make $M$ replications: \begin{itemize}
      \item pick a random subsample of specified size from the train sample;
      \item construct confidence regions (conformal, Bayesian) for the test sample;
      \item compute the coverage rate across the test sample and region size for each test object;
    \end{itemize}
  \end{enumerate}
\end{frame}

\subsection{Results} % (fold)
\label{sub:results}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Example Bayesian and conformal (RRCM) confidence regions}
  \begin{figure}%[t, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.9\linewidth]{../images/output_pdf/profile/gaussian/0.1_0.1/50/profile_gaussian_0,1_0,1_100_5p-GPR_50.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.9\linewidth]{../images/output_pdf/profile/gaussian/0.1_0.1/50/profile_gaussian_0,1_0,1_100_5p-RRCM_50.pdf}
    \end{subfigure}
    % \caption{Example Bayesian and conformal (RRCM) confidence regions.}
    \label{fig:gauss_1d_prof_gpr}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Asymptotic coverage rate in Gaussain case}
  \begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/GPR-f/coverage_gaussian_0,1_1e-06_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM/coverage_gaussian_0,1_1e-06_auto_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/GPR-f/coverage_gaussian_0,1_0,1_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM/coverage_gaussian_0,1_0,1_auto_RRCM.pdf}
    \end{subfigure}\\
    % \caption{Asymptotic coverage rate in Gaussain case.}
    \label{fig:gaussian_1d_high_noise}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Typical confidence region widths in Gaussian case with high noise}
  \begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/GPR-f/width_gaussian_0,1_1e-06_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/RRCM/width_gaussian_0,1_1e-06_auto_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/width/GPR-f/width_gaussian_0,1_0,1_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM/width_gaussian_0,1_0,1_auto_RRCM.pdf}
    \end{subfigure}\\
    % \caption{Typical confidence region widths in Gaussian case with high noise.}
    \label{fig:gaussian_1d_high_noise_width}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Typical confidence region widths in Gaussian case with high noise}
  \begin{figure}%[b, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/GPR-f/coverage_heaviside_1e-06_1e-06_100_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/RRCM/coverage_heaviside_1e-06_1e-06_100_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/GPR-f/coverage_heaviside_1e-06_0,1_100_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/RRCM/coverage_heaviside_1e-06_0,1_100_RRCM.pdf}
    \end{subfigure}\\
    % \caption{Typical confidence region widths in Gaussian case with high noise.}
    \label{fig:heaviside_1d_low_noise}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Typical coverage rate in non-Gaussian case with high noise}
  \begin{figure}%[b, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/GPR-f/coverage_heaviside_0,1_1e-06_10_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/RRCM/coverage_heaviside_0,1_1e-06_10_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/GPR-f/coverage_heaviside_0,1_0,1_10_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/RRCM/coverage_heaviside_0,1_0,1_10_RRCM.pdf}
    \end{subfigure}\\
    % \caption{Typical coverage rate in non-Gaussian case with high noise.}
    \label{fig:heaviside_1d_high_noise_arb}
  \end{figure}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{itemize}
    \item in all settings confromal confidence regions are asymptotically valid,
    regardless of KRR hyper-parameters, quality of fit or noise-to-signal ratio;
    %% page 34, columns a,c; page 36, columns a,c
    %% non-Gaussian: figures 10-12
    \item Gaussian setting with high noise-to-signal: Bayesian intervals are valid,
    conformal regions are asymptotically efficient;
    %% fig 14-17, pp. 42-45;
    %% fig 18-19, p. 46 (double "the", different hyper-parameters)
    %% ``f6'' function is much Gaussian-like; thus the comparison on fig. 22
    \item Gaussian setting with low noise-to-signal: Bayesian intervals are
    conservatively valid and wider than conformal intervals;
    \item non-Gaussian settings with high noise: ;
    %% fig. 6, 16; fig. 10-13
    \item non-Gaussian settings with low noise: Bayseian intervals fail to have
    correct coverage rate;
    \item Bayesian confidence regions are sensitive to KRR hyper-parameters;
  \end{itemize}
\end{frame}

% subsection results (end)

\subsection{Implications} % (fold)
\label{sub:implications}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  By design, the conformal prediction algorithm is oblivious to the nature and
  the domain of the used non-conformity measure.

  That is why we consider the obtained empirical evidence for the validity and
  efficiency of the KRR CCR in batch setting to be sufficient to \begin{itemize}
    \item warrant further theoretical research in asymptotic properties of conformal
    kernel ridge regression predictions;
    \item recommend the use of conformal confidence regions in applied prediction
    tasks when the assumption of i.i.d. observations is reasonable;
  \end{itemize}
\end{frame}

% subsection implications (end)

% section Numerical_study (end)

\section{Further development beyond kernel ridge regression} % (fold)
\label{sec:further_development_beyond_krr}

\subsection{Kernel bandwidth selection} % (fold)
\label{sub:kernel_bandwidth_selection}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  Inspired by the approach taken in \cite{goldenshluger1997}: in function approximation
  setting assume that the observational noise is sub-Gaussian, use it to construct
  confidence intervals the true value of $f(x_0)$ at $x_0$, and, finally, select
  a locally optimal bandwidth by minimizing the size of the confidence region. 

  \begin{block}{Idea: Conformal Bandwidth Selection}
    Conformal confidence intervals have certain coverage guarantees, which is
    why they are a good non-parametric substitute for the sub-Gaussian confidence 
    intervals.

    Main challenge -- aggregation: \begin{itemize}
      \item how to move from local to global bandwidth choice;
      \item balance computational complexity versus approximation precision;
    \end{itemize}
  \end{block}
\end{frame}

% subsection kernel_bandwidth_selection (end)

\subsection{Kernel embeddings of distributions} % (fold)
\label{sub:kernel_embeddings_of_distributions}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  A distribution $P$ on $\Xcal$ admits an embedding $\mu_P$ in an RKHS $\Hcal_K$
  induced by $K$, if $\ex_{x\sim P} \sqrt{K(x,x)} < +\infty$, in which case
  $\ex_{x\sim P} f(x) = \langle f, \mu_P \rangle_\Hcal $ for any $f\in \Hcal_K$,
  where $\mu_P\in \Hcal_K$ is unique and
  $$ \mu_P: x \mapsto \ex_{x'\sim P} K(x', x) \,. $$
  If the RKHS $\Hcal_K$ is universal and on a compact metric domain, then the
  distribution embedding $\mu_P$ is injective, \cite{gretton2012};
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{Idea: C-KDE}
    Use conformal procedure to assign abnormality probability to new observations
    based on RKHS norm of difference of embeddings of empirical distributions of
    original and pooled samples.
    \begin{itemize}
      \item Analogous to jackknife bootstrapping;
      \item In special cases might permit computation of quantile regions;
      \item Weighted observations in embedding?;
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c, shrink=0]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{figure}%[t, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=0.9\linewidth]{ocSVM.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=0.9\linewidth]{ckde.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=0.9\linewidth]{ckde-lap.pdf}
    \end{subfigure}%
    \caption{A toy example of one-class SVM (\textit{left}, normalized exp-slacks
    $e^{\xi_i}$) and the proposed conformal p-values for empirical kernel embeddings
    (\textit{middle}) with Gaussian kernel $\text{exp}(-\theta\|x-x'\|^2)$;
    \textit{middle} C-KDE with Laplacian kernel $\text{exp}(-\theta\|x-x'\|)$.}
    \label{fig:gauss_1d_prof_gpr}
  \end{figure}
  C-KDE is very sensitive to kernel bandwidth $\theta$: coalesces clusters for
  moderately large bandwidth.
\end{frame}

% subsection kernel_embeddings_of_distributions (end)

% section further_development_beyond_krr (end)

\section{References} % (fold)
\label{sec:references}

\begin{frame}[t, shrink=25]\frametitle{\insertsection}
  \printbibliography
\end{frame}

% section references (end)


\end{document}