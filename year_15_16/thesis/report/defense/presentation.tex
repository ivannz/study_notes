% Этот шаблон документа разработан в 2014 году
% Данилом Фёдоровых (danil@fedorovykh.ru) 
% для использования в курсе 
% <<Документы и презентации в \LaTeX>>, записанном НИУ ВШЭ
% для Coursera.org: http://coursera.org/course/latex .
% Исходная версия шаблона --- 
% https://www.writelatex.com/coursera/latex/5.1

\documentclass[t]{beamer}  % [t], [c], или [b] --- вертикальное выравнивание на слайдах (верх, центр, низ)
%\documentclass[handout]{beamer} % Раздаточный материал (на слайдах всё сразу)
%\documentclass[aspectratio=169]{beamer} % Соотношение сторон

%\usetheme{Berkeley} % Тема оформления
%\usetheme{Bergen}
%\usetheme{Szeged}

%\usecolortheme{beaver} % Цветовая схема
%\useinnertheme{circles}
\useinnertheme{rectangles}

% \usetheme{HSE}
\usepackage{HSE-theme/beamerthemeHSE-en}

\usepackage{cmap}         % поиск в PDF
\usepackage{mathtext}         % русские буквы в формулах
\usepackage[T2A]{fontenc}     % кодировка
\usepackage[utf8]{inputenc}     % кодировка исходного текста
\usepackage[english]{babel} % локализация и переносы

% \newtheorem{rtheorem}{Теорема}
% \newtheorem{rproof}{Доказательство}
% \newtheorem{rexample}{Пример}

\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
% \usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление
\usepackage{mathptmx}
\usepackage{algorithm2e}

%% Номера формул
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
%\usepackage{leqno} % Нумерация формул слева

%% Свои команды
% \DeclareMathOperator{\sgn}{\mathop{sgn}}

%% Перенос знаков в формулах (по Львовскому)
% \newcommand*{\hm}[1]{#1\nobreak\discretionary{}
% {\hbox{$\mathsurround=0pt #1$}}{}}

\usepackage{graphicx}  % Для вставки рисунков
% \graphicspath{{images/}{images2/}}  % папки с картинками
% \setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
% \setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
\usepackage{wrapfig} % Обтекание рисунков текстом
\usepackage{subcaption}
\usepackage{caption}

\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
% \usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

% \usepackage{etoolbox} % логические операторы

\usepackage{lastpage} % Узнать, сколько всего страниц в документе.
\usepackage{soul} % Модификаторы начертания
\usepackage{csquotes} % Еще инструменты для ссылок
\usepackage[style=alphabetic,backend=biber,sorting=nty]{biblatex}

\bibliography{../references}

% \usepackage[style=authoryear,maxcitenames=2,backend=bibtex,sorting=nty]{biblatex}
\usepackage{multicol} % Несколько колонок

% \usepackage{tikz} % Работа с графикой
% \usepackage{pgfplots}
% \usepackage{pgfplotstable}

% \usepackage{fontspec}
% \defaultfontfeatures{Ligatures={TeX},Renderer=Basic}
% \setmainfont[Ligatures={TeX,Historic}]{Myriad Pro} % install Myriad Pro or replace with Arial
% \setsansfont{Myriad Pro}  % install Myriad Pro or replace with Arial
% \setmonofont{Courier New}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\ex}{\mathop{\mathbb{E}}\nolimits}
\newcommand{\pr}{\mathop{\mathbb{P}}\nolimits}

\title{Conformal methods in multidimensional linear models and anomaly detection}
% \title[Short title]{Presentation Title} 
% \subtitle{Presentation Subtitle or Conference Title}
\author[Nazarov Ivan]{Nazarov Ivan}
% \author[Author's name]{Author's name \\ \smallskip \scriptsize \url{author@hse.ru}\\\url{http://hse.ru/en/staff/author/}}
\date{\today}
\institute[Higher School of Economics]{National Research University \\ Higher School of Economics}


\begin{document}
\selectlanguage{english}
\frame[plain]{\titlepage} % Титульный слайд

\section{Problem statement} % (fold)
\label{sec:problem_statement}
\subsection{Confidence predictions} % (fold)
\label{sub:confidence_predictions}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  Consider a training sample $(x_i, y_i)_{i=1}^n \sim \Dcal$ i.i.d., where conditional
  on $x\in \Xcal$, $\Xcal \subset \Real^{p\times 1}$, the target $y_x$ obeys
  $$ y_x = f(x) + \epsilon_x \,, $$
  for some unknown $f:\Xcal\mapsto \Ycal$, $\Ycal\subseteq \Real$ and $\ex(\epsilon_x|x) = 0$.

  In general functional approximation it is desirable to learn a faithful approximation
  $\hat{f}_{:n}$ of $f$ for $x\sim \Xcal$ within the support of $\Dcal$, and have an
  estimate of its accuracy.

  Confidence prediction is more concerned with learning a ``well behaved'' point-set
  map $x \mapsto \hat{\Gamma}_{:n}^\alpha(x)$ such that
  $$ \pr_\Dcal\bigl(y_x \notin \hat{\Gamma}_{:n}^\alpha(x)\bigr) \approx \alpha \,. $$
\end{frame}

% subsection confidence_predictions (end)

\subsection{Typical applications} % (fold)
\label{sub:typical_applications}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{itemize}
    \item anomaly detection (batch setting): learn a detector on a train sample,
    that is able to tell ``abnormal'' observations from ``normal'' ones, then
    apply to test objects;
    \begin{itemize}
      \item probabilistic models: observations outside of typical (dense) regions;
      \item predictive models: observations deviating from predictions (reconstructions)
      by the learnt model of ``normal'' regime;
    \end{itemize}
    \item time series forecasting (online setting): based on the dynamic model, be
    it ARIMA with/without seasonality, GARCH et c., make a confidence prediction
    regarding the levels and or volatilities;
    \item function approximation (batch setting): given a set of observations,
    assess the approximate error region for the underlying function;
  \end{itemize}
\end{frame}
% subsection typical_applications (end)

\subsection{Predictive models} % (fold)
\label{sub:predictive_models}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  Options for building blocks of predictive regression models: 
  \begin{block}{approximation}
    \begin{itemize}
      \item linear models, (Kernel) Ridge regression, Support Vector regression;
      \item piecewise linear Gradient Boosted Regression Trees;
      \item Neural Networks;
    \end{itemize}
  \end{block}
  \begin{block}{confidence}
      \begin{itemize}
        \item Bayesian assumptions for conditional predictive distributions;
        \item conformal prediction;
      \end{itemize}
  \end{block}
\end{frame}

% subsection predictive_models (end)

% section problem_statement (end)

\section{Conformal prediction} % (fold)
\label{sec:conformal_prediction}

\begin{frame}
  \frametitle{\insertsection}
  A distribution-free technique designed to yield a statistically valid confidence
  sets for predictions made by Machine Learning algorithms.

  \begin{itemize}
    \item can be constructed atop any ML algorithm by incorporating it into a function
    $A(Z_{:n}, z)$, reflecting non-conformity of $z$ with respect to sample $Z_{:n}$;
    \item validity guarantees for i.i.d. observations in online setting established
    in \cite{vovk2005};
    \item asymptotic efficiency in the batch setting for ridge regression shown in
    \cite{burnaevV14};
  \end{itemize}

  In particular, for predictive models the non-conformity measures $A$ are typically
  based on residuals;
\end{frame}

\subsection{General algorithm} % (fold)
\label{sub:general_algorithm}

\begin{frame}[c, shrink=10]
  \frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{algorithm}[H]
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{Non-conformity measure $A$, significance level $\alpha \in (0,1)$,
    training sample $Z_{:n} = (x_i, y_i)_{i=1}^n$, a test object $x_{n+1}\in \Xcal$.}
  \Output{Confidence set $\Gamma_{:n}^\alpha(x_{n+1})$ for $y_{n+1}\in \Ycal$.}
  \BlankLine
  $\Gamma_{:n}^\alpha \leftarrow \emptyset$\;
  \For{$y \in \Ycal$}{
    $z_{n+1} \leftarrow (x_{n+1}, y)$\;
    \For{$i = 1,\ldots, n, n+1$}{
      $Z_{-i} \leftarrow \bigl(z_j\bigr)_{j=1, j\neq i}^{n+1}$\;
      $\eta_i \leftarrow A(Z_{-i}, z_i)$\;
    }
    $p^y \leftarrow (n+1)^{-1} \bigl\lvert \{
        i \,:\, \eta_i \geq \eta_{n+1} \} \bigr\rvert $\;
    \If{$p^y > \alpha$}{
      $\Gamma_{:n}^\alpha \leftarrow \Gamma_{:n}^\alpha \cup\{y\}$\;
    }
  }
  \Return{$\Gamma_{:n}^\alpha$}\;
  \end{algorithm}
\end{frame}

% subsection general_algorithm (end)

% section conformal_prediction (end)

\section{Research goals and results} % (fold)
\label{sec:research_goals_and_results}

\begin{frame}[c, shrink=10]\frametitle{\insertsection}
  \begin{block}{Goals}
    \begin{itemize}
      \item Propose an efficient construction for conformal prediction based on
      kernel ridge regression (KRR);
      \item Experimentally verify validity guaranties and efficiency of conformal
      confidence regions over KRR with the Gaussian kernel in the batch learning
      setting;
    \end{itemize}
  \end{block}

  \begin{block}{Experimental setup}
    \begin{enumerate}
      \item Draw a large random uniform sample from $\Xcal$ to serve as a train set;
        Use a regular grid as a test sample;
      \item Generate a large sample path over the pooled sample;
      \item For each setting of hyper-parameters make $M$ replications: \begin{itemize}
        \item pick a random subsample of specified size from the train sample;
        \item construct confidence regions (conformal, Bayesian) for the test sample;
        \item compute the coverage rate across the test sample and region size for each test object;
      \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\subsection{Results: algorithm} % (fold)
\label{sub:results_algorithm}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  Outlined a specialized algorithm for KRR conformal confidence region for each
  new test observation, $\hat{\Gamma}^\alpha_{:n}(x_{n+1})$:
  \begin{itemize}
    \item Initialization: inversion of the regularized Gram matrix $\Ocal(n^3)$;
    \item Memory complexity: $\mathcal{O}(n^2)$ to store the inverse;
    \item Runtime complexity: $\Ocal(n^2)$ for canonical feature map computation
    and matrix multiplication, and $\Ocal(n \log n)$ for interval construction;
  \end{itemize}

  The procedure uses structure of sample Gram matrix and the block matrix inversion,
  to represent KRR residuals for $n+1$ sample as a linear function of the test
  observations' value.
\end{frame}

% subsection results_algorithm (end)

\subsection{Results: performance} % (fold)
\label{sub:results_performance}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  Extensive empirical study of CCR over KRR with the Gaussian kernel revealed:
  \begin{itemize}
    \item CCR have asymptotic validity, regardless of KRR hyper-parameters, quality
    of fit or noise-to-signal ratio;
    %% page 34, columns a,c; page 36, columns a,c
    %% non-Gaussian: figures 10-12
    \item In Gaussian setting with high noise-to-signal, both Bayesian and conformal
    KRR intervals are valid, and the latter are asymptotically as efficient as the
    former;
    %% fig 14-17, pp. 42-45;
    %% fig 18-19, p. 46 (double "the", different hyperparameters)
    %% ``f6'' function is much Gaussian-like; thus the comparison on fig. 22
    \item Bayesian confidence regions are severely affected by the choice of KRR
    hyper-parameters (kernel precision -- proxy for the degree of locality), and
    in non-Gaussian with low-noise settings fail to guarantee requested coverage
    rate;
    %% fig. 6, 16; fig. 10-13
  \end{itemize}
  Similar results were obtained in test cases with $1-d$ and $2-d$ domains.
\end{frame}

% \begin{frame}[c]\frametitle{\insertsection}
%   \framesubtitle{\insertsubsection}
%   Some illustrations
% \end{frame}

% subsection results_performance (end)

\subsection{Implications} % (fold)
\label{sub:implications}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  By design, the conformal prediction algorithm is oblivious to the nature and
  the domain of the used non-conformity measure.

  That is why we consider the obtained empirical evidence for the validity and
  efficiency of the KRR CCR in batch setting to be sufficient to \begin{itemize}
    \item warrant further theoretical research in asymptotic properties of conformal
    kernel ridge regression predictions;
    \item recommend the use of conformal confidence regions in applied prediction
    tasks when the assumption of i.i.d. observations is reasonable;
  \end{itemize}
\end{frame}

% subsection implications (end)

% section research_goals_and_results (end)

\section{Further development beyond kernel ridge regression} % (fold)
\label{sec:further_development_beyond_krr}

\subsection{Kernel embeddings of distributions} % (fold)
\label{sub:kernel_embeddings_of_distributions}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  A distribution $P$ on $\Xcal$ admits an embedding $\mu_P$ in an RKHS $\Hcal_K$
  induced by $K$, if $\ex_{x\sim P} \sqrt{K(x,x)} < +\infty$, in which case
  $\ex_{x\sim P} f(x) = \langle f, \mu_P \rangle_\Hcal $ for any $f\in \Hcal_K$,
  where $\mu_P\in \Hcal_K$ is unique and
  $$ \mu_P: x \mapsto \ex_{x'\sim P} K(x', x) \,. $$
  If the RKHS $\Hcal_K$ is universal and on a compact metric domain, then the
  distribution embedding $\mu_P$ is injective, \cite{gretton2012};

  \begin{block}{Idea: C-KDE}
    Use conformal procedure to assign abnormality probability to new observations
    based on RKHS norm of difference of embeddings of empirical distributions of
    original and pooled samples.
    \begin{itemize}
      \item Analogous to jackknife bootstrapping;
      \item In special cases might permit computation of quantile regions;
      \item Weighted observations in embedding?;
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c, shrink=0]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{figure}%[t, width=0.5\textwidth]
    \centering
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=0.9\linewidth]{ocSVM.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=0.9\linewidth]{ckde.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=0.9\linewidth]{ckde-lap.pdf}
    \end{subfigure}%
    \caption{A toy example of one-class SVM (\textit{left}, normalized exp-slacks
    $e^{\xi_i}$) and the proposed conformal p-values for empirical kernel embeddings
    (\textit{middle}) with Gaussian kernel $\text{exp}(-\theta\|x-x'\|^2)$;
    \textit{middle} C-KDE with Laplacian kernel $\text{exp}(-\theta\|x-x'\|)$.}
    \label{fig:gauss_1d_prof_gpr}
  \end{figure}
  C-KDE is very sensitive to kernel bandwidth $\theta$: coalesces clusters for
  moderately large bandwidth.
\end{frame}

% subsection kernel_embeddings_of_distributions (end)

\subsection{Kernel bandwidth selection} % (fold)
\label{sub:kernel_bandwidth_selection}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  Inspired by the approach taken in \cite{goldenshluger1997}: in function approximation
  setting assume that the observational noise is sub-Gaussian, use it to construct
  confidence intervals the true value of $f(x_0)$ at $x_0$, and, finally, select
  a locally optimal bandwidth by minimizing the size of the confidence region. 

  \begin{block}{Idea: Conformal Bandwidth Selection}
    Conformal confidence intervals have certain coverage guarantees, which is
    why they are a good non-parametric substitute for the sub-Gaussian confidence 
    intervals.

    Main challenge -- aggregation: \begin{itemize}
      \item how to move from local to global bandwidth choice;
      \item balance computational complexity versus approximation precision;
    \end{itemize}
  \end{block}
\end{frame}

% subsection kernel_bandwidth_selection (end)

% section further_development_beyond_krr (end)

\section{References} % (fold)
\label{sec:references}

\begin{frame}[t, shrink=25]\frametitle{\insertsection}
  \printbibliography
\end{frame}

% section references (end)


\end{document}