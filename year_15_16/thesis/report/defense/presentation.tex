% Этот шаблон документа разработан в 2014 году
% Данилом Фёдоровых (danil@fedorovykh.ru) 
% для использования в курсе 
% <<Документы и презентации в \LaTeX>>, записанном НИУ ВШЭ
% для Coursera.org: http://coursera.org/course/latex .
% Исходная версия шаблона --- 
% https://www.writelatex.com/coursera/latex/5.1

\documentclass[t]{beamer}  % [t], [c], или [b] --- вертикальное выравнивание на слайдах (верх, центр, низ)
%\documentclass[handout]{beamer} % Раздаточный материал (на слайдах всё сразу)
%\documentclass[aspectratio=169]{beamer} % Соотношение сторон

%\usetheme{Berkeley} % Тема оформления
%\usetheme{Bergen}
%\usetheme{Szeged}

%\usecolortheme{beaver} % Цветовая схема
%\useinnertheme{circles}
%\useinnertheme{rectangles}

% \usetheme{HSE}
\usepackage{HSE-theme/beamerthemeHSE-en}

\usepackage{cmap}         % поиск в PDF
\usepackage{mathtext}         % русские буквы в формулах
\usepackage[T2A]{fontenc}     % кодировка
\usepackage[utf8]{inputenc}     % кодировка исходного текста
\usepackage[english,russian]{babel} % локализация и переносы

% \newtheorem{rtheorem}{Теорема}
% \newtheorem{rproof}{Доказательство}
% \newtheorem{rexample}{Пример}

\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
% \usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление
\usepackage{mathptmx}
\usepackage{algorithm2e}

%% Номера формул
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
%\usepackage{leqno} % Нумерация формул слева

%% Свои команды
% \DeclareMathOperator{\sgn}{\mathop{sgn}}

%% Перенос знаков в формулах (по Львовскому)
% \newcommand*{\hm}[1]{#1\nobreak\discretionary{}
% {\hbox{$\mathsurround=0pt #1$}}{}}

\usepackage{graphicx}  % Для вставки рисунков
% \graphicspath{{images/}{images2/}}  % папки с картинками
% \setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
% \setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
\usepackage{wrapfig} % Обтекание рисунков текстом

\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
\usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

\usepackage{etoolbox} % логические операторы

\usepackage{lastpage} % Узнать, сколько всего страниц в документе.
\usepackage{soul} % Модификаторы начертания
\usepackage{csquotes} % Еще инструменты для ссылок
%\usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblatex}
\usepackage{multicol} % Несколько колонок

\usepackage{tikz} % Работа с графикой
\usepackage{pgfplots}
\usepackage{pgfplotstable}

% \usepackage{biblatex}

% \usepackage{fontspec}
% \defaultfontfeatures{Ligatures={TeX},Renderer=Basic}
% \setmainfont[Ligatures={TeX,Historic}]{Myriad Pro} % install Myriad Pro or replace with Arial
% \setsansfont{Myriad Pro}  % install Myriad Pro or replace with Arial
% \setmonofont{Courier New}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\ex}{\mathop{\mathbb{E}}\nolimits}
\newcommand{\pr}{\mathop{\mathbb{P}}\nolimits}

\title{Conformal methods in multidimensional linear models and anomaly detection}
% \title[Short title]{Presentation Title} 
% \subtitle{Presentation Subtitle or Conference Title}
\author[Nazarov Ivan]{Nazarov Ivan}
% \author[Author's name]{Author's name \\ \smallskip \scriptsize \url{author@hse.ru}\\\url{http://hse.ru/en/staff/author/}}
\date{\today}
\institute[Higher School of Economics]{National Research University \\ Higher School of Economics}


\begin{document}
\selectlanguage{english}
\frame[plain]{\titlepage} % Титульный слайд

\section{Problem statement} % (fold)
\label{sec:problem_statement}
\subsection{Anomaly detection} % (fold)
\label{sub:anomaly_detection}


\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{}
    Applied problem of being able to tell ``abnormal'' observations from ``normal''
    ones. % Requires a concept of ``abnormal''.
    \begin{itemize}
      \item probabilistic models: observations outside of typical (dense) regions;
      \item predictive models: observations deviating from predictions (reconstructions)
      by the learnt model of ``normal'' regime;
    \end{itemize}
  \end{block}
\end{frame}

% subsection anomaly_detection (end)

\subsection{Predictive models} % (fold)
\label{sub:predictive_models}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{}
    Consider a training sample $(x_i, y_i)_{i=1}^n \sim \Dcal$ i.i.d., where conditional
    on $x\in \Xcal$, $\Xcal \subset \Real^{p\times 1}$, the target $y_x$ obeys
    $$ y_x = f(x) + \epsilon_x \,, $$
    for some unknown $f:\Xcal\mapsto \Ycal$, $\Ycal\subseteq \Real$ and $\ex(\epsilon_x|x) = 0$.

    In general functional approximation it is desirable to learn a faithful approximation
    $\hat{f}_{:n}$ of $f$ for $x\sim \Xcal$ within the support of $\Dcal$, and have an
    estimate of its accuracy.

    Anomaly detection is more concerned with a confidence region for new observations:
    a learnt point-set map $x \mapsto \hat{\Gamma}_{:n}^\alpha(x)$ such that
    $$ \pr_\Dcal\bigl(y_x \notin \hat{\Gamma}_{:n}^\alpha(x)\bigr) \leq \alpha \,. $$
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  Options for building blocks of a predictive model in AD framework: 
  \begin{block}{approximation}
    \begin{itemize}
      \item linear models, (Kernel) Ridge regression, Support Vector regression;
      \item piecewise linear Gradient Boosted Regression Trees;
      \item Neural Networks;
    \end{itemize}
  \end{block}
  \begin{block}{confidence}
      \begin{itemize}
        \item Bayesian assumptions for conditional predictive distributions;
        \item conformal prediction;
      \end{itemize}
  \end{block}
\end{frame}

% subsection predictive_models (end)

\subsection{Conformal prediction} % (fold)
\label{sub:conformal_prediction}

\begin{frame}
  \frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  Conformal prediction is a distribution-free technique designed to yield a statistically
  valid confidence sets for predictions made by machine learning algorithms.

  \begin{itemize}
    \item can be constructed atop any ML algorithm by incorporating it into a function
    $A(Z_{:n}, z)$, reflecting non-conformity of $z$ with respect to sample $Z_{:n}$;
    \item validity guarantees for i.i.d. observations in online setting established
    in \cite{vovk2005};
    \item asymptotic efficiency in the batch setting for ridge regression shown in
    \cite{burnaevV14};
  \end{itemize}

  In particular, for predictive models the non-conformity measures $A$ are typically
  based on residuals;
\end{frame}

\begin{frame}[c, shrink=6]
  \frametitle{\insertsection}
  \framesubtitle{Conformal predictor}
  \begin{algorithm}[H]
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{Non-conformity measure $A$, significance level $\alpha \in (0,1)$,
    training sample $Z_{:n} = (x_i, y_i)_{i=1}^n$, a test object $x_{n+1}\in \Xcal$.}
  \Output{Confidence set $\Gamma_{:n}^\alpha(x_{n+1})$ for $y_{n+1}\in \Ycal$.}
  \BlankLine
  $\Gamma_{:n}^\alpha \leftarrow \emptyset$\;
  \For{$y \in \Ycal$}{
    $z_{n+1} \leftarrow (x_{n+1}, y)$\;
    \For{$i = 1,\ldots, n, n+1$}{
      $Z_{-i} \leftarrow \bigl(z_j\bigr)_{j=1, j\neq i}^{n+1}$\;
      $\eta_i \leftarrow A(Z_{-i}, z_i)$\;
    }
    $p^y \leftarrow (n+1)^{-1} \bigl\lvert \{
        i \,:\, \eta_i \geq \eta_{n+1} \} \bigr\rvert $\;
    \If{$p^y > \alpha$}{
      $\Gamma_{:n}^\alpha \leftarrow \Gamma_{:n}^\alpha \cup\{y\}$\;
    }
  }
  \Return{$\Gamma_{:n}^\alpha$}\;
  \end{algorithm}
\end{frame}

% subsection conformal_prediction (end)

% section problem_statement (end)

\section{Research goals and results} % (fold)
\label{sec:research_goals_and_results}

\begin{frame}[c]\frametitle{\insertsection}
  \begin{block}{Goals}
    \begin{itemize}
      \item Propose an efficient construction for conformal prediciton based on
      kernel ridge regression (KRR);
      \item Experimentally verify validity guaranteees and efficiency of conformal
      confidence regions over KRR with the Gaussian kernel in the batch learning
      setting;
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \begin{block}{Results: algorithm}
    Outlined a specialized algorithm for KRR conformal confidence region for each
    new test observation, $\hat{\Gamma}^\alpha_{:n}(x_{n+1})$:
    \begin{itemize}
      \item Initialization: inversion of the regularized Gram matrix $\Ocal(n^3)$;
      \item Memory compelxity: $\mathcal{O}(n^2)$ to store the inverse;
      \item Runtime complexity: $\Ocal(n^2)$ for canonical feature map computation
      and matrix multiplication, and $\Ocal(n \log n)$ for interval construction;
    \end{itemize}

    The procedure uses structre of sample Gram matrix and the block matrix inversion,
    to represent KRR residuals for $n+1$ sample as a linear function of the test
    observation's value.
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \begin{block}{Experimental setup}
    The experiments were set up as follows: \begin{itemize}
      \item 
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \begin{block}{Results: performance}
    Extensive empirical study of CCR over KRR with the Gaussian kernel revealed:
    \begin{itemize}
      \item CCR have asymptotic validity, regardless of KRR hyperparameters, quality
      of fit or noise-to-signal ratio;
      %% page 34, columns a,c; page 36, columns a,c
      %% nongaussian: figures 10-12
      \item In Gaussian setting with high noise-to-signal, both Bayesian and conformal
      KRR intervals are valid, and the latter are asymptotically as efficient as the
      former;
      %% fig 14-17, pp. 42-45;
      %% fig 18-19, p. 46 (double "the", different hyperparameters)
      %% ``f6'' function is much gaussian-like; thus the comparison on fig. 22
      \item Bayesian confidence regions are severely affected by the choice of KRR
      hyperparamters (kernel precision -- proxy for the degree of locality), and
      in non-Gaussian with low-noise settings fail to guarantee requested coverage
      rate;
      %% fig. 6, 16; fig. 10-13
    \end{itemize}
    Similar results were obtained in test cases with $1-d$ and $2-d$ domains.
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \begin{block}{Implications}
    By design, the conformal prediction algorithm is oblivious to the nature and
    the domain of the used non-conformity measure.

    That is why we consider the obtained empirical evidence for the validity and
    efficiency of the KRR CCR in batch setting to be sufficient to \begin{itemize}
      \item warrant further throretical research in asymptotic properties of conformal
      kernel ridge regression predictions;
      \item recommend the use of conformal confidence regions in applied prediction
      tasks when the assumption of i.i.d. observations is reasonable;
    \end{itemize}
  \end{block}

  \begin{block}{Typical applications}
    \begin{itemize}
      \item anomaly detection (batch);
      \item time series forecasting (online);
      \item confidence regions for function approximation (batch);
    \end{itemize}
  \end{block}

\end{frame}

% section research_goals_and_results (end)

\section{Further development beyond kernel ridge regression} % (fold)
\label{sec:further_development_beyond_krr}

\subsection{Kernel embeddings of distributions} % (fold)
\label{sub:kernel_embeddings_of_distributions}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
    A distribution $P$ on $\Xcal$ admits an embedding in an RKHS $\Hcal_K$ induced
    by $K$, if $\ex_{x\sim P} \sqrt{K(x,x)} < +\infty$, in which case $\ex_{x\sim P} f(x)
    = \langle f, \mu_P \rangle_\Hcal $ for any $f\in \Hcal_K$, where
    $$ \mu_P: x \mapsto \ex_{x'\sim P} K(x', x) \,. $$
    If the RKHS $\Hcal_K$ is universal and on compact metric domain, then the distribution
    embeddin $\mu_P$ is injective, \cite{gretton2012};

    \begin{block}{Idea}
    Use confromal procedure to assign abnormality probability to new observations
    based on RKHS norm of difference of embeddings of empirical distributions of
    original and pooled samples.
    \begin{itemize}
      \item Analogous to jackknife bootstrapping;
      \item In special cases might permit computation of quantile regions;
    \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
      Picture ocSVM versus CKDE.
\end{frame}

% subsection kernel_embeddings_of_distributions (end)

\subsection{Kernel bandwidth selection} % (fold)
\label{sub:kernel_bandwidth_selection}

\begin{frame}[c]\frametitle{\insertsection}

  The research achieved the following goals:
  % \begin{enumerate}
  % \end{enumerate}
\end{frame}

% subsection kernel_bandwidth_selection (end)

% section further_development_beyond_krr (end)

\section{References} % (fold)
\label{sec:references}

\begin{frame}[t]{References}  
  \bibliographystyle{amsplain}
  \bibliography{../references}
\end{frame}

% section references (end)


\end{document}