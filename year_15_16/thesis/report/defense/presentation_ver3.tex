% Этот шаблон документа разработан в 2014 году
% Данилом Фёдоровых (danil@fedorovykh.ru) 
% для использования в курсе 
% <<Документы и презентации в \LaTeX>>, записанном НИУ ВШЭ
% для Coursera.org: http://coursera.org/course/latex .
% Исходная версия шаблона --- 
% https://www.writelatex.com/coursera/latex/5.1

\documentclass[t]{beamer}  % [t], [c], или [b] --- вертикальное выравнивание на слайдах (верх, центр, низ)
%\documentclass[handout]{beamer} % Раздаточный материал (на слайдах всё сразу)
%\documentclass[aspectratio=169]{beamer} % Соотношение сторон

%\usetheme{Berkeley} % Тема оформления
%\usetheme{Bergen}
%\usetheme{Szeged}

%\usecolortheme{beaver} % Цветовая схема
%\useinnertheme{circles}
\useinnertheme{rectangles}

% \usetheme{HSE}
\usepackage{HSE-theme/beamerthemeHSE-en}

\usepackage{cmap}         % поиск в PDF
\usepackage{mathtext}         % русские буквы в формулах
\usepackage[T2A]{fontenc}     % кодировка
\usepackage[utf8]{inputenc}     % кодировка исходного текста
\usepackage[english]{babel} % локализация и переносы

% \newtheorem{rtheorem}{Теорема}
% \newtheorem{rproof}{Доказательство}
% \newtheorem{rexample}{Пример}

\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
% \usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление
\usepackage{mathptmx}
\usepackage[noline]{algorithm2e}

%% Номера формул
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
%\usepackage{leqno} % Нумерация формул слева

%% Свои команды
% \DeclareMathOperator{\sgn}{\mathop{sgn}}

%% Перенос знаков в формулах (по Львовскому)
% \newcommand*{\hm}[1]{#1\nobreak\discretionary{}
% {\hbox{$\mathsurround=0pt #1$}}{}}

\usepackage{graphicx}  % Для вставки рисунков
% \graphicspath{{images/}{images2/}}  % папки с картинками
% \setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
% \setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
\usepackage{wrapfig} % Обтекание рисунков текстом
\usepackage{subcaption}
\usepackage{caption}

\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
% \usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

% \usepackage{etoolbox} % логические операторы

\usepackage{lastpage} % Узнать, сколько всего страниц в документе.
\usepackage{soul} % Модификаторы начертания
\usepackage{csquotes} % Еще инструменты для ссылок
\usepackage[style=alphabetic,backend=biber,sorting=nty]{biblatex}

\bibliography{../references}

% \usepackage[style=authoryear,maxcitenames=2,backend=bibtex,sorting=nty]{biblatex}
\usepackage{multicol} % Несколько колонок

% \usepackage{tikz} % Работа с графикой
% \usepackage{pgfplots}
% \usepackage{pgfplotstable}

% \usepackage{fontspec}
% \defaultfontfeatures{Ligatures={TeX},Renderer=Basic}
% \setmainfont[Ligatures={TeX,Historic}]{Myriad Pro} % install Myriad Pro or replace with Arial
% \setsansfont{Myriad Pro}  % install Myriad Pro or replace with Arial
% \setmonofont{Courier New}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\ex}{\mathop{\mathbb{E}}\nolimits}
\newcommand{\pr}{\mathop{\mathbb{P}}\nolimits}

\title{Conformal methods in multidimensional linear models and anomaly detection}
% \subtitle{Presentation Subtitle or Conference Title}
\author[Nazarov I.]{\small Nazarov Ivan\\
{\smaller \vspace{\baselineskip}
Supervised by:\\
Burnaev Evgeniy, PhD,
Associate Professor,\\
Head of lab.~10, IITP, RAS}}

% \author[Author's name]{Author's name \\ \smallskip \scriptsize \url{author@hse.ru}\\\url{http://hse.ru/en/staff/author/}}
\date{\today}
\institute[Higher School of Economics]{National Research University \\ Higher School of Economics}

\begin{document}
\selectlanguage{english}
\frame[plain]{\titlepage} % Титульный слайд

\section{Introduction} % (fold)
\label{sec:introduction}

\begin{frame}[c]\frametitle{\insertsection}
  \begin{block}{Function approximation}
    A faithful approximation $\hat{f}_{:n}$ of $f:\Xcal \mapsto \Real$,
    $\Xcal\subset \Real^{p\times 1}$, from noisy observations
    $Z_{:n} = (X,y) = (x_i, y_i)_{i=1}^n \sim \Dcal$ with
    $$ y_i = f(x_i) + \epsilon_i \,. $$
  \end{block}
  \begin{block}{Confidence prediction}
    Construct a point-set map $x \mapsto \hat{C}_{:n}^\alpha(x)$ such that
    $$ \pr_\Dcal\bigl(y_x \notin \hat{C}_{:n}^\alpha(x)\bigr) \approx \alpha \,. $$
    Necessary for: \begin{itemize}
      \item anomaly detection, forecasting, function approximation;
    \end{itemize}
    % All these models also usually rely on some predictive model;
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Typical approach}
  Linear regression or Kernel Ridge Regression (KRR):
  $$ \|y - K\beta \|^2 + \lambda \beta' K \beta \to \min_\beta \,, $$
  $\lambda > 0$, $K$ -- sample Gram matrix of kernel $K(\cdot,\cdot)$ on inputs $X$.
  \vspace{\baselineskip}
  \begin{description}
    \item[prediction] $\hat{y}(x) = \kappa_x' \hat{\beta} = \kappa_x' Q y$;
    \item[loo-cv] $\hat{r}^{\text{loo}}_i = \frac{h(x_i)}{\lambda} e_i'(y - K Q y)$;
  \end{description}
  with $h(x) = \lambda + K(x,x) - \kappa_x' Q \kappa_x$, $Q = (\lambda I_n + K)^{-1}$
  \hfill\\ and $\kappa_z$ canonical feature maps of $X$ at $z$.
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Bayesian KRR}
  \begin{block}{Gaussian Process Regression}  
    Observations are drawn from a Gaussian process with mean $0$ and covariance
    $$ \sigma^2(\lambda\delta_{x,x'} + K(x,x')) \,,$$
    $\sigma^2$ -- variance, $\lambda$ -- noise-to-signal ratio.
  \end{block}
  \vspace{\baselineskip}
  \begin{block}{Forecast distribution}
  Conditional on the observed data $(X, y)$ at a new $x$:
  $$ y_x \sim \Ncal(\kappa_x' Q y, \sigma^2 h(x)) \,. $$
  \end{block}
\end{frame}

% section introduction (end)

\section{Conformal prediction} % (fold)
\label{sec:conformal_prediction}

\begin{frame}[t]\frametitle{\insertsection}
  % Invalid confidence intervals if Bayesian assumptions fail.

  % \vspace{\baselineskip}
  Conformal prediction -- a general distribution-free method for
  valid confidence sets.

  % \vspace{\baselineskip}
  \begin{itemize}
    \item can be constructed atop \textbf{any} ML algorithm;
    \item relies on non-conformity score of $z_{n+1}=(x_{n+1}, y)$ with respect to
    the sample $Z_{:n}$, $A(Z_{:n}, z_{n+1})$;
    \item asymptotic validity guarantees, \cite{vovk2005}:
    $$ \pr\bigl( y_{n+1} \notin \hat{\Gamma}_{:n}^\alpha(x_{n+1}) \bigr) \leq \alpha \,; $$
    \item Efficiency depends on the non-conformity measure $A$ and model assumptions,
    (ridge regression \cite{burnaevV14});
    \item Little is known about efficiency in the case of nonlinear models;
  \end{itemize}
\end{frame}

\begin{frame}[c, shrink=10]
  \frametitle{\insertsection}
  \begin{algorithm}[H]
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{Non-conformity measure $A$, significance level $\alpha \in (0,1)$,
    training sample $Z_{:n} = (x_i, y_i)_{i=1}^n$, a test object $x_{n+1}\in \Xcal$.}
  \Output{Confidence set $\Gamma_{:n}^\alpha(x_{n+1})$ for $y_{n+1}\in \Ycal \subseteq \Real$.}
  \BlankLine
  $\Gamma_{:n}^\alpha \leftarrow \emptyset$\;
  \For{$y \in \Ycal$}{
    $z_{n+1} \leftarrow (x_{n+1}, y)$\;
    \For{$i = 1,\ldots, n, n+1$}{
      $Z_{-i} \leftarrow \bigl(z_j\bigr)_{j=1, j\neq i}^{n+1}$\;
      $\eta_i \leftarrow A(Z_{-i}, z_i)$\;
    }
    $p_{:n+1} \leftarrow (n+1)^{-1} \bigl\lvert \{
        i \,:\, \eta_i \geq \eta_{n+1} \} \bigr\rvert $\;
    \If{$p_{:n+1} > \alpha$}{
      $\Gamma_{:n}^\alpha \leftarrow \Gamma_{:n}^\alpha \cup\{ y\}$\;
    }
  }
  \Return{$\Gamma_{:n}^\alpha$}\;
  \end{algorithm}
\end{frame}

\begin{frame}[t]\frametitle{Thesis objectives}
  \begin{block}{Objectives}
    \begin{itemize}
      \item a computationally efficient conformal procedure for kernel ridge regression;
      \item study efficiency of conformal confidence regions over nonlinear regression model;
      \item propose a non-parametric method for model selection on conformal predictions;
      \item a novel approach to novelty detection in unsupervised setting;
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Non-conformity measures}
  \begin{block}{KRR residuals}  
    The residual $\hat{r}_i(y; Z_{:n}, x_{n+1})$ of the $i$-th observation in
    $Z_{:n}\cup\{(x_{n+1}, y)\}$ is a linear function of $n+1$-st target value.
  \end{block}

  \begin{description}
    \item[Ridge Regression Confidence Machine] $A(Z_{-i}, z_i) = |\hat{r}_i(y; Z_{:n}, x_{n+1})|$;
    \vspace{\baselineskip}
    \item[Two-sided Conformal Region] \hfill\\
      \textit{upper} $A(Z_{-i}, z_i) = \hat{r}_i(y; Z_{:n}, x_{n+1})$;\hfill\\
      \textit{lower} $A(Z_{-i}, z_i) = -\hat{r}_i(y; Z_{:n}, x_{n+1})$;
  \end{description}

  Possible to use leave-one-out residuals:
  $$ \hat{r}^{\text{loo}}_i(y; Z_{:n}, x_{n+1})
    = \frac{h(x_i)}{\lambda} \hat{r}_i(y; Z_{:n}, x_{n+1}) \,. $$
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \begin{block}{Conformal confidence set}
    $$ \hat{\Gamma}_{Z_{:n}}^\alpha(x_{n+1})
        = \bigl\{ y \in \Ycal \,:\, p_{:n}(y) > \alpha \bigr\}
      \,. $$
  \end{block}

  \begin{block}{Conformal p-value}
    For $\eta_i(y) = A(Z_{-i}, z_i)$, $i=1,\ldots,n+1$, and $z_{n+1} = (x_{n+1}, y)$
    $$ p_{:n}(y) = (n+1)^{-1} \bigl\lvert\{ i \,:\, \eta_i(y) \geq \eta_{n+1}(y) \}\bigr\rvert
      \,. $$
  \end{block}
\end{frame}

\subsection{Ridge Regression Confidence Machine} % (fold)
\label{sub:ridge_regression_confidence_machine}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{Exhaustive search over $y\in \Real$ is not necessary}
    KRR residuals are special: proved that each non-empty region
    $S_i = \{y \in \Ycal \,:\, \eta_i(y) \geq \eta_i(y)\}$
    is either \begin{itemize}
      \item $(-\infty, a_i]$ or $[b_i, +\infty)$, or union;
      \item $[a_i, b_i]$;
      \item $\Real$, or $\{a_i\}$;
    \end{itemize}
    \vspace{\baselineskip}
    The set $\hat{\Gamma}_{Z_{:n}}^{\alpha, \text{RRCM}}(x_{n+1})$ is a union of
    \begin{itemize}
      \item closed intervals $[a, b]\cap\Real$, ($a\leq b$ any endpoint of any $S_i$);
      \item singletons $\{a_i\}$;
    \end{itemize}
    covered by at least $\alpha n$ regions $S_i$.
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
\framesubtitle{\insertsubsection}
  Fixed requirements per sample $Z_{:n}$: \begin{itemize}
    \item Initialization: $\Ocal(n^3)$ for inversion of the Gram matrix;
    \item Memory complexity: $\Ocal(n^2)$ to store the inverse;
  \end{itemize}
  \vspace{\baselineskip}
  Requirements for each $x_{n+1}$:
  \begin{itemize}
    \item Memory complexity: $\Ocal(n)$ to store endpoints;
    \item Runtime complexity: \begin{itemize}
      \item $\Ocal(n^2)$ for matrix multiplication and kernel;
      \item $\Ocal(n \log n)$ for interval construction;
      \item $\Ocal(n)$ for pruning;
    \end{itemize}
  \end{itemize}
\end{frame}

% subsection ridge_regression_confidence_machine (end)

\subsection{Two-sided conformal region} % (fold)
\label{sub:two_sided_conformal_region}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  $\hat{\Gamma}_{Z_{:n}}^{\alpha,\text{ts}}(x_{n+1})$ is an intersection of the
  \textit{upper} interval $\hat{\Gamma}_{Z_{:n}}^{\frac{\alpha}{2}, U}(x_{n+1})$ and the
  \textit{lower} interval $\hat{\Gamma}_{Z_{:n}}^{\frac{\alpha}{2}, L}(x_{n+1})$
  ($\Ocal(1)$ time);
  % A general $\hat{\Gamma}_{Z_{:n}}^\alpha(x_{n+1})$ is not necessarily connected;

  \vspace{\baselineskip}
  \begin{block}{Regularity property, \cite{burnaevV14}}
    The two-sided region is a compact interval for every sequence $(x_i)_{i=1}^n\in \Xcal$
    (compact) satisfying
    $$ f(x_{n+1})^2 \leq \sum_{i=1}^n f(x_n)^2 + \lambda \|f\|^2 \,, $$
    for all $f$ in the RKHS of a kernel $K(\cdot, \cdot)$.

    This condition is satisfied when for all sufficiently large $n\geq 1$
    $$ K(x_{n+1}, x_{n+1}) - \kappa_{:n}(x_{n+1})' K^{-1}_{:n} \kappa_{:n}(x_{n+1}) < \lambda \, $$
  \end{block}
\end{frame}

% subsection two_sided_conformal_region (end)

\subsection{Preliminary results} % (fold)
\label{sub:preliminary_results}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{So far:}
  \begin{itemize}
    \item an efficient polynomial-time algorithm for construction of conformal
    confidence regions;
    \item a sufficient condition for regularity of two-sided intervals for KRR;
  \end{itemize}
  \end{block}
  \begin{block}{Next step}
    compare the conformal confidence region to the Bayesian confidence interval.
  \end{block}
\end{frame}

% subsection preliminary_results (end)

% section conformal_prediction (end)

\section{Numerical study} % (fold)
\label{sec:Numerical_study}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Bayesian interval}
  Gaussian Process Regression readily yields confidence intervals.
  \vspace{\baselineskip}
  \begin{block}{Bayesian predictions}
    $$ \hat{y}_{:n}(x) = \kappa_x' (\lambda I_n + K)^{-1} y_{:n} \,, $$
    where $\kappa_x = (K(x_i, x))_{i=1}^n$.
  \end{block}

  \begin{block}{Prediction confidence interval}
    $$ \frac{y_{n+1} - \hat{y}_{:n}(x_{n+1})}{\sigma \sqrt{h(x_{n+1})}} \in
      \bigl[z_{\frac{\alpha}{2}}, z_{1-\frac{\alpha}{2}} \bigr] \,, $$
    where $ h(x) = \lambda + K(x,x) - \kappa_x' (\lambda I_n + K)^{-1} \kappa_x$,
    and $z_\alpha$ is the $\alpha$ quantile of the standard Normal distribution.
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  In the study we consider kernel ridge regression with the Gaussian kernel:
  $$ K(x,x') = \text{exp}\bigl\{ -\theta \|x-x'\|^2 \bigr\} \,. $$
  \begin{block}{Objectives}
    \begin{itemize}
      \item verify validity guaranties of conformal confidence regions for KRR in
      the batch learning setting;
      \item measure efficiency of conformal confidence regions under Gaussian
      assumptions.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \begin{block}{Experimental setup}
  \end{block}
  \begin{itemize}
    \item Test functions on compact $1$-d and $2$-d domains; \begin{itemize}
      \item a piecewise continuous;
      \item a piecewise smooth;
      \item a sample path of a Gaussian process;
    \end{itemize}
    \vspace{\baselineskip}
    \item Low or high Gaussian noise;
    \item KRR hyper-parameters: regularization $\lambda$, kernel precision $\theta$;
    \item Train sample size $n$;
  \end{itemize}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Experimental setup}
  \begin{itemize}
    \item A large random train sample from $\Xcal$, a fixed test sample;
    \item A sample path over the pooled sample;
    \vspace{\baselineskip}
    \item For each setting of hyper-parameters: \begin{enumerate}
      \item pick a random train sub-sample;
      \item get confidence regions (conformal, Bayesian) for the test;
      \vspace{\baselineskip}
      \item get overall coverage rate; get size of each confidence region;
      \item compute summary values across $M$ replications:\begin{itemize}
        \item median, $5\%$, $95\%$ quantiles, $\max$;
      \end{itemize}
    \end{enumerate}
  \end{itemize}
\end{frame}

\subsection{Results} % (fold)
\label{sub:results}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{Example Bayesian and conformal (RRCM) confidence regions}
  \begin{figure}%[t, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.9\linewidth]{../images/output_pdf/profile/gaussian/0.1_0.1/200/profile_gaussian_0,1_0,1_100_5p-GPR_200.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.9\linewidth]{../images/output_pdf/profile/gaussian/0.1_0.1/200/profile_gaussian_0,1_0,1_100_5p-RRCM_200.pdf}
    \end{subfigure}
    \caption{Example Bayesian and conformal (RRCM) confidence regions $n=200$.}
    \label{fig:gauss_1d_prof_gpr}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Asymptotic coverage rate in the Gaussian case with high noise}
  \begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/GPR-f/coverage_gaussian_0,1_1e-06_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/coverage/RRCM/coverage_gaussian_0,1_1e-06_auto_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/GPR-f/coverage_gaussian_0,1_0,1_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/coverage/RRCM/coverage_gaussian_0,1_0,1_auto_RRCM.pdf}
    \end{subfigure}\\
    % \label{fig:gaussian_1d_high_noise}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Typical confidence region widths in the Gaussian case with high noise}
  \begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/GPR-f/width_gaussian_0,1_1e-06_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_1e-06/width/RRCM/width_gaussian_0,1_1e-06_auto_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/width/GPR-f/width_gaussian_0,1_0,1_auto_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/gaussian/0.1_0.1/width/RRCM/width_gaussian_0,1_0,1_auto_RRCM.pdf}
    \end{subfigure}\\
    % \caption{\textit{Upward}~triangles indicate the $5\%$ sample quantile;
    %   \textit{downward}~triangles indicate the maximal width.}
    % \label{fig:gaussian_1d_high_noise_width}
  \end{figure}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{itemize}
    \item conformal regions are asymptotically valid, regardless of KRR hyper-parameters,
    quality of fit, or noise-to-signal ratio;
    %% page 34, columns a,c; page 36, columns a,c
    %% non-Gaussian: figures 10-12
    \item High noise: Bayesian intervals are valid, conformal regions
    are asymptotically efficient;
    %% fig 14-17, pp. 42-45;
    %% fig 18-19, p. 46 (double "the", different hyper-parameters)
    %% ``f6'' function is much Gaussian-like; thus the comparison on fig. 22
    \vspace{\baselineskip}
    \item Low noise: Bayesian intervals are conservatively valid and
    wider than conformal intervals;
    %% fig. 6, 16; fig. 10-13
    \item Bayesian intervals fail to have correct coverage rate in non-Gaussian
    settings with low noise;
    \item Bayesian regions are sensitive to KRR hyper-parameters;
  \end{itemize}
\end{frame}

% subsection results (end)

\subsection{Implications} % (fold)
\label{sub:implications}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  By design, conformal prediction procedure is oblivious to the nature of
  the non-conformity measure.

  \vspace{\baselineskip}
  Gathered empirical evidence is sufficient to \begin{itemize}
    % for validity and efficiency of the KRR conformal confidence regions in batch setting
    \item warrant further theoretical research in asymptotic properties of conformal
    kernel ridge regression predictions;
    \item recommend the use of conformal confidence regions in applied prediction
    tasks when the assumption of i.i.d. observations is reasonable;
  \end{itemize}
\end{frame}

% subsection implications (end)

% section Numerical_study (end)

\section{Further development beyond kernel ridge regression} % (fold)
\label{sec:further_development_beyond_krr}

\subsection{Kernel bandwidth selection} % (fold)
\label{sub:kernel_bandwidth_selection}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{Possible approaches}
    Choose a kernel bandwidth (reciprocal of precision $\theta$).

    \begin{itemize}
      \item full Bayesian assumptions: MLE of $\theta$;
      \item Gaussian noise: local bandwidth estimate based on approximate
        confidence interval, \cite{goldenshluger1997};
      \item distribution-free: conformal confidence regions;
    \end{itemize}
  \end{block}

  \begin{block}{Conformal Bandwidth Selection}
    Conformal confidence regions $\hat{\Gamma}_{Z_{:n}}^\alpha(x_i)$ have guaranteed
    coverage\hfill\\ $\Rightarrow$ at each $x_i$ locally ``optimal'' bandwidth gives
    the smallest region \begin{itemize}
      \item take the sample median of local bandwidths across the sample.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Selected precision for a noisy ``triangle'' signal}
  \begin{figure}%[b, width=0.45\textwidth]
    \centering
    \includegraphics[width=0.95\linewidth]{cbs_images/triangle_00_0,5.pdf}
  \end{figure}
\end{frame}

% subsection kernel_bandwidth_selection (end)

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Anomaly detection}
  \begin{block}{Supervised setting}
    Use KRR prediction conformal confidence region to discriminate between
    ``normal'' and ``anomalous'' observations.
  \end{block}

  \begin{block}{Unsupervised setting}
    Given a sample $(x_i)_{i=1}^n\sim \Dcal$, decide if $x_{n+1}$ was generated
    by $\Dcal$ as well. Possible approaches:
    \begin{itemize}
      \item one-class SVM: a kernel method for novelty detection;
      \item kernel distribution embedding with conformal abnormal probability (C-KDE);
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{A toy example}
  \begin{figure}%[t, width=0.45\textwidth]
    \centering
    \includegraphics[width=0.5\linewidth]{old/toy_ckde.pdf}
    \caption{Toy $2$-d data with three ``normal'' clusters (\textit{colored}).}
    \label{fig:toy}
  \end{figure}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{A toy example}
  \begin{figure}%[t, width=0.45\textwidth]
    \centering
    \includegraphics[width=0.5\linewidth]{old/ocSVM.pdf}
    \caption{one-class SVM with Gaussian kernel; normalized exp-slacks ($e^{\xi_i}$)
    are used as proxies for abnormality probability.}
    \label{fig:ocsvm}
  \end{figure}
  % one-class SVM work poorly when anomalies are relatively frequent, or form
  % a cluster.
\end{frame}

\subsection{Kernel embeddings of distributions} % (fold)
\label{sub:kernel_embeddings_of_distributions}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  A distribution $P$ on $\Xcal$ admits an embedding $\mu_P$ in an RKHS $\Hcal_K$
  induced by $K$, if $\ex_{x\sim P} \sqrt{K(x,x)} < +\infty$.
  
  \vspace{\baselineskip}
  In this case $\ex_{x\sim P} f(x) = \langle f, \mu_P \rangle_\Hcal$ for any
  $f\in \Hcal_K$, where $\mu_P\in \Hcal_K$ is unique and
  $$ \mu_P: x \mapsto \ex_{x'\sim P} K(x', x) \,. $$
  
  \vspace{\baselineskip}
  If $\Hcal_K$ is a universal RKHS of functions on a compact metric domain, then
  the embedding $\mu_P$ is injective, \cite{gretton2012};
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{\insertsubsection}
  \begin{block}{C-KDE}
    Non-conformity measure
    $$ A(X_{-i}, x_i) = n^2 \| \mu_{\hat{P}_{-i}} - \mu_{\hat{P}} \|^2 \,. $$

    Define the abnormality probability as the conformal p-value with respect to $A$.

    \begin{itemize}
      \item Analogous to jackknife bootstrapping;
      \item In special cases might permit computation of quantile regions;
      \item Extension: weighted observations in embedding;
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]\frametitle{\insertsection}
  \framesubtitle{A toy example}
  \begin{figure}%[t, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.9\linewidth]{old/ckde.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.9\linewidth]{old/ckde-lap.pdf}
    \end{subfigure}%
    \caption{Abnormality p-values for empirical kernel embedding with
    (\textit{left})~Gaussian kernel, and
    (\textit{right})~Laplacian kernel, $\text{exp}(-\theta\|x-x'\|)$.}
    \label{fig:ckde}
  \end{figure}
  % C-KDE is very sensitive to kernel bandwidth $\theta$: coalesces clusters for
  % moderately large bandwidth.
\end{frame}

% subsection kernel_embeddings_of_distributions (end)

% section further_development_beyond_krr (end)

\section{Summary} % (fold)
\label{sec:summary}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Achieved objectives}
  \begin{itemize}
    \item An efficient algorithm for valid non-parametric confidence regions for
    kernel ridge regression;

    \vspace{\baselineskip}
    \item Experimental verification of validity guarantees of conformal confidence
    regions over KRR with the Gaussian kernel in the batch learning setting;

    % \vspace{\baselineskip}
    \item Extensive comparison of conformal confidence region to Bayesian confidence
    interval and investigation of its efficiency;

    \vspace{\baselineskip}
    \item Further extensions of conformal prediction with kernel methods:
    \begin{itemize}
      \item kernel bandwidth selection;
      \item conformalized kernel distribution embeddings;
    \end{itemize}
  \end{itemize}
\end{frame}

% section summary (end)

\section{References} % (fold)
\label{sec:references}

\begin{frame}[t, shrink=25]\frametitle{\insertsection}
  \printbibliography
\end{frame}

% section references (end)

\section{Appendix} % (fold)
\label{sec:appendix}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Typical coverage rate in non-Gaussian case with low noise}
  \begin{figure}%[b, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/GPR-f/coverage_heaviside_1e-06_1e-06_100_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_1e-06/coverage/RRCM/coverage_heaviside_1e-06_1e-06_100_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/GPR-f/coverage_heaviside_1e-06_0,1_100_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/1e-06_0.1/coverage/RRCM/coverage_heaviside_1e-06_0,1_100_RRCM.pdf}
    \end{subfigure}\\
    % \label{fig:heaviside_1d_low_noise}
  \end{figure}
\end{frame}

\begin{frame}[t]\frametitle{\insertsection}
  \framesubtitle{Typical coverage rate in non-Gaussian case with high noise}
  \begin{figure}%[b, width=0.45\textwidth]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/GPR-f/coverage_heaviside_0,1_1e-06_10_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_1e-06/coverage/RRCM/coverage_heaviside_0,1_1e-06_10_RRCM.pdf}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/GPR-f/coverage_heaviside_0,1_0,1_10_GPR-f.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
      \includegraphics[width=0.95\linewidth]{../images/output_pdf/exp_1d/heaviside/0.1_0.1/coverage/RRCM/coverage_heaviside_0,1_0,1_10_RRCM.pdf}
    \end{subfigure}\\
    % \label{fig:heaviside_1d_high_noise_arb}
  \end{figure}
\end{frame}

% section appendix (end)

\end{document}
